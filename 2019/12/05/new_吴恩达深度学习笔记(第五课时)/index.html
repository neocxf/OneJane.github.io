<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><title>吴恩达深度学习笔记(第五课时) - OneJane</title><meta charset="UTF-8"><meta name="description" content="微服务,高可用,高并发,人工智能"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="msvalidate.01" content="396E9693347B4D18AAE96D9E75B9B686"><link rel="shortcut icon" href="/images/a.ico" type="image/png"><meta name="description" content="吴恩达深度学习笔记"><meta name="keywords" content="nlp"><meta property="og:type" content="article"><meta property="og:title" content="吴恩达深度学习笔记(第五课时)"><meta property="og:url" content="https://onejane.github.io/2019/12/05/new_吴恩达深度学习笔记(第五课时)/index.html"><meta property="og:site_name" content="OneJane"><meta property="og:description" content="吴恩达深度学习笔记"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/ae2970d80a119cd341ef31c684bfac49.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/cccbc03192af67a089b53d7940659505.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/a45c8066f935c6f29d00a95e36cb6662.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/8deca8a84f06466155d2d8d53d26e05d.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1653ec3b8eb718ca817d3423ae3ca643.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/cb041c33b65e17600842ebf87174c4f2.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/140529e4d7531babb5ba21778cd88bc3.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/19cbb2d356a2a6e0f35aa2a946b23a2a.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/27afdd27f45ad8ddf78677af2a3eeaf8.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/rnn-f.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/998c7af4f90cd0de0c88f138b61f0168.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/ad9dd74b6ce9bcea14baa289df530d6b.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/71a0ed918704f6d35091d8b6d60793e4.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/rnn_cell_backprop.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/329b0748f7282efc206ea8de6a709833.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/14e1df0a7a8cdd1584b2e92e87e23aa7.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/db580f1dfd6095d672fc62cce74ce5e2.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1daa38085604dd04e91ebc5e609d1179.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/fa17a85c0e7d9b14633013f2223c877b.png"><meta property="og:image" content="https://markdown.xiaoshujiang.com/img/spinner.gif"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/986226c39270a1e14643e8658fe6c374.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/794b8461aeb485ee61c368c90738523e.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/8b901fc8fcab9e16b1fe26b92f4ec546.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1d31771da8ced333968541fbbf67e6f1.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/4c7e0878e85865c8fed558375e14b938.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/8fb1c4afe30b7a0ede26522b355068ba.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/ac5d647140997ba713c376fb097ea0e2.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/db720dae1606767241df59d8fd6079ee.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1521560729.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/13cee4a2d0ad3897924c22d729c5b02d.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/5e55fb9f6de649031e7f9a4b249f4fea.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/32f3b94e4e4523bc413d9687ec29e801.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/cfa628f62f1c57ee6213793a438957a3.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/c1df3f793dcb1ec681db6757b4974cee.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/daf9e8fa30888b6a1b407ca6ea303984.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/523650730db3f0d5c05a7192da02f878.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/9456d50c55cf0408a3fb2b6e903d85d6.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/94e871edbd87337937ce374e71d56e42.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/LSTM.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/LSTM_rnn.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/63be3fd3701604f94b45a08f1d8b460d.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/48c787912f7f8daee638dd311583d6cf.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/053831ff43d039bd5e734df96d8794cb.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/8378c2bfe73e1ac9f85d6aa79b71b5eb.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/455863a3c8c2dfaa0e5474bfa2c6824d.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/68d7c930146724f39782cb57d33161e9.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/ce30c9ae7912bdb3562199bf85eca1cd.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/59fb45cfdf7faa53571ec7b921b78358.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/b4bf4b0cdcef0c9d021707c47d5aecda.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/8a1d58b7ade17208053c10728b2bf3b6.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/43943c791844cc7f077f6c6f98f1f629.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/12242657bd982acd1d80570cc090b4fe.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/5a42eea162ddc75a1d37520618b4bcd2.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/012c07b7692aed382a2875292ea8e81b.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/cd01dd1ced86605f712cf080681db022.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/cosine_sim.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/ad1c7b1e85d39f56756c28787ccef892.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/fa320bd001f9dca8ec33c7a426e20d80.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/31347eca490e0ae8541140fb01c04d72.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/747e619260737ded586ae51b3b4f07d6.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/638c103855ffeb25122259dd6b669850.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/0800c19895cbf1a360379b5dc5493902.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/4ebf216a59d46efa2136f72b51fd49bd.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/776044225ea4a736a4f2b38ea61fae4c.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/89743b5ade106cad1318b8f3f4547a7f.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/a41190391bd3c506f49124798952e2d2.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1575537312018.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/54beb302688f6a298b63178534281575.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/f36df292b7444e9b7379fa7c14626fa2.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/b05dd0362a19496bb0ad91b8494e374c.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/70e282d4d1abb86fd15ff7b175f4e579.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/f6fc2cec52f4ecb15567511aae822914.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/ec4b604d619dd617f14c2a34945c075d.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/bf6f5879d33ae4ef09b32f77df84948e.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/ea844a0290e66d1c76a31e34b632dc0c.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/de4b6513a8d1866bccf1fac3c0d0d6d2.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/25430afa93f24dc6caa6f85503bbad27.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/cf60f429ef532a2b3bbad3db98b054c5.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/4102795b004ff090ed83dc654f585852.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/9b27d865dff73a2f10abbdc1c7fc966b.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/equalize10.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/2d41c0090fd3d71e6f28eade62b7c97b.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/b9492d18803ebe3853e936098f08661c.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/a8b8c64483ee84d57135829ab025da53.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/fd97f885fe1e6e1a7c70bb965595210a.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/c50a0e8948bbdb9a1d23f2f8a768bf5c.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/8a22dfb5d3c0a4b5d2fdfa716dc3f3b2.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/14a940ae2ea7932b7b7190eceb79f79e.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/be75f3d5f932151e26d9bcab05c92a93.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/507c9081ee77c686bb96a009248087fd.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/6a0b785dd54fcbc439bd82794eeefcf8.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/725eec5b76123bf45c9495e1231b6584.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/96e50aec6e1d7287e5bac70a0c4d92f4.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/748283d682db5be4855e61b90e96c427.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1bc0b442db9d5a1aa19dfe9a477a3c3e.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/2689876529562b7d9a79bf868e7cbad7.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/5e854a803e36991a6e0dd4e33ecab930.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/0bc25316900ccd1d4dd25a35ec7c45c4.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/7f48951acf8ca7c7b63f6c4c455ada18.png"><meta property="og:image" content="https://markdown.xiaoshujiang.com/img/spinner.gif"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/59279ff91bb69a94280e6735eba8ab99.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/3dcdd58eaa544a09e67eb892f8c732bf.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1e6b86a4e3690b4a0c6b8146ffa2f791.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/b22dff4a3b1a4ea8c1ab201446e98889.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/fa6769127cd98ea0058f600449833a21.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/8da3e9cf049139a8e4a78503bd72e7fd.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/4130b85a0694549f02bdf60f7c47a3d7.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/8f409fc3980b0be00dca49bf4fac2659.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/28443f8b28afa63adb6cf89ce19c4f6d.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/f2da69f9fa6462c8e591e79db452f6c1.png"><meta property="og:updated_time" content="2019-12-05T09:35:14.743Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="吴恩达深度学习笔记(第五课时)"><meta name="twitter:description" content="吴恩达深度学习笔记"><meta name="twitter:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/ae2970d80a119cd341ef31c684bfac49.png"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdui@0.4.3/dist/css/mdui.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.15.8/styles/atom-one-dark.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1038733_0xvrvpg9c0r.css"><link rel="stylesheet" href="/css/style.css?v=1578196807505"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.js"></script></head><body class="mdui-drawer-body-left"><div id="nexmoe-background"><div class="nexmoe-bg" style="background-image:url(https://www.github.com/OneJane/blog/raw/master/小书匠/1566388885395.png)"></div><div class="mdui-appbar mdui-shadow-0"><div class="mdui-toolbar"> <a mdui-drawer="{target: '#drawer', swipe: true}" title="menu" class="mdui-btn mdui-btn-icon"><i class="mdui-icon material-icons">menu</i></a><div class="mdui-toolbar-spacer"></div> <a href="/" title="OneJane" class="mdui-btn mdui-btn-icon"><img src="https://www.github.com/OneJane/blog/raw/master/小书匠/1566388846021.png"></a></div></div></div><div id="nexmoe-header"><div class="nexmoe-drawer mdui-drawer" id="drawer"><div class="nexmoe-avatar mdui-ripple"> <a href="/" title="OneJane"><img src="https://www.github.com/OneJane/blog/raw/master/小书匠/1566388846021.png" alt="OneJane"></a></div><div class="nexmoe-count"><div><span>文章</span>69</div><div><span>标签</span>83</div><div><span>分类</span>12</div></div><ul class="nexmoe-list mdui-list" mdui-collapse="{accordion: true}"><a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/" title="回到首页"><i class="mdui-list-item-icon nexmoefont icon-home"></i><div class="mdui-list-item-content"> 回到首页</div></a><a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/about.html" title="关于博客"><i class="mdui-list-item-icon nexmoefont icon-info-circle"></i><div class="mdui-list-item-content"> 关于博客</div></a><a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/py.html" title="我的朋友"><i class="mdui-list-item-icon nexmoefont icon-unorderedlist"></i><div class="mdui-list-item-content"> 我的朋友</div></a></ul><aside id="nexmoe-sidebar"><div class="nexmoe-widget-wrap"><h3 class="nexmoe-widget-title">社交按钮</h3><div class="nexmoe-widget nexmoe-social"><a class="mdui-ripple" href="https://github.com/OneJane" target="_blank" mdui-tooltip="{content: 'GitHub'}" style="color:#191717;background-color:rgba(25,23,23,.15)"><i class="nexmoefont icon-github"></i></a></div></div><div class="nexmoe-widget-wrap"><h3 class="nexmoe-widget-title">文章分类</h3><div class="nexmoe-widget"><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/人工智能/">人工智能</a><span class="category-list-count">16</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/博客/">博客</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/定时器/">定时器</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/持续集成/">持续集成</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据库/">数据库</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/注册中心/">注册中心</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/测试/">测试</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/系统/">系统</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/自然语言处理/">自然语言处理</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/项目实战/">项目实战</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/高可用/">高可用</a><span class="category-list-count">4</span></li></ul></div></div><div class="nexmoe-widget-wrap"><h3 class="nexmoe-widget-title">标签云</h3><div class="nexmoe-widget tagcloud"> <a href="/tags/Gensim/" style="font-size:10px">Gensim</a> <a href="/tags/Hanlp/" style="font-size:10px">Hanlp</a> <a href="/tags/NLTK/" style="font-size:10px">NLTK</a> <a href="/tags/OpenCV/" style="font-size:12.86px">OpenCV</a> <a href="/tags/Stanford-NLP/" style="font-size:10px">Stanford NLP</a> <a href="/tags/Tensorflow/" style="font-size:15.71px">Tensorflow</a> <a href="/tags/ant-design/" style="font-size:10px">ant design</a> <a href="/tags/ant-design-pro/" style="font-size:11.43px">ant design pro</a> <a href="/tags/auc/" style="font-size:10px">auc</a> <a href="/tags/bottle/" style="font-size:10px">bottle</a> <a href="/tags/chatterbot/" style="font-size:10px">chatterbot</a> <a href="/tags/cnn/" style="font-size:12.86px">cnn</a> <a href="/tags/crf/" style="font-size:12.86px">crf</a> <a href="/tags/doc2vec/" style="font-size:10px">doc2vec</a> <a href="/tags/docker/" style="font-size:17.14px">docker</a> <a href="/tags/dubbo/" style="font-size:11.43px">dubbo</a> <a href="/tags/elasticsearch/" style="font-size:10px">elasticsearch</a> <a href="/tags/elastisearch/" style="font-size:10px">elastisearch</a> <a href="/tags/email/" style="font-size:10px">email</a> <a href="/tags/es6/" style="font-size:10px">es6</a> <a href="/tags/feign/" style="font-size:10px">feign</a> <a href="/tags/flask/" style="font-size:11.43px">flask</a> <a href="/tags/folium/" style="font-size:10px">folium</a> <a href="/tags/freemarker/" style="font-size:10px">freemarker</a> <a href="/tags/function/" style="font-size:10px">function</a> <a href="/tags/gateway/" style="font-size:10px">gateway</a> <a href="/tags/gensim/" style="font-size:11.43px">gensim</a> <a href="/tags/gitlab/" style="font-size:11.43px">gitlab</a> <a href="/tags/gru/" style="font-size:11.43px">gru</a> <a href="/tags/hanlp/" style="font-size:11.43px">hanlp</a> <a href="/tags/haproxy/" style="font-size:10px">haproxy</a> <a href="/tags/hmm/" style="font-size:10px">hmm</a> <a href="/tags/jenkins/" style="font-size:11.43px">jenkins</a> <a href="/tags/jieba/" style="font-size:15.71px">jieba</a> <a href="/tags/jmeter/" style="font-size:10px">jmeter</a> <a href="/tags/keepalived/" style="font-size:10px">keepalived</a> <a href="/tags/lda/" style="font-size:11.43px">lda</a> <a href="/tags/linux/" style="font-size:10px">linux</a> <a href="/tags/lstm/" style="font-size:12.86px">lstm</a> <a href="/tags/maven/" style="font-size:11.43px">maven</a> <a href="/tags/multi-druid/" style="font-size:10px">multi druid</a> <a href="/tags/mybatis/" style="font-size:10px">mybatis</a> <a href="/tags/mybatisplus/" style="font-size:10px">mybatisplus</a> <a href="/tags/mysql/" style="font-size:10px">mysql</a> <a href="/tags/n-gram/" style="font-size:10px">n-gram</a> <a href="/tags/nacos/" style="font-size:11.43px">nacos</a> <a href="/tags/neo4j/" style="font-size:11.43px">neo4j</a> <a href="/tags/nexmoe/" style="font-size:10px">nexmoe</a> <a href="/tags/nlp/" style="font-size:20px">nlp</a> <a href="/tags/numpy/" style="font-size:10px">numpy</a> <a href="/tags/partition/" style="font-size:10px">partition</a> <a href="/tags/procedure/" style="font-size:10px">procedure</a> <a href="/tags/pxc/" style="font-size:10px">pxc</a> <a href="/tags/pyhanlp/" style="font-size:11.43px">pyhanlp</a> <a href="/tags/python/" style="font-size:10px">python</a> <a href="/tags/rabbitmq/" style="font-size:10px">rabbitmq</a> <a href="/tags/react/" style="font-size:11.43px">react</a> <a href="/tags/redis/" style="font-size:11.43px">redis</a> <a href="/tags/redis-cluster/" style="font-size:10px">redis-cluster</a> <a href="/tags/replication/" style="font-size:11.43px">replication</a> <a href="/tags/rnn/" style="font-size:10px">rnn</a> <a href="/tags/rocketmq/" style="font-size:11.43px">rocketmq</a> <a href="/tags/scrapy/" style="font-size:12.86px">scrapy</a> <a href="/tags/selenium/" style="font-size:12.86px">selenium</a> <a href="/tags/sentinel/" style="font-size:14.29px">sentinel</a> <a href="/tags/seq2seq/" style="font-size:10px">seq2seq</a> <a href="/tags/session/" style="font-size:10px">session</a> <a href="/tags/sklearn/" style="font-size:10px">sklearn</a> <a href="/tags/skywalking/" style="font-size:11.43px">skywalking</a> <a href="/tags/snownlp/" style="font-size:10px">snownlp</a> <a href="/tags/spring-cloud-alibaba/" style="font-size:18.57px">spring cloud alibaba</a> <a href="/tags/springboot/" style="font-size:14.29px">springboot</a> <a href="/tags/svm/" style="font-size:10px">svm</a> <a href="/tags/swagger/" style="font-size:10px">swagger</a> <a href="/tags/textrank/" style="font-size:10px">textrank</a> <a href="/tags/tf-idf/" style="font-size:12.86px">tf-idf</a> <a href="/tags/tk-mybatis/" style="font-size:10px">tk mybatis</a> <a href="/tags/umi/" style="font-size:10px">umi</a> <a href="/tags/validate/" style="font-size:10px">validate</a> <a href="/tags/word2vec/" style="font-size:10px">word2vec</a> <a href="/tags/wordcloud/" style="font-size:10px">wordcloud</a> <a href="/tags/xxl-job/" style="font-size:11.43px">xxl-job</a> <a href="/tags/zookeeper/" style="font-size:10px">zookeeper</a></div></div><div class="nexmoe-widget-wrap"><h3 class="nexmoe-widget-title">文章归档</h3><div class="nexmoe-widget"><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">十月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">九月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li></ul></div></div></aside><div class="nexmoe-copyright"> &copy; 2020 OneJane</div></div></div><div id="nexmoe-content"><div class="nexmoe-primary"><div class="nexmoe-post"><div class="nexmoe-post-cover"> <img src="https://www.github.com/OneJane/blog/raw/master/小书匠/1566388401380.png"><h1>吴恩达深度学习笔记(第五课时)</h1></div><div class="nexmoe-post-meta"><a><i class="nexmoefont icon-calendar-fill"></i> 2019年12月05日</a><a><i class="nexmoefont icon-areachart"></i> 69.7k 字</a><a><i class="nexmoefont icon-time-circle-fill"></i> 大概 296 分钟</a> <a class="nexmoefont icon-appstore-fill -link" href="/categories/人工智能/">人工智能</a> <a class="nexmoefont icon-tag-fill -link" href="/tags/nlp/">nlp</a></div><article><p><a href="https://onejane.github.io/">吴恩达深度学习笔记</a></p><a id="more"></a><h1 id="序列模型-Sequence-Models"><a href="#序列模型-Sequence-Models" class="headerlink" title="序列模型(Sequence Models)"></a>序列模型(Sequence Models)</h1><h2 id="第一周-循环序列模型（Recurrent-Neural-Networks）"><a href="#第一周-循环序列模型（Recurrent-Neural-Networks）" class="headerlink" title="第一周 循环序列模型（Recurrent Neural Networks）"></a>第一周 循环序列模型（Recurrent Neural Networks）</h2><h3 id="1-1-为什么选择序列模型？（Why-Sequence-Models-）"><a href="#1-1-为什么选择序列模型？（Why-Sequence-Models-）" class="headerlink" title="1.1 为什么选择序列模型？（Why Sequence Models?）"></a>1.1 为什么选择序列模型？（Why Sequence Models?）</h3><p>在本课程中你将学会序列模型，它是深度学习中最令人激动的内容之一。循环神经网络（<strong>RNN</strong>）之类的模型在语音识别、自然语言处理和其他领域中引起变革。在本节课中，你将学会如何自行创建这些模型。我们先看一些例子，这些例子都有效使用了序列模型。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/ae2970d80a119cd341ef31c684bfac49.png" alt="ae2970d80a119cd341ef31c684bfac49"><br>在进行语音识别时，给定了一个输入音频片段 $X$，并要求输出对应的文字记录 $Y$。这个例子里输入和输出数据都是序列模型，因为 $X$是一个按时播放的音频片段，输出 $Y$是一系列单词。所以之后将要学到的一些序列模型，如循环神经网络等等在语音识别方面是非常有用的。</p><p>音乐生成问题是使用序列数据的另一个例子，在这个例子中，只有输出数据 $Y$是序列，而输入数据可以是空集，也可以是个单一的整数，这个数可能指代你想要生成的音乐风格，也可能是你想要生成的那首曲子的头几个音符。输入的 $X$可以是空的，或者就是个数字，然后输出序列 $Y$。</p><p>在处理情感分类时，输入数据 $X$是序列，你会得到类似这样的输入：“<strong>There is nothing to like in this movie.</strong>”，你认为这句评论对应几星？</p><p>系列模型在<strong>DNA</strong>序列分析中也十分有用，你的<strong>DNA</strong>可以用<strong>A</strong>、<strong>C</strong>、<strong>G</strong>、<strong>T</strong>四个字母来表示。所以给定一段<strong>DNA</strong>序列，你能够标记出哪部分是匹配某种蛋白质的吗？</p><p>在机器翻译过程中，你会得到这样的输入句：“<strong>Voulez-vou chante avecmoi?</strong>”（法语：要和我一起唱么？），然后要求你输出另一种语言的翻译结果。</p><p>在进行视频行为识别时，你可能会得到一系列视频帧，然后要求你识别其中的行为。</p><p>在进行命名实体识别时，可能会给定一个句子要你识别出句中的人名。</p><p>所以这些问题都可以被称作使用标签数据 $(X,Y)$作为训练集的监督学习。但从这一系列例子中你可以看出序列问题有很多不同类型。有些问题里，输入数据 $X$和输出数据$Y$都是序列，但就算在那种情况下，$X$和$Y$有时也会不一样长。或者像上图编号1所示和上图编号2的$X$和$Y$有相同的数据长度。在另一些问题里，只有 $X$或者只有$Y$是序列。</p><p>所以在本节我们学到适用于不同情况的序列模型。</p><p>下节中我们会定义一些定义序列问题要用到的符号。</p><h3 id="1-2-数学符号（Notation）"><a href="#1-2-数学符号（Notation）" class="headerlink" title="1.2 数学符号（Notation）"></a>1.2 数学符号（Notation）</h3><p>本节先从定义符号开始一步步构建序列模型。</p><p>比如说你想要建立一个序列模型，它的输入语句是这样的：“<strong>Harry Potter and Herminoe Granger invented a new spell.</strong>”，(这些人名都是出自于<strong>J.K.Rowling</strong>笔下的系列小说<strong>Harry Potter</strong>)。假如你想要建立一个能够自动识别句中人名位置的序列模型，那么这就是一个命名实体识别问题，这常用于搜索引擎，比如说索引过去24小时内所有新闻报道提及的人名，用这种方式就能够恰当地进行索引。命名实体识别系统可以用来查找不同类型的文本中的人名、公司名、时间、地点、国家名和货币名等等。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/cccbc03192af67a089b53d7940659505.png" alt="cccbc03192af67a089b53d7940659505"><br>现在给定这样的输入数据$x$，假如你想要一个序列模型输出$y$，使得输入的每个单词都对应一个输出值，同时这个$y$能够表明输入的单词是否是人名的一部分。技术上来说这也许不是最好的输出形式，还有更加复杂的输出形式，它不仅能够表明输入词是否是人名的一部分，它还能够告诉你这个人名在这个句子里从哪里开始到哪里结束。比如<strong>Harry Potter</strong>（上图编号1所示）、<strong>Hermione Granger</strong>（上图标号2所示）。</p><p>更简单的那种输出形式:</p><p>这个输入数据是9个单词组成的序列，所以最终我们会有9个特征集和来表示这9个单词，并按序列中的位置进行索引，$x^{&lt;1&gt;}$、$x^{&lt;2&gt;}$、$x^{&lt;3&gt;}$等等一直到$x^{&lt;9&gt;}$来索引不同的位置，我将用$x^{&lt;t&gt;}$来索引这个序列的中间位置。$t$意味着它们是时序序列，但不论是否是时序序列，我们都将用$t$来索引序列中的位置。</p><p>输出数据也是一样，我们还是用$y^{&lt;1&gt;}$、$y^{&lt;2&gt;}$、$y^{&lt;3&gt;}$等等一直到$y^{&lt;9&gt;}$来表示输出数据。同时我们用$T_{x}$来表示输入序列的长度，这个例子中输入是9个单词，所以$T_{x}= 9$。我们用$T_{y}$来表示输出序列的长度。在这个例子里$T_{x} =T_{y}$，上个视频里你知道$T_{x}$和$T_{y}$可以有不同的值。</p><p>你应该记得我们之前用的符号，我们用$x^{(i)}$来表示第$i$个训练样本，所以为了指代第$t$个元素，或者说是训练样本i的序列中第$t$个元素用$x^{\left(i \right) &lt;t&gt;}$这个符号来表示。如果$T_{x}$是序列长度，那么你的训练集里不同的训练样本就会有不同的长度，所以$T_{x}^{(i)}$就代表第$i$个训练样本的输入序列长度。同样$y^{\left( i \right) &lt; t&gt;}$代表第$i$个训练样本中第$t$个元素，$T_{y}^{(i)}$就是第$i$个训练样本的输出序列的长度。</p><p>所以在这个例子中，$T_{x}^{(i)}=9$，但如果另一个样本是由15个单词组成的句子，那么对于这个训练样本，$T_{x}^{(i)}=15$。</p><p>既然我们这个例子是<strong>NLP</strong>，也就是自然语言处理，这是我们初次涉足自然语言处理，一件我们需要事先决定的事是怎样表示一个序列里单独的单词，你会怎样表示像<strong>Harry</strong>这样的单词，$x^{&lt;1&gt;}$实际应该是什么？</p><p>接下来我们讨论一下怎样表示一个句子里单个的词。想要表示一个句子里的单词，第一件事是做一张词表，有时也称为词典，意思是列一列你的表示方法中用到的单词。这个词表（下图所示）中的第一个词是<strong>a</strong>，也就是说词典中的第一个单词是<strong>a</strong>，第二个单词是<strong>Aaron</strong>，然后更下面一些是单词<strong>and</strong>，再后面你会找到<strong>Harry</strong>，然后找到<strong>Potter</strong>，这样一直到最后，词典里最后一个单词可能是<strong>Zulu</strong>。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/a45c8066f935c6f29d00a95e36cb6662.png" alt="a45c8066f935c6f29d00a95e36cb6662"><br>因此<strong>a</strong>是第一个单词，<strong>Aaron</strong>是第二个单词，在这个词典里，<strong>and</strong>出现在367这个位置上，<strong>Harry</strong>是在4075这个位置，<strong>Potter</strong>在6830，词典里的最后一个单词<strong>Zulu</strong>可能是第10,000个单词。所以在这个例子中我用了10,000个单词大小的词典，这对现代自然语言处理应用来说太小了。对于商业应用来说，或者对于一般规模的商业应用来说30,000到50,000词大小的词典比较常见，但是100,000词的也不是没有，而且有些大型互联网公司会用百万词，甚至更大的词典。许多商业应用用的词典可能是30,000词，也可能是50,000词。不过我将用10,000词大小的词典做说明，因为这是一个很好用的整数。</p><p>如果你选定了10,000词的词典，构建这个词典的一个方法是遍历你的训练集，并且找到前10,000个常用词，你也可以去浏览一些网络词典，它能告诉你英语里最常用的10,000个单词，接下来你可以用<strong>one-hot</strong>表示法来表示词典里的每个单词。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/8deca8a84f06466155d2d8d53d26e05d.png" alt="8deca8a84f06466155d2d8d53d26e05d"><br>举个例子，在这里$x^{&lt;1&gt;}$表示<strong>Harry</strong>这个单词，它就是一个第4075行是1，其余值都是0的向量（上图编号1所示），因为那是<strong>Harry</strong>在这个词典里的位置。</p><p>同样$x^{&lt;2&gt;}$是个第6830行是1，其余位置都是0的向量（上图编号2所示）。</p><p><strong>and</strong>在词典里排第367，所以$x^{&lt;3&gt;}$就是第367行是1，其余值都是0的向量（上图编号3所示）。如果你的词典大小是10,000的话，那么这里的每个向量都是10,000维的。</p><p>因为<strong>a</strong>是字典第一个单词，$x^{&lt;7&gt;}$对应<strong>a</strong>，那么这个向量的第一个位置为1，其余位置都是0的向量（上图编号4所示）。</p><p>所以这种表示方法中，$x^{&lt;t&gt;}$指代句子里的任意词，它就是个<strong>one-hot</strong>向量，因为它只有一个值是1，其余值都是0，所以你会有9个<strong>one-hot</strong>向量来表示这个句中的9个单词，目的是用这样的表示方式表示$X$，用序列模型在$X$和目标输出$Y$之间学习建立一个映射。我会把它当作监督学习的问题，我确信会给定带有$(x，y)$标签的数据。</p><p>那么还剩下最后一件事，我们将在之后的视频讨论，如果你遇到了一个不在你词表中的单词，答案就是创建一个新的标记，也就是一个叫做<strong>Unknow Word</strong>的伪造单词，用&amp;lt;<strong>UNK</strong>&amp;gt;作为标记，来表示不在词表中的单词，我们之后会讨论更多有关这个的内容。</p><p>总结一下本节课的内容，我们描述了一套符号用来表述你的训练集里的序列数据$x$和$y$，在下节课我们开始讲述循环神经网络中如何构建$X$到$Y$的映射。</p><h3 id="1-3-循环神经网络模型（Recurrent-Neural-Network-Model）"><a href="#1-3-循环神经网络模型（Recurrent-Neural-Network-Model）" class="headerlink" title="1.3 循环神经网络模型（Recurrent Neural Network Model）"></a>1.3 循环神经网络模型（Recurrent Neural Network Model）</h3><p>上节视频中，你了解了我们用来定义序列学习问题的符号。现在我们讨论一下怎样才能建立一个模型，建立一个神经网络来学习$X$到$Y$的映射。</p><p>可以尝试的方法之一是使用标准神经网络，在我们之前的例子中，我们有9个输入单词。想象一下，把这9个输入单词，可能是9个<strong>one-hot</strong>向量，然后将它们输入到一个标准神经网络中，经过一些隐藏层，最终会输出9个值为0或1的项，它表明每个输入单词是否是人名的一部分。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1653ec3b8eb718ca817d3423ae3ca643.png" alt="1653ec3b8eb718ca817d3423ae3ca643"><br>但结果表明这个方法并不好，主要有两个问题，</p><p>一、是输入和输出数据在不同例子中可以有不同的长度，不是所有的例子都有着同样输入长度$T_{x}$或是同样输出长度的$T_{y}$。即使每个句子都有最大长度，也许你能够填充（<strong>pad</strong>）或零填充（<strong>zero pad</strong>）使每个输入语句都达到最大长度，但仍然看起来不是一个好的表达方式。</p><p>二、一个像这样单纯的神经网络结构，它并不共享从文本的不同位置上学到的特征。具体来说，如果神经网络已经学习到了在位置1出现的<strong>Harry</strong>可能是人名的一部分，那么如果<strong>Harry</strong>出现在其他位置，比如$x^{&lt;t&gt;}$时，它也能够自动识别其为人名的一部分的话，这就很棒了。这可能类似于你在卷积神经网络中看到的，你希望将部分图片里学到的内容快速推广到图片的其他部分，而我们希望对序列数据也有相似的效果。和你在卷积网络中学到的类似，用一个更好的表达方式也能够让你减少模型中参数的数量。</p><p>之前我们提到过这些（上图编号1所示的$x^{&lt;1&gt;}$……$x^{&lt;t&gt;}$……$x^{&lt; T_{x}&gt;}$）都是10,000维的<strong>one-hot</strong>向量，因此这会是十分庞大的输入层。如果总的输入大小是最大单词数乘以10,000，那么第一层的权重矩阵就会有着巨量的参数。但循环神经网络就没有上述的两个问题。</p><p>那么什么是循环神经网络呢？我们先建立一个（下图编号1所示）。如果你以从左到右的顺序读这个句子，第一个单词就是，假如说是$x^{&lt;1&gt;}$，我们要做的就是将第一个词输入一个神经网络层，我打算这样画，第一个神经网络的隐藏层，我们可以让神经网络尝试预测输出，判断这是否是人名的一部分。循环神经网络做的是，当它读到句中的第二个单词时，假设是$x^{&lt;2&gt;}$，它不是仅用$x^{&lt;2&gt;}$就预测出${\hat{y} }^{&lt;2&gt;}$，他也会输入一些来自时间步1的信息。具体而言，时间步1的激活值就会传递到时间步2。然后，在下一个时间步，循环神经网络输入了单词$x^{&lt;3&gt;}$，然后它尝试预测输出了预测结果${\hat{y} }^{&lt;3&gt;}$，等等，一直到最后一个时间步，输入了$x^{&lt;T_{x}&gt;}$，然后输出了${\hat{y} }^{&lt; T_{y} &gt;}$。至少在这个例子中$T_{x} =T_{y}$，同时如果$T_{x}$和$T_{y}$不相同，这个结构会需要作出一些改变。所以在每一个时间步中，循环神经网络传递一个激活值到下一个时间步中用于计算。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/cb041c33b65e17600842ebf87174c4f2.png" alt="cb041c33b65e17600842ebf87174c4f2"><br>要开始整个流程，在零时刻需要构造一个激活值$a^{&lt;0&gt;}$，这通常是零向量。有些研究人员会随机用其他方法初始化$a^{&lt;0&gt;}$，不过使用零向量作为零时刻的伪激活值是最常见的选择，因此我们把它输入神经网络。</p><p>在一些研究论文中或是一些书中你会看到这类神经网络，用这样的图形来表示（上图编号2所示），在每一个时间步中，你输入$x^{&lt;t&gt;}$然后输出$y^{&lt;t&gt;}$。然后为了表示循环连接有时人们会像这样画个圈，表示输回网络层，有时他们会画一个黑色方块，来表示在这个黑色方块处会延迟一个时间步。我个人认为这些循环图很难理解，所以在本次课程中，我画图更倾向于使用左边这种分布画法（上图编号1所示）。不过如果你在教材中或是研究论文中看到了右边这种图表的画法（上图编号2所示），它可以在心中将这图展开成左图那样。</p><p>循环神经网络是从左向右扫描数据，同时每个时间步的参数也是共享的，所以下页幻灯片中我们会详细讲述它的一套参数，我们用$W_{\text{ax} }$来表示管理着从$x^{&lt;1&gt;}$到隐藏层的连接的一系列参数，每个时间步使用的都是相同的参数$W_{\text{ax} }$。而激活值也就是水平联系是由参数$W_{aa}$决定的，同时每一个时间步都使用相同的参数$W_{aa}$，同样的输出结果由$W_{\text{ya} }$决定。下图详细讲述这些参数是如何起作用。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/140529e4d7531babb5ba21778cd88bc3.png" alt="140529e4d7531babb5ba21778cd88bc3"><br>在这个循环神经网络中，它的意思是在预测${\hat{y} }^{&lt; 3 &gt;}$时，不仅要使用$x^{&lt;3&gt;}$的信息，还要使用来自$x^{&lt;1&gt;}$和$x^{&lt;2&gt;}$的信息，因为来自$x^{&lt;1&gt;}$的信息可以通过这样的路径（上图编号1所示的路径）来帮助预测${\hat{y} }^{&lt;3&gt;}$。这个循环神经网络的一个缺点就是它只使用了这个序列中之前的信息来做出预测，尤其当预测${\hat{y} }^{&lt;3&gt;}$时，它没有用到$x^{&lt;4&gt;}$，$x^{&lt;5&gt;}$，$x^{&lt;6&gt;}$等等的信息。所以这就有一个问题，因为如果给定了这个句子，“<strong>Teddy Roosevelt was a great President.</strong>”，为了判断<strong>Teddy</strong>是否是人名的一部分，仅仅知道句中前两个词是完全不够的，还需要知道句中后部分的信息，这也是十分有用的，因为句子也可能是这样的，“<strong>Teddy bears are on sale!</strong>”。因此如果只给定前三个单词，是不可能确切地知道<strong>Teddy</strong>是否是人名的一部分，第一个例子是人名，第二个例子就不是，所以你不可能只看前三个单词就能分辨出其中的区别。</p><p>所以这样特定的神经网络结构的一个限制是它在某一时刻的预测仅使用了从序列之前的输入信息并没有使用序列中后部分的信息，我们会在之后的双向循环神经网络（<strong>BRNN</strong>）的视频中处理这个问题。但对于现在，这个更简单的单向神经网络结构就够我们来解释关键概念了，之后只要在此基础上作出修改就能同时使用序列中前面和后面的信息来预测${\hat{y} }^{&lt;3&gt;}$，不过我们会在之后的视频讲述这些内容，接下来我们具体地写出这个神经网络计算了些什么。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/19cbb2d356a2a6e0f35aa2a946b23a2a.png" alt="19cbb2d356a2a6e0f35aa2a946b23a2a"><br>这里是一张清理后的神经网络示意图，和我之前提及的一样，一般开始先输入$a^{&lt;0&gt;}$，它是一个零向量。接着就是前向传播过程，先计算激活值$a^{&lt;1&gt;}$，然后再计算$y^{&lt;1&gt;}$。</p> $a^{&lt;1&gt;} = g_{1}(W_{ {aa} }a^{&lt; 0 &gt;} + W_{ {ax} }x^{&lt; 1 &gt;} + b_{a})$ $\hat y^{&lt; 1 &gt;} = g_{2}(W_{ {ya} }a^{&lt; 1 &gt;} + b_{y})$<p>我将用这样的符号约定来表示这些矩阵下标，举个例子$W_{\text{ax} }$，第二个下标意味着$W_{\text{ax} }$要乘以某个$x$类型的量，然后第一个下标$a$表示它是用来计算某个$a$类型的变量。同样的，可以看出这里的$W_{\text{ya} }$乘上了某个$a$类型的量，用来计算出某个$\hat {y}$类型的量。</p><p>循环神经网络用的激活函数经常是<strong>tanh</strong>，不过有时候也会用<strong>ReLU</strong>，但是<strong>tanh</strong>是更通常的选择，我们有其他方法来避免梯度消失问题，我们将在之后进行讲述。选用哪个激活函数是取决于你的输出$y$，如果它是一个二分问题，那么我猜你会用<strong>sigmoid</strong>函数作为激活函数，如果是$k$类别分类问题的话，那么可以选用<strong>softmax</strong>作为激活函数。不过这里激活函数的类型取决于你有什么样类型的输出$y$，对于命名实体识别来说$y$只可能是0或者1，那我猜这里第二个激活函数$g$可以是<strong>sigmoid</strong>激活函数。</p><p>更一般的情况下，在$t$时刻，</p> $a^{&lt; t &gt;} = g_{1}(W_{aa}a^{&lt; t - 1 &gt;} + W_{ax}x^{&lt; t &gt;} + b_{a})$ $\hat y^{&lt; t &gt;} = g_{2}(W_{ {ya} }a^{&lt; t &gt;} + b_{y})$<p>所以这些等式定义了神经网络的前向传播，你可以从零向量$a^{&lt;0&gt;}$开始，然后用$a^{&lt;0&gt;}$和$x^{&lt;1&gt;}$来计算出$a^{&lt;1&gt;}$和$\hat y^{&lt;1&gt;}$，然后用$x^{&lt;2&gt;}$和$a^{&lt;1&gt;}$一起算出$a^{&lt;2&gt;}$和$\hat y^{&lt;2&gt;}$等等，像图中这样，从左到右完成前向传播。</p><p>现在为了帮我们建立更复杂的神经网络，我实际要将这个符号简化一下，我在下一张幻灯片里复制了这两个等式（上图编号1所示的两个等式）。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/27afdd27f45ad8ddf78677af2a3eeaf8.png" alt="27afdd27f45ad8ddf78677af2a3eeaf8"><br>接下来为了简化这些符号，我要将这部分（$W_{\text{aa} }a^{&lt;t -1&gt;} +W_{\text{ax} }x^{&lt;t&gt;}$）（上图编号1所示）以更简单的形式写出来，我把它写做$a^{&lt;t&gt;} =g(W_{a}\left\lbrack a^{&lt; t-1 &gt;},x^{&lt;t&gt;} \right\rbrack +b_{a})$（上图编号2所示），那么左右两边划线部分应该是等价的。所以我们定义$W_{a}$的方式是将矩阵$W_{aa}$和矩阵$W_{ {ax} }$水平并列放置，$[ { {W}_{aa} }\vdots { {W}_{ax} }]=W_{a}$（上图编号3所示）。举个例子，如果$a$是100维的，然后延续之前的例子，$x$是10,000维的，那么$W_{aa}$就是个$（100，100）$维的矩阵，$W_{ax}$就是个$（100，10,000）$维的矩阵，因此如果将这两个矩阵堆起来，$W_{a}$就会是个$（100，10,100）$维的矩阵。</p><p>用这个符号（$\left\lbrack a^{&lt; t - 1 &gt;},x^{&lt; t &gt;}\right\rbrack$）的意思是将这两个向量堆在一起，我会用这个符号表示，即$\begin{bmatrix}a^{&lt; t-1 &gt;} \\ x^{&lt; t &gt;} \\\end{bmatrix}$（上图编号4所示），最终这就是个10,100维的向量。你可以自己检查一下，用这个矩阵乘以这个向量，刚好能够得到原来的量，因为此时，矩阵$[ { {W}_{aa} }\vdots { {W}_{ax} }]$乘以$\begin{bmatrix} a^{&lt; t - 1 &gt;} \\ x^{&lt; t &gt;} \\ \end{bmatrix}$，刚好等于$W_{ {aa} }a^{&lt;t-1&gt;} + W_{ {ax} }x^{&lt;t&gt;}$，刚好等于之前的这个结论（上图编号5所示）。这种记法的好处是我们可以不使用两个参数矩阵$W_{ {aa} }$和$W_{ {ax} }$，而是将其压缩成一个参数矩阵$W_{a}$，所以当我们建立更复杂模型时这就能够简化我们要用到的符号。</p><p>同样对于这个例子（$\hat y^{&lt;t&gt;} = g(W_{ya}a^{&lt;t&gt;} +b_{y})$），我会用更简单的方式重写，$\hat y^{&lt; t &gt;} = g(W_{y}a^{&lt; t &gt;} +b_{y})$（上图编号6所示）。现在$W_{y}$和$b_{y}$符号仅有一个下标，它表示在计算时会输出什么类型的量，所以$W_{y}$就表明它是计算$y$类型的量的权重矩阵，而上面的$W_{a}$和$b_{a}$则表示这些参数是用来计算$a$类型或者说是激活值的。</p><p><strong>RNN</strong>前向传播示意图：</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/rnn-f.png" alt="rnn-f"><br>好就这么多，你现在知道了基本的循环神经网络，下节课我们会一起来讨论反向传播，以及你如何能够用<strong>RNN</strong>进行学习。</p><h3 id="1-4-通过时间的反向传播（Backpropagation-through-time）"><a href="#1-4-通过时间的反向传播（Backpropagation-through-time）" class="headerlink" title="1.4 通过时间的反向传播（Backpropagation through time）"></a>1.4 通过时间的反向传播（Backpropagation through time）</h3><p>之前我们已经学过了循环神经网络的基础结构，在本节视频中我们将来了解反向传播是怎样在循环神经网络中运行的。和之前一样，当你在编程框架中实现循环神经网络时，编程框架通常会自动处理反向传播。但我认为，在循环神经网络中，对反向传播的运行有一个粗略的认识还是非常有用的，让我们来一探究竟。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/998c7af4f90cd0de0c88f138b61f0168.png" alt="998c7af4f90cd0de0c88f138b61f0168"><br>在之前你已经见过对于前向传播（上图蓝色箭头所指方向）怎样在神经网络中从左到右地计算这些激活项，直到输出所有地预测结果。而对于反向传播，我想你已经猜到了，反向传播地计算方向（上图红色箭头所指方向）与前向传播基本上是相反的。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/ad9dd74b6ce9bcea14baa289df530d6b.png" alt="ad9dd74b6ce9bcea14baa289df530d6b"><br>我们来分析一下前向传播的计算，现在你有一个输入序列，$x^{&lt;1&gt;}$，$x^{&lt;2&gt;}$，$x^{&lt;3&gt;}$一直到$x^{&lt; T_{x} &gt;}$，然后用$x^{&lt;1&gt;}$还有$a^{&lt;0&gt;}$计算出时间步1的激活项，再用$x^{&lt;2&gt;}$和$a^{&lt;1&gt;}$计算出$a^{&lt;2&gt;}$，然后计算$a^{&lt;3&gt;}$等等，一直到$a^{&lt; T_{x} &gt;}$。</p><p>为了真正计算出$a^{&lt;1&gt;}$，你还需要一些参数，$W_{a}$和$b_{a}$，用它们来计算出$a^{&lt;1&gt;}$。这些参数在之后的每一个时间步都会被用到，于是继续用这些参数计算$a^{&lt;2&gt;}$，$a^{&lt;3&gt;}$等等，所有的这些激活项都要取决于参数$W_{a}$和$b_{a}$。有了$a^{&lt;1&gt;}$，神经网络就可以计算第一个预测值$\hat y^{&lt;1&gt;}$，接着到下一个时间步，继续计算出$\hat y^{&lt;2&gt;}$，$\hat y^{&lt;3&gt;}$，等等，一直到$\hat y^{&lt;T_{y}&gt;}$。为了计算出${\hat{y} }$，需要参数$W_{y}$和$b_{y}$，它们将被用于所有这些节点。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/71a0ed918704f6d35091d8b6d60793e4.png" alt="71a0ed918704f6d35091d8b6d60793e4"><br>然后为了计算反向传播，你还需要一个损失函数。我们先定义一个元素损失函数（上图编号1所示）</p> $L^{&lt;t&gt;}( \hat y^{&lt;t&gt;},y^{&lt;t&gt;}) = - y^{&lt;t&gt;}\log\hat y^{&lt;t&gt;}-( 1- y^{&lt;t&gt;})log(1-\hat y^{&lt;t&gt;})$<p>它对应的是序列中一个具体的词，如果它是某个人的名字，那么$y^{&lt;t&gt;}$的值就是1，然后神经网络将输出这个词是名字的概率值，比如0.1。我将它定义为标准逻辑回归损失函数，也叫交叉熵损失函数（<strong>Cross Entropy Loss</strong>），它和之前我们在二分类问题中看到的公式很像。所以这是关于单个位置上或者说某个时间步$t$上某个单词的预测值的损失函数。</p><p>现在我们来定义整个序列的损失函数，将$L$定义为（上图编号2所示）</p> $L(\hat y,y) = \ \sum_{t = 1}^{T_{x} }{L^{&lt; t &gt;}(\hat y^{&lt; t &gt;},y^{&lt; t &gt;})}$<p>在这个计算图中，通过$\hat y^{&lt;1&gt;}$可以计算对应的损失函数，于是计算出第一个时间步的损失函数（上图编号3所示），然后计算出第二个时间步的损失函数，然后是第三个时间步，一直到最后一个时间步，最后为了计算出总体损失函数，我们要把它们都加起来，通过下面的等式（上图编号2所示的等式）计算出最后的$L$（上图编号4所示），也就是把每个单独时间步的损失函数都加起来。</p><p>这就是完整的计算图，在之前的例子中，你已经见过反向传播，所以你应该能够想得到反向传播算法需要在相反的方向上进行计算和传递信息，最终你做的就是把前向传播的箭头都反过来，在这之后你就可以计算出所有合适的量，然后你就可以通过导数相关的参数，用梯度下降法来更新参数。</p><p>在这个反向传播的过程中，最重要的信息传递或者说最重要的递归运算就是这个从右到左的运算，这也就是为什么这个算法有一个很别致的名字，叫做<strong>“通过（穿越）时间反向传播</strong>（<strong>backpropagation through time</strong>）”。取这个名字的原因是对于前向传播，你需要从左到右进行计算，在这个过程中，时刻$t$不断增加。而对于反向传播，你需要从右到左进行计算，就像时间倒流。“通过时间反向传播”，就像穿越时光，这种说法听起来就像是你需要一台时光机来实现这个算法一样。</p><p><strong>RNN</strong>反向传播示意图：</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/rnn_cell_backprop.png" alt="rnn_cell_backprop"><br>希望你大致了解了前向和反向传播是如何在<strong>RNN</strong>中工作的，到目前为止，你只见到了<strong>RNN</strong>中一个主要的例子，其中输入序列的长度和输出序列的长度是一样的。在下节课将展示更多的<strong>RNN</strong>架构，这将让你能够处理一些更广泛的应用。</p><h3 id="1-5-不同类型的循环神经网络（Different-types-of-RNNs）"><a href="#1-5-不同类型的循环神经网络（Different-types-of-RNNs）" class="headerlink" title="1.5 不同类型的循环神经网络（Different types of RNNs）"></a>1.5 不同类型的循环神经网络（Different types of <strong>RNN</strong>s）</h3><p>现在你已经了解了一种<strong>RNN</strong>结构，它的输入量$T_{x}$等于输出数量$T_{y}$。事实上，对于其他一些应用，$T_{x}$和$T_{y}$并不一定相等。在这个视频里，你会看到更多的<strong>RNN</strong>的结构。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/329b0748f7282efc206ea8de6a709833.png" alt="329b0748f7282efc206ea8de6a709833"><br>你应该还记得这周第一个视频中的那个幻灯片，那里有很多例子输入$x$和输出$y$，有各种类型，并不是所有的情况都满足$T_{x}=T_{y}$。</p><p>比如音乐生成这个例子，$T_{x}$可以是长度为1甚至为空集。再比如电影情感分类，输出$y$可以是1到5的整数，而输入是一个序列。在命名实体识别中，这个例子中输入长度和输出长度是一样的。</p><p>还有一些情况，输入长度和输出长度不同，他们都是序列但长度不同，比如机器翻译，一个法语句子和一个英语句子不同数量的单词却能表达同一个意思。</p><p>所以我们应该修改基本的<strong>RNN</strong>结构来处理这些问题，这个视频的内容参考了<strong>Andrej Karpathy</strong>的博客，一篇叫做《循环神经网络的非理性效果》（“<strong>The Unreasonable Effectiveness of Recurrent Neural Networks</strong>”）的文章，我们看一些例子。</p><p>你已经见过$T_{x} = T_{y}$的例子了（下图编号1所示），也就是我们输入序列$x^{&lt;1&gt;}$，$x^{&lt;2&gt;}$，一直到$x^{&lt; T_{x}&gt;}$，我们的循环神经网络这样工作，输入$x^{&lt;1&gt;}$来计算$\hat y^{&lt;1&gt;}$，$\hat y^{&lt;2&gt;}$等等一直到$\hat y^{&lt;T_{y}&gt;}$。在原先的图里，我会画一串圆圈表示神经元，大部分时候为了让符号更加简单，此处就以简单的小圈表示。这个就叫做“多对多”（<strong>many-to-many</strong>）的结构，因为输入序列有很多的输入，而输出序列也有很多输出。</p><p>现在我们看另外一个例子，假如说，你想处理情感分类问题（下图编号2所示），这里$x$可能是一段文本，比如一个电影的评论，“<strong>These is nothing to like in this movie.</strong>”（“这部电影没什么还看的。”），所以$x$就是一个序列，而$y$可能是从1到5的一个数字，或者是0或1，这代表正面评价和负面评价，而数字1到5代表电影是1星，2星，3星，4星还是5星。所以在这个例子中，我们可以简化神经网络的结构，输入$x^{&lt;1 &gt;}$，$x^{&lt; 2 &gt;}$，一次输入一个单词，如果输入文本是“<strong>These is nothing to like in this movie</strong>”，那么单词的对应如下图编号2所示。我们不再在每个时间上都有输出了，而是让这个<strong>RNN</strong>网络读入整个句子，然后在最后一个时间上得到输出，这样输入的就是整个句子，所以这个神经网络叫做“多对一”（<strong>many-to-one</strong>）结构，因为它有很多输入，很多的单词，然后输出一个数字。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/14e1df0a7a8cdd1584b2e92e87e23aa7.png" alt="14e1df0a7a8cdd1584b2e92e87e23aa7"><br>为了完整性，还要补充一个“<strong>一对一</strong>”（<strong>one-to-one</strong>）的结构（上图编号3所示），这个可能没有那么重要，这就是一个小型的标准的神经网络，输入$x$然后得到输出$y$，我们这个系列课程的前两个课程已经讨论过这种类型的神经网络了。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/db580f1dfd6095d672fc62cce74ce5e2.png" alt="db580f1dfd6095d672fc62cce74ce5e2"><br>除了“<strong>多对一</strong>”的结构，也可以有“<strong>一对多</strong>”（<strong>one-to-many</strong>）的结构。对于一个“一对多”神经网络结构的例子就是音乐生成（上图编号1所示），事实上，你会在这个课后编程练习中去实现这样的模型，你的目标是使用一个神经网络输出一些音符。对应于一段音乐，输入$x$可以是一个整数，表示你想要的音乐类型或者是你想要的音乐的第一个音符，并且如果你什么都不想输入，$x$可以是空的输入，可设为0向量。</p><p>这样这个神经网络的结构，首先是你的输入$x$，然后得到<strong>RNN</strong>的输出，第一个值，然后就没有输入了，再得到第二个输出，接着输出第三个值等等，一直到合成这个音乐作品的最后一个音符，这里也可以写上输入$a^{&lt;0&gt;}$（上图编号3所示）。有一个后面才会讲到的技术细节，当你生成序列时通常会把第一个合成的输出也喂给下一层（上图编号4所示），所以实际的网络结构最终就像这个样子。</p><p>我们已经讨论了“<strong>多对多</strong>”、“<strong>多对一</strong>”、“<strong>一对一</strong>”和“<strong>一对多</strong>”的结构，对于“多对多”的结构还有一个有趣的例子值得详细说一下，就是输入和输出长度不同的情况。你刚才看过的多对多的例子，它的输入长度和输出长度是完全一样的。而对于像机器翻译这样的应用，输入句子的单词的数量，比如说一个法语的句子，和输出句子的单词数量，比如翻译成英语，这两个句子的长度可能不同，所以还需要一个新的网络结构，一个不同的神经网络（上图编号2所示）。首先读入这个句子，读入这个输入，比如你要将法语翻译成英语，读完之后，这个网络就会输出翻译结果。有了这种结构$T_{x}$和$T_{y}$就可以是不同的长度了。同样，你也可以画上这个$a^{&lt;0&gt;}$。这个网络的结构有两个不同的部分，这（上图编号5所示）是一个编码器，获取输入，比如法语句子，这（上图编号6所示）是解码器，它会读取整个句子，然后输出翻译成其他语言的结果。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1daa38085604dd04e91ebc5e609d1179.png" alt="1daa38085604dd04e91ebc5e609d1179"><br>这就是一个“<strong>多对多</strong>”结构的例子，到这周结束的时候，你就能对这些各种各样结构的基本构件有一个很好的理解。严格来说，还有一种结构，我们会在第四周涉及到，就是“注意力”（<strong>attention based</strong>）结构，但是根据我们现在画的这些图不好理解这个模型。</p><p>总结一下这些各种各样的<strong>RNN</strong>结构，这（上图编号1所示）是“<strong>一对一</strong>”的结构，当去掉$a^{&lt;0&gt;}$时它就是一种标准类型的神经网络。还有一种“<strong>一对多</strong>”的结构（上图编号2所示），比如音乐生成或者序列生成。还有“<strong>多对一</strong>”，这（上图编号3所示）是情感分类的例子，首先读取输入，一个电影评论的文本，然后判断他们是否喜欢电影还是不喜欢。还有“<strong>多对多</strong>”的结构（上图编号4所示），命名实体识别就是“<strong>多对多</strong>”的例子，其中$T_{x}=T_{y}$。最后还有一种“<strong>多对多</strong>”结构的其他版本（上图编号5所示），对于像机器翻译这样的应用，$T_{x}$和$T_{y}$就可以不同了。</p><p>现在，你已经了解了大部分基本的模块，这些就是差不多所有的神经网络了，除了序列生成，有些细节的问题我们会在下节课讲解。</p><p>我希望你从本视频中了解到用这些<strong>RNN</strong>的基本模块，把它们组合在一起就可以构建各种各样的模型。但是正如我前面提到的，序列生成还有一些不一样的地方，在这周的练习里，你也会实现它，你需要构建一个语言模型，结果好的话会得到一些有趣的序列或者有意思的文本。下节课深入探讨序列生成。</p><h3 id="1-6-语言模型和序列生成（Language-model-and-sequence-generation）"><a href="#1-6-语言模型和序列生成（Language-model-and-sequence-generation）" class="headerlink" title="1.6 语言模型和序列生成（Language model and sequence generation）"></a>1.6 语言模型和序列生成（Language model and sequence generation）</h3><p>在自然语言处理中，构建语言模型是最基础的也是最重要的工作之一，并且能用<strong>RNN</strong>很好地实现。在本视频中，你将学习用<strong>RNN</strong>构建一个语言模型，在本周结束的时候，还会有一个很有趣的编程练习，你能在练习中构建一个语言模型，并用它来生成莎士比亚文风的文本或其他类型文本。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/fa17a85c0e7d9b14633013f2223c877b.png" alt="fa17a85c0e7d9b14633013f2223c877b"><br>所以什么是语言模型呢？比如你在做一个语音识别系统，你听到一个句子，“<strong>the apple and pear（pair） salad was delicious.</strong>”，所以我究竟说了什么？我说的是 “<strong>the apple and pair salad</strong>”，还是“<strong>the apple and pear salad</strong>”？（<strong>pear</strong>和<strong>pair</strong>是近音词）。你可能觉得我说的应该更像第二种，事实上，这就是一个好的语音识别系统要帮助输出的东西，即使这两句话听起来是如此相似。而让语音识别系统去选择第二个句子的方法就是使用一个语言模型，他能计算出这两句话各自的可能性。</p><p>举个例子，一个语音识别模型可能算出第一句话的概率是$P( \text{The apple and pair salad}) = 3.2 \times 10^{-13}$，而第二句话的概率是$P\left(\text{The apple and pear salad} \right) = 5.7 \times 10^{-10}$，比较这两个概率值，显然我说的话更像是第二种，因为第二句话的概率比第一句高出1000倍以上，这就是为什么语音识别系统能够在这两句话中作出选择。</p><p>所以语言模型所做的就是，它会告诉你某个特定的句子它出现的概率是多少，根据我所说的这个概率，假设你随机拿起一张报纸，打开任意邮件，或者任意网页或者听某人说下一句话，并且这个人是你的朋友，这个你即将从世界上的某个地方得到的句子会是某个特定句子的概率是多少，例如“<strong>the apple and pear salad</strong>”。它是两种系统的基本组成部分，一个刚才所说的语音识别系统，还有机器翻译系统，它要能正确输出最接近的句子。而语言模型做的最基本工作就是输入一个句子，准确地说是一个文本序列，$y^{&lt;1&gt;}$，$y^{&lt;2&gt;}$一直到$y^{&lt;T_{y}&gt;}$。对于语言模型来说，用$y$来表示这些序列比用$x$来表示要更好，然后语言模型会估计某个句子序列中各个单词出现的可能性。</p><p>那么如何建立一个语言模型呢？为了使用<strong>RNN</strong>建立出这样的模型，你首先需要一个训练集，包含一个很大的英文文本语料库（<strong>corpus</strong>）或者其它的语言，你想用于构建模型的语言的语料库。语料库是自然语言处理的一个专有名词，意思就是很长的或者说数量众多的英文句子组成的文本。</p><p><img src="https://markdown.xiaoshujiang.com/img/spinner.gif" alt="54dfcac1220e3cf567a2c656383e40ec" title="[[[1575536829342]]]"><br>假如说，你在训练集中得到这么一句话，“<strong>Cats average 15 hours of sleep a day.</strong>”(猫一天睡15小时)，你要做的第一件事就是将这个句子标记化，意思就是像之前视频中一样，建立一个字典，然后将每个单词都转换成对应的<strong>one-hot</strong>向量，也就是字典中的索引。可能还有一件事就是你要定义句子的结尾，一般的做法就是增加一个额外的标记，叫做<strong>EOS</strong>（上图编号1所示），它表示句子的结尾，这样能够帮助你搞清楚一个句子什么时候结束，我们之后会详细讨论这个。<strong>EOS</strong>标记可以被附加到训练集中每一个句子的结尾，如果你想要你的模型能够准确识别句子结尾的话。在本周的练习中我们不需要使用这个<strong>EOS</strong>标记，不过在某些应用中你可能会用到它，不过稍后就能见到它的用处。于是在本例中我们，如果你加了<strong>EOS</strong>标记，这句话就会有9个输入，有$y^{&lt;1&gt;}$，$y^{&lt;2&gt;}$一直到$y^{&lt;9&gt;}$。在标记化的过程中，你可以自行决定要不要把标点符号看成标记，在本例中，我们忽略了标点符号，所以我们只把<strong>day</strong>看成标志，不包括后面的句号，如果你想把句号或者其他符号也当作标志，那么你可以将句号也加入你的字典中。</p><p>现在还有一个问题如果你的训练集中有一些词并不在你的字典里，比如说你的字典有10,000个词，10,000个最常用的英语单词。现在这个句，“<strong>The Egyptian Mau is a bread of cat.</strong>”其中有一个词<strong>Mau</strong>，它可能并不是预先的那10,000个最常用的单词，在这种情况下，你可以把<strong>Mau</strong>替换成一个叫做<strong>UNK</strong>的代表未知词的标志，我们只针对<strong>UNK</strong>建立概率模型，而不是针对这个具体的词<strong>Mau</strong>。</p><p>完成标识化的过程后，这意味着输入的句子都映射到了各个标志上，或者说字典中的各个词上。下一步我们要构建一个<strong>RNN</strong>来构建这些序列的概率模型。在下一张幻灯片中会看到的一件事就是最后你会将$x^{&lt;t&gt;}$设为$y^{&lt;t-1&gt;}$。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/986226c39270a1e14643e8658fe6c374.png" alt="986226c39270a1e14643e8658fe6c374"><br>现在我们来建立<strong>RNN</strong>模型，我们继续使用“<strong>Cats average 15 hours of sleep a day.</strong>”这个句子来作为我们的运行样例，我将会画出一个<strong>RNN</strong>结构。在第0个时间步，你要计算激活项$a^{&lt;1&gt;}$，它是以$x^{&lt;1 &gt;}$作为输入的函数，而$x^{&lt;1&gt;}$会被设为全为0的集合，也就是0向量。在之前的$a^{&lt;0&gt;}$按照惯例也设为0向量，于是$a^{&lt;1&gt;}$要做的就是它会通过<strong>softmax</strong>进行一些预测来计算出第一个词可能会是什么，其结果就是$\hat y^{&lt;1&gt;}$（上图编号1所示），这一步其实就是通过一个<strong>softmax</strong>层来预测字典中的任意单词会是第一个词的概率，比如说第一个词是$a$的概率有多少，第一个词是<strong>Aaron</strong>的概率有多少，第一个词是<strong>cats</strong>的概率又有多少，就这样一直到<strong>Zulu</strong>是第一个词的概率是多少，还有第一个词是<strong>UNK</strong>（未知词）的概率有多少，还有第一个词是句子结尾标志的概率有多少，表示不必阅读。所以$\hat y^{&lt;1&gt;}$的输出是<strong>softmax</strong>的计算结果，它只是预测第一个词的概率，而不去管结果是什么。在我们的例子中，最终会得到单词<strong>Cats</strong>。所以<strong>softmax</strong>层输出10,000种结果，因为你的字典中有10,000个词，或者会有10,002个结果，因为你可能加上了未知词，还有句子结尾这两个额外的标志。</p><p>然后<strong>RNN</strong>进入下个时间步，在下一时间步中，仍然使用激活项$a^{&lt;1&gt;}$，在这步要做的是计算出第二个词会是什么。现在我们依然传给它正确的第一个词，我们会告诉它第一个词就是<strong>Cats</strong>，也就是$\hat y^{&lt;1&gt;}$，告诉它第一个词就是<strong>Cats</strong>，这就是为什么$y^{&lt;1&gt;} = x^{&lt;2&gt;}$（上图编号2所示）。然后在第二个时间步中，输出结果同样经过<strong>softmax</strong>层进行预测，<strong>RNN</strong>的职责就是预测这些词的概率（上图编号3所示），而不会去管结果是什么，可能是b或者<strong>arron</strong>，可能是<strong>Cats</strong>或者<strong>Zulu</strong>或者<strong>UNK</strong>（未知词）或者<strong>EOS</strong>或者其他词，它只会考虑之前得到的词。所以在这种情况下，我猜正确答案会是<strong>average</strong>，因为句子确实就是<strong>Cats average</strong>开头的。</p><p>然后再进行<strong>RNN</strong>的下个时间步，现在要计算$a^{&lt;3&gt;}$。为了预测第三个词，也就是15，我们现在给它之前两个词，告诉它<strong>Cats average</strong>是句子的前两个词，所以这是下一个输入，$x^{&lt;3&gt;} = y^{&lt;2&gt;}$，输入<strong>average</strong>以后，现在要计算出序列中下一个词是什么，或者说计算出字典中每一个词的概率（上图编号4所示），通过之前得到的<strong>Cats</strong>和<strong>average</strong>，在这种情况下，正确结果会是15，以此类推。</p><p>一直到最后，没猜错的话，你会停在第9个时间步，然后把$x^{&lt;9&gt;}$也就是$y^{&lt;8&gt;}$传给它（上图编号5所示），也就是单词<strong>day</strong>，这里是$a^{&lt;9&gt;}$，它会输出$y^{&lt;9&gt;}$，最后的得到结果会是<strong>EOS</strong>标志，在这一步中，通过前面这些得到的单词，不管它们是什么，我们希望能预测出<strong>EOS</strong>句子结尾标志的概率会很高（上图编号6所示）。</p><p>所以<strong>RNN</strong>中的每一步都会考虑前面得到的单词，比如给它前3个单词（上图编号7所示），让它给出下个词的分布，这就是<strong>RNN</strong>如何学习从左往右地每次预测一个词。</p><p>接下来为了训练这个网络，我们要定义代价函数。于是，在某个时间步$t$，如果真正的词是$y^{&lt;t&gt;}$，而神经网络的<strong>softmax</strong>层预测结果值是$y^{&lt;t&gt;}$，那么这（上图编号8所示）就是<strong>softmax</strong>损失函数，$L\left( \hat y^{&lt;t&gt;},y^{&lt;t&gt;}&gt;\right) = - \sum_{i}^{}{y_{i}^{&lt;t&gt;}\log\hat y_{i}^{&lt;t&gt;} }$。而总体损失函数（上图编号9所示）$L = \sum_{t}^{}{L^{&lt; t &gt;}\left( \hat y^{&lt;t&gt;},y^{&lt;t&gt;} \right)}$，也就是把所有单个预测的损失函数都相加起来。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/794b8461aeb485ee61c368c90738523e.png" alt="794b8461aeb485ee61c368c90738523e"><br>如果你用很大的训练集来训练这个<strong>RNN</strong>，你就可以通过开头一系列单词像是<strong>Cars average 15</strong>或者<strong>Cars average 15 hours of</strong>来预测之后单词的概率。现在有一个新句子，它是$y^{&lt;1&gt;}$，$y^{&lt;2&gt;}$，$y^{&lt;3&gt;}$，为了简单起见，它只包含3个词（如上图所示），现在要计算出整个句子中各个单词的概率，方法就是第一个<strong>softmax</strong>层会告诉你$y^{&lt;1&gt;}$的概率（上图编号1所示），这也是第一个输出，然后第二个<strong>softmax</strong>层会告诉你在考虑$y^{&lt;1&gt;}$的情况下$y^{&lt;2&gt;}$的概率（上图编号2所示），然后第三个<strong>softmax</strong>层告诉你在考虑$y^{&lt;1&gt;}$和$y^{&lt;2&gt;}$的情况下$y^{&lt;3&gt;}$的概率（上图编号3所示），把这三个概率相乘，最后得到这个含3个词的整个句子的概率。</p><p>这就是用<strong>RNN</strong>训练一个语言模型的基础结构，可能我说的这些东西听起来有些抽象，不过别担心，你可以在编程练习中亲自实现这些东西。下一节课用语言模型做的一件最有趣的事就是从模型中进行采样。</p><h3 id="1-7-对新序列采样（Sampling-novel-sequences）"><a href="#1-7-对新序列采样（Sampling-novel-sequences）" class="headerlink" title="1.7 对新序列采样（Sampling novel sequences）"></a>1.7 对新序列采样（Sampling novel sequences）</h3><p>在你训练一个序列模型之后，要想了解到这个模型学到了什么，一种非正式的方法就是进行一次新序列采样，来看看到底应该怎么做。</p><p>记住一个序列模型模拟了任意特定单词序列的概率，我们要做的就是对这些概率分布进行采样来生成一个新的单词序列。下图编号1所示的网络已经被上方所展示的结构训练训练过了，而为了进行采样（下图编号2所示的网络），你要做一些截然不同的事情。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/8b901fc8fcab9e16b1fe26b92f4ec546.png" alt="8b901fc8fcab9e16b1fe26b92f4ec546"><br>第一步要做的就是对你想要模型生成的第一个词进行采样，于是你输入$x^{&lt;1&gt;} =0$，$a^{&lt;0&gt;} =0$，现在你的第一个时间步得到的是所有可能的输出是经过<strong>softmax</strong>层后得到的概率，然后根据这个<strong>softmax</strong>的分布进行随机采样。<strong>Softmax</strong>分布给你的信息就是第一个词<strong>a</strong>的概率是多少，第一个词是<strong>aaron</strong>的概率是多少，第一个词是<strong>zulu</strong>的概率是多少，还有第一个词是<strong>UNK</strong>（未知标识）的概率是多少，这个标识可能代表句子的结尾，然后对这个向量使用例如<strong>numpy</strong>命令，<code>np.random.choice</code>（上图编号3所示），来根据向量中这些概率的分布进行采样，这样就能对第一个词进行采样了。</p><p>然后继续下一个时间步，记住第二个时间步需要$\hat y^{&lt;1&gt;}$作为输入，而现在要做的是把刚刚采样得到的$\hat y^{&lt;1&gt;}$放到$a^{&lt;2&gt;}$（上图编号4所示），作为下一个时间步的输入，所以不管你在第一个时间步得到的是什么词，都要把它传递到下一个位置作为输入，然后<strong>softmax</strong>层就会预测$\hat y^{&lt;2&gt;}$是什么。举个例子，假如说对第一个词进行抽样后，得到的是<strong>The</strong>，<strong>The</strong>作为第一个词的情况很常见，然后把<strong>The</strong>当成$x^{&lt;2&gt;}$，现在$x^{&lt;2&gt;}$就是$\hat y^{&lt;1&gt;}$，现在你要计算出在第一词是<strong>The</strong>的情况下，第二个词应该是什么（上图编号5所示），然后得到的结果就是$\hat y^{&lt;2&gt;}$，然后再次用这个采样函数来对$\hat y^{&lt;2&gt;}$进行采样。</p><p>然后再到下一个时间步，无论你得到什么样的用<strong>one-hot</strong>码表示的选择结果，都把它传递到下一个时间步，然后对第三个词进行采样。不管得到什么都把它传递下去，一直这样直到最后一个时间步。</p><p>那么你要怎样知道一个句子结束了呢？方法之一就是，如果代表句子结尾的标识在你的字典中，你可以一直进行采样直到得到<strong>EOS</strong>标识（上图编号6所示），这代表着已经抵达结尾，可以停止采样了。另一种情况是，如果你的字典中没有这个词，你可以决定从20个或100个或其他个单词进行采样，然后一直将采样进行下去直到达到所设定的时间步。不过这种过程有时候会产生一些未知标识（上图编号7所示），如果你要确保你的算法不会输出这种标识，你能做的一件事就是拒绝采样过程中产生任何未知的标识，一旦出现就继续在剩下的词中进行重采样，直到得到一个不是未知标识的词。如果你不介意有未知标识产生的话，你也可以完全不管它们。</p><p>这就是你如何从你的<strong>RNN</strong>语言模型中生成一个随机选择的句子。直到现在我们所建立的是基于词汇的<strong>RNN</strong>模型，意思就是字典中的词都是英语单词（下图编号1所示）。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1d31771da8ced333968541fbbf67e6f1.png" alt="1d31771da8ced333968541fbbf67e6f1"><br>根据你实际的应用，你还可以构建一个基于字符的<strong>RNN</strong>结构，在这种情况下，你的字典仅包含从<strong>a</strong>到<strong>z</strong>的字母，可能还会有空格符，如果你需要的话，还可以有数字0到9，如果你想区分字母大小写，你可以再加上大写的字母，你还可以实际地看一看训练集中可能会出现的字符，然后用这些字符组成你的字典（上图编号2所示）。</p><p>如果你建立一个基于字符的语言模型，比起基于词汇的语言模型，你的序列$\hat y^{&lt;1&gt;}$，$\hat y^{&lt;2&gt;}$，$\hat y^{&lt;3&gt;}$在你的训练数据中将会是单独的字符，而不是单独的词汇。所以对于前面的例子来说，那个句子（上图编号3所示），“<strong>Cats average 15 hours of sleep a day.</strong>”，在该例中<strong>C</strong>就是$\hat y^{&lt;1&gt;}$，<strong>a</strong>就是$\hat y^{&lt;2&gt;}$，<strong>t</strong>就是$\hat y^{&lt;3&gt;}$，空格符就是$\hat y^{&lt;4&gt;}$等等。</p><p>使用基于字符的语言模型有有点也有缺点，优点就是你不必担心会出现未知的标识，例如基于字符的语言模型会将<strong>Mau</strong>这样的序列也视为可能性非零的序列。而对于基于词汇的语言模型，如果<strong>Mau</strong>不在字典中，你只能把它当作未知标识<strong>UNK</strong>。不过基于字符的语言模型一个主要缺点就是你最后会得到太多太长的序列，大多数英语句子只有10到20个的单词，但却可能包含很多很多字符。所以基于字符的语言模型在捕捉句子中的依赖关系也就是句子较前部分如何影响较后部分不如基于词汇的语言模型那样可以捕捉长范围的关系，并且基于字符的语言模型训练起来计算成本比较高昂。所以我见到的自然语言处理的趋势就是，绝大多数都是使用基于词汇的语言模型，但随着计算机性能越来越高，会有更多的应用。在一些特殊情况下，会开始使用基于字符的模型。但是这确实需要更昂贵的计算力来训练，所以现在并没有得到广泛地使用，除了一些比较专门需要处理大量未知的文本或者未知词汇的应用，还有一些要面对很多专有词汇的应用。</p><p>在现有的方法下，现在你可以构建一个<strong>RNN</strong>结构，看一看英文文本的语料库，然后建立一个基于词汇的或者基于字符的语言模型，然后从训练的语言模型中进行采样。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/4c7e0878e85865c8fed558375e14b938.png" alt="4c7e0878e85865c8fed558375e14b938"><br>这里有一些样本，它们是从一个语言模型中采样得到的，准确来说是基于字符的语言模型，你可以在编程练习中自己实现这样的模型。如果模型是用新闻文章训练的，它就会生成左边这样的文本，这有点像一篇不太合乎语法的新闻文本，不过听起来，这句“<strong>Concussion epidemic</strong>”，<strong>to be examined</strong>，确实有点像新闻报道。用莎士比亚的文章训练后生成了右边这篇东西，听起来很像是莎士比亚写的东西：</p><p>“<strong>The mortal moon hath her eclipse in love.</strong></p><p><strong>And subject of this thou art another this fold.</strong></p><p><strong>When besser be my love to me see sabl’s.</strong></p><p><strong>For whose are ruse of mine eyes heaves.</strong>”</p><p>这些就是基础的<strong>RNN</strong>结构和如何去建立一个语言模型并使用它，对于训练出的语言模型进行采样。在之后的视频中，我想探讨在训练<strong>RNN</strong>时一些更加深入的挑战以及如何适应这些挑战，特别是梯度消失问题来建立更加强大的<strong>RNN</strong>模型。下节课，我们将谈到梯度消失并且会开始谈到<strong>GRU</strong>，也就是门控循环单元和<strong>LSTM</strong>长期记忆网络模型。</p><h3 id="1-8-循环神经网络的梯度消失（Vanishing-gradients-with-RNNs）"><a href="#1-8-循环神经网络的梯度消失（Vanishing-gradients-with-RNNs）" class="headerlink" title="1.8 循环神经网络的梯度消失（Vanishing gradients with RNNs）"></a>1.8 循环神经网络的梯度消失（Vanishing gradients with <strong>RNN</strong>s）</h3><p>你已经了解了<strong>RNN</strong>时如何工作的了，并且知道如何应用到具体问题上，比如命名实体识别，比如语言模型，你也看到了怎么把反向传播用于<strong>RNN</strong>。其实，基本的<strong>RNN</strong>算法还有一个很大的问题，就是梯度消失的问题。这节课我们会讨论，在下几节课我们会讨论一些方法用来解决这个问题。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/8fb1c4afe30b7a0ede26522b355068ba.png" alt="8fb1c4afe30b7a0ede26522b355068ba"><br>你已经知道了<strong>RNN</strong>的样子，现在我们举个语言模型的例子，假如看到这个句子（上图编号1所示），“<strong>The cat, which already ate ……, was full.</strong>”，前后应该保持一致，因为<strong>cat</strong>是单数，所以应该用<strong>was</strong>。“<strong>The cats, which ate ……, were full.</strong>”（上图编号2所示），<strong>cats</strong>是复数，所以用<strong>were</strong>。这个例子中的句子有长期的依赖，最前面的单词对句子后面的单词有影响。但是我们目前见到的基本的<strong>RNN</strong>模型（上图编号3所示的网络模型），不擅长捕获这种长期依赖效应，解释一下为什么。</p><p>你应该还记得之前讨论的训练很深的网络，我们讨论了梯度消失的问题。比如说一个很深很深的网络（上图编号4所示），100层，甚至更深，对这个网络从左到右做前向传播然后再反向传播。我们知道如果这是个很深的神经网络，从输出$\hat y$得到的梯度很难传播回去，很难影响靠前层的权重，很难影响前面层（编号5所示的层）的计算。</p><p>对于有同样问题的<strong>RNN</strong>，首先从左到右前向传播，然后反向传播。但是反向传播会很困难，因为同样的梯度消失的问题，后面层的输出误差（上图编号6所示）很难影响前面层（上图编号7所示的层）的计算。这就意味着，实际上很难让一个神经网络能够意识到它要记住看到的是单数名词还是复数名词，然后在序列后面生成依赖单复数形式的<strong>was</strong>或者<strong>were</strong>。而且在英语里面，这中间的内容（上图编号8所示）可以任意长，对吧？所以你需要长时间记住单词是单数还是复数，这样后面的句子才能用到这些信息。也正是这个原因，所以基本的<strong>RNN</strong>模型会有很多局部影响，意味着这个输出$\hat y^{&lt;3&gt;}$（上图编号9所示）主要受$\hat y^{&lt;3&gt;}$附近的值（上图编号10所示）的影响，上图编号11所示的一个数值主要与附近的输入（上图编号12所示）有关，上图编号6所示的输出，基本上很难受到序列靠前的输入（上图编号10所示）的影响，这是因为不管输出是什么，不管是对的，还是错的，这个区域都很难反向传播到序列的前面部分，也因此网络很难调整序列前面的计算。这是基本的<strong>RNN</strong>算法的一个缺点，我们会在下几节视频里处理这个问题。如果不管的话，<strong>RNN</strong>会不擅长处理长期依赖的问题。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/ac5d647140997ba713c376fb097ea0e2.png" alt="ac5d647140997ba713c376fb097ea0e2"><br>尽管我们一直在讨论梯度消失问题，但是，你应该记得我们在讲很深的神经网络时，我们也提到了梯度爆炸，我们在反向传播的时候，随着层数的增多，梯度不仅可能指数型的下降，也可能指数型的上升。事实上梯度消失在训练<strong>RNN</strong>时是首要的问题，尽管梯度爆炸也是会出现，但是梯度爆炸很明显，因为指数级大的梯度会让你的参数变得极其大，以至于你的网络参数崩溃。所以梯度爆炸很容易发现，因为参数会大到崩溃，你会看到很多<strong>NaN</strong>，或者不是数字的情况，这意味着你的网络计算出现了数值溢出。如果你发现了梯度爆炸的问题，一个解决方法就是用梯度修剪。梯度修剪的意思就是观察你的梯度向量，如果它大于某个阈值，缩放梯度向量，保证它不会太大，这就是通过一些最大值来修剪的方法。所以如果你遇到了梯度爆炸，如果导数值很大，或者出现了<strong>NaN</strong>，就用梯度修剪，这是相对比较鲁棒的，这是梯度爆炸的解决方法。然而梯度消失更难解决，这也是我们下几节视频的主题。</p><p>总结一下，在前面的课程，我们了解了训练很深的神经网络时，随着层数的增加，导数有可能指数型的下降或者指数型的增加，我们可能会遇到梯度消失或者梯度爆炸的问题。加入一个<strong>RNN</strong>处理1,000个时间序列的数据集或者10,000个时间序列的数据集，这就是一个1,000层或者10,000层的神经网络，这样的网络就会遇到上述类型的问题。梯度爆炸基本上用梯度修剪就可以应对，但梯度消失比较棘手。我们下节会介绍<strong>GRU</strong>，门控循环单元网络，这个网络可以有效地解决梯度消失的问题，并且能够使你的神经网络捕获更长的长期依赖，我们去下个视频一探究竟吧。</p><h3 id="1-9-GRU单元（Gated-Recurrent-Unit（GRU））"><a href="#1-9-GRU单元（Gated-Recurrent-Unit（GRU））" class="headerlink" title="1.9 GRU单元（Gated Recurrent Unit（GRU））"></a>1.9 <strong>GRU</strong>单元（Gated Recurrent Unit（<strong>GRU</strong>））</h3><p>你已经了解了基础的<strong>RNN</strong>模型的运行机制，在本节视频中你将会学习门控循环单元，它改变了<strong>RNN</strong>的隐藏层，使其可以更好地捕捉深层连接，并改善了梯度消失问题，让我们看一看。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/db720dae1606767241df59d8fd6079ee.png" alt="db720dae1606767241df59d8fd6079ee"><br>你已经见过了这个公式，$a^{&lt; t &gt;} = g(W_{a}\left\lbrack a^{&lt; t - 1 &gt;},x^{&lt; t &gt;}\right\rbrack +b_{a})$，在<strong>RNN</strong>的时间$t$处，计算激活值。我把这个画个图，把<strong>RNN</strong>的单元画个图，画一个方框，输入$a^{&lt;t-1&gt;}$（上图编号1所示），即上一个时间步的激活值，再输入$x^{&lt;t&gt;}$（上图编号2所示），再把这两个并起来，然后乘上权重项，在这个线性计算之后（上图编号3所示），如果$g$是一个<strong>tanh</strong>激活函数，再经过<strong>tanh</strong>计算之后，它会计算出激活值$a^{&lt;t&gt;}$。然后激活值$a^{&lt;t&gt;}$将会传<strong>softmax</strong>单元（上图编号4所示），或者其他用于产生输出$y^{&lt;t&gt;}$的东西。就这张图而言，这就是<strong>RNN</strong>隐藏层的单元的可视化呈现。我向展示这张图，因为我们将使用相似的图来讲解门控循环单元。<br><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1521560729.png" alt="1521560729"></p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/13cee4a2d0ad3897924c22d729c5b02d.png" alt="13cee4a2d0ad3897924c22d729c5b02d"><br>许多<strong>GRU</strong>的想法都来分别自于<strong>Yu Young Chang, Kagawa，Gaza Hera, Chang Hung Chu</strong>和<br><strong>Jose Banjo</strong>的两篇论文。我再引用上个视频中你已经见过的这个句子，“<strong>The cat, which already ate……, was full.</strong>”，你需要记得猫是单数的，为了确保你已经理解了为什么这里是<strong>was</strong>而不是<strong>were</strong>，“<strong>The cat was full.</strong>”或者是“<strong>The cats were full</strong>”。当我们从左到右读这个句子，<strong>GRU</strong>单元将会有个新的变量称为$c$，代表细胞（<strong>cell</strong>），即记忆细胞（下图编号1所示）。记忆细胞的作用是提供了记忆的能力，比如说一只猫是单数还是复数，所以当它看到之后的句子的时候，它仍能够判断句子的主语是单数还是复数。于是在时间$t$处，有记忆细胞$c^{&lt;t&gt;}$，然后我们看的是，<strong>GRU</strong>实际上输出了激活值$a^{&lt;t&gt;}$，$c^{&lt;t&gt;} = a^{&lt;t&gt;}$（下图编号2所示）。于是我们想要使用不同的符号$c$和$a$来表示记忆细胞的值和输出的激活值，即使它们是一样的。我现在使用这个标记是因为当我们等会说到<strong>LSTMs</strong>的时候，这两个会是不同的值，但是现在对于<strong>GRU</strong>，$c^{&lt;t&gt;}$的值等于$a^{&lt;t&gt;}$的激活值。</p><p>所以这些等式表示了<strong>GRU</strong>单元的计算，在每个时间步，我们将用一个候选值重写记忆细胞，即${\tilde{c} }^{&lt;t&gt;}$的值，所以它就是个候选值，替代了$c^{&lt;t&gt;}$的值。然后我们用<strong>tanh</strong>激活函数来计算，${\tilde{c} }^{&lt;t&gt;} =tanh(W_{c}\left\lbrack c^{&lt;t-1&gt;},x^{&lt;t&gt;} \right\rbrack +b_{c})$，所以${\tilde{c} }^{&lt;t&gt;}$的值就是个替代值，代替表示$c^{&lt;t&gt;}$的值（下图编号3所示）。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/5e55fb9f6de649031e7f9a4b249f4fea.png" alt="5e55fb9f6de649031e7f9a4b249f4fea"><br>重点来了，在<strong>GRU</strong>中真正重要的思想是我们有一个门，我先把这个门叫做$\Gamma_{u}$（上图编号4所示），这是个下标为$u$的大写希腊字母$\Gamma$，$u$代表更新门，这是一个0到1之间的值。为了让你直观思考<strong>GRU</strong>的工作机制，先思考$\Gamma_{u}$，这个一直在0到1之间的门值，实际上这个值是把这个式子带入<strong>sigmoid</strong>函数得到的，$\Gamma_{u}= \sigma(W_{u}\left\lbrack c^{&lt;t-1&gt;},x^{&lt;t&gt;} \right\rbrack +b_{u})$。我们还记得<strong>sigmoid</strong>函数是上图编号5所示这样的，它的输出值总是在0到1之间，对于大多数可能的输入，<strong>sigmoid</strong>函数的输出总是非常接近0或者非常接近1。在这样的直觉下，可以想到$\Gamma_{u}$在大多数的情况下非常接近0或1。然后这个字母<strong>u</strong>表示“<strong>update</strong>”，我选了字母$\Gamma$是因为它看起来像门。还有希腊字母<strong>G</strong>，<strong>G</strong>是门的首字母，所以<strong>G</strong>表示门。</p><p>然后<strong>GRU</strong>的关键部分就是上图编号3所示的等式，我们刚才写出来的用$\tilde{c}$更新$c$的等式。然后门决定是否要真的更新它。于是我们这么看待它，记忆细胞$c^{&lt;t&gt;}$将被设定为0或者1，这取决于你考虑的单词在句子中是单数还是复数，因为这里是单数情况，所以我们先假定它被设为了1，或者如果是复数的情况我们就把它设为0。然后<strong>GRU</strong>单元将会一直记住$c^{&lt;t&gt;}$的值，直到上图编号7所示的位置，$c^{&lt;t&gt;}$的值还是1，这就告诉它，噢，这是单数，所以我们用<strong>was</strong>。于是门，即$\Gamma_{u}$的作用就是决定什么时候你会更新这个值，特别是当你看到词组<strong>the cat</strong>，即句子的主语猫，这就是一个好时机去更新这个值。然后当你使用完它的时候，“<strong>The cat, which already ate……, was full.</strong>”，然后你就知道，我不需要记住它了，我可以忘记它了。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/32f3b94e4e4523bc413d9687ec29e801.png" alt="32f3b94e4e4523bc413d9687ec29e801"><br>所以我们接下来要给<strong>GRU</strong>用的式子就是$c^{&lt;t&gt;} = \Gamma_{u}<em>{\tilde{c} }^{&lt;t&gt;} +\left( 1- \Gamma_{u} \right)</em>c^{&lt;t-1&gt;}$（上图编号1所示）。你应该注意到了，如果这个更新值$\Gamma_{u} =1$，也就是说把这个新值，即$c^{&lt;t&gt;}$设为候选值（$\Gamma_{u} =1$时简化上式，$c^{&lt;t&gt;} = {\tilde{c} }^{&lt;t&gt;}$）。将门值设为1（上图编号2所示），然后往前再更新这个值。对于所有在这中间的值，你应该把门的值设为0，即$\Gamma_{u}= 0$，意思就是说不更新它，就用旧的值。因为如果$\Gamma_{u} = 0$，则$c^{&lt;t&gt;} =c^{&lt;t-1&gt;}$，$c^{&lt;t&gt;}$等于旧的值。甚至你从左到右扫描这个句子，当门值为0的时候（上图编号3所示，中间$\Gamma_{u}=0$一直为0，表示一直不更新），就是说不更新它的时候，不要更新它，就用旧的值，也不要忘记这个值是什么，这样即使你一直处理句子到上图编号4所示，$c^{&lt;t&gt;}$应该会一直等$c^{&lt;t-1&gt;}$，于是它仍然记得猫是单数的。</p><p>让我再画个图来（下图所示）解释一下<strong>GRU</strong>单元，顺便说一下，当你在看网络上的博客或者教科书或者教程之类的，这些图对于解释<strong>GRU</strong>和我们稍后会讲的<strong>LSTM</strong>是相当流行的，我个人感觉式子在图片中比较容易理解，那么即使看不懂图片也没关系，我就画画，万一能帮得上忙就最好了。</p><p><strong>GRU</strong>单元输入$c^{&lt;t-1&gt;}$（下图编号1所示），对于上一个时间步，先假设它正好等于$a^{&lt;t-1&gt;}$，所以把这个作为输入。然后$x^{&lt;t&gt;}$也作为输入（下图编号2所示），然后把这两个用合适权重结合在一起，再用<strong>tanh</strong>计算，算出${\tilde{c} }^{&lt;t&gt;}$，${\tilde{c} }^{&lt;t&gt;} =tanh(W_{c}\left\lbrack c^{&lt;t-1&gt;},x^{&lt;t&gt;} \right\rbrack +b_{c})$，即$c^{&lt;t&gt;}$的替代值。</p><p>再用一个不同的参数集，通过<strong>sigmoid</strong>激活函数算出$\Gamma_{u}$，$\Gamma_{u}= \sigma(W_{u}\left\lbrack c^{&lt;t-1&gt;},x^{&lt;t&gt;} \right\rbrack +b_{u})$，即更新门。最后所有的值通过另一个运算符结合，我并不会写出公式，但是我用紫色阴影标注的这个方框（下图编号5所示，其所代表的运算过程即下图编号13所示的等式），代表了这个式子。所以这就是紫色运算符所表示的是，它输入一个门值（下图编号6所示），新的候选值（下图编号7所示），这再有一个门值（下图编号8所示）和$c^{&lt;t&gt;}$的旧值（下图编号9所示），所以它把这个（下图编号1所示）、这个（下图编号3所示）和这个（下图编号4所示）作为输入一起产生记忆细胞的新值$c^{&lt;t&gt;}$，所以$c^{&lt;t&gt;}$等于$a^{&lt;t&gt;}$。如果你想，你也可以也把这个代入<strong>softmax</strong>或者其他预测$y^{&lt;t&gt;}$的东西。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/cfa628f62f1c57ee6213793a438957a3.png" alt="cfa628f62f1c57ee6213793a438957a3"><br>这就是<strong>GRU</strong>单元或者说是一个简化过的<strong>GRU</strong>单元，它的优点就是通过门决定，当你从左（上图编号10所示）到右扫描一个句子的时候，这个时机是要更新某个记忆细胞，还是不更新，不更新（上图编号11所示，中间$\Gamma_{u}=0$一直为0，表示一直不更新）直到你到你真的需要使用记忆细胞的时候（上图编号12所示），这可能在句子之前就决定了。因为sigmoid的值，现在因为门很容易取到0值，只要这个值是一个很大的负数，再由于数值上的四舍五入，上面这些门大体上就是0，或者说非常非常非常接近0。所以在这样的情况下，这个更新式子（上图编号13所示的等式）就会变成$c^{&lt;t&gt;} = c^{&lt;t-1&gt;}$，这非常有利于维持细胞的值。因为$\Gamma_{u}$很接近0，可能是0.000001或者更小，这就不会有梯度消失的问题了。因为$\Gamma_{u}$很接近0，这就是说$c^{&lt;t&gt;}$几乎就等于$c^{&lt;t-1&gt;}$，而且$c^{&lt;t&gt;}$的值也很好地被维持了，即使经过很多很多的时间步（上图编号14所示）。这就是缓解梯度消失问题的关键，因此允许神经网络运行在非常庞大的依赖词上，比如说<strong>cat</strong>和<strong>was</strong>单词即使被中间的很多单词分割开。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/c1df3f793dcb1ec681db6757b4974cee.png" alt="c1df3f793dcb1ec681db6757b4974cee"><br>现在我想说下一些实现的细节，在这个我写下的式子中$c^{&lt;t&gt;}$可以是一个向量（上图编号1所示），如果你有100维的隐藏的激活值，那么$c^{&lt;t&gt;}$也是100维的，${\tilde{c} }^{&lt;t&gt;}$也是相同的维度（${\tilde{c} }^{&lt;t&gt;} =tanh(W_{c}\left\lbrack c^{&lt;t-1&gt;},x^{&lt;t&gt;} \right\rbrack +b_{c})$），$\Gamma_{u}$也是相同的维度（$\Gamma_{u}= \sigma(W_{u}\left\lbrack c^{&lt;t-1&gt;},x^{&lt;t&gt;} \right\rbrack +b_{u})$），还有画在框中的其他值。这样的话“*”实际上就是元素对应的乘积（$c^{&lt;t&gt;} = \Gamma_{u}<em>{\tilde{c} }^{&lt;t&gt;} +\left( 1- \Gamma_{u} \right)</em>c^{&lt;t-1&gt;}$），所以这里的$\Gamma_{u}$：（$\Gamma_{u}= \sigma(W_{u}\left\lbrack c^{&lt;t-1&gt;},x^{&lt;t&gt;} \right\rbrack +b_{u})$），即如果门是一个100维的向量，$\Gamma_{u}$也就100维的向量，里面的值几乎都是0或者1，就是说这100维的记忆细胞$c^{&lt;t&gt;}$（$c^{&lt;t&gt;}=a^{&lt;t&gt;}$上图编号1所示）就是你要更新的比特。</p><p>当然在实际应用中$\Gamma_{u}$不会真的等于0或者1，有时候它是0到1的一个中间值（上图编号5所示），但是这对于直观思考是很方便的，就把它当成确切的0，完全确切的0或者就是确切的1。元素对应的乘积做的就是告诉<strong>GRU</strong>单元哪个记忆细胞的向量维度在每个时间步要做更新，所以你可以选择保存一些比特不变，而去更新其他的比特。比如说你可能需要一个比特来记忆猫是单数还是复数，其他比特来理解你正在谈论食物，因为你在谈论吃饭或者食物，然后你稍后可能就会谈论“<strong>The cat was full.</strong>”，你可以每个时间点只改变一些比特。</p><p>你现在已经理解<strong>GRU</strong>最重要的思想了，幻灯片中展示的实际上只是简化过的<strong>GRU</strong>单元，现在来描述一下完整的<strong>GRU</strong>单元。</p><p>对于完整的<strong>GRU</strong>单元我要做的一个改变就是在我们计算的第一个式子中给记忆细胞的新候选值加上一个新的项，我要添加一个门$\Gamma_{r}$（下图编号1所示），你可以认为$r$代表相关性（<strong>relevance</strong>）。这个$\Gamma_{r}$门告诉你计算出的下一个$c^{&lt;t&gt;}$的候选值${\tilde{c} }^{&lt;t&gt;}$跟$c^{&lt;t-1&gt;}$有多大的相关性。计算这个门$\Gamma_{r}$需要参数，正如你看到的这个，一个新的参数矩阵$W_{r}$，$\Gamma_{r}= \sigma(W_{r}\left\lbrack c^{&lt;t-1&gt;},x^{&lt;t&gt;} \right\rbrack + b_{r})$。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/daf9e8fa30888b6a1b407ca6ea303984.png" alt="daf9e8fa30888b6a1b407ca6ea303984"><br>正如你所见，有很多方法可以来设计这些类型的神经网络，然后我们为什么有$\Gamma_{r}$？为什么不用上一张幻灯片里的简单的版本？这是因为多年来研究者们试验过很多很多不同可能的方法来设计这些单元，去尝试让神经网络有更深层的连接，去尝试产生更大范围的影响，还有解决梯度消失的问题，<strong>GRU</strong>就是其中一个研究者们最常使用的版本，也被发现在很多不同的问题上也是非常健壮和实用的。你可以尝试发明新版本的单元，只要你愿意。但是<strong>GRU</strong>是一个标准版本，也就是最常使用的。你可以想象到研究者们也尝试了很多其他版本，类似这样的但不完全是，比如我这里写的这个。然后另一个常用的版本被称为<strong>LSTM</strong>，表示长短时记忆网络，这个我们会在下节视频中讲到，但是<strong>GRU</strong>和<strong>LSTM</strong>是在神经网络结构中最常用的两个具体实例。</p><p>还有在符号上的一点，我尝试去定义固定的符号让这些概念容易理解，如果你看学术文章的话，你有的时候会看到有些人使用另一种符号$\tilde{x}$，$u$，$r$和$h$表示这些量。但我试着在<strong>GRU</strong>和<strong>LSTM</strong>之间用一种更固定的符号，比如使用更固定的符号$\Gamma$来表示门，所以希望这能让这些概念更好理解。</p><p>所以这就是<strong>GRU</strong>，即门控循环单元，这是<strong>RNN</strong>的其中之一。这个结构可以更好捕捉非常长范围的依赖，让<strong>RNN</strong>更加有效。然后我简单提一下其他常用的神经网络，比较经典的是这个叫做<strong>LSTM</strong>，即长短时记忆网络，我们在下节视频中讲解。</p><p>（<strong>Chung J, Gulcehre C, Cho K H, et al. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling[J]. Eprint Arxiv, 2014.</strong></p><p><strong>Cho K, Merrienboer B V, Bahdanau D, et al. On the Properties of Neural Machine Translation: Encoder-Decoder Approaches[J]. Computer Science, 2014.</strong>）</p><h3 id="1-10-长短期记忆（LSTM（long-short-term-memory）unit）"><a href="#1-10-长短期记忆（LSTM（long-short-term-memory）unit）" class="headerlink" title="1.10 长短期记忆（LSTM（long short term memory）unit）"></a>1.10 长短期记忆（<strong>LSTM</strong>（long short term memory）unit）</h3><p>在上一个视频中你已经学了<strong>GRU</strong>（门控循环单元）。它能够让你可以在序列中学习非常深的连接。其他类型的单元也可以让你做到这个，比如<strong>LSTM</strong>即长短时记忆网络，甚至比<strong>GRU</strong>更加有效，让我们看看。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/523650730db3f0d5c05a7192da02f878.png" alt="523650730db3f0d5c05a7192da02f878"><br>这里是上个视频中的式子，对于<strong>GRU</strong>我们有$a^{&lt; t &gt;} = c^{&lt;t&gt;}$。</p><p>还有两个门:</p><p>更新门$\Gamma_{u}$（<strong>the update gate</strong>）</p><p>相关门$\Gamma_{r}$（<strong>the relevance gate</strong>）</p> ${\tilde{c} }^{&lt;t&gt;}$，这是代替记忆细胞的候选值，然后我们使用更新门$\Gamma_{u}$来决定是否要用${\tilde{c} }^{&lt;t&gt;}$ 更新$c^{&lt;t&gt;}$。<p><strong>LSTM</strong>是一个比<strong>GRU</strong>更加强大和通用的版本，这多亏了 <strong>Sepp Hochreiter</strong>和 <strong>Jurgen Schmidhuber</strong>，感谢那篇开创性的论文，它在序列模型上有着巨大影响。我感觉这篇论文是挺难读懂的，虽然我认为这篇论文在深度学习社群有着重大的影响，它深入讨论了梯度消失的理论，我感觉大部分的人学到<strong>LSTM</strong>的细节是在其他的地方，而不是这篇论文。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/9456d50c55cf0408a3fb2b6e903d85d6.png" alt="9456d50c55cf0408a3fb2b6e903d85d6"><br>这就是<strong>LSTM</strong>主要的式子（上图编号2所示），我们继续回到记忆细胞<strong>c</strong>上面来，使用${\tilde{c} }^{&lt;t&gt;} = tanh(W_{c}\left\lbrack a^{&lt;t-1&gt;},x^{&lt;t&gt;} \right\rbrack +b_{c}$来更新它的候选值${\tilde{c} }^{&lt;t&gt;}$（上图编号3所示）。注意了，在<strong>LSTM</strong>中我们不再有$a^{&lt;t&gt;} = c^{&lt;t&gt;}$的情况，这是现在我们用的是类似于左边这个式子（上图编号4所示），但是有一些改变，现在我们专门使用$a^{&lt;t&gt;}$或者$a^{&lt;t-1&gt;}$，而不是用$c^{&lt;t-1&gt;}$，我们也不用$\Gamma_{r}$，即相关门。虽然你可以使用<strong>LSTM</strong>的变体，然后把这些东西（左边所示的<strong>GRU</strong>公式）都放回来，但是在更加典型的<strong>LSTM</strong>里面，我们先不那样做。</p><p>我们像以前那样有一个更新门$\Gamma_{u}$和表示更新的参数$W_{u}$，$\Gamma_{u}= \sigma(W_{u}\left\lbrack a^{&lt;t-1&gt;},x^{&lt;t&gt;} \right\rbrack +b_{u})$（上图编号5所示）。一个<strong>LSTM</strong>的新特性是不只有一个更新门控制，这里的这两项（上图编号6，7所示），我们将用不同的项来代替它们，要用别的项来取代$\Gamma_{u}$和$1-\Gamma_{u}$，这里（上图编号6所示）我们用$\Gamma_{u}$。</p><p>然后这里（上图编号7所示）用遗忘门（<strong>the forget gate</strong>），我们叫它$\Gamma_{f}$，所以这个$\Gamma_{f} =\sigma(W_{f}\left\lbrack a^{&lt;t-1&gt;},x^{&lt;t&gt;} \right\rbrack +b_{f})$（上图编号8所示）；</p><p>然后我们有一个新的输出门，$\Gamma_{o} =\sigma(W_{o}\left\lbrack a^{&lt;t-1&gt;},x^{&lt;t&gt;} \right\rbrack +&gt;b_{o})$（上图编号9所示）；</p><p>于是记忆细胞的更新值$c^{&lt;t&gt;} =\Gamma_{u}<em>{\tilde{c} }^{&lt;t&gt;} + \Gamma_{f}</em>c^{&lt;t-1&gt;}$（上图编号10所示）；</p><p>所以这给了记忆细胞选择权去维持旧的值$c^{&lt;t-1&gt;}$或者就加上新的值${\tilde{c} }^{&lt;t&gt;}$，所以这里用了单独的更新门$\Gamma_{u}$和遗忘门$\Gamma_{f}$，</p><p>然后这个表示更新门（$\Gamma_{u}= \sigma(W_{u}\left\lbrack a^{&lt;t-1&gt;},x^{&lt;t&gt;} \right\rbrack +b_{u})$上图编号5所示）；</p><p>遗忘门（$\Gamma_{f} =\sigma(W_{f}\left\lbrack a^{&lt;t-1&gt;},x^{&lt;t&gt;} \right\rbrack +b_{f})$上图编号8所示）和输出门（上图编号9所示）。</p><p>最后$a^{&lt;t&gt;} = c^{&lt;t&gt;}$的式子会变成$a^{&lt;t&gt;} = \Gamma_{o}<em>c^{&lt;t&gt;}$。这就是*</em>LSTM**主要的式子了，然后这里（上图编号11所示）有三个门而不是两个，这有点复杂，它把门放到了和之前有点不同的地方。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/94e871edbd87337937ce374e71d56e42.png" alt="94e871edbd87337937ce374e71d56e42"><br>再提一下，这些式子就是控制<strong>LSTM</strong>行为的主要的式子了（上图编号1所示）。像之前一样用图片稍微解释一下，先让我把图画在这里（上图编号2所示）。如果图片过于复杂，别担心，我个人感觉式子比图片好理解，但是我画图只是因为它比较直观。这个右上角的图的灵感来自于<strong>Chris Ola</strong>的一篇博客，标题是《理解<strong>LSTM</strong>网络》（<strong>Understanding LSTM Network</strong>），这里的这张图跟他博客上的图是很相似的，但关键的不同可能是这里的这张图用了$a^{&lt;t-1&gt;}$和$x^{&lt;t&gt;}$来计算所有门值（上图编号3，4所示），在这张图里是用$a^{&lt;t-1&gt;}$， $x^{&lt;t&gt;}$一起来计算遗忘门$\Gamma_{f}$的值，还有更新门$\Gamma_{u}$以及输出门$\Gamma_{o}$（上图编号4所示）。然后它们也经过<strong>tanh</strong>函数来计算${\tilde{c} }^{&lt;t&gt;}$（上图编号5所示），这些值被用复杂的方式组合在一起，比如说元素对应的乘积或者其他的方式来从之前的$c^{&lt;t-1&gt;}$（上图编号6所示）中获得$c^{&lt;t&gt;}$（上图编号7所示）。</p><p>这里其中一个元素很有意思，如你在这一堆图（上图编号8所示的一系列图片）中看到的，这是其中一个，再把他们连起来，就是把它们按时间次序连起来，这里（上图编号9所示）输入$x^{&lt;1&gt;}$，然后$x^{&lt;2&gt;}$，$x^{&lt;3&gt;}$，然后你可以把这些单元依次连起来，这里输出了上一个时间的$a$，$a$会作为下一个时间步的输入，$c$同理。在下面这一块，我把图简化了一下（相对上图编号2所示的图有所简化）。然后这有个有意思的事情，你会注意到上面这里有条线（上图编号10所示的线），这条线显示了只要你正确地设置了遗忘门和更新门，<strong>LSTM</strong>是相当容易把$c^{&lt;0&gt;}$的值（上图编号11所示）一直往下传递到右边，比如$c^{&lt;3&gt;} = c^{&lt;0&gt;}$（上图编号12所示）。这就是为什么<strong>LSTM</strong>和<strong>GRU</strong>非常擅长于长时间记忆某个值，对于存在记忆细胞中的某个值，即使经过很长很长的时间步。</p><p>这就是<strong>LSTM</strong>，你可能会想到这里和一般使用的版本会有些不同，最常用的版本可能是门值不仅取决于$a^{&lt;t-1&gt;}$和$x^{&lt;t&gt;}$，有时候也可以偷窥一下$c^{&lt;t-1&gt;}$的值（上图编号13所示），这叫做“窥视孔连接”（<strong>peephole connection</strong>）。虽然不是个好听的名字，但是你想，“<strong>偷窥孔连接</strong>”其实意思就是门值不仅取决于$a^{&lt;t-1&gt;}$和$x^{&lt;t&gt;}$，也取决于上一个记忆细胞的值（$c^{&lt;t-1&gt;}$），然后“偷窥孔连接”就可以结合这三个门（$\Gamma_{u}$、$\Gamma_{f}$、$\Gamma_{o}$）来计算了。</p><p>如你所见<strong>LSTM</strong>主要的区别在于一个技术上的细节，比如这（上图编号13所示）有一个100维的向量，你有一个100维的隐藏的记忆细胞单元，然后比如第50个$c^{&lt;t-1&gt;}$的元素只会影响第50个元素对应的那个门，所以关系是一对一的，于是并不是任意这100维的$c^{&lt;t-1&gt;}$可以影响所有的门元素。相反的，第一个$c^{&lt;t-1&gt;}$的元素只能影响门的第一个元素，第二个元素影响对应的第二个元素，如此类推。但如果你读过论文，见人讨论“<strong>偷窥孔连接</strong>”，那就是在说$c^{&lt;t-1&gt;}$也能影响门值。</p><p><strong>LSTM</strong>前向传播图：</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/LSTM.png" alt="LSTM"><br><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/LSTM_rnn.png" alt="LSTM_rnn"></p><p><strong>LSTM</strong>反向传播计算：</p><p><strong>门求偏导：</strong></p><p>$d \Gamma_o^{\langle t \rangle} = da_{next}<em>\tanh(c_{next}) * \Gamma_o^{\langle t \rangle}</em>(1-\Gamma_o^{\langle t \rangle})\tag{1}$</p><p>$d\tilde c^{\langle t \rangle} = dc_{next}*\Gamma_i^{\langle t \rangle}+ \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) * i_t * da_{next} * \tilde c^{\langle t \rangle} * (1-\tanh(\tilde c)^2) \tag{2}$</p><p>$d\Gamma_u^{\langle t \rangle} = dc_{next}<em>\tilde c^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) * \tilde c^{\langle t \rangle} * da_{next}*\Gamma_u^{\langle t \rangle}</em>(1-\Gamma_u^{\langle t \rangle})\tag{3}$</p><p>$d\Gamma_f^{\langle t \rangle} = dc_{next}<em>\tilde c_{prev} + \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) * c_{prev} * da_{next}*\Gamma_f^{\langle t \rangle}</em>(1-\Gamma_f^{\langle t \rangle})\tag{4}$</p><p><strong>参数求偏导 ：</strong></p><p>$ dW_f = d\Gamma_f^{\langle t \rangle} * \begin{pmatrix} a_{prev} \ x_t\end{pmatrix}^T \tag{5} $<br>$ dW_u = d\Gamma_u^{\langle t \rangle} * \begin{pmatrix} a_{prev} \ x_t\end{pmatrix}^T \tag{6} $<br>$ dW_c = d\tilde c^{\langle t \rangle} * \begin{pmatrix} a_{prev} \ x_t\end{pmatrix}^T \tag{7} $<br>$ dW_o = d\Gamma_o^{\langle t \rangle} * \begin{pmatrix} a_{prev} \ x_t\end{pmatrix}^T \tag{8}$</p><p>为了计算$db_f, db_u, db_c, db_o$ 需要各自对$d\Gamma_f^{\langle t \rangle}, d\Gamma_u^{\langle t \rangle}, d\tilde c^{\langle t \rangle}, d\Gamma_o^{\langle t \rangle}$ 求和。</p><p>最后，计算隐藏状态、记忆状态和输入的偏导数：</p><p>$ da_{prev} = W_f^T*d\Gamma_f^{\langle t \rangle} + W_u^T * d\Gamma_u^{\langle t \rangle}+ W_c^T * d\tilde c^{\langle t \rangle} + W_o^T * d\Gamma_o^{\langle t \rangle} \tag{9}$</p><p>$ dc_{prev} = dc_{next}\Gamma_f^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} * (1- \tanh(c_{next})^2)<em>\Gamma_f^{\langle t \rangle}</em>da_{next} \tag{10}$<br>$ dx^{\langle t \rangle} = W_f^T*d\Gamma_f^{\langle t \rangle} + W_u^T * d\Gamma_u^{\langle t \rangle}+ W_c^T * d\tilde c_t + W_o^T * d\Gamma_o^{\langle t \rangle}\tag{11} $</p><p>这就是<strong>LSTM</strong>，我们什么时候应该用<strong>GRU</strong>？什么时候用<strong>LSTM</strong>？这里没有统一的准则。而且即使我先讲解了<strong>GRU</strong>，在深度学习的历史上，<strong>LSTM</strong>也是更早出现的，而<strong>GRU</strong>是最近才发明出来的，它可能源于<strong>Pavia</strong>在更加复杂的<strong>LSTM</strong>模型中做出的简化。研究者们在很多不同问题上尝试了这两种模型，看看在不同的问题不同的算法中哪个模型更好，所以这不是个学术和高深的算法，我才想要把这两个模型展示给你。</p><p><strong>GRU</strong>的优点是这是个更加简单的模型，所以更容易创建一个更大的网络，而且它只有两个门，在计算性上也运行得更快，然后它可以扩大模型的规模。</p><p>但是<strong>LSTM</strong>更加强大和灵活，因为它有三个门而不是两个。如果你想选一个使用，我认为<strong>LSTM</strong>在历史进程上是个更优先的选择，所以如果你必须选一个，我感觉今天大部分的人还是会把<strong>LSTM</strong>作为默认的选择来尝试。虽然我认为最近几年<strong>GRU</strong>获得了很多支持，而且我感觉越来越多的团队也正在使用<strong>GRU</strong>，因为它更加简单，而且还效果还不错，它更容易适应规模更加大的问题。</p><p>所以这就是<strong>LSTM</strong>，无论是<strong>GRU</strong>还是<strong>LSTM</strong>，你都可以用它们来构建捕获更加深层连接的神经网络。</p><p>（<strong>Hochreiter S, Schmidhuber J. Long Short-Term Memory[J]. Neural Computation, 1997, 9(8):1735-1780.</strong>）</p><h3 id="1-11-双向循环神经网络（Bidirectional-RNN）"><a href="#1-11-双向循环神经网络（Bidirectional-RNN）" class="headerlink" title="1.11 双向循环神经网络（Bidirectional RNN）"></a>1.11 双向循环神经网络（Bidirectional <strong>RNN</strong>）</h3><p>现在，你已经了解了大部分<strong>RNN</strong>模型的关键的构件，还有两个方法可以让你构建更好的模型，其中之一就是双向<strong>RNN</strong>模型，这个模型可以让你在序列的某点处不仅可以获取之前的信息，还可以获取未来的信息，我们会在这个视频里讲解。第二个就是深层的<strong>RNN</strong>，我们会在下个视频里见到，现在先从双向<strong>RNN</strong>开始吧。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/63be3fd3701604f94b45a08f1d8b460d.png" alt="63be3fd3701604f94b45a08f1d8b460d"><br>为了了解双向<strong>RNN</strong>的动机，我们先看一下之前在命名实体识别中已经见过多次的神经网络。这个网络有一个问题，在判断第三个词<strong>Teddy</strong>（上图编号1所示）是不是人名的一部分时，光看句子前面部分是不够的，为了判断$\hat y^{&lt;3&gt;}$（上图编号2所示）是0还是1，除了前3个单词，你还需要更多的信息，因为根据前3个单词无法判断他们说的是<strong>Teddy熊</strong>，还是前美国总统<strong>Teddy Roosevelt</strong>，所以这是一个非双向的或者说只有前向的<strong>RNN</strong>。我刚才所说的总是成立的，不管这些单元（上图编号3所示）是标准的<strong>RNN</strong>块，还是<strong>GRU</strong>单元或者是<strong>LSTM</strong>单元，只要这些构件都是只有前向的。</p><p>那么一个双向的<strong>RNN</strong>是如何解决这个问题的？下面解释双向<strong>RNN</strong>的工作原理。为了简单，我们用四个输入或者说一个只有4个单词的句子，这样输入只有4个，$x^{&lt;1&gt;}$到$x^{&lt;4&gt;}$。从这里开始的这个网络会有一个前向的循环单元叫做${\overrightarrow{a} }^{&lt;1&gt;}$，${\overrightarrow{a} }^{&lt;2&gt;}$，${\overrightarrow{a} }^{&lt;3&gt;}$还有${\overrightarrow{a} }^{&lt;4&gt;}$，我在这上面加个向右的箭头来表示前向的循环单元，并且他们这样连接（下图编号1所示）。这四个循环单元都有一个当前输入$x$输入进去，得到预测的$\hat y^{&lt;1&gt;}$，$\hat y^{&lt;2&gt;}$，$\hat y^{&lt;3&gt;}$和$\hat y^{&lt;4&gt;}$。</p><p>到目前为止，我还没做什么，仅仅是把前面幻灯片里的<strong>RNN</strong>画在了这里，只是在这些地方画上了箭头。我之所以在这些地方画上了箭头是因为我们想要增加一个反向循环层，这里有个${\overleftarrow{a} }^{&lt;1&gt;}$，左箭头代表反向连接，${\overleftarrow{a} }^{&lt;2&gt;}$反向连接，${\overleftarrow{a} }^{&lt;3&gt;}$反向连接，${\overleftarrow{a} }^{&lt;4&gt;}$反向连接，所以这里的左箭头代表反向连接。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/48c787912f7f8daee638dd311583d6cf.png" alt="48c787912f7f8daee638dd311583d6cf"><br>同样，我们把网络这样向上连接，这个$a$反向连接就依次反向向前连接（上图编号2所示）。这样，这个网络就构成了一个无环图。给定一个输入序列$x^{&lt;1&gt;}$到$x^{&lt;4&gt;}$，这个序列首先计算前向的${\overrightarrow{a} }^{&lt;1&gt;}$，然后计算前向的${\overrightarrow{a} }^{&lt;2&gt;}$，接着${\overrightarrow{a} }^{&lt;3&gt;}$，${\overrightarrow{a} }^{&lt;4&gt;}$。而反向序列从计算${\overleftarrow{a} }^{&lt;4&gt;}$开始，反向进行，计算反向的${\overleftarrow{a} }^{&lt;3&gt;}$。你计算的是网络激活值，这不是反向而是前向的传播，而图中这个前向传播一部分计算是从左到右，一部分计算是从右到左。计算完了反向的${\overleftarrow{a} }^{&lt;3&gt;}$，可以用这些激活值计算反向的${\overleftarrow{a} }^{&lt;2&gt;}$，然后是反向的${\overleftarrow{a} }^{&lt;1&gt;}$，把所有这些激活值都计算完了就可以计算预测结果了。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/053831ff43d039bd5e734df96d8794cb.png" alt="053831ff43d039bd5e734df96d8794cb"><br>举个例子，为了预测结果，你的网络会有如$\hat y^{&lt;t&gt;}$，$\hat y^{&lt;t&gt;} =g(W_{g}\left\lbrack {\overrightarrow{a} }^{&lt; t &gt;},{\overleftarrow{a} }^{&lt; t &gt;} \right\rbrack +b_{y})$（上图编号1所示）。比如你要观察时间3这里的预测结果，信息从$x^{&lt;1&gt;}$过来，流经这里，前向的${\overrightarrow{a} }^{&lt;1&gt;}$到前向的${\overrightarrow{a} }^{&lt;2&gt;}$，这些函数里都有表达，到前向的${\overrightarrow{a} }^{&lt;3&gt;}$再到$\hat y^{&lt;3&gt;}$（上图编号2所示的路径），所以从$x^{&lt;1&gt;}$，$x^{&lt;2&gt;}$，$x^{&lt;3&gt;}$来的信息都会考虑在内，而从$x^{&lt;4&gt;}$来的信息会流过反向的${\overleftarrow{a} }^{&lt;4&gt;}$，到反向的${\overleftarrow{a} }^{&lt;3&gt;}$再到$\hat y^{&lt;3&gt;}$（上图编号3所示的路径）。这样使得时间3的预测结果不仅输入了过去的信息，还有现在的信息，这一步涉及了前向和反向的传播信息以及未来的信息。给定一个句子”<strong>He said Teddy Roosevelt…</strong>“来预测<strong>Teddy</strong>是不是人名的一部分，你需要同时考虑过去和未来的信息。</p><p>这就是双向循环神经网络，并且这些基本单元不仅仅是标准<strong>RNN</strong>单元，也可以是<strong>GRU</strong>单元或者<strong>LSTM</strong>单元。事实上，很多的<strong>NLP</strong>问题，对于大量有自然语言处理问题的文本，有<strong>LSTM</strong>单元的双向<strong>RNN</strong>模型是用的最多的。所以如果有<strong>NLP</strong>问题，并且文本句子都是完整的，首先需要标定这些句子，一个有<strong>LSTM</strong>单元的双向<strong>RNN</strong>模型，有前向和反向过程是一个不错的首选。</p><p>以上就是双向<strong>RNN</strong>的内容，这个改进的方法不仅能用于基本的<strong>RNN</strong>结构，也能用于<strong>GRU</strong>和<strong>LSTM</strong>。通过这些改变，你就可以用一个用<strong>RNN</strong>或<strong>GRU</strong>或<strong>LSTM</strong>构建的模型，并且能够预测任意位置，即使在句子的中间，因为模型能够考虑整个句子的信息。这个双向<strong>RNN</strong>网络模型的缺点就是你需要完整的数据的序列，你才能预测任意位置。比如说你要构建一个语音识别系统，那么双向<strong>RNN</strong>模型需要你考虑整个语音表达，但是如果直接用这个去实现的话，你需要等待这个人说完，然后获取整个语音表达才能处理这段语音，并进一步做语音识别。对于实际的语音识别的应用通常会有更加复杂的模块，而不是仅仅用我们见过的标准的双向<strong>RNN</strong>模型。但是对于很多自然语言处理的应用，如果你总是可以获取整个句子，这个标准的双向<strong>RNN</strong>算法实际上很高效。</p><p>好的，这就是双向<strong>RNN</strong>，下一个视频，也是这周的最后一个，我们会讨论如何用这些概念，标准的<strong>RNN</strong>，<strong>LSTM</strong>单元，<strong>GRU</strong>单元，还有双向的版本，构建更深的网络。</p><h3 id="1-12-深层循环神经网络（Deep-RNNs）"><a href="#1-12-深层循环神经网络（Deep-RNNs）" class="headerlink" title="1.12 深层循环神经网络（Deep RNNs）"></a>1.12 深层循环神经网络（Deep <strong>RNN</strong>s）</h3><p>目前你学到的不同<strong>RNN</strong>的版本，每一个都可以独当一面。但是要学习非常复杂的函数，通常我们会把<strong>RNN</strong>的多个层堆叠在一起构建更深的模型。这节视频里我们会学到如何构建这些更深的<strong>RNN</strong>。</p><p>一个标准的神经网络，首先是输入$x$，然后堆叠上隐含层，所以这里应该有激活值，比如说第一层是$a^{\left\lbrack 1 \right\rbrack}$，接着堆叠上下一层，激活值$a^{\left\lbrack 2 \right\rbrack}$，可以再加一层$a^{\left\lbrack 3 \right\rbrack}$，然后得到预测值$\hat{y}$。深层的<strong>RNN</strong>网络跟这个有点像，用手画的这个网络（下图编号1所示），然后把它按时间展开就是了，我们看看。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/8378c2bfe73e1ac9f85d6aa79b71b5eb.png" alt="8378c2bfe73e1ac9f85d6aa79b71b5eb"><br>这是我们一直见到的标准的<strong>RNN</strong>（上图编号3所示方框内的<strong>RNN</strong>），只是我把这里的符号稍微改了一下，不再用原来的$a^{&lt;0 &gt;}$表示0时刻的激活值了，而是用$a^{\lbrack 1\rbrack &lt;0&gt;}$来表示第一层（上图编号4所示），所以我们现在用$a^{\lbrack l\rbrack &lt;t&gt;}$来表示第l层的激活值，这个$\&lt;t\&gt;$表示第$t$个时间点，这样就可以表示。第一层第一个时间点的激活值$a^{\lbrack 1\rbrack &lt;1&gt;}$，这（$a^{\lbrack 1\rbrack &lt;2&gt;}$）就是第一层第二个时间点的激活值，$a^{\lbrack 1\rbrack &lt;3&gt;}$和$a^{\lbrack 1\rbrack &lt;4&gt;}$。然后我们把这些（上图编号4方框内所示的部分）堆叠在上面，这就是一个有三个隐层的新的网络。</p><p>我们看个具体的例子，看看这个值（$a^{\lbrack 2\rbrack &lt;3&gt;}$，上图编号5所示）是怎么算的。激活值$a^{\lbrack 2\rbrack &lt;3&gt;}$有两个输入，一个是从下面过来的输入（上图编号6所示），还有一个是从左边过来的输入（上图编号7所示），$a^{\lbrack 2\rbrack &lt; 3 &gt;} = g(W_{a}^{\left\lbrack 2 \right\rbrack}\left\lbrack a^{\left\lbrack 2 \right\rbrack &lt; 2 &gt;},a^{\left\lbrack 1 \right\rbrack &lt; 3 &gt;} \right\rbrack + b_{a}^{\left\lbrack 2 \right\rbrack})$，这就是这个激活值的计算方法。参数$W_{a}^{\left\lbrack 2 \right\rbrack}$和$b_{a}^{\left\lbrack 2 \right\rbrack}$在这一层的计算里都一样，相对应地第一层也有自己的参数$W_{a}^{\left\lbrack 1 \right\rbrack}$和$b_{a}^{\left\lbrack 1 \right\rbrack}$。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/455863a3c8c2dfaa0e5474bfa2c6824d.png" alt="455863a3c8c2dfaa0e5474bfa2c6824d"><br>对于像左边这样标准的神经网络，你可能见过很深的网络，甚至于100层深，而对于<strong>RNN</strong>来说，有三层就已经不少了。由于时间的维度，<strong>RNN</strong>网络会变得相当大，即使只有很少的几层，很少会看到这种网络堆叠到100层。但有一种会容易见到，就是在每一个上面堆叠循环层，把这里的输出去掉（上图编号1所示），然后换成一些深的层，这些层并不水平连接，只是一个深层的网络，然后用来预测$y^{&lt;1&gt;}$。同样这里（上图编号2所示）也加上一个深层网络，然后预测$y^{&lt;2&gt;}$。这种类型的网络结构用的会稍微多一点，这种结构有三个循环单元，在时间上连接，接着一个网络在后面接一个网络，当然$y^{&lt;3&gt;}$和$y^{&lt;4&gt;}$也一样，这是一个深层网络，但没有水平方向上的连接，所以这种类型的结构我们会见得多一点。通常这些单元（上图编号3所示）没必要非是标准的<strong>RNN</strong>，最简单的<strong>RNN</strong>模型，也可以是<strong>GRU</strong>单元或者<strong>LSTM</strong>单元，并且，你也可以构建深层的双向<strong>RNN</strong>网络。由于深层的<strong>RNN</strong>训练需要很多计算资源，需要很长的时间，尽管看起来没有多少循环层，这个也就是在时间上连接了三个深层的循环层，你看不到多少深层的循环层，不像卷积神经网络一样有大量的隐含层。</p><p>这就是深层<strong>RNN</strong>的内容，从基本的<strong>RNN</strong>网络，基本的循环单元到<strong>GRU</strong>，<strong>LSTM</strong>，再到双向<strong>RNN</strong>，还有深层版的模型。这节课后，你已经可以构建很不错的学习序列的模型了。</p><h2 id="自然语言处理与词嵌入（Natural-Language-Processing-and-Word-Embeddings）"><a href="#自然语言处理与词嵌入（Natural-Language-Processing-and-Word-Embeddings）" class="headerlink" title="自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）"></a>自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）</h2><h3 id="2-1-词汇表征（Word-Representation）"><a href="#2-1-词汇表征（Word-Representation）" class="headerlink" title="2.1 词汇表征（Word Representation）"></a>2.1 词汇表征（Word Representation）</h3><p>上周我们学习了<strong>RNN</strong>、<strong>GRU</strong>单元和<strong>LSTM</strong>单元。本周你会看到我们如何把这些知识用到<strong>NLP</strong>上，用于自然语言处理，深度学习已经给这一领域带来了革命性的变革。其中一个很关键的概念就是词嵌入（<strong>word embeddings</strong>），这是语言表示的一种方式，可以让算法自动的理解一些类似的词，比如男人对女人，比如国王对王后，还有其他很多的例子。通过词嵌入的概念你就可以构建<strong>NLP</strong>应用了，即使你的模型标记的训练集相对较小。这周的最后我们会消除词嵌入的偏差，就是去除不想要的特性，或者学习算法有时会学到的其他类型的偏差。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/68d7c930146724f39782cb57d33161e9.png" alt="68d7c930146724f39782cb57d33161e9"><br>现在我们先开始讨论词汇表示，目前为止我们一直都是用词汇表来表示词，上周提到的词汇表，可能是10000个单词，我们一直用<strong>one-hot</strong>向量来表示词。比如如果<strong>man</strong>（上图编号1所示）在词典里是第5391个，那么就可以表示成一个向量，只在第5391处为1（上图编号2所示），我们用$O_{5391}$代表这个量，这里的$O$代表<strong>one-hot</strong>。接下来，如果<strong>woman</strong>是编号9853（上图编号3所示），那么就可以用$O_{9853}$来表示，这个向量只在9853处为1（上图编号4所示），其他为0，其他的词<strong>king</strong>、<strong>queen</strong>、<strong>apple</strong>、<strong>orange</strong>都可以这样表示出来这种表示方法的一大缺点就是它把每个词孤立起来，这样使得算法对相关词的泛化能力不强。</p><p>举个例子，假如你已经学习到了一个语言模型，当你看到“<strong>I want a glass of orange ___</strong>”，那么下一个词会是什么？很可能是<strong>juice</strong>。即使你的学习算法已经学到了“<strong>I want a glass of orange juice</strong>”这样一个很可能的句子，但如果看到“<strong>I want a glass of apple ___</strong>”，因为算法不知道<strong>apple</strong>和<strong>orange</strong>的关系很接近，就像<strong>man</strong>和<strong>woman</strong>，<strong>king</strong>和<strong>queen</strong>一样。所以算法很难从已经知道的<strong>orange</strong> <strong>juice</strong>是一个常见的东西，而明白<strong>apple</strong> <strong>juice</strong>也是很常见的东西或者说常见的句子。这是因为任何两个<strong>one-hot</strong>向量的内积都是0，如果你取两个向量，比如<strong>king</strong>和<strong>queen</strong>，然后计算它们的内积，结果就是0。如果用<strong>apple</strong>和<strong>orange</strong>来计算它们的内积，结果也是0。很难区分它们之间的差别，因为这些向量内积都是一样的，所以无法知道<strong>apple</strong>和<strong>orange</strong>要比<strong>king</strong>和<strong>orange</strong>，或者<strong>queen</strong>和<strong>orange</strong>相似地多。</p><p>换一种表示方式会更好，如果我们不用<strong>one-hot</strong>表示，而是用特征化的表示来表示每个词，<strong>man</strong>，<strong>woman</strong>，<strong>king</strong>，<strong>queen</strong>，<strong>apple</strong>，<strong>orange</strong>或者词典里的任何一个单词，我们学习这些词的特征或者数值。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/ce30c9ae7912bdb3562199bf85eca1cd.png" alt="ce30c9ae7912bdb3562199bf85eca1cd"><br>举个例子，对于这些词，比如我们想知道这些词与<strong>Gender</strong>（<strong>性别</strong>）的关系。假定男性的性别为-1，女性的性别为+1，那么<strong>man</strong>的性别值可能就是-1，而<strong>woman</strong>就是-1。最终根据经验<strong>king</strong>就是-0.95，<strong>queen</strong>是+0.97，<strong>apple</strong>和<strong>orange</strong>没有性别可言。</p><p>另一个特征可以是这些词有多<strong>Royal</strong>（<strong>高贵</strong>），所以这些词，<strong>man</strong>，<strong>woman</strong>和高贵没太关系，所以它们的特征值接近0。而<strong>king</strong>和<strong>queen</strong>很高贵，<strong>apple</strong>和<strong>orange</strong>跟高贵也没太大关系。</p><p>那么<strong>Age</strong>（<strong>年龄</strong>）呢？<strong>man</strong>和<strong>woman</strong>一般没有年龄的意思，也许<strong>man</strong>和<strong>woman</strong>隐含着成年人的意思，但也可能是介于<strong>young</strong>和<strong>old</strong>之间，所以它们（<strong>man</strong>和<strong>woman</strong>）的值也接近0。而通常<strong>king</strong>和<strong>queen</strong>都是成年人，<strong>apple</strong>和<strong>orange</strong>跟年龄更没什么关系了。</p><p>还有一个特征，这个词是否是<strong>Food</strong>（<strong>食物</strong>），<strong>man</strong>不是食物，<strong>woman</strong>不是食物，<strong>king</strong>和<strong>queen</strong>也不是，但<strong>apple</strong>和<strong>orange</strong>是食物。</p><p>当然还可以有很多的其他特征，从<strong>Size</strong>（<strong>尺寸大小</strong>），<strong>Cost</strong>（<strong>花费多少</strong>），这个东西是不是<strong>alive</strong>（<strong>活的</strong>），是不是一个<strong>Action</strong>（<strong>动作</strong>），或者是不是<strong>Noun</strong>（<strong>名词</strong>）或者是不是<strong>Verb</strong>（<strong>动词</strong>），还是其他的等等。</p><p>所以你可以想很多的特征，为了说明，我们假设有300个不同的特征，这样的话你就有了这一列数字（上图编号1所示），这里我只写了4个，实际上是300个数字，这样就组成了一个300维的向量来表示<strong>man</strong>这个词。接下来，我想用$e_{5391}$这个符号来表示，就像这样（上图编号2所示）。同样这个300维的向量，我用$e_{9853}$代表这个300维的向量用来表示<strong>woman</strong>这个词（上图编号3所示），这些其他的例子也一样。现在，如果用这种表示方法来表示<strong>apple</strong>和<strong>orange</strong>这些词，那么<strong>apple</strong>和<strong>orange</strong>的这种表示肯定会非常相似，可能有些特征不太一样，因为<strong>orange</strong>的颜色口味，<strong>apple</strong>的颜色口味，或者其他的一些特征会不太一样，但总的来说<strong>apple</strong>和<strong>orange</strong>的大部分特征实际上都一样，或者说都有相似的值。这样对于已经知道<strong>orange juice</strong>的算法很大几率上也会明白<strong>apple</strong><br><strong>juice</strong>这个东西，这样对于不同的单词算法会泛化的更好。</p><p>后面的几个视频，我们会找到一个学习词嵌入的方式，这里只是希望你能理解这种高维特征的表示能够比<strong>one-hot</strong>更好的表示不同的单词。而我们最终学习的特征不会像这里一样这么好理解，没有像第一个特征是性别，第二个特征是高贵，第三个特征是年龄等等这些，新的特征表示的东西肯定会更难搞清楚。尽管如此，接下来要学的特征表示方法却能使算法高效地发现<strong>apple</strong>和<strong>orange</strong>会比<strong>king</strong>和<strong>orange</strong>，<strong>queen</strong>和<strong>orange</strong>更加相似。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/59fb45cfdf7faa53571ec7b921b78358.png" alt="59fb45cfdf7faa53571ec7b921b78358"><br>如果我们能够学习到一个300维的特征向量，或者说300维的词嵌入，通常我们可以做一件事，把这300维的数据嵌入到一个二维空间里，这样就可以可视化了。常用的可视化算法是<strong>t-SNE算法</strong>，来自于<strong>Laurens van der Maaten</strong> 和 <strong>Geoff Hinton</strong>的论文。如果观察这种词嵌入的表示方法，你会发现<strong>man</strong>和<strong>woman</strong>这些词聚集在一块（上图编号1所示），<strong>king</strong>和<strong>queen</strong>聚集在一块（上图编号2所示），这些都是人，也都聚集在一起（上图编号3所示）。动物都聚集在一起（上图编号4所示），水果也都聚集在一起（上图编号5所示），像1、2、3、4这些数字也聚集在一起（上图编号6所示）。如果把这些生物看成一个整体，他们也聚集在一起（上图编号7所示）。</p><p>在网上你可能会看到像这样的图用来可视化，300维或者更高维度的嵌入。希望你能有个整体的概念，这种词嵌入算法对于相近的概念，学到的特征也比较类似，在对这些概念可视化的时候，这些概念就比较相似，最终把它们映射为相似的特征向量。这种表示方式用的是在300维空间里的特征表示，这叫做嵌入（<strong>embeddings</strong>）。之所以叫嵌入的原因是，你可以想象一个300维的空间，我画不出来300维的空间，这里用个3维的代替（上图编号8所示）。现在取每一个单词比如<strong>orange</strong>，它对应一个3维的特征向量，所以这个词就被嵌在这个300维空间里的一个点上了（上图编号9所示），<strong>apple</strong>这个词就被嵌在这个300维空间的另一个点上了（上图编号10所示）。为了可视化，<strong>t-SNE算法</strong>把这个空间映射到低维空间，你可以画出一个2维图像然后观察，这就是这个术语嵌入的来源。</p><p>词嵌入已经是<strong>NLP</strong>领域最重要的概念之一了，在自然语言处理领域。本节视频中你已经知道为什么要学习或者使用词嵌入了，下节视频我们会深入讲解如何用这些算法构建<strong>NLP</strong>算法。</p><h3 id="2-2-使用词嵌入（Using-Word-Embeddings）"><a href="#2-2-使用词嵌入（Using-Word-Embeddings）" class="headerlink" title="2.2 使用词嵌入（Using Word Embeddings）"></a>2.2 使用词嵌入（Using Word Embeddings）</h3><p>上一个视频中，你已经了解不同单词的特征化表示了。这节你会看到我们如何把这种表示方法应用到<strong>NLP</strong>应用中。</p><p>我们从一个例子开始，我们继续用命名实体识别的例子，如果你要找出人名，假如有一个句子：“<strong>Sally Johnson is an orange farmer.</strong>”（<strong>Sally Johnson是一个种橙子的农民</strong>），你会发现<strong>Sally Johnson</strong>就是一个人名，所以这里的输出为1。之所以能确定<strong>Sally Johnson</strong>是一个人名而不是一个公司名，是因为你知道种橙子的农民一定是一个人，前面我们已经讨论过用<strong>one-hot</strong>来表示这些单词，$x^{&lt;1&gt;}$ ，$x^{&lt; 2 &gt;}$等等。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/b4bf4b0cdcef0c9d021707c47d5aecda.png" alt="b4bf4b0cdcef0c9d021707c47d5aecda"><br>但是如果你用特征化表示方法，嵌入的向量，也就是我们在上个视频中讨论的。那么用词嵌入作为输入训练好的模型，如果你看到一个新的输入：“<strong>Robert Lin is an apple farmer.</strong>”（<strong>Robert Lin是一个种苹果的农民</strong>），因为知道<strong>orange</strong>和<strong>apple</strong>很相近，那么你的算法很容易就知道<strong>Robert Lin</strong>也是一个人，也是一个人的名字。一个有意思的情况是，要是测试集里这句话不是“<strong>Robert Lin is an apple farmer.</strong>”，而是不太常见的词怎么办？要是你看到：“<strong>Robert Lin is a durian cultivator.</strong>”（<strong>Robert Lin是一个榴莲培育家</strong>）怎么办？<strong>榴莲</strong>（<strong>durian</strong>）是一种比较稀罕的水果，这种水果在新加坡和其他一些国家流行。如果对于一个命名实体识别任务，你只有一个很小的标记的训练集，你的训练集里甚至可能没有<strong>durian</strong>（<strong>榴莲</strong>）或者<strong>cultivator</strong>（<strong>培育家</strong>）这两个词。但是如果你有一个已经学好的词嵌入，它会告诉你<strong>durian</strong>（<strong>榴莲</strong>）是水果，就像<strong>orange</strong>（<strong>橙子</strong>）一样，并且<strong>cultivator</strong>（<strong>培育家</strong>），做培育工作的人其实跟<strong>farmer</strong>（<strong>农民</strong>）差不多，那么你就有可能从你的训练集里的“<strong>an orange farmer</strong>”（<strong>种橙子的农民</strong>）归纳出“<strong>a durian cultivator</strong>”（<strong>榴莲培育家</strong>）也是一个人。</p><p>词嵌入能够达到这种效果，其中一个原因就是学习词嵌入的算法会考察非常大的文本集，也许是从网上找到的，这样你可以考察很大的数据集可以是1亿个单词，甚至达到100亿也都是合理的，大量的无标签的文本的训练集。通过考察大量的无标签文本，很多都是可以免费下载的，你可以发现<strong>orange</strong>（<strong>橙子</strong>）和<strong>durian</strong>（<strong>榴莲</strong>）相近，<strong>farmer</strong>（<strong>农民</strong>）和<strong>cultivator</strong>（<strong>培育家</strong>）相近。因此学习这种嵌入表达，把它们都聚集在一块，通过读取大量的互联网文本发现了<strong>orange</strong>（<strong>橙子</strong>）和<strong>durian</strong>（<strong>榴莲</strong>）都是水果。接下来你可以把这个词嵌入应用到你的命名实体识别任务当中，尽管你只有一个很小的训练集，也许训练集里有100,000个单词，甚至更小，这就使得你可以使用迁移学习，把你从互联网上免费获得的大量的无标签文本中学习到的知识，能够分辨<strong>orange</strong>（<strong>橙子</strong>）、<strong>apple</strong>（<strong>苹果</strong>）和<strong>durian</strong>（<strong>榴莲</strong>）都是水果的知识，然后把这些知识迁移到一个任务中，比如你只有少量标记的训练数据集的命名实体识别任务中。当然了，这里为了简化我只画了单向的<strong>RNN</strong>，事实上如果你想用在命名实体识别任务上，你应该用一个双向的<strong>RNN</strong>，而不是这样一个简单的。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/8a1d58b7ade17208053c10728b2bf3b6.png" alt="8a1d58b7ade17208053c10728b2bf3b6"><br>总结一下，这是如何用词嵌入做迁移学习的步骤。</p><p>第一步，先从大量的文本集中学习词嵌入。一个非常大的文本集，或者可以下载网上预训练好的词嵌入模型，网上你可以找到不少，词嵌入模型并且都有许可。</p><p>第二步，你可以用这些词嵌入模型把它迁移到你的新的只有少量标注训练集的任务中，比如说用这个300维的词嵌入来表示你的单词。这样做的一个好处就是你可以用更低维度的特征向量代替原来的10000维的<strong>one-hot</strong>向量，现在你可以用一个300维更加紧凑的向量。尽管<strong>one-hot</strong>向量很快计算，而学到的用于词嵌入的300维的向量会更加紧凑。</p><p>第三步，当你在你新的任务上训练模型时，在你的命名实体识别任务上，只有少量的标记数据集上，你可以自己选择要不要继续微调，用新的数据调整词嵌入。实际中，只有这个第二步中有很大的数据集你才会这样做，如果你标记的数据集不是很大，通常我不会在微调词嵌入上费力气。</p><p>当你的任务的训练集相对较小时，词嵌入的作用最明显，所以它广泛用于<strong>NLP</strong>领域。我只提到一些，不要太担心这些术语（下问列举的一些<strong>NLP</strong>任务），它已经用在命名实体识别，用在文本摘要，用在文本解析、指代消解，这些都是非常标准的<strong>NLP</strong>任务。</p><p>词嵌入在语言模型、机器翻译领域用的少一些，尤其是你做语言模型或者机器翻译任务时，这些任务你有大量的数据。在其他的迁移学习情形中也一样，如果你从某一任务<strong>A</strong>迁移到某个任务<strong>B</strong>，只有<strong>A</strong>中有大量数据，而<strong>B</strong>中数据少时，迁移的过程才有用。所以对于很多<strong>NLP</strong>任务这些都是对的，而对于一些语言模型和机器翻译则不然。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/43943c791844cc7f077f6c6f98f1f629.png" alt="43943c791844cc7f077f6c6f98f1f629"><br>最后，词嵌入和人脸编码之间有奇妙的关系，你已经在前面的课程学到了关于人脸编码的知识了，如果你上了卷积神经网络的课程的话。你应该还记得对于人脸识别，我们训练了一个<strong>Siamese</strong>网络结构，这个网络会学习不同人脸的一个128维表示，然后通过比较编码结果来判断两个图片是否是同一个人脸，这个词嵌入的意思和这个差不多。在人脸识别领域大家喜欢用编码这个词来指代这些向量$f(x^{\left(i \right)})$，$f(x^{\left( j\right)})$（上图编号1所示），人脸识别领域和这里的词嵌入有一个不同就是，在人脸识别中我们训练一个网络，任给一个人脸照片，甚至是没有见过的照片，神经网络都会计算出相应的一个编码结果。上完后面几节课，你会更明白，我们学习词嵌入则是有一个固定的词汇表，比如10000个单词，我们学习向量$e_{1}$到$e_{10000}$，学习一个固定的编码，每一个词汇表的单词的固定嵌入，这就是人脸识别与我们接下来几节视频要讨论的算法之间的一个不同之处。这里的术语编码（<strong>encoding</strong>）和嵌入（<strong>embedding</strong>）可以互换，所以刚才讲的差别不是因为术语不一样，这个差别就是，人脸识别中的算法未来可能涉及到海量的人脸照片，而自然语言处理有一个固定的词汇表，而像一些没有出现过的单词我们就记为未知单词。</p><p>这节视频里，你看到如何用词嵌入来实现这种类型的迁移学习，并且通过替换原来的<strong>one-hot</strong>表示，而是用之前的嵌入的向量，你的算法会泛化的更好，你也可以从较少的标记数据中进行学习。接下来我会给你展示一些词嵌入的特性，这之后再讨论学习这些词嵌入的算法。下个视频我们会看到词嵌入在做类比推理中发挥的作用。</p><h3 id="2-3-词嵌入的特性（Properties-of-Word-Embeddings）"><a href="#2-3-词嵌入的特性（Properties-of-Word-Embeddings）" class="headerlink" title="2.3 词嵌入的特性（Properties of Word Embeddings）"></a>2.3 词嵌入的特性（Properties of Word Embeddings）</h3><p>到现在，你应该明白了词嵌入是如何帮助你构建自然语言处理应用的。词嵌入还有一个迷人的特性就是它还能帮助实现类比推理，尽管类比推理可能不是自然语言处理应用中最重要的，不过它能帮助人们理解词嵌入做了什么，以及词嵌入能够做什么，让我们来一探究竟。</p><p>这是一系列你希望词嵌入可以捕捉的单词的特征表示，假如我提出一个问题，<strong>man</strong>如果对应<strong>woman</strong>，那么<strong>king</strong>应该对应什么？你们应该都能猜到<strong>king</strong>应该对应<strong>queen</strong>。能否有一种算法来自动推导出这种关系，下面就是实现的方法。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/12242657bd982acd1d80570cc090b4fe.png" alt="12242657bd982acd1d80570cc090b4fe"><br>我们用一个四维向量来表示<strong>man</strong>，我们用$e_{5391}$来表示，不过在这节视频中我们先把它（上图编号1所示）称为$e_{\text{man} }$，而旁边这个（上图编号2所示）表示<strong>woman</strong>的嵌入向量，称它为$e_{\text{woman} }$，对<strong>king</strong>和<strong>queen</strong>也是用一样的表示方法。在该例中，假设你用的是四维的嵌入向量，而不是比较典型的50到1000维的向量。这些向量有一个有趣的特性，就是假如你有向量$e_{\text{man} }$和$e_{\text{woman} }$，将它们进行减法运算，即</p><p>$$<br>e_{\text{man} } - e_{\text{woman} } = \begin{bmatrix}</p><ul><li>1 \</li></ul><p>0.01 \<br>0.03 \<br>0.09 \<br>\end{bmatrix} - \begin{bmatrix}<br>1 \<br>0.02 \<br>0.02 \<br>0.01 \<br>\end{bmatrix} = \begin{bmatrix}</p><ul><li>2 \</li><li>0.01 \</li></ul><p>0.01 \<br>0.08 \<br>\end{bmatrix} \approx \begin{bmatrix}</p><ul><li>2 \<br>0 \<br>0 \<br>0 \<br>\end{bmatrix}<br>$$</li></ul><p>类似的，假如你用$e_{\text{king} }$减去$e_{\text{queen} }$，最后也会得到一样的结果，即</p><p>$$<br>e_{\text{king} } - e_{\text{queen} } = \begin{bmatrix}</p><ul><li>0.95 \</li></ul><p>0.93 \<br>0.70 \<br>0.02 \<br>\end{bmatrix} - \begin{bmatrix}<br>0.97 \<br>0.95 \<br>0.69 \<br>0.01 \<br>\end{bmatrix} = \begin{bmatrix}</p><ul><li>1.92 \</li><li>0.02 \</li></ul><p>0.01 \<br>0.01 \<br>\end{bmatrix} \approx \begin{bmatrix}</p><ul><li>2 \<br>0 \<br>0 \<br>0 \<br>\end{bmatrix}<br>$$</li></ul><p>这个结果表示，<strong>man</strong>和<strong>woman</strong>主要的差异是<strong>gender</strong>（<strong>性别</strong>）上的差异，而<strong>king</strong>和<strong>queen</strong>之间的主要差异，根据向量的表示，也是<strong>gender</strong>（<strong>性别</strong>）上的差异，这就是为什么$e_{\text{man} }- e_{\text{woman} }$和$e_{\text{king} } - e_{\text{queen} }$结果是相同的。所以得出这种类比推理的结论的方法就是，当算法被问及<strong>man</strong>对<strong>woman</strong>相当于<strong>king</strong>对什么时，算法所做的就是计算$e_{\text{man} }-e_{\text{woman} }$，然后找出一个向量也就是找出一个词，使得$e_{\text{man} }-e_{\text{woman} }$≈$\ e_{\text{king} }- e_{?}$，也就是说，当这个新词是<strong>queen</strong>时，式子的左边会近似地等于右边。这种思想首先是被<strong>Tomas Mikolov</strong> 和 <strong>Wen-tau Yih</strong>还有<strong>Geoffrey Zweig</strong>提出的，这是词嵌入领域影响力最为惊人和显著的成果之一，这种思想帮助了研究者们对词嵌入领域建立了更深刻的理解。</p><p>（<strong>Mikolov T, Yih W T, Zweig G. Linguistic regularities in continuous space word representations[J]. In HLT-NAACL, 2013.</strong>）</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/5a42eea162ddc75a1d37520618b4bcd2.png" alt="5a42eea162ddc75a1d37520618b4bcd2"><br>让我们来正式地探讨一下应该如何把这种思想写成算法。在图中，词嵌入向量在一个可能有300维的空间里，于是单词<strong>man</strong>代表的就是空间中的一个点，另一个单词<strong>woman</strong>代表空间另一个点，单词<strong>king</strong>也代表一个点，还有单词<strong>queen</strong>也在另一点上（上图编号1方框内所示的点）。事实上，我们在上个幻灯片所展示的就是向量<strong>man</strong>和<strong>woman</strong>的差值非常接近于向量<strong>king</strong>和<strong>queen</strong>之间的差值，我所画的这个箭头（上图编号2所示）代表的就是向量在<strong>gender</strong>（<strong>性别</strong>）这一维的差，不过不要忘了这些点是在300维的空间里。为了得出这样的类比推理，计算当<strong>man</strong>对于<strong>woman</strong>，那么<strong>king</strong>对于什么，你能做的就是找到单词<strong>w</strong>来使得，$e_{\text{man} }-e_{\text{woman} }≈ e_{\text{king} } - e_{w}$这个等式成立，你需要的就是找到单词<strong>w</strong>来最大化$e_{w}$与$e_{\text{king} } - e_{\text{man} } + e_{\text{woman} }$的相似度，即</p> $Find\ word\ w:argmax \ Sim(e_{w},e_{\text{king} } - e_{\text{man} } + e_{\text{woman} })$<p>所以我做的就是我把这个$e_{w}$全部放到等式的一边，于是等式的另一边就会是$e_{\text{king} }- e_{\text{man} } + e_{\text{woman} }$。我们有一些用于测算$e_{w}$和$e_{\text{king} } -e_{\text{man} } + e_{\text{woman} }$之间的相似度的函数，然后通过方程找到一个使得相似度最大的单词，如果结果理想的话会得到单词<strong>queen</strong>。值得注意的是这种方法真的有效，如果你学习一些词嵌入，通过算法来找到使得相似度最大化的单词<strong>w</strong>，你确实可以得到完全正确的答案。不过这取决于过程中的细节，如果你查看一些研究论文就不难发现，通过这种方法来做类比推理准确率大概只有30%~75%，只要算法猜中了单词，就把该次计算视为正确，从而计算出准确率，在该例子中，算法选出了单词<strong>queen</strong>。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/012c07b7692aed382a2875292ea8e81b.png" alt="012c07b7692aed382a2875292ea8e81b"><br>在继续下一步之前，我想再说明一下左边的这幅图（上图编号1所示），在之前我们谈到过用<strong>t-SNE</strong>算法来将单词可视化。<strong>t-SNE算法</strong>所做的就是把这些300维的数据用一种非线性的方式映射到2维平面上，可以得知<strong>t-SNE</strong>中这种映射很复杂而且很非线性。在进行<strong>t-SNE</strong>映射之后，你不能总是期望使等式成立的关系，会像左边那样成一个平行四边形，尽管在这个例子最初的300维的空间内你可以依赖这种平行四边形的关系来找到使等式成立的一对类比，通过<strong>t-SNE算法</strong>映射出的图像可能是正确的。但在大多数情况下，由于<strong>t-SNE</strong>的非线性映射，你就没法再指望这种平行四边形了，很多这种平行四边形的类比关系在<strong>t-SNE</strong>映射中都会失去原貌。</p><p>现在，再继续之前，我想再快速地列举一个最常用的相似度函数，这个最常用的相似度函数叫做余弦相似度。这是我们上个幻灯片所得到的等式（下图编号1所示），在余弦相似度中，假如在向量$u$和$v$之间定义相似度:$\text{sim}\left( u,v \right) = \frac{u^{T}v}{\left| \left| u \right| \right|_{2}\left| \left| v \right| \right|_{2} }$</p><p>现在我们先不看分母，分子其实就是$u$和$v$的内积。如果u和v非常相似，那么它们的内积将会很大，把整个式子叫做余弦相似度，其实就是因为该式是$u$和$v$的夹角的余弦值，所以这个角（下图编号2所示）就是Φ角，这个公式实际就是计算两向量夹角Φ角的余弦。你应该还记得在微积分中，Φ角的余弦图像是这样的（下图编号3所示），所以夹角为0度时，余弦相似度就是1，当夹角是90度角时余弦相似度就是0，当它们是180度时，图像完全跑到了相反的方向，这时相似度等于-1，这就是为什么余弦相似度对于这种类比工作能起到非常好的效果。<br>距离用平方距离或者欧氏距离来表示:$\left| \left| u - v \right| \right|^{2}$<br><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/cd01dd1ced86605f712cf080681db022.png" alt="cd01dd1ced86605f712cf080681db022"><br><strong>参考资料：余弦相似度</strong><br>为了测量两个词的相似程度，我们需要一种方法来测量两个词的两个嵌入向量之间的相似程度。 给定两个向量$u$和$v$，余弦相似度定义如下：</p> ${CosineSimilarity(u, v)} = \frac {u . v} {||u||_2 ||v||_2} = cos(\theta) \tag{1}$<p>其中 $u.v$ 是两个向量的点积（或内积），$||u||_2$是向量$u$的范数（或长度），并且 $\theta$ 是向量$u$和$v$之间的角度。这种相似性取决于角度在向量$u$和$v$之间。如果向量$u$和$v$非常相似，它们的余弦相似性将接近1; 如果它们不相似，则余弦相似性将取较小的值。<br><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/cosine_sim.png" alt="cosine_sim"><br><strong>图1：</strong>两个向量之间角度的余弦是衡量它们有多相似的指标，角度越小，两个向量越相似。</p><p>从学术上来说，比起测量相似度，这个函数更容易测量的是相异度，所以我们需要对其取负，这个函数才能正常工作，不过我还是觉得余弦相似度用得更多一点，这两者的主要区别是它们对$u$和$v$之间的距离标准化的方式不同。</p><p>词嵌入的一个显著成果就是，可学习的类比关系的一般性。举个例子，它能学会<strong>man</strong>对于<strong>woman</strong>相当于<strong>boy</strong>对于<strong>girl</strong>，因为<strong>man</strong>和<strong>woman</strong>之间和<strong>king</strong>和<strong>queen</strong>之间，还有<strong>boy</strong>和<strong>girl</strong>之间的向量差在<strong>gender</strong>（<strong>性别</strong>）这一维都是一样的。它还能学习<strong>Canada</strong>（<strong>加拿大</strong>）的首都是<strong>Ottawa</strong>（<strong>渥太华</strong>），而渥太华对于加拿大相当于<strong>Nairobi</strong>（<strong>内罗毕</strong>）对于<strong>Kenya</strong>（<strong>肯尼亚</strong>），这些都是国家中首都城市名字。它还能学习<strong>big</strong>对于<strong>bigger</strong>相当于<strong>tall</strong>对于<strong>taller</strong>，还能学习<strong>Yen</strong>（<strong>円</strong>）对于<strong>Janpan</strong>（<strong>日本</strong>），<strong>円</strong>是日本的货币单位，相当于<strong>Ruble</strong>（<strong>卢比</strong>）对于<strong>Russia</strong>（<strong>俄罗斯</strong>）。这些东西都能够学习，只要你在大型的文本语料库上实现一个词嵌入学习算法，只要从足够大的语料库中进行学习，它就能自主地发现这些模式。</p><p>在本节视频中，你见到了词嵌入是如何被用于类比推理的，可能你不会自己动手构建一个类比推理系统作为一项应用，不过希望在这些可学习的类特征的表示方式能够给你一些直观的感受。你还看知道了余弦相似度可以作为一种衡量两个词嵌入向量间相似度的办法，我们谈了许多有关这些嵌入的特性，以及如何使用它们。下节视频中，我们来讨论如何真正的学习这些词嵌入。</p><h3 id="2-4-嵌入矩阵（Embedding-Matrix）"><a href="#2-4-嵌入矩阵（Embedding-Matrix）" class="headerlink" title="2.4 嵌入矩阵（Embedding Matrix）"></a>2.4 嵌入矩阵（Embedding Matrix）</h3><p>接下来我们要将学习词嵌入这一问题具体化，当你应用算法来学习词嵌入时，实际上是学习一个嵌入矩阵，我们来看一下这是什么意思。</p><p>和之前一样，假设我们的词汇表含有10,000个单词，词汇表里有<strong>a</strong>，<strong>aaron</strong>，<strong>orange</strong>，<strong>zulu</strong>，可能还有一个未知词标记&amp;lt;<strong>UNK</strong>&amp;gt;。我们要做的就是学习一个嵌入矩阵$E$，它将是一个300×10,000的矩阵，如果你的词汇表里有10,000个，或者加上未知词就是10,001维。这个矩阵的各列代表的是词汇表中10,000个不同的单词所代表的不同向量。假设<strong>orange</strong>的单词编号是6257（下图编号1所示），代表词汇表中第6257个单词，我们用符号$O_{6527}$<br>来表示这个<strong>one-hot</strong>向量，这个向量除了第6527个位置上是1（下图编号2所示），其余各处都为0，显然它是一个10,000维的列向量，它只在一个位置上有1，它不像图上画的那么短，它的高度应该和左边的嵌入矩阵的宽度相等。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/ad1c7b1e85d39f56756c28787ccef892.png" alt="ad1c7b1e85d39f56756c28787ccef892"><br>假设这个嵌入矩阵叫做矩阵$E$，注意如果用$E$去乘以右边的<strong>one-hot</strong>向量（上图编号3所示），也就是$O_{6527}$，那么就会得到一个300维的向量，$E$是300×10,000的，$O_{6527}$是10,000×1的，所以它们的积是300×1的，即300维的向量。要计算这个向量的第一个元素，你需要做的是把$E$的第一行（上图编号4所示）和$O_{6527}$的整列相乘，不过$O_{6527}$的所有元素都是0，只有6257位置上是1，最后你得到的这个向量的第一个元素（上图编号5所示）就是<strong>orange</strong>这一列下的数字（上图编号6所示）。然后我们要计算这个向量的第二个元素，就是把$E$的第二行（上图编号7所示）和这个$O_{6527}$相乘，和之前一样，然后得到第二个元素（上图编号8所示），以此类推，直到你得到这个向量剩下的所有元素（上图编号9所示）。</p><p>这就是为什么把矩阵$E$和这个<strong>one-hot</strong>向量相乘，最后得到的其实就是这个300维的列，就是单词<strong>orange</strong>下的这一列，它等于$e_{6257}$，这个符号是我们用来表示这个300×1的嵌入向量的符号，它表示的单词是<strong>orange</strong>。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/fa320bd001f9dca8ec33c7a426e20d80.png" alt="fa320bd001f9dca8ec33c7a426e20d80"><br>更广泛来说，假如说有某个单词<strong>w</strong>，那么$e_{w}$就代表单词<strong>w</strong>的嵌入向量。同样，$EO_{j}$，$O_{j}$就是只有第$j$个位置是1的<strong>one-hot</strong>向量，得到的结果就是$e_{j}$，它表示的是字典中单词<strong>j</strong>的嵌入向量。</p><p>在这一小节中，要记住的一件事就是我们的目标是学习一个嵌入矩阵$E$。在下节视频中你将会随机地初始化矩阵$E$，然后使用梯度下降法来学习这个300×10,000的矩阵中的各个参数，$E$乘以这个<strong>one-hot</strong>向量（上图编号1所示）会得到嵌入向量。再多说一点，当我们写这个等式（上图编号2所示）的时候，写出这些符号是很方便的，代表用矩阵$E$乘以<strong>one-hot</strong>向量$O_{j}$。但当你动手实现时，用大量的矩阵和向量相乘来计算它，效率是很低下的，因为<strong>one-hot</strong>向量是一个维度非常高的向量，并且几乎所有元素都是0，所以矩阵向量相乘效率太低，因为我们要乘以一大堆的0。所以在实践中你会使用一个专门的函数来单独查找矩阵$E$的某列，而不是用通常的矩阵乘法来做，但是在画示意图时（上图所示，即矩阵$E$乘以<strong>one-hot</strong>向量示意图），这样写比较方便。但是例如在<strong>Keras</strong>中就有一个嵌入层，然后我们用这个嵌入层更有效地从嵌入矩阵中提取出你需要的列，而不是对矩阵进行很慢很复杂的乘法运算。</p><p>在本视频中你见到了在学习嵌入向量的过程中用来描述这些算法的符号以及关键术语，矩阵$E$它包含了词汇表中所有单词的嵌入向量。在下节视频中，我们将讨论学习矩阵$E$的具体算法。</p><h3 id="2-5-学习词嵌入（Learning-Word-Embeddings）"><a href="#2-5-学习词嵌入（Learning-Word-Embeddings）" class="headerlink" title="2.5 学习词嵌入（Learning Word Embeddings）"></a>2.5 学习词嵌入（Learning Word Embeddings）</h3><p>在本节视频中，你将要学习一些具体的算法来学习词嵌入。在深度学习应用于学习词嵌入的历史上，人们一开始使用的算法比较复杂，但随着时间推移，研究者们不断发现他们能用更加简单的算法来达到一样好的效果，特别是在数据集很大的情况下。但有一件事情就是，现在很多最流行的算法都十分简单，如果我一开始就介绍这些简单的算法，你可能会觉得这有点神奇，这么简单的算法究竟是怎么起作用的？稍微复杂一些的算法开始，因为我觉得这样更容易对算法的运作方式有一个更直观的了解，之后我们会对这些算法进行简化，使你能够明白即使一些简单的算法也能得到非常好的结果，我们开始吧。</p><p>假如你在构建一个语言模型，并且用神经网络来实现这个模型。于是在训练过程中，你可能想要你的神经网络能够做到比如输入：“<strong>I want a glass of orange ___.</strong>”，然后预测这句话的下一个词。在每个单词下面，我都写上了这些单词对应词汇表中的索引。实践证明，建立一个语言模型是学习词嵌入的好方法，我提出的这些想法是源于<strong>Yoshua Bengio</strong>，<strong>Rejean Ducharme</strong>，<strong>Pascal Vincent</strong>，<strong>Rejean Ducharme</strong>，<strong>Pascal Vincent</strong>还有<strong>Christian Jauvin</strong>。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/31347eca490e0ae8541140fb01c04d72.png" alt="31347eca490e0ae8541140fb01c04d72"><br>下面我将介绍如何建立神经网络来预测序列中的下一个单词，让我为这些词列一个表格，“<strong>I want a glass of orange</strong>”，我们从第一个词<strong>I</strong>开始，建立一个<strong>one-hot</strong>向量表示这个单词<strong>I</strong>。这是一个<strong>one-hot</strong>向量（上图编号1所示），在第4343个位置是1，它是一个10,000维的向量。然后要做的就是生成一个参数矩阵$E$，然后用$E$乘以$O_{4343}$，得到嵌入向量$e_{4343}$，这一步意味着$e_{4343}$是由矩阵$E$乘以<strong>one-hot</strong>向量得到的（上图编号2所示）。然后我们对其他的词也做相同的操作，单词<strong>want</strong>在第9665个，我们将$E$与这个<strong>one-hot</strong>向量（$O_{9665}$）相乘得到嵌入向量$e_{9665}$。对其他单词也是一样，<strong>a</strong>是字典中的第一个词，因为<strong>a</strong>是第一个字母，由$O_{1}$得到$e_{1}$。同样地，其他单词也这样操作。</p><p>于是现在你有许多300维的嵌入向量。我们能做的就是把它们全部放进神经网络中（上图编号3所示），经过神经网络以后再通过<strong>softmax</strong>层（上图编号4所示），这个<strong>softmax</strong>也有自己的参数，然后这个<strong>softmax</strong>分类器会在10,000个可能的输出中预测结尾这个单词。假如说在训练集中有<strong>juice</strong>这个词，训练过程中<strong>softmax</strong>的目标就是预测出单词<strong>juice</strong>，就是结尾的这个单词。这个隐藏层（上图编号3所示）有自己的参数，我这里用$W^{\left\lbrack1 \right\rbrack}$和$b^{\left\lbrack 1\right\rbrack}$来表示，这个<strong>softmax</strong>层（上图编号4所示）也有自己的参数$W^{\left\lbrack2 \right\rbrack}$和$b^{\left\lbrack 2\right\rbrack}$。如果它们用的是300维大小的嵌入向量，而这里有6个词，所以用6×300，所以这个输入会是一个1800维的向量，这是通过将这6个嵌入向量堆在一起得到的。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/747e619260737ded586ae51b3b4f07d6.png" alt="747e619260737ded586ae51b3b4f07d6"><br>实际上更常见的是有一个固定的历史窗口，举个例子，你总是想预测给定四个单词（上图编号1所示）后的下一个单词，注意这里的4是算法的超参数。这就是如何适应很长或者很短的句子，方法就是总是只看前4个单词，所以说我只用这4个单词（上图编号2所示）而不去看这几个词（上图编号3所示）。如果你一直使用一个4个词的历史窗口，这就意味着你的神经网络会输入一个1200维的特征变量到这个层中（上图编号4所示），然后再通过<strong>softmax</strong>来预测输出，选择有很多种，用一个固定的历史窗口就意味着你可以处理任意长度的句子，因为输入的维度总是固定的。所以这个模型的参数就是矩阵$E$，对所有的单词用的都是同一个矩阵$E$，而不是对应不同的位置上的不同单词用不同的矩阵。然后这些权重（上图编号5所示）也都是算法的参数，你可以用反向传播来进行梯度下降来最大化训练集似然，通过序列中给定的4个单词去重复地预测出语料库中下一个单词什么。</p><p>事实上通过这个算法能很好地学习词嵌入，原因是，如果你还记得我们的<strong>orange jucie</strong>，<strong>apple juice</strong>的例子，在这个算法的激励下，<strong>apple</strong>和<strong>orange</strong>会学到很相似的嵌入，这样做能够让算法更好地拟合训练集，因为它有时看到的是<strong>orange juice</strong>，有时看到的是<strong>apple juice</strong>。如果你只用一个300维的特征向量来表示所有这些词，算法会发现要想最好地拟合训练集，就要使<strong>apple</strong>（<strong>苹果</strong>）、<strong>orange</strong>（<strong>橘子</strong>）、<strong>grape</strong>（<strong>葡萄</strong>）和<strong>pear</strong>（<strong>梨</strong>）等等，还有像<strong>durian</strong>（<strong>榴莲</strong>）这种很稀有的水果都拥有相似的特征向量。</p><p>这就是早期最成功的学习词嵌入，学习这个矩阵$E$的算法之一。现在我们先概括一下这个算法，看看我们该怎样来推导出更加简单的算法。现在我想用一个更复杂的句子作为例子来解释这些算法，假设在你的训练集中有这样一个更长的句子：“<strong>I want a glass of orange juice to go along with my cereal.</strong>”。我们在上个幻灯片看到的是算法预测出了某个单词<strong>juice</strong>，我们把它叫做目标词（下图编号1所示），它是通过一些上下文，在本例中也就是这前4个词（下图编号2所示）推导出来的。如果你的目标是学习一个嵌入向量，研究人员已经尝试过很多不同类型的上下文。如果你要建立一个语言模型，那么一般选取目标词之前的几个词作为上下文。但如果你的目标不是学习语言模型本身的话，那么你可以选择其他的上下文。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/638c103855ffeb25122259dd6b669850.png" alt="638c103855ffeb25122259dd6b669850"><br>比如说，你可以提出这样一个学习问题，它的上下文是左边和右边的四个词，你可以把目标词左右各4个词作为上下文（上图编号3所示）。这就意味着我们提出了一个这样的问题，算法获得左边4个词，也就是<strong>a glass of orange</strong>，还有右边四个词<strong>to go along with</strong>，然后要求预测出中间这个词（上图编号4所示）。提出这样一个问题，这个问题需要将左边的还有右边这4个词的嵌入向量提供给神经网络，就像我们之前做的那样来预测中间的单词是什么，来预测中间的目标词，这也可以用来学习词嵌入。</p><p>或者你想用一个更简单的上下文，也许只提供目标词的前一个词，比如只给出<strong>orange</strong>这个词来预测<strong>orange</strong>后面是什么（上图编号5所示），这将会是不同的学习问题。只给出一个词<strong>orange</strong>来预测下一个词是什么（上图编号6所示），你可以构建一个神经网络，只把目标词的前一个词或者说前一个词的嵌入向量输入神经网络来预测该词的下一个词。</p><p>还有一个效果非常好的做法就是上下文是附近一个单词，它可能会告诉你单词<strong>glass</strong>（上图编号7所示）是一个邻近的单词。或者说我看见了单词<strong>glass</strong>，然后附近有一个词和<strong>glass</strong>位置相近，那么这个词会是什么（上图编号8所示）？这就是用附近的一个单词作为上下文。我们将在下节视频中把它公式化，这用的是一种<strong>Skip-Gram</strong>模型的思想。这是一个简单算法的例子，因为上下文相当的简单，比起之前4个词，现在只有1个，但是这种算法依然能工作得很好。</p><p>研究者发现，如果你真想建立一个语言模型，用目标词的前几个单词作为上下文是常见做法（上图编号9所示）。但如果你的目标是学习词嵌入，那么你就可以用这些其他类型的上下文（上图编号10所示），它们也能得到很好的词嵌入。我会在下节视频详细介绍这些，我们会谈到<strong>Word2Vec</strong>模型。</p><p>总结一下，在本节视频中你学习了语言模型问题，模型提出了一个机器学习问题，即输入一些上下文，例如目标词的前4个词然后预测出目标词，学习了提出这些问题是怎样帮助学习词嵌入的。在下节视频，你将看到如何用更简单的上下文和更简单的算法来建立从上下文到目标词的映射，这将让你能够更好地学习词嵌入，一起进入下节视频学习<strong>Word2Vec</strong>模型。</p><h3 id="2-6-Word2Vec"><a href="#2-6-Word2Vec" class="headerlink" title="2.6 Word2Vec"></a>2.6 Word2Vec</h3><p>在上个视频中你已经见到了如何学习一个神经语言模型来得到更好的词嵌入，在本视频中你会见到 <strong>Word2Vec</strong>算法，这是一种简单而且计算时更加高效的方式来学习这种类型的嵌入，让我们来看看。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/0800c19895cbf1a360379b5dc5493902.png" alt="0800c19895cbf1a360379b5dc5493902"><br>本视频中的大多数的想法来源于<strong>Tomas Mikolov</strong>，<strong>Kai Chen</strong>，<strong>Greg Corrado</strong> 和 <strong>Jeff Dean</strong>。</p><p>（<strong>Mikolov T, Chen K, Corrado G, et al. Efficient Estimation of Word Representations in Vector Space[J]. Computer Science, 2013.</strong>）</p><p>假设在训练集中给定了一个这样的句子：“<strong>I want a glass of orange juice to go along with my cereal.</strong>”，在<strong>Skip-Gram</strong>模型中，我们要做的是抽取上下文和目标词配对，来构造一个监督学习问题。上下文不一定总是目标单词之前离得最近的四个单词，或最近的$n$个单词。我们要的做的是随机选一个词作为上下文词，比如选<strong>orange</strong>这个词，然后我们要做的是随机在一定词距内选另一个词，比如在上下文词前后5个词内或者前后10个词内，我们就在这个范围内选择目标词。可能你正好选到了<strong>juice</strong>作为目标词，正好是下一个词（表示<strong>orange</strong>的下一个词），也有可能你选到了前面第二个词，所以另一种配对目标词可以是<strong>glass</strong>，还可能正好选到了单词<strong>my</strong>作为目标词。</p><p>于是我们将构造一个监督学习问题，它给定上下文词，要求你预测在这个词正负10个词距或者正负5个词距内随机选择的某个目标词。显然，这不是个非常简单的学习问题，因为在单词<strong>orange</strong>的正负10个词距之间，可能会有很多不同的单词。但是构造这个监督学习问题的目标并不是想要解决这个监督学习问题本身，而是想要使用这个学习问题来学到一个好的词嵌入模型。</p><p>接下来说说模型的细节，我们继续假设使用一个10,000词的词汇表，有时训练使用的词汇表会超过一百万词。但我们要解决的基本的监督学习问题是学习一种映射关系，从上下文<strong>c</strong>，比如单词<strong>orange</strong>，到某个目标词，记为<strong>t</strong>，可能是单词<strong>juice</strong>或者单词<strong>glass</strong>或者单词<strong>my</strong>。延续上一张幻灯片的例子，在我们的词汇表中，<strong>orange</strong>是第6257个单词，<strong>juice</strong>是10,000个单词中的第4834个，这就是你想要的映射到输出$y$的输入$x$。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/4ebf216a59d46efa2136f72b51fd49bd.png" alt="4ebf216a59d46efa2136f72b51fd49bd"><br>为了表示输入，比如单词<strong>orange</strong>，你可以先从<strong>one-hot</strong>向量开始，我们将其写作$O_{c}$，这就是上下文词的<strong>one-hot</strong>向量（上图编号1所示）。然后和你在上节视频中看到的类似，你可以拿嵌入矩阵$E$乘以向量$O_{c}$，然后得到了输入的上下文词的嵌入向量，于是这里$e_{c}=EO_{c}$。在这个神经网络中（上图编号2所示），我们将把向量$e_{c}$喂入一个<strong>softmax</strong>单元。我通常把<strong>softmax</strong>单元画成神经网络中的一个节点（上图编号3所示），这不是字母<strong>O</strong>，而是<strong>softmax</strong>单元，<strong>softmax</strong>单元要做的就是输出$\hat y$。然后我们再写出模型的细节，这是<strong>softmax</strong>模型（上图编号4所示），预测不同目标词的概率：</p> $Softmax:p\left( t \middle| c \right) = \frac{e^{\theta_{t}^{T}e_{c} }}{\sum_{j = 1}^{10,000}e^{\theta_{j}^{T}e_{c} }}$<p>这里$\theta_{t}$是一个与输出$t$有关的参数，即某个词$t$和标签相符的概率是多少。我省略了<strong>softmax</strong>中的偏差项，想要加上的话也可以加上。</p><p>最终<strong>softmax</strong>的损失函数就会像之前一样，我们用$y$表示目标词，我们这里用的$y$和$\hat y$都是用<strong>one-hot</strong>表示的，于是损失函数就会是：</p> $L\left( \hat y,y \right) = - \sum_{i = 1}^{10,000}{y_{i}\log \hat y_{i} }$<p>这是常用的<strong>softmax</strong>损失函数，$y$ 就是只有一个1其他都是0的<strong>one-hot</strong>向量，如果目标词是<strong>juice</strong>，那么第4834个元素就是1，其余是0（上图编号5所示）。类似的$\hat y$是一个从<strong>softmax</strong>单元输出的10,000维的向量，这个向量是所有可能目标词的概率。</p><p>总结一下，这大体上就是一个可以找到词嵌入的简化模型和神经网络（上图编号2所示），其实就是个<strong>softmax</strong>单元。矩阵$E$将会有很多参数，所以矩阵$E$有对应所有嵌入向量$e_{c}$的参数（上图编号6所示），<strong>softmax</strong>单元也有$\theta_{t}$的参数（上图编号3所示）。如果优化这个关于所有这些参数的损失函数，你就会得到一个较好的嵌入向量集，这个就叫做<strong>Skip-Gram</strong>模型。它把一个像<strong>orange</strong>这样的词作为输入，并预测这个输入词，从左数或从右数的某个词，预测上下文词的前面一些或者后面一些是什么词。</p><p>实际上使用这个算法会遇到一些问题，首要的问题就是计算速度。尤其是在<strong>softmax</strong>模型中，每次你想要计算这个概率，你需要对你词汇表中的所有10,000个词做求和计算，可能10,000个词的情况还不算太差。如果你用了一个大小为100,000或1,000,000的词汇表，那么这个分母的求和操作是相当慢的，实际上10,000已经是相当慢的了，所以扩大词汇表就更加困难了。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/776044225ea4a736a4f2b38ea61fae4c.png" alt="776044225ea4a736a4f2b38ea61fae4c"><br>这里有一些解决方案，如分级（<strong>hierarchical</strong>）的<strong>softmax</strong>分类器和<strong>负采样</strong>（<strong>Negative Sampling</strong>）。</p><p>在文献中你会看到的方法是使用一个分级（<strong>hierarchical</strong>）的<strong>softmax</strong>分类器，意思就是说不是一下子就确定到底是属于10,000类中的哪一类。想象如果你有一个分类器（上图编号1所示），它告诉你目标词是在词汇表的前5000个中还是在词汇表的后5000个词中，假如这个二分类器告诉你这个词在前5000个词中（上图编号2所示），然后第二个分类器会告诉你这个词在词汇表的前2500个词中，或者在词汇表的第二组2500个词中，诸如此类，直到最终你找到一个词准确所在的分类器（上图编号3所示），那么就是这棵树的一个叶子节点。像这样有一个树形的分类器，意味着树上内部的每一个节点都可以是一个二分类器，比如逻辑回归分类器，所以你不需要再为单次分类，对词汇表中所有的10,000个词求和了。实际上用这样的分类树，计算成本与词汇表大小的对数成正比（上图编号4所示），而不是词汇表大小的线性函数，这个就叫做分级<strong>softmax</strong>分类器。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/89743b5ade106cad1318b8f3f4547a7f.png" alt="89743b5ade106cad1318b8f3f4547a7f"><br>我要提一下，在实践中分级<strong>softmax</strong>分类器不会使用一棵完美平衡的分类树或者说一棵左边和右边分支的词数相同的对称树（上图编号1所示的分类树）。实际上，分级的<strong>softmax</strong>分类器会被构造成常用词在顶部，然而不常用的词像<strong>durian</strong>会在树的更深处（上图编号2所示的分类树），因为你想更常见的词会更频繁，所以你可能只需要少量检索就可以获得常用单词像<strong>the</strong>和<strong>of</strong>。然而你更少见到的词比如<strong>durian</strong>就更合适在树的较深处，因为你一般不需要到那样的深处，所以有不同的经验法则可以帮助构造分类树形成分级<strong>softmax</strong>分类器。所以这是你能在文献中见到的一个加速<strong>softmax</strong>分类的方法，但是我不会再花太多时间在这上面了，你可以从我在第一张幻灯片中提到的<strong>Tomas Mikolov</strong>等人的论文中参阅更多的细节，所以我不会再花更多时间讲这个了。因为在下个视频中，我们会讲到另一个方法叫做负采样，我感觉这个会更简单一点，对于加速<strong>softmax</strong>和解决需要在分母中对整个词汇表求和的问题也很有作用，下个视频中你会看到更多的细节。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/a41190391bd3c506f49124798952e2d2.png" alt="a41190391bd3c506f49124798952e2d2"><br>但是在进入下个视频前，我想要你理解一个东西，那就是怎么对上下文<strong>c</strong>进行采样，一旦你对上下文<strong>c</strong>进行采样，那么目标词<strong>t</strong>就会在上下文<strong>c</strong>的正负10个词距内进行采样。但是你要如何选择上下文<strong>c</strong>？一种选择是你可以就对语料库均匀且随机地采样，如果你那么做，你会发现有一些词，像<strong>the</strong>、<strong>of</strong>、<strong>a</strong>、<strong>and</strong>、<strong>to</strong>诸如此类是出现得相当频繁的，于是你那么做的话，你会发现你的上下文到目标词的映射会相当频繁地得到这些种类的词，但是其他词，像<strong>orange</strong>、<strong>apple</strong>或<strong>durian</strong>就不会那么频繁地出现了。你可能不会想要你的训练集都是这些出现得很频繁的词，因为这会导致你花大部分的力气来更新这些频繁出现的单词的$e_{c}$（上图编号1所示），但你想要的是花时间来更新像<strong>durian</strong>这些更少出现的词的嵌入，即$e_{\text{durian} }$。实际上词$p(c)$的分布并不是单纯的在训练集语料库上均匀且随机的采样得到的，而是采用了不同的分级来平衡更常见的词和不那么常见的词。</p><p>这就是<strong>Word2Vec</strong>的<strong>Skip-Gram</strong>模型，如果你读过我之前提到的论文原文，你会发现那篇论文实际上有两个不同版本的<strong>Word2Vec</strong>模型，<strong>Skip-Gram</strong>只是其中的一个，另一个叫做<strong>CBOW</strong>，即连续词袋模型（<strong>Continuous</strong><br><strong>Bag-Of-Words Model</strong>），它获得中间词两边的的上下文，然后用周围的词去预测中间的词，这个模型也很有效，也有一些优点和缺点。</p><p>总结下：<strong>CBOW</strong>是从原始语句推测目标字词；而<strong>Skip-Gram</strong>正好相反，是从目标字词推测出原始语句。<strong>CBOW</strong>对小型数据库比较合适，而<strong>Skip-Gram</strong>在大型语料中表现更好。 （下图左边为<strong>CBOW</strong>，右边为<strong>Skip-Gram</strong>）</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1575537312018.png" alt="enter description here"><br>而刚才讲的<strong>Skip-Gram</strong>模型，关键问题在于<strong>softmax</strong>这个步骤的计算成本非常昂贵，因为它需要在分母里对词汇表中所有词求和。通常情况下，<strong>Skip-Gram</strong>模型用到更多点。在下个视频中，我会展示给你一个算法，它修改了训练目标使其可以运行得更有效，因此它可以让你应用在一个更大的训练集上面，也可以学到更好的词嵌入。</p><h3 id="2-7-负采样（Negative-Sampling）"><a href="#2-7-负采样（Negative-Sampling）" class="headerlink" title="2.7 负采样（Negative Sampling）"></a>2.7 负采样（Negative Sampling）</h3><p>在上个视频中，你见到了<strong>Skip-Gram</strong>模型如何帮助你构造一个监督学习任务，把上下文映射到了目标词上，它如何让你学到一个实用的词嵌入。但是它的缺点就在于<strong>softmax</strong>计算起来很慢。在本视频中，你会看到一个改善过的学习问题叫做负采样，它能做到与你刚才看到的<strong>Skip-Gram</strong>模型相似的事情，但是用了一个更加有效的学习算法，让我们来看看这是怎么做到的。</p><p>在本视频中大多数的想法源于<strong>Tomas Mikolov</strong>，<strong>Ilya Sutskever</strong>，<strong>Kai Chen</strong>，<strong>Greg Corrado</strong> 和 <strong>Jeff Dean</strong>。</p><p>（<strong>Mikolov T, Sutskever I, Chen K, et al. Distributed Representations of Words and Phrases and their Compositionality[J]. 2013, 26:3111-3119.</strong>）</p><p>我们在这个算法中要做的是构造一个新的监督学习问题，那么问题就是给定一对单词，比如<strong>orange</strong>和<strong>juice</strong>，我们要去预测这是否是一对上下文词-目标词（<strong>context-target</strong>）。</p><p>在这个例子中<strong>orange</strong>和<strong>juice</strong>就是个正样本，那么<strong>orange</strong>和<strong>king</strong>就是个负样本，我们把它标为0。我们要做的就是采样得到一个上下文词和一个目标词，在这个例子中就是<strong>orange</strong> 和<strong>juice</strong>，我们用1作为标记，我把中间这列（下图编号1所示）叫做词（<strong>word</strong>）。这样生成一个正样本，正样本跟上个视频中生成的方式一模一样，先抽取一个上下文词，在一定词距内比如说正负10个词距内选一个目标词，这就是生成这个表的第一行，即<strong>orange– juice -1</strong>的过程。然后为了生成一个负样本，你将用相同的上下文词，再在字典中随机选一个词，在这里我随机选了单词<strong>king</strong>，标记为0。然后我们再拿<strong>orange</strong>，再随机从词汇表中选一个词，因为我们设想，如果随机选一个词，它很可能跟<strong>orange</strong>没关联，于是<strong>orange–book–0</strong>。我们再选点别的，<strong>orange</strong>可能正好选到<strong>the</strong>，然后是0。还是<strong>orange</strong>，再可能正好选到<strong>of</strong>这个词，再把这个标记为0，注意<strong>of</strong>被标记为0，即使<strong>of</strong>的确出现在<strong>orange</strong>词的前面。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/54beb302688f6a298b63178534281575.png" alt="54beb302688f6a298b63178534281575"><br>总结一下，生成这些数据的方式是我们选择一个上下文词（上图编号2所示），再选一个目标词（上图编号3所示），这（上图编号4所示）就是表的第一行，它给了一个正样本，上下文，目标词，并给定标签为1。然后我们要做的是给定几次，比如$K$次（上图编号5所示），我们将用相同的上下文词，再从字典中选取随机的词，<strong>king</strong>、<strong>book</strong>、<strong>the</strong>、<strong>of</strong>等，从词典中任意选取的词，并标记0，这些就会成为负样本（上图编号6所示）。出现以下情况也没关系，就是如果我们从字典中随机选到的词，正好出现在了词距内，比如说在上下文词<strong>orange</strong>正负10个词之内。</p><p>接下来我们将构造一个监督学习问题，其中学习算法输入$x$，输入这对词（上图编号7所示），要去预测目标的标签（上图编号8所示），即预测输出$y$。因此问题就是给定一对词，像<strong>orange</strong>和<strong>juice</strong>，你觉得它们会一起出现么？你觉得这两个词是通过对靠近的两个词采样获得的吗？或者你觉得我是分别在文本和字典中随机选取得到的？这个算法就是要分辨这两种不同的采样方式，这就是如何生成训练集的方法。</p><p>那么如何选取$K$？<strong>Mikolov</strong>等人推荐小数据集的话，$K$从5到20比较好。如果你的数据集很大，$K$就选的小一点。对于更大的数据集$K$就等于2到5，数据集越小$K$就越大。那么在这个例子中，我们就用$K=4$。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/f36df292b7444e9b7379fa7c14626fa2.png" alt="f36df292b7444e9b7379fa7c14626fa2"><br>下面我们讲讲学习从$x$映射到$y$的监督学习模型，这（上图编号1所示:$Softmax:p\left( t \middle| c \right) = \frac{e^{\theta_{t}^{T}e_{c} }}{\sum_{j = 1}^{10,000}e^{\theta_{j}^{T}e_{c} }}$）的<strong>softmax</strong>模型。这是我们从上张幻灯片中得到的训练集，这个（上图编号2所示）将是新的输入$x$，这个（上图编号3所示）将是你要预测的值$y$。为了定义模型，我们将使用记号$c$表示上下文词，记号$t$表示可能的目标词，我再用$y$表示0和1，表示是否是一对上下文-目标词。我们要做的就是定义一个逻辑回归模型，给定输入的$c$，$t$对的条件下，$y=1$的概率，即：</p> $P\left( y = 1 \middle| c,t \right) = \sigma(\theta_{t}^{T}e_{c})$<p>这个模型基于逻辑回归模型，但不同的是我们将一个<strong>sigmoid</strong>函数作用于$\theta_{t}^{T}e_{c}$，参数和之前一样，你对每一个可能的目标词有一个参数向量$\theta_{t}$和另一个参数向量$e_{c}$，即每一个可能上下文词的的嵌入向量，我们将用这个公式估计$y=1$的概率。如果你有$K$个样本，你可以把这个看作$\frac{1}{K}$的正负样本比例，即每一个正样本你都有$K$个对应的负样本来训练一个类似逻辑回归的模型。</p><p>我们把这个画成一个神经网络，如果输入词是<strong>orange</strong>，即词6257，你要做的就是输入<strong>one-hot</strong>向量，再传递给$E$，通过两者相乘获得嵌入向量$e_{6257}$，你就得到了10,000个可能的逻辑回归分类问题，其中一个（上图编号4所示）将会是用来判断目标词是否是<strong>juice</strong>的分类器，还有其他的词，比如说可能下面的某个分类器（上图编号5所示）是用来预测<strong>king</strong>是否是目标词，诸如此类，预测词汇表中这些可能的单词。把这些看作10,000个二分类逻辑回归分类器，但并不是每次迭代都训练全部10,000个，我们只训练其中的5个，我们要训练对应真正目标词那一个分类器，再训练4个随机选取的负样本，这就是$K=4$的情况。所以不使用一个巨大的10,000维度的<strong>softmax</strong>，因为计算成本很高，而是把它转变为10,000个二分类问题，每个都很容易计算，每次迭代我们要做的只是训练它们其中的5个，一般而言就是$K+1$个，其中$K$个负样本和1个正样本。这也是为什么这个算法计算成本更低，因为只需更新$K+1$个逻辑单元，$K+1$个二分类问题，相对而言每次迭代的成本比更新10,000维的<strong>softmax</strong>分类器成本低。</p><p>你也会在本周的编程练习中用到这个算法，这个技巧就叫负采样。因为你做的是，你有一个正样本词<strong>orange</strong>和<strong>juice</strong>，然后你会特意生成一系列负样本，这些（上图编号6所示）是负样本，所以叫负采样，即用这4个负样本训练，4个额外的二分类器，在每次迭代中你选择4个不同的随机的负样本词去训练你的算法。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/b05dd0362a19496bb0ad91b8494e374c.png" alt="b05dd0362a19496bb0ad91b8494e374c"><br>这个算法有一个重要的细节就是如何选取负样本，即在选取了上下文词<strong>orange</strong>之后，你如何对这些词进行采样生成负样本？一个办法是对中间的这些词进行采样，即候选的目标词，你可以根据其在语料中的经验频率进行采样，就是通过词出现的频率对其进行采样。但问题是这会导致你在<strong>like</strong>、<strong>the</strong>、<strong>of</strong>、<strong>and</strong>诸如此类的词上有很高的频率。另一个极端就是用1除以词汇表总词数，即$\frac{1}{\left|v\right|}$，均匀且随机地抽取负样本，这对于英文单词的分布是非常没有代表性的。所以论文的作者<strong>Mikolov</strong>等人根据经验，他们发现这个经验值的效果最好，它位于这两个极端的采样方法之间，既不用经验频率，也就是实际观察到的英文文本的分布，也不用均匀分布，他们采用以下方式：</p> $P\left( w_{i} \right) = \frac{f\left( w_{i} \right)^{\frac{3}{4} }}{\sum_{j = 1}^{10,000}{f\left( w_{j} \right)^{\frac{3}{4} }} }$<p>进行采样，所以如果$f(w_{i})$是观测到的在语料库中的某个英文词的词频，通过$\frac{3}{4}$次方的计算，使其处于完全独立的分布和训练集的观测分布两个极端之间。我并不确定这是否有理论证明，但是很多研究者现在使用这个方法，似乎也效果不错。</p><p>总结一下，你已经知道了在<strong>softmax</strong>分类器中如何学到词向量，但是计算成本很高。在这个视频中，你见到了如何通过将其转化为一系列二分类问题使你可以非常有效的学习词向量。如果你使用这个算法，你将可以学到相当好的词向量。当然和深度学习的其他领域一样，有很多开源的实现，当然也有预训练过的词向量，就是其他人训练过的然后授权许可发布在网上的，所以如果你想要在<strong>NLP</strong>问题上取得进展，去下载其他人的词向量是很好的方法，在此基础上改进。</p><p><strong>Skip-Gram</strong>模型就介绍到这里，在下个视频中，我会跟你分享另一个版本的词嵌入学习算法<strong>GloVe</strong>，而且这可能比你之前看到的都要简单。</p><h3 id="2-8-GloVe-词向量（GloVe-Word-Vectors）"><a href="#2-8-GloVe-词向量（GloVe-Word-Vectors）" class="headerlink" title="2.8 GloVe 词向量（GloVe Word Vectors）"></a>2.8 GloVe 词向量（GloVe Word Vectors）</h3><p>你已经了解了几个计算词嵌入的算法，另一个在<strong>NLP</strong>社区有着一定势头的算法是<strong>GloVe</strong>算法，这个算法并不如<strong>Word2Vec</strong>或是<strong>Skip-Gram</strong>模型用的多，但是也有人热衷于它，我认为可能是因为它简便吧，我们来看看这个算法。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/70e282d4d1abb86fd15ff7b175f4e579.png" alt="70e282d4d1abb86fd15ff7b175f4e579"><br><strong>Glove</strong>算法是由<strong>Jeffrey Pennington</strong>，<strong>Richard Socher</strong>和<strong>Chris Manning</strong>发明的。</p><p>(<strong>Pennington J, Socher R, Manning C. Glove: Global Vectors for Word Representation[C]// Conference on Empirical Methods in Natural Language Processing. 2014:1532-1543.</strong>)</p><p><strong>GloVe</strong>代表用词表示的全局变量（<strong>global vectors for word representation</strong>）。在此之前，我们曾通过挑选语料库中位置相近的两个词，列举出词对，即上下文和目标词，<strong>GloVe</strong>算法做的就是使其关系开始明确化。假定$X_{ {ij} }$是单词$i$在单词$j$上下文中出现的次数，那么这里$i$和$j$就和$t$和$c$的功能一样，所以你可以认为$X_{ {ij} }$等同于$X_{ {tc} }$。你也可以遍历你的训练集，然后数出单词$i$在不同单词$j$上下文中出现的个数，单词$t$在不同单词$c$的上下文中共出现多少次。根据上下文和目标词的定义，你大概会得出$X_{ {ij} }$等于$X_{ji}$这个结论。事实上，如果你将上下文和目标词的范围定义为出现于左右各10词以内的话，那么就会有一种对称关系。如果你对上下文的选择是，上下文总是目标词前一个单词的话，那么$X_{ {ij} }$和$X_{ji}$就不会像这样对称了。不过对于<strong>GloVe</strong>算法，我们可以定义上下文和目标词为任意两个位置相近的单词，假设是左右各10词的距离，那么$X_{ {ij} }$就是一个能够获取单词$i$和单词$j$出现位置相近时或是彼此接近的频率的计数器。</p><p><strong>GloVe</strong>模型做的就是进行优化，我们将他们之间的差距进行最小化处理：</p> $\text{mini}\text{mize}\sum_{i = 1}^{10,000}{\sum_{j = 1}^{10,000}{f\left( X_{ {ij} } \right)\left( \theta_{i}^{T}e_{j} + b_{i} + b_{j}^{'} - logX_{ {ij} } \right)^{2} }}$<p>其中$\theta_{i}^{T}e_{j}$，想一下$i$和$j$与$t$和$c$的功能一样，因此这就和你之前看的有些类似了，即$\theta_{t}^{T}e_{c}$。同时对于这个（$\theta_{t}^{T}e_{c}$，下图编号1所示）来说，你想要知道的是告诉你这两个单词之间有多少联系，$t$和$c$之间有多紧密，$i$和$j$之间联系程度如何，换句话说就是他们同时出现的频率是多少，这是由这个$X_{ {ij} }$影响的。然后，我们要做的是解决参数$\theta$和$e$的问题，然后准备用梯度下降来最小化上面的公式，你只想要学习一些向量，这样他们的输出能够对这两个单词同时出现的频率进行良好的预测。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/f6fc2cec52f4ecb15567511aae822914.png" alt="f6fc2cec52f4ecb15567511aae822914"><br>现在一些附加的细节是如果$X_{ {ij} }$是等于0的话，那么$log0$就是未定义的，是负无穷大的，所以我们想要对$X_{ {ij} }$为0时进行求和，因此要做的就是添加一个额外的加权项$f\left(X_{ {ij} }\right)$（上图编号2所示）。如果$X_{ {ij} }$等于0的话，同时我们会用一个约定，即$0log0= 0$，这个的意思是如果$X_{ {ij} } =0$，先不要进行求和，所以这个$log0$项就是不相关项。上面的求和公式表明，这个和仅是一个上下文和目标词关系里连续出现至少一次的词对的和。$f\left(X_{ {ij} }\right)$的另一个作用是，有些词在英语里出现十分频繁，比如说<strong>this</strong>，<strong>is</strong>，<strong>of</strong>，<strong>a</strong>等等，有些情况，这叫做<strong>停止词</strong>，但是在频繁词和不常用词之间也会有一个连续统（<strong>continuum</strong>）。不过也有一些不常用的词，比如<strong>durion</strong>，你还是想将其考虑在内，但又不像那些常用词这样频繁。因此，这个加权因子$f\left(X_{ {ij} }\right)$就可以是一个函数，即使是像<strong>durion</strong>这样不常用的词，它也能给予大量有意义的运算，同时也能够给像<strong>this</strong>，<strong>is</strong>，<strong>of</strong>，<strong>a</strong>这样在英语里出现更频繁的词更大但不至于过分的权重。因此有一些对加权函数f的选择有着启发性的原则，就是既不给这些词（<strong>this</strong>，<strong>is</strong>，<strong>of</strong>，<strong>a</strong>）过分的权重，也不给这些不常用词（<strong>durion</strong>）太小的权值。如果你想要知道f是怎么能够启发性地完成这个功能的话，你可以看一下我之前的幻灯片里引用的<strong>GloVe</strong>算法论文。</p><p>最后，一件有关这个算法有趣的事是$\theta$和$e$现在是完全对称的，所以那里的$\theta_{i}$和$e_{j}$就是对称的。如果你只看数学式的话，他们（$\theta_{i}$和$e_{j}$）的功能其实很相近，你可以将它们颠倒或者将它们进行排序，实际上他们都输出了最佳结果。因此一种训练算法的方法是一致地初始化$\theta$和$e$，然后使用梯度下降来最小化输出，当每个词都处理完之后取平均值，所以，给定一个词$w$，你就会有$e_{w}^{(final)}= \frac{e_{w} +\theta_{w} }{2}$。因为$\theta$和$e$在这个特定的公式里是对称的，而不像之前视频里我们了解的模型，$\theta$和$e$功能不一样，因此也不能像那样取平均。</p><p>这就是<strong>GloVe</strong>算法的内容，我认为这个算法的一个疑惑之处是如果你看着这个等式，它实在是太简单了，对吧？仅仅是最小化，像这样的一个二次代价函数（上图编号3所示）是怎么能够让你学习有意义的词嵌入的呢？但是结果证明它确实有效，发明者们发明这个算法的过程是他们以历史上更为复杂的算法，像是<strong>newer language</strong>模型，以及之后的<strong>Word2Vec</strong>、<strong>Skip-Gram</strong>模型等等为基础，同时希望能够简化所有之前的算法才发明的。</p><p>在我们总结词嵌入学习算法之前，有一件更优先的事，我们会简单讨论一下。就是说，我们以这个特制的表格作为例子来开始学习词向量，我们说，第一行的嵌入向量是来表示<strong>Gender</strong>的，第二行是来表示<strong>Royal</strong>的，然后是是<strong>Age</strong>，在之后是<strong>Food</strong>等等。但是当你在使用我们了解过的算法的一种来学习一个词嵌入时，例如我们之前的幻灯片里提到的<strong>GloVe</strong>算法，会发生一件事就是你不能保证嵌入向量的独立组成部分是能够理解的，为什么呢？</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/ec4b604d619dd617f14c2a34945c075d.png" alt="ec4b604d619dd617f14c2a34945c075d"><br>假设说有个空间，里面的第一个轴（上图编号1所示）是<strong>Gender</strong>，第二个轴（上图编号2所示）是<strong>Royal</strong>，你能够保证的是第一个嵌入向量对应的轴（上图编号3所示）是和这个轴（上面提到的第一和第二基轴，编号1，2所示）有联系的，它的意思可能是<strong>Gender</strong>、<strong>Royal</strong>、<strong>Age</strong>和<strong>Food</strong>。具体而言，这个学习算法会选择这个（上图编号3所示）作为第一维的轴，所以给定一些上下文词，第一维可能是这个轴（上图编号3所示），第二维也许是这个（上图编号4所示），或者它可能不是正交的，它也可能是第二个非正交轴（上图编号5所示），它可以是你学习到的词嵌入中的第二部分。当我们看到这个（上图编号6所示）的时候，如果有某个可逆矩阵$A$，那么这项（上图编号6所示）就可以简单地替换成$\left(A\theta_{i} \right)^{T}(A^{- T}e_{j})$，因为我们将其展开：</p> $\left( A\theta_{i} \right)^{T}\left( A^{- T}e_{j} \right) = \theta_{i}^{T}A^{T}A^{- T}e_{j} = \theta_{i}^{T}e_{j}$<p>不必担心，如果你没有学过线性代数的话会，和这个算法一样有一个简单证明过程。你不能保证这些用来表示特征的轴能够等同于人类可能简单理解的轴，具体而言，第一个特征可能是个<strong>Gender</strong>、<strong>Roya</strong>、<strong>Age</strong>、<strong>Food Cost</strong>和<strong>Size</strong>的组合，它也许是名词或是一个行为动词和其他所有特征的组合，所以很难看出独立组成部分，即这个嵌入矩阵的单行部分，然后解释出它的意思。尽管有这种类型的线性变换，这个平行四边形映射也说明了我们解决了这个问题，当你在类比其他问题时，这个方法也是行得通的。因此尽管存在特征量潜在的任意线性变换，你最终还是能学习出解决类似问题的平行四边形映射。</p><p>这就是词嵌入学习的内容，你现在已经了解了一些学习词嵌入的算法了，你可以在本周的编程练习里更多地运用它们。下节课讲解怎样使用这些算法来解决情感分类问题。</p><h3 id="2-9-情感分类（Sentiment-Classification）"><a href="#2-9-情感分类（Sentiment-Classification）" class="headerlink" title="2.9 情感分类（Sentiment Classification）"></a>2.9 情感分类（Sentiment Classification）</h3><p>情感分类任务就是看一段文本，然后分辨这个人是否喜欢他们在讨论的这个东西，这是<strong>NLP</strong>中最重要的模块之一，经常用在许多应用中。情感分类一个最大的挑战就是可能标记的训练集没有那么多，但是有了词嵌入，即使只有中等大小的标记的训练集，你也能构建一个不错的情感分类器，让我们看看是怎么做到的。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/bf6f5879d33ae4ef09b32f77df84948e.png" alt="bf6f5879d33ae4ef09b32f77df84948e"><br>这是一个情感分类问题的一个例子（上图所示），输入$x$是一段文本，而输出$y$是你要预测的相应情感。比如说是一个餐馆评价的星级，</p><p>比如有人说，”<strong>The dessert is excellent.</strong>“（甜点很棒），并给出了四星的评价；</p><p>“<strong>Service was quite slow</strong>“（服务太慢），两星评价；</p><p>“<strong>Good for a quick meal but nothing special</strong>“（适合吃快餐但没什么亮点），三星评价；</p><p>还有比较刁钻的评论，”<strong>Completely lacking in good taste, good service and good ambiance.</strong>“（完全没有好的味道，好的服务，好的氛围），给出一星评价。</p><p>如果你能训练一个从$x$到$y$的映射，基于这样的标记的数据集，那么你就可以用来搜集大家对你运营的餐馆的评价。一些人可能会把你的餐馆信息放到一些社交媒体上，<strong>Twitter</strong>、<strong>Facebook</strong>、<strong>Instagram</strong>或者其他的社交媒体，如果你有一个情感分类器，那么它就可以看一段文本然后分析出这个人对你的餐馆的评论的情感是正面的还是负面的，这样你就可以一直记录是否存在一些什么问题，或者你的餐馆是在蒸蒸日上还是每况愈下。</p><p>情感分类一个最大的挑战就是可能标记的训练集没有那么多。对于情感分类任务来说，训练集大小从10,000到100,000个单词都很常见，甚至有时会小于10,000个单词，采用了词嵌入能够带来更好的效果，尤其是只有很小的训练集时。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/ea844a0290e66d1c76a31e34b632dc0c.png" alt="ea844a0290e66d1c76a31e34b632dc0c"><br>接下来你可以这样做，这节我们会讲几个不同的算法。这是一个简单的情感分类的模型，假设有一个句子”<strong>dessert is excellent</strong>“，然后在词典里找这些词，我们通常用10,000个词的词汇表。我们要构建一个分类器能够把它映射成输出四个星，给定这四个词（”<strong>dessert is excellent</strong>“），我们取这些词，找到相应的<strong>one-hot</strong>向量，所以这里（上图编号1所示）就是$o_{8928}$，乘以嵌入矩阵$E$，$E$可以从一个很大的文本集里学习到，比如它可以从一亿个词或者一百亿个词里学习嵌入，然后用来提取单词<strong>the</strong>的嵌入向量$e_{8928}$，对<strong>dessert</strong>、<strong>is</strong>、<strong>excellent</strong>做同样的步骤。</p><p>如果在很大的训练集上训练$E$，比如一百亿的单词，这样你就会获得很多知识，甚至从有些不常用的词中获取，然后应用到你的问题上，即使你的标记数据集里没有这些词。我们可以这样构建一个分类器，取这些向量（上图编号2所示），比如是300维度的向量。然后把它们求和或者求平均，这里我画一个大点的平均值计算单元（上图编号3所示），你也可以用求和或者平均。这个单元（上图编号3所示）会得到一个300维的特征向量，把这个特征向量送进<strong>softmax</strong>分类器，然后输出$\hat y$。这个<strong>softmax</strong>能够输出5个可能结果的概率值，从一星到五星，这个就是5个可能输出的<strong>softmax</strong>结果用来预测$y$的值。</p><p>这里用的平均值运算单元，这个算法适用于任何长短的评论，因为即使你的评论是100个词长，你也可以对这一百个词的特征向量求和或者平均它们，然后得到一个表示一个300维的特征向量表示，然后把它送进你的<strong>softmax</strong>分类器，所以这个平均值运算效果不错。它实际上会把所有单词的意思给平均起来，或者把你的例子中所有单词的意思加起来就可以用了。</p><p>这个算法有一个问题就是没考虑词序，尤其是这样一个负面的评价，”<strong>Completely lacking in good taste, good service, and good ambiance.</strong>“，但是<strong>good</strong>这个词出现了很多次，有3个<strong>good</strong>，如果你用的算法跟这个一样，忽略词序，仅仅把所有单词的词嵌入加起来或者平均下来，你最后的特征向量会有很多<strong>good</strong>的表示，你的分类器很可能认为这是一个好的评论，尽管事实上这是一个差评，只有一星的评价。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/de4b6513a8d1866bccf1fac3c0d0d6d2.png" alt="de4b6513a8d1866bccf1fac3c0d0d6d2"><br>我们有一个更加复杂的模型，不用简单的把所有的词嵌入都加起来，我们用一个<strong>RNN</strong>来做情感分类。我们这样做，首先取这条评论，”<strong>Completely lacking in good taste, good service, and good ambiance.</strong>“，找出每一个<strong>one-hot</strong>向量，这里我跳过去每一个<strong>one-hot</strong>向量的表示。用每一个<strong>one-hot</strong>向量乘以词嵌入矩阵$E$，得到词嵌入表达$e$，然后把它们送进<strong>RNN</strong>里。<strong>RNN</strong>的工作就是在最后一步（上图编号1所示）计算一个特征表示，用来预测$\hat y$，这是一个多对一的网络结构的例子，我们之前已经见过了。有了这样的算法，考虑词的顺序效果就更好了，它就能意识到”<strong>things are lacking in good taste</strong>“，这是个负面的评价，“<strong>not good</strong>”也是一个负面的评价。而不像原来的算法一样，只是把所有的加在一起得到一个大的向量，根本意识不到“<strong>not good</strong>”和 “<strong>good</strong>”不是一个意思，”<strong>lacking in good taste</strong>“也是如此，等等。</p><p>如果你训练一个这样的算法，最后会得到一个很合适的情感分类的算法。由于你的词嵌入是在一个更大的数据集里训练的，这样效果会更好，更好的泛化一些没有见过的新的单词。比如其他人可能会说，”<strong>Completely absent of good taste, good service, and good ambiance.</strong>“，即使<strong>absent</strong>这个词不在标记的训练集里，如果是在一亿或者一百亿单词集里训练词嵌入，它仍然可以正确判断，并且泛化的很好，甚至这些词是在训练集中用于训练词嵌入的，但是可以不在专门用来做情感分类问题的标记的训练集中。</p><p>以上就是情感分类的问题，我希望你能大体了解。一旦你学习到或者从网上下载词嵌入，你就可以很快构建一个很有效的<strong>NLP</strong>系统。</p><h3 id="2-10-词嵌入除偏（Debiasing-Word-Embeddings）"><a href="#2-10-词嵌入除偏（Debiasing-Word-Embeddings）" class="headerlink" title="2.10 词嵌入除偏（Debiasing Word Embeddings）"></a>2.10 词嵌入除偏（Debiasing Word Embeddings）</h3><p>现在机器学习和人工智能算法正渐渐地被信任用以辅助或是制定极其重要的决策，因此我们想尽可能地确保它们不受非预期形式偏见影响，比如说性别歧视、种族歧视等等。本节视频中我会向你展示词嵌入中一些有关减少或是消除这些形式的偏见的办法。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/25430afa93f24dc6caa6f85503bbad27.png" alt="25430afa93f24dc6caa6f85503bbad27"><br>本节视频中当我使用术语<strong>bias</strong>时，我不是指<strong>bias</strong>本身这个词，或是偏见这种感觉，而是指性别、种族、性取向方面的偏见，那是不同的偏见，同时这也通常用于机器学习的学术讨论中。不过我们讨论的大部分内容是词嵌入是怎样学习类比像<strong>Man</strong>：<strong>Woman</strong>，就像<strong>King</strong>：<strong>Queen</strong>，不过如果你这样问，如果<strong>Man</strong>对应<strong>Computer Programmer</strong>，那么<strong>Woman</strong>会对应什么呢？所以这篇论文（上图编号1所示:<strong>Bolukbasi T, Chang K W, Zou J, et al. Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings[J]. 2016.</strong>）的作者<strong>Tolga Bolukbasi</strong>、<strong>Kai-Wei Chang</strong>、<strong>James Zou</strong>、<strong>Venkatesh Saligrama</strong>和 <strong>Adam Kalai</strong>发现了一个十分可怕的结果，就是说一个已经完成学习的词嵌入可能会输出<strong>Man</strong>：<strong>Computer Programmer</strong>，同时输出<strong>Woman</strong>：<strong>Homemaker</strong>，那个结果看起来是错的，并且它执行了一个十分不良的性别歧视。如果算法输出的是<strong>Man</strong>：<strong>Computer Programmer</strong>，同时<strong>Woman</strong>：<strong>Computer Programmer</strong>这样子会更合理。同时他们也发现如果<strong>Father</strong>：<strong>Doctor</strong>，那么<strong>Mother</strong>应该对应什么呢？一个十分不幸的结果是，有些完成学习的词嵌入会输出<strong>Mother</strong>：<strong>Nurse</strong>。</p><p>因此根据训练模型所使用的文本，词嵌入能够反映出性别、种族、年龄、性取向等其他方面的偏见，一件我尤其热衷的事是，这些偏见都和社会经济状态相关，我认为每个人不论你出身富裕还是贫穷，亦或是二者之间，我认为每个人都应当拥有好的机会，同时因为机器学习算法正用来制定十分重要的决策，它也影响着世间万物，从大学录取到人们找工作的途径，到贷款申请，不论你的的贷款申请是否会被批准，再到刑事司法系统，甚至是判决标准，学习算法都在作出非常重要的决策，所以我认为我们尽量修改学习算法来尽可能减少或是理想化消除这些非预期类型的偏见是十分重要的。</p><p>至于词嵌入，它们能够轻易学会用来训练模型的文本中的偏见内容，所以算法获取到的偏见内容就可以反映出人们写作中的偏见。在漫长的世纪里，我认为人类已经在减少这些类型的偏见上取得了进展，幸运的是对于人工智能来说，实际上我认为有更好的办法来实现更快地减少<strong>AI</strong>领域中相比与人类社会中的偏见。虽然我认为我们仍未实现人工智能，仍然有许多研究许多难题需要完成来减少学习算法中这些类型的偏见。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/cf60f429ef532a2b3bbad3db98b054c5.png" alt="cf60f429ef532a2b3bbad3db98b054c5"><br>本节视频里我想要做的是与你们分享一个例子，它是一篇论文的一套办法，就是下面引用的这篇由<strong>Bolukbasi</strong>和其他人共同撰写的论文，它是研究减少词嵌入中偏见问题的。就是这些，假设说我们已经完成一个词嵌入的学习，那么<strong>babysitter</strong>就是在这里，<strong>doctor</strong>在这里，<strong>grandmother</strong>在这里，<strong>grandfather</strong>在这里，也许<strong>girl</strong>嵌入在这里，<strong>boy</strong>嵌入在这里，也许<strong>she</strong>嵌在这里，<strong>he</strong>在这里（上图编号1所示的区域内），所以首先我们要做的事就是辨别出我们想要减少或想要消除的特定偏见的趋势。</p><p>为了便于说明，我会集中讨论性别歧视，不过这些想法对于所有我在上个幻灯片里提及的其他类型的偏见都是通用的。这个例子中，你会怎样辨别出与这个偏见相似的趋势呢？主要有以下三个步骤：</p><p>一、对于性别歧视这种情况来说，我们能做的是$e_{\text{he} }-e_{\text{she} }$，因为它们的性别不同，然后将$e_{\text{male} }-e_{\text{female} }$，然后将这些值取平均（上图编号2所示），将这些差简单地求平均。这个趋势（上图编号3所示）看起来就是性别趋势或说是偏见趋势，然后这个趋势（上图编号4所示）与我们想要尝试处理的特定偏见并不相关，因此这就是个无偏见趋势。在这种情况下，偏见趋势可以将它看做<strong>1D</strong>子空间，所以这个无偏见趋势就会是<strong>299D</strong>的子空间。我已经略微简化了，原文章中的描述这个偏见趋势可以比1维更高，同时相比于取平均值，如同我在这里描述的这样，实际上它会用一个更加复杂的算法叫做<strong>SVU</strong>，也就是奇异值分解，如果你对主成分分析（<strong>Principle Component Analysis</strong>）很熟悉的话，奇异值分解这个算法的一些方法和主成分分析 (<strong>PCA</strong>)其实很类似。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/4102795b004ff090ed83dc654f585852.png" alt="4102795b004ff090ed83dc654f585852"><br>二、中和步骤，所以对于那些定义不确切的词可以将其处理一下，避免偏见。有些词本质上就和性别有关，像<strong>grandmother</strong>、<strong>grandfather</strong>、<strong>girl</strong>、<strong>boy</strong>、<strong>she</strong>、<strong>he</strong>，他们的定义中本就含有性别的内容，不过也有一些词像<strong>doctor</strong>和<strong>babysitter</strong>我们想使之在性别方面是中立的。同时在更通常的情况下，你可能会希望像<strong>doctor</strong>或<strong>babysitter</strong>这些词成为种族中立的，或是性取向中立的等等，不过这里我们仍然只用性别来举例说明。对于那些定义不明确的词，它的基本意思是不像<strong>grandmother</strong>和<strong>grandfather</strong>这种定义里有着十分合理的性别含义的，因为从定义上来说<strong>grandmothers</strong>是女性，<strong>grandfather</strong>是男性。所以对于像<strong>doctor</strong>和<strong>babysitter</strong>这种单词我们就可以将它们在这个轴（上图编号1所示）上进行处理，来减少或是消除他们的性别歧视趋势的成分，也就是说减少他们在这个水平方向上的距离（上图编号2方框内所示的投影），所以这就是第二个中和步。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/9b27d865dff73a2f10abbdc1c7fc966b.png" alt="9b27d865dff73a2f10abbdc1c7fc966b"><br>三、均衡步，意思是说你可能会有这样的词对，<strong>grandmother</strong>和<strong>grandfather</strong>，或者是<strong>girl</strong>和<strong>boy</strong>，对于这些词嵌入，你只希望性别是其区别。那为什么要那样呢？在这个例子中，<strong>babysitter</strong>和<strong>grandmother</strong>之间的距离或者说是相似度实际上是小于<strong>babysitter</strong>和<strong>grandfather</strong>之间的（上图编号1所示），因此这可能会加重不良状态，或者可能是非预期的偏见，也就是说<strong>grandmothers</strong>相比于<strong>grandfathers</strong>最终更有可能输出<strong>babysitting</strong>。所以在最后的均衡步中，我们想要确保的是像<strong>grandmother</strong>和<strong>grandfather</strong>这样的词都能够有一致的相似度，或者说是相等的距离，和<strong>babysitter</strong>或是<strong>doctor</strong>这样性别中立的词一样。这其中会有一些线性代数的步骤，但它主要做的就是将<strong>grandmother</strong>和<strong>grandfather</strong>移至与中间轴线等距的一对点上（上图编号2所示），现在性别歧视的影响也就是这两个词与<strong>babysitter</strong>的距离就完全相同了（上图编号3所示）。所以总体来说，会有许多对像<strong>grandmother-grandfather</strong>，<strong>boy-girl</strong>，<strong>sorority-fraternity</strong>，<strong>girlhood-boyhood</strong>，<strong>sister-brother</strong>，<strong>niece-nephew</strong>，<strong>daughter-son</strong>这样的词对，你可能想要通过均衡步来解决他们。</p><p>最后一个细节是你怎样才能够决定哪个词是中立的呢？对于这个例子来说<strong>doctor</strong>看起来像是一个应该对其中立的单词来使之性别不确定或是种族不确定。相反地，<strong>grandmother</strong>和<strong>grandfather</strong>就不应是性别不确定的词。也会有一些像是<strong>beard</strong>词，一个统计学上的事实是男性相比于比女性更有可能拥有胡子，因此也许<strong>beard</strong>应该比<strong>female</strong>更靠近<strong>male</strong>一些。</p><p>因此论文作者做的就是训练一个分类器来尝试解决哪些词是有明确定义的，哪些词是性别确定的，哪些词不是。结果表明英语里大部分词在性别方面上是没有明确定义的，意思就是说性别并是其定义的一部分，只有一小部分词像是<strong>grandmother-grandfather</strong>，<strong>girl-boy</strong>，<strong>sorority-fraternity</strong>等等，不是性别中立的。因此一个线性分类器能够告诉你哪些词能够通过中和步来预测这个偏见趋势，或将其与这个本质是<strong>299D</strong>的子空间进行处理。</p><p>最后，你需要平衡的词对的数实际上是很小的，至少对于性别歧视这个例子来说，用手都能够数出来你需要平衡的大部分词对。完整的算法会比我在这里展示的更复杂一些，你可以去看一下这篇论文了解详细内容，你也可以通过编程作业来练习一下这些想法。</p><p><strong>参考资料：针对性别特定词汇的均衡算法</strong></p><p>如何对两个单词除偏，比如：”<strong>actress</strong>“（“<strong>女演员</strong>”）和“<strong>actor</strong>”（“<strong>演员</strong>”）。 均衡算法适用于您可能希望仅通过性别属性不同的单词对。 举一个具体的例子，假设”<strong>actress</strong>“（“<strong>女演员</strong>”）比“<strong>actor</strong>”（“<strong>演员</strong>”）更接近“保姆”。 通过将中和应用于”<strong>babysit</strong>“（“<strong>保姆</strong>”），我们可以减少与保姆相关的性别刻板印象。 但是这仍然不能保证”<strong>actress</strong>“（“<strong>女演员</strong>”）和“<strong>actor</strong>”（“<strong>演员</strong>”）与”<strong>babysit</strong>“（“<strong>保姆</strong>”）等距。 均衡算法可以解决这个问题。</p><p>均衡背后的关键思想是确保一对特定的单词与49维$g_\perp$距离相等 。均衡步骤还可以确保两个均衡步骤现在与$e_{receptionist}^{debiased}$ 距离相同，或者用其他方法进行均衡。下图演示了均衡算法的工作原理：</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/equalize10.png" alt="equalize10"><br>公式的推导有点复杂(参考论文：<strong>Bolukbasi T, Chang K W, Zou J, et al. Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings[J]. 2016.</strong>)</p><p>主要步骤如下:</p> $ \mu = \frac{e_{w1} + e_{w2} }{2}\tag{1}$<p>$ \mu_{B} = \frac {\mu * \text{bias_axis} }{||\text{bias_axis}||_2} + ||\text{bias_axis}||_2 *\text{bias_axis}\tag{2}$</p> $\mu_{\perp} = \mu - \mu_{B} \tag{3}$<p>$e_{w1B} = \sqrt{ |{1 - ||\mu_{\perp} ||^2_2} |} * \frac{(e_{ {w1} } - \mu_{\perp}) - \mu_B} {|(e_{w1} - \mu_{\perp}) - \mu_B)|} \tag{4}$</p><p>$e_{w2B} = \sqrt{ |{1 - ||\mu_{\perp} ||^2_2} |} * \frac{(e_{\text{w2} } - \mu_{\perp}) - \mu_B} {|(e_{w2} - \mu_{\perp}) - \mu_B)|} \tag{5}$</p> $e_1 = e_{w1B} + \mu_{\perp} \tag{6}$ $e_2 = e_{w2B} + \mu_{\perp} \tag{7}$<p>总结一下，减少或者是消除学习算法中的偏见问题是个十分重要的问题，因为这些算法会用来辅助制定越来越多的社会中的重要决策，在本节视频中分享了一套如何尝试处理偏见问题的办法，不过这仍是一个许多学者正在进行主要研究的领域。</p><h2 id="序列模型和注意力机制（Sequence-models-amp-Attention-mechanism）"><a href="#序列模型和注意力机制（Sequence-models-amp-Attention-mechanism）" class="headerlink" title="序列模型和注意力机制（Sequence models &amp; Attention mechanism）**"></a>序列模型和注意力机制（<strong>Sequence models &amp; Attention mechanism</strong>）**</h2><h3 id="3-1-基础模型（Basic-Models）"><a href="#3-1-基础模型（Basic-Models）" class="headerlink" title="3.1 基础模型（Basic Models）"></a>3.1 基础模型（Basic Models）</h3><p>在这一周，你将会学习<strong>seq2seq</strong>（<strong>sequence to sequence</strong>）模型，从机器翻译到语音识别，它们都能起到很大的作用，从最基本的模型开始。之后你还会学习集束搜索（<strong>Beam search</strong>）和注意力模型（<strong>Attention Model</strong>），一直到最后的音频模型，比如语音。</p><p>现在就开始吧，比如你想通过输入一个法语句子，比如这句 “<strong>Jane visite I’Afrique en septembre.</strong>”，将它翻译成一个英语句子，“<strong>Jane is visiting Africa in September.</strong>”。和之前一样，我们用$x^{&lt;1&gt;}$ 一直到$x^{&lt; 5&gt;}$来表示输入的句子的单词，然后我们用$y^{&lt;1&gt;}$到$y^{&lt;6&gt;}$来表示输出的句子的单词，那么，如何训练出一个新的网络来输入序列$x$和输出序列$y$呢？</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/2d41c0090fd3d71e6f28eade62b7c97b.png" alt="2d41c0090fd3d71e6f28eade62b7c97b"><br>这里有一些方法，这些方法主要都来自于两篇论文，作者是<strong>Sutskever</strong>，<strong>Oriol Vinyals</strong> 和 <strong>Quoc Le</strong>，另一篇的作者是<strong>Kyunghyun Cho</strong>，<strong>Bart van Merrienboer</strong>，<strong>Caglar Gulcehre</strong>，<strong>Dzmitry Bahdanau</strong>，<strong>Fethi Bougares</strong>，<strong>Holger Schwen</strong> 和 <strong>Yoshua Bengio</strong>。</p><p>首先，我们先建立一个网络，这个网络叫做编码网络（<strong>encoder network</strong>）（上图编号1所示），它是一个<strong>RNN</strong>的结构， <strong>RNN</strong>的单元可以是<strong>GRU</strong> 也可以是<strong>LSTM</strong>。每次只向该网络中输入一个法语单词，将输入序列接收完毕后，这个<strong>RNN</strong>网络会输出一个向量来代表这个输入序列。之后你可以建立一个解码网络，我把它画出来（上图编号2所示），它以编码网络的输出作为输入，编码网络是左边的黑色部分（上图编号1所示），之后它可以被训练为每次输出一个翻译后的单词，一直到它输出序列的结尾或者句子结尾标记，这个解码网络的工作就结束了。和往常一样我们把每次生成的标记都传递到下一个单元中来进行预测，就像之前用语言模型合成文本时一样。</p><p>深度学习在近期最卓越的成果之一就是这个模型确实有效，在给出足够的法语和英语文本的情况下，如果你训练这个模型，通过输入一个法语句子来输出对应的英语翻译，这个模型将会非常有效。这个模型简单地用一个编码网络来对输入的法语句子进行编码，然后用一个解码网络来生成对应的英语翻译。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/b9492d18803ebe3853e936098f08661c.png" alt="b9492d18803ebe3853e936098f08661c"><br>还有一个与此类似的结构被用来做图像描述，给出一张图片，比如这张猫的图片（上图编号1所示），它能自动地输出该图片的描述，一只猫坐在椅子上，那么你如何训练出这样的网络？通过输入图像来输出描述，像这个句子一样。</p><p>方法如下，在之前的卷积网络课程中，你已经知道了如何将图片输入到卷积神经网络中，比如一个预训练的<strong>AlexNet</strong>结构（上图编号2方框所示），然后让其学习图片的编码，或者学习图片的一系列特征。现在幻灯片所展示的就是<strong>AlexNet</strong>结构，我们去掉最后的<strong>softmax</strong>单元（上图编号3所示），这个预训练的<strong>AlexNet</strong>结构会给你一个4096维的特征向量，向量表示的就是这只猫的图片，所以这个预训练网络可以是图像的编码网络。现在你得到了一个4096维的向量来表示这张图片，接着你可以把这个向量输入到<strong>RNN</strong>中（上图编号4方框所示），RNN要做的就是生成图像的描述，每次生成一个单词，这和我们在之前将法语译为英语的机器翻译中看到的结构很像，现在你输入一个描述输入的特征向量，然后让网络生成一个输出序列，或者说一个一个地输出单词序列。</p><p>事实证明在图像描述领域，这种方法相当有效，特别是当你想生成的描述不是特别长时。据我所知，这种模型首先是由<strong>Junhua Mao</strong>，<strong>Wei Xu</strong>，<strong>Yi Yang</strong>，<strong>Jiang Wang</strong>，<strong>Zhiheng Huang</strong>和<strong>Alan Yuille</strong>提出的，尽管有几个团队都几乎在同一时间构造出了非常相似的模型，因为还有另外两个团队也在同一时间得出了相似的结论。我觉得有可能<strong>Mao</strong>的团队和<strong>Oriol Vinyals</strong>，<strong>Alexander Toshev</strong>，<strong>Samy Bengio</strong>和<strong>Dumitru Erhan</strong>，还有<strong>Andrej Karpathy</strong>和<strong>Fei-Fei Y</strong>i是同一个团队。</p><p>现在你知道了基本的<strong>seq2seq</strong>模型是怎样运作的，以及<strong>image to sequence</strong>模型或者说图像描述模型是怎样运作的。不过这两个模型运作方式有一些不同，主要体现在如何用语言模型合成新的文本，并生成对应序列的方面。一个主要的区别就是你大概不会想得到一个随机选取的翻译，你想要的是最准确的翻译，或者说你可能不想要一个随机选取的描述，你想要的是最好的最贴切的描述，我们将在下节视频中介绍如何生成这些序列。</p><h3 id="3-2-选择最可能的句子（Picking-the-most-likely-sentence）"><a href="#3-2-选择最可能的句子（Picking-the-most-likely-sentence）" class="headerlink" title="3.2 选择最可能的句子（Picking the most likely sentence）"></a>3.2 选择最可能的句子（Picking the most likely sentence）</h3><p>在<strong>seq2seq</strong>机器翻译模型和我们在第一周课程所用的语言模型之间有很多相似的地方，但是它们之间也有许多重要的区别，让我们来一探究竟。</p><p>你可以把机器翻译想成是建立一个条件语言模型，在语言模型中上方是一个我们在第一周所建立的模型，这个模型可以让你能够估计句子的可能性，这就是语言模型所做的事情。你也可以将它用于生成一个新的句子，如果你在图上的该处（下图编号1所示），有$x^{&lt;1&gt;}$和$x^{&lt;2&gt;}$，那么在该例中$x^{&lt;2&gt;} = y^{&lt;1&gt;}$，但是$x^{&lt;1&gt;}$、$x^{&lt;2&gt;}$等在这里并不重要。为了让图片看起来更简洁，我把它们先抹去，可以理解为$x^{&lt;1&gt;}$是一个全为0的向量，然后$x^{&lt;2&gt;}$、$x^{&lt;3&gt;}$等都等于之前所生成的输出，这就是所说的语言模型。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/a8b8c64483ee84d57135829ab025da53.png" alt="a8b8c64483ee84d57135829ab025da53"><br>而机器翻译模型是下面这样的，我这里用两种不同的颜色来表示，即绿色和紫色，用绿色（上图编号2所示）表示<strong>encoder</strong>网络，用紫色（上图编号3所示）表示<strong>decoder</strong>网络。你会发现<strong>decoder</strong>网络看起来和刚才所画的语言模型几乎一模一样，机器翻译模型其实和语言模型非常相似，不同在于语言模型总是以零向量（上图编号4所示）开始，而<strong>encoder</strong>网络会计算出一系列向量（上图编号2所示）来表示输入的句子。有了这个输入句子，<strong>decoder</strong>网络就可以以这个句子开始，而不是以零向量开始，所以我把它叫做条件语言模型（<strong>conditional language model</strong>）。相比语言模型，输出任意句子的概率，翻译模型会输出句子的英文翻译（上图编号5所示），这取决于输入的法语句子（上图编号6所示）。换句话说，你将估计一个英文翻译的概率，比如估计这句英语翻译的概率，”<strong>Jane is visiting Africa in September.</strong>“，这句翻译是取决于法语句子，”<strong>Jane visite I’Afrique en septembre.</strong>“，这就是英语句子相对于输入的法语句子的可能性，所以它是一个条件语言模型。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/fd97f885fe1e6e1a7c70bb965595210a.png" alt="fd97f885fe1e6e1a7c70bb965595210a"><br>现在，假如你想真正地通过模型将法语翻译成英文，通过输入的法语句子模型将会告诉你各种英文翻译所对应的可能性。$x$在这里是法语句子”<strong>Jane visite l’Afrique en septembre.</strong>“，而它将告诉你不同的英语翻译所对应的概率。显然你不想让它随机地进行输出，如果你从这个分布中进行取样得到$P(y|x)$，可能取样一次就能得到很好的翻译，”<strong>Jane is visiting Africa in September.</strong>“。但是你可能也会得到一个截然不同的翻译，”<strong>Jane is going to be visiting Africa in September.</strong>“，这句话听起来有些笨拙，但它不是一个糟糕的翻译，只是不是最好的而已。有时你也会偶然地得到这样的翻译，”<strong>In September, Jane will visit Africa.</strong>“，或者有时候你还会得到一个很糟糕的翻译，”<strong>Her African friend welcomed Jane in September.</strong>“。所以当你使用这个模型来进行机器翻译时，你并不是从得到的分布中进行随机取样，而是你要找到一个英语句子$y$（上图编号1所示），使得条件概率最大化。所以在开发机器翻译系统时，你需要做的一件事就是想出一个算法，用来找出合适的$y$值，使得该项最大化，而解决这种问题最通用的算法就是束搜索(<strong>Beam Search</strong>)，你将会在下节课见到它。</p><p>不过在了解束搜索之前，你可能会问一个问题，为什么不用贪心搜索(<strong>Greedy Search</strong>)呢？贪心搜索是一种来自计算机科学的算法，生成第一个词的分布以后，它将会根据你的条件语言模型挑选出最有可能的第一个词进入你的机器翻译模型中，在挑选出第一个词之后它将会继续挑选出最有可能的第二个词，然后继续挑选第三个最有可能的词，这种算法就叫做贪心搜索，但是你真正需要的是一次性挑选出整个单词序列，从$y^{&lt;1&gt;}$、$y^{&lt;2&gt;}$到$y^{&lt;T_{y}&gt;}$来使得整体的概率最大化。所以这种贪心算法先挑出最好的第一个词，在这之后再挑最好的第二词，然后再挑第三个，这种方法其实并不管用，为了证明这个观点，我们来考虑下面两种翻译。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/c50a0e8948bbdb9a1d23f2f8a768bf5c.png" alt="c50a0e8948bbdb9a1d23f2f8a768bf5c"><br>第一串（上图编号1所示）翻译明显比第二个（上图编号2所示）好，所以我们希望机器翻译模型会说第一个句子的$P(y|x)$比第二个句子要高，第一个句子对于法语原文来说更好更简洁，虽然第二个也不错，但是有些啰嗦，里面有很多不重要的词。但如果贪心算法挑选出了”<strong>Jane is</strong>“作为前两个词，因为在英语中<strong>going</strong>更加常见，于是对于法语句子来说”<strong>Jane is going</strong>“相比”<strong>Jane is visiting</strong>“会有更高的概率作为法语的翻译，所以很有可能如果你仅仅根据前两个词来估计第三个词的可能性，得到的就是<strong>going</strong>，最终你会得到一个欠佳的句子，在$P(y|x)$模型中这不是一个最好的选择。</p><p>我知道这种说法可能比较粗略，但是它确实是一种广泛的现象，当你想得到单词序列$y^{&lt;1&gt;}$、$y^{&lt;2&gt;}$一直到最后一个词总体的概率时，一次仅仅挑选一个词并不是最佳的选择。当然，在英语中各种词汇的组合数量还有很多很多，如果你的字典中有10,000个单词，并且你的翻译可能有10个词那么长，那么可能的组合就有10,000的10次方这么多，这仅仅是10个单词的句子，从这样大一个字典中来挑选单词，所以可能的句子数量非常巨大，不可能去计算每一种组合的可能性。所以这时最常用的办法就是用一个近似的搜索算法，这个近似的搜索算法做的就是它会尽力地，尽管不一定总会成功，但它将挑选出句子$y$使得条件概率最大化，尽管它不能保证找到的$y$值一定可以使概率最大化，但这已经足够了。</p><p>最后总结一下，在本视频中，你看到了机器翻译是如何用来解决条件语言模型问题的，这个模型和之前的语言模型一个主要的区别就是，相比之前的模型随机地生成句子，在该模型中你要找到最有可能的英语句子，最可能的英语翻译，但是可能的句子组合数量过于巨大，无法一一列举，所以我们需要一种合适的搜索算法，让我们在下节课中学习集束搜索。</p><h3 id="3-3-集束搜索（Beam-Search）"><a href="#3-3-集束搜索（Beam-Search）" class="headerlink" title="3.3 集束搜索（Beam Search）"></a>3.3 集束搜索（Beam Search）</h3><p>这节视频中你会学到集束搜索（<strong>beam search</strong>）算法，上节视频中我们讲了对于机器翻译来说，给定输入，比如法语句子，你不会想要输出一个随机的英语翻译结果，你想要一个最好的，最可能的英语翻译结果。对于语音识别也一样，给定一个输入的语音片段，你不会想要一个随机的文本翻译结果，你想要最好的，最接近原意的翻译结果，集束搜索就是解决这个最常用的算法。这节视频里，你会明白怎么把集束搜索算法应用到你自己的工作中，就用我们的法语句子的例子来试一下集束搜索吧。</p><p>“<strong>Jane visite l’Afrique en Septembre.</strong>”（法语句子），我们希望翻译成英语，”<strong>Jane is visiting Africa in September</strong>“.（英语句子），集束搜索算法首先做的就是挑选要输出的英语翻译中的第一个单词。这里我列出了10,000个词的词汇表（下图编号1所示），为了简化问题，我们忽略大小写，所有的单词都以小写列出来。在集束搜索的第一步中我用这个网络部分，绿色是编码部分（下图编号2所示），紫色是解码部分（下图编号3所示），来评估第一个单词的概率值，给定输入序列$x$，即法语作为输入，第一个输出$y$的概率值是多少。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/8a22dfb5d3c0a4b5d2fdfa716dc3f3b2.png" alt="8a22dfb5d3c0a4b5d2fdfa716dc3f3b2"><br>贪婪算法只会挑出最可能的那一个单词，然后继续。而集束搜索则会考虑多个选择，集束搜索算法会有一个参数<strong>B</strong>，叫做集束宽（<strong>beam width</strong>）。在这个例子中我把这个集束宽设成3，这样就意味着集束搜索不会只考虑一个可能结果，而是一次会考虑3个，比如对第一个单词有不同选择的可能性，最后找到<strong>in</strong>、<strong>jane</strong>、<strong>september</strong>，是英语输出的第一个单词的最可能的三个选项，然后集束搜索算法会把结果存到计算机内存里以便后面尝试用这三个词。如果集束宽设的不一样，如果集束宽这个参数是10的话，那么我们跟踪的不仅仅3个，而是10个第一个单词的最可能的选择。所以要明白，为了执行集束搜索的第一步，你需要输入法语句子到编码网络，然后会解码这个网络，这个<strong>softmax</strong>层（上图编号3所示）会输出10,000个概率值，得到这10,000个输出的概率值，取前三个存起来。</p><p>让我们看看集束搜索算法的第二步，已经选出了<strong>in</strong>、<strong>jane</strong>、<strong>september</strong>作为第一个单词三个最可能的选择，集束算法接下来会针对每个第一个单词考虑第二个单词是什么，单词<strong>in</strong>后面的第二个单词可能是<strong>a</strong>或者是<strong>aaron</strong>，我就是从词汇表里把这些词列了出来，或者是列表里某个位置，<strong>september</strong>，可能是列表里的 <strong>visit</strong>，一直到字母<strong>z</strong>，最后一个单词是<strong>zulu</strong>（下图编号1所示）。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/14a940ae2ea7932b7b7190eceb79f79e.png" alt="14a940ae2ea7932b7b7190eceb79f79e"><br>为了评估第二个词的概率值，我们用这个神经网络的部分，绿色是编码部分（上图编号2所示），而对于解码部分，当决定单词<strong>in</strong>后面是什么，别忘了解码器的第一个输出$y^{&lt;1&gt;}$，我把$y^{&lt;1&gt;}$设为单词<strong>in</strong>（上图编号3所示），然后把它喂回来，这里就是单词<strong>in</strong>（上图编号4所示），因为它的目的是努力找出第一个单词是<strong>in</strong>的情况下，第二个单词是什么。这个输出就是$y^{&lt;2&gt;}$（上图编号5所示），有了这个连接（上图编号6所示），就是这里的第一个单词<strong>in</strong>（上图编号4所示）作为输入，这样这个网络就可以用来评估第二个单词的概率了，在给定法语句子和翻译结果的第一个单词<strong>in</strong>的情况下。</p><p>注意，在第二步里我们更关心的是要找到最可能的第一个和第二个单词对，所以不仅仅是第二个单词有最大的概率，而是第一个、第二个单词对有最大的概率（上图编号7所示）。按照条件概率的准则，这个可以表示成第一个单词的概率（上图编号8所示）乘以第二个单词的概率（上图编号9所示），这个可以从这个网络部分里得到（上图编号10所示），对于已经选择的<strong>in</strong>、<strong>jane</strong>、<strong>september</strong>这三个单词，你可以先保存这个概率值（上图编号8所示），然后再乘以第二个概率值（上图编号9所示）就得到了第一个和第二个单词对的概率（上图编号7所示）。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/be75f3d5f932151e26d9bcab05c92a93.png" alt="be75f3d5f932151e26d9bcab05c92a93"><br>现在你已经知道在第一个单词是in的情况下如何评估第二个单词的概率，现在第一个单词是<strong>jane</strong>，道理一样，句子可能是”<strong>jane a</strong>“、”<strong>jane aaron</strong>“，等等到”<strong>jane is</strong>“、”<strong>jane visits</strong>“等等（上图编号1所示）。你会用这个新的网络部分（上图编号2所示），我在这里画一条线，代表从$y^{&lt;1&gt;}$，即<strong>jane</strong>，$y^{&lt; 1 &gt;}$连接<strong>jane</strong>（上图编号3所示），那么这个网络部分就可以告诉你给定输入$x$和第一个词是<strong>jane</strong>下，第二个单词的概率了（上图编号4所示），和上面一样，你可以乘以$P(y^{&lt;1&gt;}|x)$得到$P(y^{&lt;1&gt;},y^{&lt;2&gt;}|x)$。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/507c9081ee77c686bb96a009248087fd.png" alt="507c9081ee77c686bb96a009248087fd"><br>针对第二个单词所有10,000个不同的选择，最后对于单词<strong>september</strong>也一样，从单词<strong>a</strong>到单词<strong>zulu</strong>，用这个网络部分，我把它画在这里。来看看如果第一个单词是<strong>september</strong>，第二个单词最可能是什么。所以对于集束搜索的第二步，由于我们一直用的集束宽为3，并且词汇表里有10,000个单词，那么最终我们会有3乘以10,000也就是30,000个可能的结果，因为这里（上图编号1所示）是10,000，这里（上图编号2所示）是10,000，这里（上图编号3所示）是10,000，就是集束宽乘以词汇表大小，你要做的就是评估这30,000个选择。按照第一个词和第二个词的概率，然后选出前三个，这样又减少了这30,000个可能性，又变成了3个，减少到集束宽的大小。假如这30,000个选择里最可能的是“<strong>in September</strong>”（上图编号4所示）和“<strong>jane is</strong>”（上图编号5所示），以及“<strong>jane visits</strong>”（上图编号6所示），画的有点乱，但这就是这30,000个选择里最可能的三个结果，集束搜索算法会保存这些结果，然后用于下一次集束搜索。</p><p>注意一件事情，如果集束搜索找到了第一个和第二个单词对最可能的三个选择是“<strong>in September</strong>”或者“<strong>jane is</strong>”或者“<strong>jane visits</strong>”，这就意味着我们去掉了<strong>september</strong>作为英语翻译结果的第一个单词的选择，所以我们的第一个单词现在减少到了两个可能结果，但是我们的集束宽是3，所以还是有$y^{&lt;1&gt;}$，$y^{&lt;2&gt;}$对的三个选择。</p><p>在我们进入集束搜索的第三步之前，我还想提醒一下因为我们的集束宽等于3，每一步我们都复制3个，同样的这种网络来评估部分句子和最后的结果，由于集束宽等于3，我们有三个网络副本（上图编号7所示），每个网络的第一个单词不同，而这三个网络可以高效地评估第二个单词所有的30,000个选择。所以不需要初始化30,000个网络副本，只需要使用3个网络的副本就可以快速的评估<strong>softmax</strong>的输出，即$y^{&lt;2&gt;}$的10,000个结果。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/6a0b785dd54fcbc439bd82794eeefcf8.png" alt="6a0b785dd54fcbc439bd82794eeefcf8"><br>让我们快速解释一下集束搜索的下一步，前面说过前两个单词最可能的选择是“<strong>in September</strong>”和“<strong>jane is</strong>”以及“<strong>jane visits</strong>”，对于每一对单词我们应该保存起来，给定输入$x$，即法语句子作为$x$的情况下，$y^{&lt;1&gt;}$和$y^{&lt;2&gt;}$的概率值和前面一样，现在我们考虑第三个单词是什么，可以是“<strong>in September a</strong>”，可以是“<strong>in September aaron</strong>”，一直到“<strong>in September zulu</strong>”。为了评估第三个单词可能的选择，我们用这个网络部分，第一单词是<strong>in</strong>（上图编号1所示），第二个单词是<strong>september</strong>（上图编号2所示），所以这个网络部分可以用来评估第三个单词的概率，在给定输入的法语句子$x$和给定的英语输出的前两个单词“<strong>in September</strong>”情况下（上图编号3所示）。对于第二个片段来说也一样，就像这样一样（上图编号4所示），对于“<strong>jane visits</strong>”也一样，然后集束搜索还是会挑选出针对前三个词的三个最可能的选择，可能是“<strong>in september jane</strong>”（上图编号5所示），“<strong>Jane is visiting</strong>”也很有可能（上图编号6所示），也很可能是“<strong>Jane visits Africa</strong>”（上图编号7所示）。</p><p>然后继续，接着进行集束搜索的第四步，再加一个单词继续，最终这个过程的输出一次增加一个单词，集束搜索最终会找到“<strong>Jane visits africa in september</strong>”这个句子，终止在句尾符号（上图编号8所示），用这种符号的系统非常常见，它们会发现这是最有可能输出的一个英语句子。在本周的练习中，你会看到更多的执行细节，同时，你会运用到这个集束算法，在集束宽为3时，集束搜索一次只考虑3个可能结果。注意如果集束宽等于1，只考虑1种可能结果，这实际上就变成了贪婪搜索算法，上个视频里我们已经讨论过了。但是如果同时考虑多个，可能的结果比如3个，10个或者其他的个数，集束搜索通常会找到比贪婪搜索更好的输出结果。</p><p>你已经了解集束搜索是如何工作的了，事实上还有一些额外的提示和技巧的改进能够使集束算法更高效，我们在下个视频中一探究竟。</p><h3 id="3-4-改进集束搜索（Refinements-to-Beam-Search）"><a href="#3-4-改进集束搜索（Refinements-to-Beam-Search）" class="headerlink" title="3.4 改进集束搜索（Refinements to Beam Search）"></a>3.4 改进集束搜索（Refinements to Beam Search）</h3><p>上个视频中, 你已经学到了基本的束搜索算法(<strong>the basic beam search algorithm</strong>)，这个视频里,我们会学到一些技巧, 能够使算法运行的更好。长度归一化（<strong>Length normalization</strong>）就是对束搜索算法稍作调整的一种方式，帮助你得到更好的结果，下面介绍一下它。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/725eec5b76123bf45c9495e1231b6584.png" alt="725eec5b76123bf45c9495e1231b6584"><br>前面讲到束搜索就是最大化这个概率，这个乘积就是$P(y^{&lt; 1 &gt;}\ldots y^{&lt; T_{y} }|X)$，可以表示成:$P(y^{&lt;1&gt;}|X)$ $P(y^{&lt; 2 &gt;}|X,y^{&lt; 1 &gt;})$ $P(y^{&lt; 3 &gt;}|X,y^{&lt; 1 &gt;},y^{&lt; 2&gt;})$…$P(y^{&lt; T_{y} &gt;}|X,y^{&lt;1 &gt;},y^{&lt;2 &gt;}\ldots y^{&lt; T_{y} - 1 &gt;})$</p><p>这些符号看起来可能比实际上吓人，但这就是我们之前见到的乘积概率（<strong>the product probabilities</strong>）。如果计算这些，其实这些概率值都是小于1的，通常远小于1。很多小于1的数乘起来，会得到很小很小的数字，会造成数值下溢（<strong>numerical underflow</strong>）。数值下溢就是数值太小了，导致电脑的浮点表示不能精确地储存，因此在实践中,我们不会最大化这个乘积，而是取$log$值。如果在这加上一个$log$，最大化这个$log$求和的概率值，在选择最可能的句子$y$时，你会得到同样的结果。所以通过取$log$，我们会得到一个数值上更稳定的算法，不容易出现四舍五入的误差，数值的舍入误差（<strong>rounding errors</strong>）或者说数值下溢（<strong>numerical underflow</strong>）。因为$log$函数它是严格单调递增的函数，最大化$P(y)$，因为对数函数，这就是$log$函数，是严格单调递增的函数，所以最大化$logP(y|x)$和最大化$P(y|x)$结果一样。如果一个$y$值能够使前者最大，就肯定能使后者也取最大。所以实际工作中，我们总是记录概率的对数和（<strong>the sum of logs of the probabilities</strong>），而不是概率的乘积（<strong>the production of probabilities</strong>）。</p><p>对于目标函数（<strong>this objective function</strong>），还可以做一些改变，可以使得机器翻译表现的更好。如果参照原来的目标函数（<strong>this original objective</strong>），如果有一个很长的句子，那么这个句子的概率会很低，因为乘了很多项小于1的数字来估计句子的概率。所以如果乘起来很多小于1的数字，那么就会得到一个更小的概率值，所以这个目标函数有一个缺点，它可能不自然地倾向于简短的翻译结果，它更偏向短的输出，因为短句子的概率是由更少数量的小于1的数字乘积得到的，所以这个乘积不会那么小。顺便说一下，这里也有同样的问题，概率的$log$值通常小于等于1，实际上在$log$的这个范围内，所以加起来的项越多，得到的结果越负，所以对这个算法另一个改变也可以使它表现的更好，也就是我们不再最大化这个目标函数了，我们可以把它归一化，通过除以翻译结果的单词数量（<strong>normalize this by the number of words in your translation</strong>）。这样就是取每个单词的概率对数值的平均了，这样很明显地减少了对输出长的结果的惩罚（<strong>this significantly reduces the penalty for outputting longer translations.</strong>）。</p><p>在实践中，有个探索性的方法，相比于直接除$T_{y}$，也就是输出句子的单词总数，我们有时会用一个更柔和的方法（<strong>a softer approach</strong>），在$T_{y}$上加上指数$a$，$a$可以等于0.7。如果$a$等于1，就相当于完全用长度来归一化，如果$a$等于0，$T_{y}$的0次幂就是1，就相当于完全没有归一化，这就是在完全归一化和没有归一化之间。$a$就是算法另一个超参数（<strong>hyper parameter</strong>），需要调整大小来得到最好的结果。不得不承认，这样用$a$实际上是试探性的，它并没有理论验证。但是大家都发现效果很好，大家都发现实践中效果不错，所以很多人都会这么做。你可以尝试不同的$a$值，看看哪一个能够得到最好的结果。</p><p>总结一下如何运行束搜索算法。当你运行束搜索时，你会看到很多长度等于1的句子，很多长度等于2的句子，很多长度等于3的句子，等等。可能运行束搜索30步，考虑输出的句子可能达到，比如长度30。因为束宽为3，你会记录所有这些可能的句子长度，长度为1、2、 3、 4 等等一直到30的三个最可能的选择。然后针对这些所有的可能的输出句子，用这个式子（上图编号1所示）给它们打分，取概率最大的几个句子，然后对这些束搜索得到的句子，计算这个目标函数。最后从经过评估的这些句子中，挑选出在归一化的$log$ 概率目标函数上得分最高的一个（<strong>you pick the one that achieves the highest value on this normalized log probability objective.</strong>），有时这个也叫作<strong>归一化的对数似然目标函数</strong>（<strong>a normalized log likelihood objective</strong>）。这就是最终输出的翻译结果，这就是如何实现束搜索。这周的练习中你会自己实现这个算法。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/96e50aec6e1d7287e5bac70a0c4d92f4.png" alt="96e50aec6e1d7287e5bac70a0c4d92f4"><br>最后还有一些实现的细节，如何选择束宽<strong>B</strong>。<strong>B</strong>越大，你考虑的选择越多，你找到的句子可能越好，但是<strong>B</strong>越大，你的算法的计算代价越大，因为你要把很多的可能选择保存起来。最后我们总结一下关于如何选择束宽<strong>B</strong>的一些想法。接下来是针对或大或小的<strong>B</strong>各自的优缺点。如果束宽很大，你会考虑很多的可能，你会得到一个更好的结果，因为你要考虑很多的选择，但是算法会运行的慢一些，内存占用也会增大，计算起来会慢一点。而如果你用小的束宽，结果会没那么好，因为你在算法运行中，保存的选择更少，但是你的算法运行的更快，内存占用也小。在前面视频里，<strong>我们例子中用了束宽为3，所以会保存3个可能选择，在实践中这个值有点偏小</strong>。<strong>在产品中，经常可以看到把束宽设到10，</strong>我认为束宽为100对于产品系统来说有点大了，这也取决于不同应用。但是对科研而言，人们想压榨出全部性能，这样有个最好的结果用来发论文，也经常看到大家用束宽为1000或者3000，这也是取决于特定的应用和特定的领域。在你实现你的应用时，尝试不同的束宽的值，当B很大的时候，性能提高会越来越少。对于很多应用来说，从束宽1，也就是贪心算法，到束宽为3、到10，你会看到一个很大的改善。但是当束宽从1000增加到3000时，效果就没那么明显了。对于之前上过计算机科学课程的同学来说，如果你熟悉计算机科学里的搜索算法（<strong>computer science search algorithms</strong>）, 比如广度优先搜索（<strong>BFS, Breadth First Search algorithms</strong>），或者深度优先搜索（<strong>DFS, Depth First Search</strong>），你可以这样想束搜索，不像其他你在计算机科学算法课程中学到的算法一样。如果你没听说过这些算法也不要紧，但是如果你听说过广度优先搜索和深度优先搜索，不同于这些算法，这些都是精确的搜索算法（<strong>exact search algorithms</strong>），束搜索运行的更快，但是不能保证一定能找到<strong>argmax</strong>的准确的最大值。如果你没听说过广度优先搜索和深度优先搜索，也不用担心，这些对于我们的目标也不重要，如果你听说过，这就是束搜索和其他算法的关系。</p><p>好，这就是束搜索。这个算法广泛应用在多产品系统或者许多商业系统上，在深度学习系列课程中的第三门课中，我们讨论了很多关于误差分析（<strong>error analysis</strong>）的问题。事实上在束搜索上做误差分析是我发现的最有用的工具之一。有时你想知道是否应该增大束宽，我的束宽是否足够好，你可以计算一些简单的东西来指导你需要做什么，来改进你的搜索算法。我们在下个视频里进一步讨论。</p><h3 id="3-5-集束搜索的误差分析（Error-analysis-in-beam-search）"><a href="#3-5-集束搜索的误差分析（Error-analysis-in-beam-search）" class="headerlink" title="3.5 集束搜索的误差分析（Error analysis in beam search）"></a>3.5 集束搜索的误差分析（Error analysis in beam search）</h3><p>在这五门课中的第三门课里，你了解了误差分析是如何能够帮助你集中时间做你的项目中最有用的工作，束搜索算法是一种近似搜索算法（<strong>an approximate search algorithm</strong>），也被称作启发式搜索算法（<strong>a heuristic search algorithm</strong>），它不总是输出可能性最大的句子，它仅记录着<strong>B</strong>为前3或者10或是100种可能。那么如果束搜索算法出现错误会怎样呢?</p><p>本节视频中，你将会学习到误差分析和束搜索算法是如何相互起作用的，以及你怎样才能发现是束搜索算法出现了问题，需要花时间解决，还是你的<strong>RNN</strong>模型出了问题，要花时间解决。我们先来看看如何对束搜索算法进行误差分析。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/748283d682db5be4855e61b90e96c427.png" alt="748283d682db5be4855e61b90e96c427"><br>我们来用这个例子说明： “<strong>Jane visite l’Afrique en septembre</strong>”。假如说，在你的机器翻译的<strong>dev</strong>集中，也就是开发集（<strong>development set</strong>），人工是这样翻译的: <strong>Jane visits Africa in September</strong>,我会将这个标记为$y^<em>$。这是一个十分不错的人工翻译结果，不过假如说，当你在已经完成学习的*</em>RNN<strong>模型，也就是已完成学习的翻译模型中运行束搜索算法时，它输出了这个翻译结果：</strong>Jane visited Africa last September**，我们将它标记为$\hat y$。这是一个十分糟糕的翻译，它实际上改变了句子的原意，因此这不是个好翻译。</p><p>你的模型有两个主要部分，一个是神经网络模型，或说是序列到序列模型（<strong>sequence to sequence model</strong>），我们将这个称作是<strong>RNN</strong>模型，它实际上是个编码器和解码器（ <strong>an encoder and a decoder</strong>）。另一部分是束搜索算法，以某个集束宽度<strong>B</strong>运行。如果你能够找出造成这个错误，这个不太好的翻译的原因，是两个部分中的哪一个，不是很好吗? <strong>RNN</strong> (<strong>循环神经网络</strong>)是更可能是出错的原因呢，还是束搜索算法更可能是出错的原因呢？你在第三门课中了解到了大家很容易想到去收集更多的训练数据，这总归没什么坏处。所以同样的，大家也会觉得不行就增大束宽，也是不会错的，或者说是很大可能是没有危害的。但是就像单纯获取更多训练数据，可能并不能得到预期的表现结果。相同的，单纯增大束宽也可能得不到你想要的结果，不过你怎样才能知道是不是值得花时间去改进搜索算法呢?<br>下面我们来分解这个问题弄清楚什么情况下该用什么解决办法。</p><p><strong>RNN</strong> (<strong>循环神经网络</strong>)实际上是个编码器和解码器（<strong>the encoder and the decoder</strong>），它会计算$P(y|x)$。所以举个例子，对于这个句子：<strong>Jane visits Africa in September</strong>，你将<strong>Jane visits Africa</strong>填入这里（上图编号1所示），同样，我现在忽略了字母的大小写，后面也是一样，然后这个就会计算。$P(y|x)$结果表明，你此时能做的最有效的事就是用这个模型来计算$P(y^<em>|x)$，同时也用你的RNN模型来计算$P(\hat y|x)$，然后比较一下这两个值哪个更大。有可能是左边大于右边，也有可能是$P(y^</em>)$小于$P(\hat y)$，其实应该是小于或等于，对吧。取决于实际是哪种情况，你就能够更清楚地将这个特定的错误归咎于<strong>RNN</strong>或是束搜索算法，或说是哪个负有更大的责任。我们来探究一下其中的逻辑。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1bc0b442db9d5a1aa19dfe9a477a3c3e.png" alt="1bc0b442db9d5a1aa19dfe9a477a3c3e"><br>这是之前幻灯片里的两个句子。记住，我们是要计算$P(y^*|x)$和$P(\hat y|x)$，然后比较这两个哪个更大，所以就会有两种情况。</p><p>第一种情况，<strong>RNN</strong>模型的输出结果$P(y^<em>|x)$ 大于$P(\hat y|x)$，这意味着什么呢?<br>束搜索算法选择了$\hat y$ ，对吧?<br>你得到$\hat y$的方式是，你用一个<strong>RNN</strong>模型来计算$P(y|x)$，然后束搜索算法做的就是尝试寻找使$P(y|x)$最大的$y$，不过在这种情况下，相比于$\hat y$，$y^</em>$的值更$P(y|x)$大，因此你能够得出束搜索算法实际上不能够给你一个能使$P(y|x)$最大化的$y$值，因为束搜索算法的任务就是寻找一个$y$的值来使这项更大，但是它却选择了$\hat y$，而$y^*$实际上能得到更大的值。因此这种情况下你能够得出是束搜索算法出错了。那另一种情况是怎样的呢?</p><p>第二种情况是$P(y^<em>|x)$小于或等于$P(\hat y|x)$对吧？这两者之中总有一个是真的。情况1或是情况2总有一个为真。情况2你能够总结出什么呢?<br>在我们的例子中，$y^</em>$ 是比 $\hat y$更好的翻译结果，不过根据RNN模型的结果，$P(y^<em>)$ 是小于$P(\hat y)$的，也就是说，相比于$\hat y$，$y^</em>$成为输出的可能更小。因此在这种情况下，看来是<strong>RNN</strong>模型出了问题。同时可能值得在<strong>RNN</strong>模型上花更多时间。这里我少讲了一些有关长度归一化（<strong>length normalizations</strong>）的细节。这里我略过了有关长度归一化的细节，如果你用了某种长度归一化，那么你要做的就不是比较这两种可能性大小，而是比较长度归一化后的最优化目标函数值。不过现在先忽略这种复杂的情况。第二种情况表明虽然$y^<em>$是一个更好的翻译结果，*</em>RNN<strong>模型却赋予它更低的可能性，是</strong>RNN**模型出现了问题。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/2689876529562b7d9a79bf868e7cbad7.png" alt="2689876529562b7d9a79bf868e7cbad7"><br>所以误差分析过程看起来就像下面这样。你先遍历开发集，然后在其中找出算法产生的错误，这个例子中，假如说$P(y^<em>|x)$的值为2 x 10-10，而$P(\hat y|x)$的值为 1 x10-10，根据上页幻灯片中的逻辑关系，这种情况下我们得知束搜索算法实际上选择了比$y^</em>$可能性更低的$\hat y$，因此我会说束搜索算法出错了。我将它缩写为<strong>B</strong>。接着你继续遍历第二个错误，再来看这些可能性。也许对于第二个例子来说，你认为是<strong>RNN</strong>模型出现了问题，我会用缩写<strong>R</strong>来代表<strong>RNN</strong>。再接着你遍历了更多的例子，有时是束搜索算法出现了问题，有时是模型出现了问题，等等。通过这个过程，你就能够执行误差分析，得出束搜索算法和<strong>RNN</strong>模型出错的比例是多少。有了这样的误差分析过程，你就可以对开发集中每一个错误例子，即算法输出了比人工翻译更差的结果的情况，尝试确定这些错误，是搜索算法出了问题，还是生成目标函数(束搜索算法使之最大化)的<strong>RNN</strong>模型出了问题。并且通过这个过程，你能够发现这两个部分中哪个是产生更多错误的原因，并且只有当你发现是束搜索算法造成了大部分错误时，才值得花费努力增大集束宽度。相反地，如果你发现是<strong>RNN</strong>模型出了更多错，那么你可以进行更深层次的分析，来决定是需要增加正则化还是获取更多的训练数据，抑或是尝试一个不同的网络结构，或是其他方案。你在第三门课中，了解到各种技巧都能够应用在这里。</p><p>这就是束搜索算法中的误差分析，我认为这个特定的误差分析过程是十分有用的，它可以用于分析近似最佳算法(如束搜索算法)，这些算法被用来优化学习算法(例如序列到序列模型/<strong>RNN</strong>)输出的目标函数。也就是我们这些课中一直讨论的。学会了这个方法，我希望你能够在你的应用里更有效地运用好这些类型的模型。</p><h3 id="3-6-Bleu-得分（选修）（Bleu-Score-optional-）"><a href="#3-6-Bleu-得分（选修）（Bleu-Score-optional-）" class="headerlink" title="3.6 Bleu 得分（选修）（Bleu Score (optional)）"></a>3.6 Bleu 得分（选修）（Bleu Score (optional)）</h3><p>机器翻译（<strong>machine translation</strong>）的一大难题是一个法语句子可以有多种英文翻译而且都同样好，所以当有多个同样好的答案时，怎样评估一个机器翻译系统呢？不像图像识别（<strong>image recognition</strong>），只有一个正确答案，就只要测量准确性就可以了。如果有多个不错的答案，要怎样衡量准确性呢?<br>常见的解决办法是，通过一个叫做<strong>BLEU</strong>得分（<strong>the BLEU score</strong>）的东西来解决。所以，在这个选修视频中，我想与你分享，我想让你了解<strong>BLEU</strong>得分是怎样工作的。</p><p>假如给你一个法语句子：<strong>Le chat est sur le tapis</strong>，然后给你一个这个句子的人工翻译作参考：<strong>The cat is on the mat</strong>。不过有多种相当不错的翻译。所以一个不同的人，也许会将其翻译为：<strong>There is a cat on the mat</strong>，同时，实际上这两个都是很好的，都准确地翻译了这个法语句子。<strong>BLEU得分做的就是，给定一个机器生成的翻译，它能够自动地计算一个分数来衡量机器翻译的好坏。</strong>直觉告诉我们，只要这个机器生成的翻译与任何一个人工翻译的结果足够接近，那么它就会得到一个高的<strong>BLEU</strong>分数。顺便提一下<strong>BLEU</strong>代表<strong>bilingual evaluation understudy</strong> (双语评估替补)。在戏剧界，侯补演员(<strong>understudy</strong>)学习资深的演员的角色，这样在必要的时候，他们就能够接替这些资深演员。而<strong>BLEU</strong>的初衷是相对于请评估员（<strong>ask human evaluators</strong>），人工评估机器翻译系统（<strong>the machine translation system</strong>），<strong>BLEU</strong>得分就相当于一个侯补者，它可以代替人类来评估机器翻译的每一个输出结果。<strong>BLEU</strong>得分是由<strong>Kishore Papineni,</strong> <strong>Salim Roukos</strong>，<strong>Todd Ward</strong>和<strong>Wei-Jing Zhu</strong>发表的这篇论文十分有影响力并且实际上也是一篇很好读的文章。所以如果有时间的话，我推荐你读一下。<strong>BLEU</strong>得分背后的理念是观察机器生成的翻译，然后看生成的词是否出现在少一个人工翻译参考之中。因此这些人工翻译的参考会包含在开发集或是测试集中。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/5e854a803e36991a6e0dd4e33ecab930.png" alt="5e854a803e36991a6e0dd4e33ecab930"><br>（<strong>参考论文</strong>：<strong>Papineni, Kishore&amp; Roukos, Salim &amp; Ward, Todd &amp; Zhu, Wei-jing. (2002). BLEU: a Method for Automatic Evaluation of Machine Translation.10.3115/1073083.1073135.</strong>）</p><p>现在，我们来看一个极端的例子。我们假设机器翻译系统缩写为<strong>MT</strong>。机器翻译 (<strong>MT</strong>)的输出是：<strong>the the the the the the the</strong>。这显然是一个十分糟糕的翻译。衡量机器翻译输出质量的方法之一是观察输出结果的每一个词看其是否出现在参考中，这被称做是机器翻译的精确度（<strong>a precision of the machine translation output</strong>）。这个情况下，机器翻译输出了七个单词并且这七个词中的每一个都出现在了参考1或是参考2。单词<strong>the</strong>在两个参考中都出现了，所以看上去每个词都是很合理的。因此这个输出的精确度就是7/7，看起来是一个极好的精确度。这就是为什么把出现在参考中的词在<strong>MT</strong>输出的所有词中所占的比例作为精确度评估标准并不是很有用的原因。因为它似乎意味着，例子中<strong>MT</strong>输出的翻译有很高的精确度，因此取而代之的是我们要用的这个改良后的精确度评估方法，我们把每一个单词的记分上限定为它在参考句子中出现的最多次数。在参考1中，单词<strong>the</strong>出现了两次，在参考2中，单词<strong>the</strong>只出现了一次。而2比1大，所以我们会说，单词<strong>the</strong>的得分上限为2。有了这个改良后的精确度，我们就说，这个输出句子的得分为2/7，因为在7个词中，我们最多只能给它2分。所以这里分母就是7个词中单词<strong>the</strong>总共出现的次数，而分子就是单词<strong>the</strong>出现的计数。我们在达到上限时截断计数，这就是改良后的精确度评估（<strong>the modified precision measure</strong>）。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/0bc25316900ccd1d4dd25a35ec7c45c4.png" alt="0bc25316900ccd1d4dd25a35ec7c45c4"><br>到目前为止，我们都只是关注单独的单词，在<strong>BLEU</strong>得分中，你不想仅仅考虑单个的单词，你也许也想考虑成对的单词，我们定义一下二元词组（<strong>bigrams</strong>）的<strong>BLEU</strong>得分。bigram的意思就是相邻的两个单词。现在我们来看看怎样用二元词组来定义<strong>BLEU</strong>得分，并且这仅仅只是最终的BLEU得分的一部分。我们会考虑一元词组（<strong>unigrams</strong>）也就是单个单词以及二元词组（<strong>bigrams</strong>），即成对的词，同时也许会有更长的单词序列，比如说三元词组（<strong>trigrams</strong>）。意思是三个挨在一起的词。我们继续刚才的例子，还是前面出现过的参考1和2，不过现在我们假定机器翻译输出了稍微好一点的翻译:<strong>The cat the cat on the mat</strong>，仍然不是一个好的翻译，不过也许比上一个好一些。这里，可能的二元词组有<strong>the cat</strong> ，忽略大小写，接着是<strong>cat the</strong>， 这是另一个二元词组，然后又是<strong>the cat</strong>。不过我已经有了，所以我们跳过它，然后下一个是<strong>cat on</strong>，然后是<strong>on the</strong>，再然后是<strong>the mat</strong>。所以这些就是机器翻译中的二元词组。好，我们来数一数每个二元词组出现了多少次。<strong>the cat</strong>出现了两次 ，<strong>cat the</strong>出现了一次，剩下的都只出现了一次。最后 ，我们来定义一下截取计数（<strong>the clipped count</strong>）。也就是<strong>Count_clip</strong>。为了定义它，我们以这列的值为基础，但是给算法设置得分上限，上限值为二元词组出现在参考1或2中的最大次数。<strong>the cat</strong>在两个参考中最多出现一次，所以我将截取它的计数为1。<strong>cat the</strong>它并没有出现在参考1和参考2中，所以我将它截取为0。<strong>cat on</strong> ，好，它出现了一次，我们就记1分。<strong>on the</strong>出现一次就记1分，<strong>the mat</strong>出现了一次，所以这些就是截取完的计数（<strong>the clipped counts</strong>）。我们把所有的这些计数都截取了一遍，实际上就是将它们降低使之不大于二元词组出现在参考中的次数。最后，修改后的二元词组的精确度就是<strong>count_clip</strong>之和。因此那就是4除以二元词组的总个数，也就是6。因此是4/6也就是2/3为二元词组改良后的精确度。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/7f48951acf8ca7c7b63f6c4c455ada18.png" alt="7f48951acf8ca7c7b63f6c4c455ada18"><br>现在我们将它公式化。基于我们在一元词组中学到的内容，我们将改良后的一元词组精确度定义为$P_1$，$P$代表的是精确度。这里的下标1的意思是一元词组。不过它定义为一元词组之和，也就是对机器翻译结果中所有单词求和，<strong>MT</strong> 输出就是$\hat y$，<strong>Countclip</strong>(<strong>unigram</strong>)。除以机器翻译输出中的一元词组出现次数之和。因此这个就是最终结果应该是两页幻灯片前得到的2/7。这里的1指代的是一元词组，意思是我们在考虑单独的词，你也可以定义$P_n$为$n$元词组精确度，用<strong>n-gram</strong>替代掉一元词组。所以这就是机器翻译输出中的$n$元词组的<strong>countclip</strong>之和除以$n$元词组的出现次数之和。因此这些精确度或说是这些改良后的精确度得分评估的是一元词组或是二元词组。就是我们前页幻灯片中做的，或者是三元词组，也就是由三个词组成的，甚至是$n$取更大数值的$n$元词组。这个方法都能够让你衡量机器翻译输出中与参考相似重复的程度。另外，你能够确信如果机器翻译输出与参考1或是参考2完全一致的话，那么所有的这些$P_1$、$P_2$等等的值，都会等于1.0。为了得到改良后的1.0的精确度，只要你的输出与参考之一完全相同就能满足，不过有时即使输出结果并不完全与参考相同，这也是有可能实现的。你可以将它们以另一种方式组合，但愿仍能得到不错的翻译结果。</p><p><img src="https://markdown.xiaoshujiang.com/img/spinner.gif" alt="0f9646d825a0c254376e094b48523ed3" title="[[[1575537462610]]]"><br>最后，我们将这些组合一下来构成最终的<strong>BLEU</strong>得分。$P_n$就是$n$元词组这一项的<strong>BLEU</strong>得分，也是计算出的$n$元词组改良后的精确度，按照惯例，为了用一个值来表示你需要计算$P_1$，$P_2$， $P_3$，$P_4$。然后将它们用这个公式组合在一起，就是取平均值。按照惯例<strong>BLEU</strong>得分被定义为，$exp (\frac{1}{4}\sum\limits_{n=1}^{4}{ {{P}_{n} }})$，对这个线性运算进行乘方运算，乘方是严格单调递增的运算，我们实际上会用额外的一个叫做<strong>BP</strong> 的惩罚因子（<strong>the BP penalty</strong>）来调整这项。<strong>BP</strong>的意思是“简短惩罚”（ <strong>brevity penalty</strong>）。这些细节也许并不是十分重要，但是你可以大致了解一下。<br>事实表明，如果你输出了一个非常短的翻译，那么它会更容易得到一个高精确度。因为输出的大部分词可能都出现在参考之中，不过我们并不想要特别短的翻译结果。因此简短惩罚(<strong>BP</strong>)就是一个调整因子，它能够惩罚输出了太短翻译结果的翻译系统。<strong>BP</strong>的公式如上图所示。如果你的机器翻译系统实际上输出了比人工翻译结果更长的翻译，那么它就等于1，其他情况下就是像这样的公式，惩罚所有更短的翻译，细节部分你能够在这篇论文中找到。</p><p>再说一句，在之前的视频中，你了解了拥有单一实数评估指标（<strong>a single real number evaluation metric</strong>）的重要性，因为它能够让你尝试两种想法，然后看一下哪个得分更高，尽量选择得分更高的那个，<strong>BLEU</strong>得分对于机器翻译来说，具有革命性的原因是因为它有一个相当不错的虽然不是完美的但是非常好的单一实数评估指标，因此它加快了整个机器翻译领域的进程，我希望这节视频能够让你了解<strong>BLEU</strong>得分是如何操作的。实践中，很少人会从零实现一个<strong>BLEU</strong>得分（<strong>implement a BLEU score from scratch</strong>），有很多开源的实现结果，你可以下载下来然后直接用来评估你的系统。不过今天，<strong>BLEU</strong>得分被用来评估许多生成文本的系统（<strong>systems that generate text</strong>），比如说机器翻译系统（<strong>machine translation systems</strong>），也有我之前简单提到的图像描述系统（<strong>image captioning systems</strong>）。也就是说你会用神经网络来生成图像描述，然后使用<strong>BLEU</strong>得分来看一下，结果在多大程度上与参考描述或是多个人工完成的参考描述内容相符。<strong>BLEU</strong>得分是一个有用的单一实数评估指标，用于评估生成文本的算法，判断输出的结果是否与人工写出的参考文本的含义相似。不过它并没有用于语音识别（<strong>speech recognition</strong>）。因为在语音识别当中，通常只有一个答案，你可以用其他的评估方法，来看一下你的语音识别结果，是否十分相近或是字字正确（<strong>pretty much, exactly word for word correct</strong>）。不过在图像描述应用中，对于同一图片的不同描述，可能是同样好的。或者对于机器翻译来说，有多个一样好的翻译结果，<strong>BLEU</strong>得分就给了你一个能够自动评估的方法，帮助加快算法开发进程。说了这么多，希望你明白了<strong>BLEU</strong>得分是怎么运行的。</p><h3 id="3-7-注意力模型直观理解（Attention-Model-Intuition）"><a href="#3-7-注意力模型直观理解（Attention-Model-Intuition）" class="headerlink" title="3.7 注意力模型直观理解（Attention Model Intuition）"></a>3.7 注意力模型直观理解（Attention Model Intuition）</h3><p>在本周大部分时间中，你都在使用这个编码解码的构架（<strong>a Encoder-Decoder architecture</strong>）来完成机器翻译。当你使用<strong>RNN</strong>读一个句子，于是另一个会输出一个句子。我们要对其做一些改变，称为注意力模型（<strong>the Attention Model</strong>），并且这会使它工作得更好。注意力模型或者说注意力这种思想（<strong>The attention algorithm, the attention idea</strong>）已经是深度学习中最重要的思想之一，我们看看它是怎么运作的。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/59279ff91bb69a94280e6735eba8ab99.png" alt="59279ff91bb69a94280e6735eba8ab99"><br>像这样给定一个很长的法语句子，在你的神经网络中，这个绿色的编码器要做的就是读整个句子，然后记忆整个句子，再在感知机中传递（<strong>to read in the whole sentence and then memorize the whole sentences and store it in the activations conveyed her</strong>）。而对于这个紫色的神经网络，即解码网络（<strong>the decoder network</strong>）将生成英文翻译，<strong>Jane</strong>去年九月去了非洲，非常享受非洲文化，遇到了很多奇妙的人，她回来就嚷嚷道，她经历了一个多棒的旅行，并邀请我也一起去。人工翻译并不会通过读整个法语句子，再记忆里面的东西，然后从零开始，机械式地翻译成一个英语句子。而人工翻译，首先会做的可能是先翻译出句子的部分，再看下一部分，并翻译这一部分。看一部分，翻译一部分，一直这样下去。你会通过句子，一点一点地翻译，因为记忆整个的像这样的的句子是非常困难的。你在下面这个编码解码结构中，会看到它对于短句子效果非常好，于是它会有一个相对高的<strong>Bleu</strong>分（<strong>Bleu score</strong>），但是对于长句子而言，比如说大于30或者40词的句子，它的表现就会变差。<strong>Bleu</strong>评分看起来就会像是这样，随着单词数量变化，短的句子会难以翻译，因为很难得到所有词。对于长的句子，效果也不好，因为在神经网络中，记忆非常长句子是非常困难的。在这个和下个视频中，你会见识到注意力模型，它翻译得很像人类，一次翻译句子的一部分。而且有了注意力模型，机器翻译系统的表现会像这个一样，因为翻译只会翻译句子的一部分，你不会看到这个有一个巨大的下倾（<strong>huge dip</strong>），这个下倾实际上衡量了神经网络记忆一个长句子的能力，这是我们不希望神经网络去做的事情。在这个视频中，我想要给你们注意力机制运行的一些直观的东西。然后在下个视频中，完善细节。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/3dcdd58eaa544a09e67eb892f8c732bf.png" alt="3dcdd58eaa544a09e67eb892f8c732bf"><br>注意力模型源于<strong>Dimitri,</strong> <strong>Bahdanau</strong>, <strong>Camcrun Cho</strong>, <strong>Yoshe Bengio</strong>。（<strong>Bahdanau D, Cho K, Bengio Y. Neural Machine Translation by Jointly Learning to Align and Translate[J]. Computer Science,2014.</strong>）虽然这个模型源于机器翻译，但它也推广到了其他应用领域。我认为在深度学习领域，这个是个非常有影响力的，非常具有开创性的论文。</p><p>让我们用一个短句举例说明一下，即使这些思想可能是应用于更长的句子。但是用短句来举例说明，讲解这些思想会更简单。我们有一个很平常的句子：(法语)<strong>Jane visite l’Afrique en Septembre</strong>。假定我们使用<strong>RNN</strong>，在这个情况中，我们将使用一个双向的<strong>RNN</strong>（<strong>a bidirectional RNN</strong>），为了计算每个输入单词的的特征集（<strong>set of features</strong>），你必须要理解输出$\hat y^{&lt;1&gt;}$到$\hat y^{&lt;3&gt;}$一直到$\hat y^{&lt;5&gt;}$的双向<strong>RNN</strong>。但是我们并不是只翻译一个单词，让我们先去掉上面的$Y$，就用双向的<strong>RNN</strong>。我们要对单词做的就是，对于句子里的每五个单词，计算一个句子中单词的特征集，也有可能是周围的词，让我们试试，生成英文翻译。我们将使用另一个<strong>RNN</strong>生成英文翻译，这是我平时用的<strong>RNN</strong>记号。我不用$A$来表示感知机（<strong>the activation</strong>），这是为了避免和这里的感知机（<strong>the activations</strong>）混淆。我会用另一个不同的记号，我会用$S$来表示<strong>RNN</strong>的隐藏状态（<strong>the hidden state in this RNN</strong>），不用$A^{&lt;1&gt;}$，而是用$S^{&lt;1&gt;}$。我们希望在这个模型里第一个生成的单词将会是<strong>Jane</strong>，为了生成<strong>Jane visits Africa in September</strong>。于是等式就是，当你尝试生成第一个词，即输出，那么我们应该看输入的法语句子的哪个部分？似乎你应该先看第一个单词，或者它附近的词。但是你别看太远了，比如说看到句尾去了。所以注意力模型就会计算注意力权重（<strong>a set of attention weights</strong>），我们将用$a^{&lt;1,1&gt;}$来表示当你生成第一个词时你应该放多少注意力在这个第一块信息处。然后我们算第二个，这个叫注意力权重，$a^{&lt;1,2&gt;}$它告诉我们当你尝试去计算第一个词<strong>Jane</strong>时，我们应该花多少注意力在输入的第二个词上面。同理这里是$a^{&lt;1,3&gt;}$，接下去也同理。这些将会告诉我们，我们应该花多少注意力在记号为$C$的内容上。这就是<strong>RNN</strong>的一个单元，如何尝试生成第一个词的，这是<strong>RNN</strong>的其中一步，我们将会在下个视频中讲解细节。对于<strong>RNN</strong>的第二步，我们将有一个新的隐藏状态$S^{&lt;2&gt;}$，我们也会用一个新的注意力权值集<strong>(a new set of the attention weights</strong>),我们将用$a^{&lt;2,1&gt;}$来告诉我们什么时候生成第二个词, 那么<strong>visits</strong>就会是第二个标签了(<strong>the ground trip label</strong>)。我们应该花多少注意力在输入的第一个法语词上。然后同理$a^{&lt;2,2&gt;}$，接下去也同理，我们应该花多少注意力在<strong>visite</strong>词上，我们应该花多少注意在词<strong>l’Afique</strong>上面。当然我们第一个生成的词<strong>Jane</strong>也会输入到这里，于是我们就有了需要花注意力的上下文。第二步，这也是个输入，然后会一起生成第二个词，这会让我们来到第三步$S^{&lt;3&gt;}$，这是输入，我们再有上下文<strong>C</strong>，它取决于在不同的时间集（<strong>time sets</strong>）,上面的$a^{&lt;3&gt;}$。这个告诉了我们我们要花注意力在不同的法语的输入词上面。<br>然后同理。有些事情我还没说清楚，但是在下个视频中，我会讲解一些细节，比如如何准确定义上下文，还有第三个词的上下文，是否真的需要去注意句子中的周围的词。这里要用到的公式以及如何计算这些注意力权重（<strong>these attention weights</strong>），将会在下个视频中讲解到。在下个视频中你会看到$a^{&lt;3,t&gt;}$，即当你尝试去生成第三个词，应该是<strong>l’Afique</strong>，就得到了右边这个输出，这个<strong>RNN</strong>步骤应该要花注意力在$t$时的法语词上，这取决于在$t$时的双向<strong>RNN</strong>的激活值。那么它应该是取决于第四个激活值，它会取决于上一步的状态，它会取决于$S^{&lt;2&gt;}$。然后这些一起影响你应该花多少注意在输入的法语句子的某个词上面。我们会在下个视频中讲解这些细节。但是直观来想就是<strong>RNN</strong>向前进一次生成一个词，在每一步直到最终生成可能是<strong>&amp;lt;EOS&amp;gt;</strong>。这些是注意力权重，即$a^{&lt;t,t&gt;}$告诉你，当你尝试生成第$t$个英文词，它应该花多少注意力在第$t$个法语词上面。当生成一个特定的英文词时，这允许它在每个时间步去看周围词距内的法语词要花多少注意力。</p><p>我希望这个视频传递了关于注意力模型的一些直观的东西。我们现在可能对算法的运行有了大概的感觉，让我们进入到下个视频中，看看具体的细节。</p><h3 id="3-8注意力模型（Attention-Model）"><a href="#3-8注意力模型（Attention-Model）" class="headerlink" title="3.8注意力模型（Attention Model）"></a>3.8注意力模型（Attention Model）</h3><p>在上个视频中你已经见到了,<strong>注意力模型如何让一个神经网络只注意到一部分的输入句子。当它在生成句子的时候，更像人类翻译</strong>。让我们把这些想法转化成确切的式子，来实现注意力模型。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1e6b86a4e3690b4a0c6b8146ffa2f791.png" alt="1e6b86a4e3690b4a0c6b8146ffa2f791"><br>跟上个视频一样，我们先假定有一个输入句子，并使用双向的<strong>RNN</strong>，或者双向的<strong>GRU</strong>或者双向的<strong>LSTM</strong>，去计算每个词的特征。实际上<strong>GRU</strong>和<strong>LSTM</strong>经常应用于这个，可能<strong>LSTM</strong>更经常一点。对于前向传播（<strong>the forward occurrence</strong>），你有第一个时间步的前向传播的激活值（<strong>a forward occurrence first time step</strong>），第一个时间步后向传播的激活值，后向的激活值，以此类推。他们一共向前了五个时间步，也向后了五个时间步，技术上我们把这里设置为0。我们也可以后向传播6次，设一个都是0的因子，实际上就是个都是0的因子。为了简化每个时间步的记号，即使你在双向<strong>RNN</strong>已经计算了前向的特征值和后向的特征值，我就用$a^{&lt;t&gt;}$来一起表示这些联系。所以$a^{&lt;t&gt;}$就是时间步$t$上的特征向量。<strong>但是为了保持记号的一致性，我们用第二个，也就是$t'$，实际上我将用$t'$来索引法语句子里面的词</strong>。接下来我们只进行前向计算，就是说这是个单向的<strong>RNN</strong>，用状态$S$表示生成翻译。所以第一个时间步，它应该生成$y^{&lt;1&gt;}$，当你输入上下文$C$的时候就会这样，如果你想用时间来索引它，你可以写$C^{&lt;1&gt;}$，但有时候我就写个$C$，就是没有上标的$C$，这个会取决于注意力参数，即$a^{&lt;1,1&gt;}$，$a^{&lt;1,2&gt;}$以此类推，告诉我们应该花多少注意力。同样的，这个$a$参数告诉我们上下文有多少取决于我们得到的特征，或者我们从不同时间步中得到的激活值。所以我们定义上下文的方式实际上来源于被注意力权重加权的不同时间步中的特征值。于是更公式化的注意力权重将会满足非负的条件，所以这就是个0或正数，它们加起来等于1。我们等会会见到我们如何确保这个成立，我们将会有上下文，或者说在$t=1$时的上下文，我会经常省略上标，这就会变成对$t'$的求和。这个权重的所有的$t'$值，加上这些激活值。所以这里的这项（上图编号1所示）就是注意力权重，这里的这项（上图编号2）来自于这里（上图编号3），于是$a^{&lt;t,t'&gt;}$就是$y^{&lt;t&gt;}$应该在$t'$时花在$a$上注意力的数量。换句话来说，当你在$t$处生成输出词，你应该花多少注意力在第$t'$个输入词上面，这是生成输出的其中一步。然后下一个时间步，你会生成第二个输出。于是相似的，你现在有了一个新的注意力权重集，再找到一个新的方式将它们相加，这就产生了一个新的上下文，这个也是输入，且允许你生成第二个词。只有现在才用这种方式相加，它会变成第二个时间步的上下文。即对$t'$的$a^{&lt;2,t'&gt;}$进行求和，于是使用这些上下文向量，$C^{&lt;1&gt;}$写到这里，$C^{&lt;2&gt;}$也同理。这里的神经网络看起来很像相当标准的<strong>RNN</strong>序列，这里有着上下文向量作为输出，我们可以一次一个词地生成翻译，我们也定义了如何通过这些注意力权重和输入句子的特征值来计算上下文向量。剩下唯一要做的事情就是定义如何计算这些注意力权重。让我们下张幻灯片看看。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/b22dff4a3b1a4ea8c1ab201446e98889.png" alt="b22dff4a3b1a4ea8c1ab201446e98889"><br>回忆一下$a^{&lt;t,t'&gt;}$，是你应该花费在$a^{&lt;t'&gt;}$上的注意力的数量，当你尝试去生成第$t$个输出的翻译词，让我们先把式子写下来，再讨论它是怎么来的。这个式子你可以用来计算$a^{&lt;t,t'&gt;}$，在此之前我们要先计算$e^{&lt;t,t'&gt;}$，关键要用<strong>softmax</strong>，来确保这些权重加起来等于1。如果你对$t'$求和，比如每一个固定的$t$值，这些加起来等于1。如果你对$t'$求和，然后优先使用<strong>softmax</strong>，确保这些值加起来等于1。</p><p>现在我们如何计算这些$e$项，一种我们可以用的方式是用下面这样的小的神经网络，于是$s^{&lt;t-1&gt;}$就是神经网络在上个时间步的状态，于是这里我们有一个神经网络，如果你想要生成$y^{&lt;t&gt;}$，那么$s^{&lt;t-1&gt;}$就是上一时间步的隐藏状态，即$s^{&lt;t&gt;}$。这是给小神经网络的其中一个输入，也就是在神经网络中的一个隐藏层，因为你需要经常计算它们，然后$a^{&lt;t'&gt;}$，即上个时间步的的特征是另一个输入。直观来想就是，如果你想要决定要花多少注意力在$t'$的激活值上。于是，似乎它会很大程度上取决于你上一个时间步的的隐藏状态的激活值。你还没有当前状态的激活值，因为上下文会输入到这里，所以你还没计算出来，但是看看你生成上一个翻译的<strong>RNN</strong>的隐藏状态，然后对于每一个位置，每一个词都看向他们的特征值，这看起来很自然，即$a^{&lt;t,t'&gt;}$和$e^{&lt;t,t'&gt;}$应该取决于这两个量。但是我们不知道具体函数是什么，所以我们可以做的事情就是训练一个很小的神经网络，去学习这个函数到底是什么。相信反向传播算法，相信梯度下降算法学到一个正确的函数。这表示，如果你应用这整个的模型，然后用梯度下降来训练它，这是可行的。这个小型的神经网络做了一件相当棒的事情，告诉你$y^{&lt;t&gt;}$应该花多少注意力在$a^{&lt;t&gt;}$上面，然后这个式子确保注意力权重加起来等于1，于是当你持续地一次生成一个词，这个神经网络实际上会花注意力在右边的这个输入句子上，它会完全自动的通过梯度下降来学习。</p><p>这个算法的一个缺点就是它要花费三次方的时间，就是说这个算法的复杂是$O(n3)$的，如果你有$T_x$个输入单词和$T_y$个输出单词，于是注意力参数的总数就会是$T_x\times T_y$，所以这个算法有着三次方的消耗。但是在机器翻译的应用上，输入和输出的句子一般不会太长，可能三次方的消耗是可以接受，但也有很多研究工作，尝试去减少这样的消耗。那么讲解注意想法在机器翻译中的应用，就到此为止了。虽然没有讲到太多的细节，但这个想法也被应用到了其他的很多问题中去了，比如图片加标题（<strong>image captioning</strong>），图片加标题就是看一张图，写下这张图的标题。底下的这篇论文来源于<strong>Kevin Chu</strong>，<strong>Jimmy Barr</strong>, <strong>Ryan Kiros</strong>, <strong>Kelvin Shaw</strong>, <strong>Aaron Korver</strong>, <strong>Russell Zarkutnov</strong>, <strong>Virta Zemo</strong>, 和 <strong>Andrew Benjo</strong>。他们也显示了你可以有一个很相似的结构看图片，然后，当你在写图片标题的时候，一次只花注意力在一部分的图片上面。如果你感兴趣，那么我鼓励你，也去看看这篇论文，做一些编程练习。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/fa6769127cd98ea0058f600449833a21.png" alt="fa6769127cd98ea0058f600449833a21"><br>因为机器翻译是一个非常复杂的问题，在之前的练习中，你应用了注意力，在日期标准化的问题（<strong>the date normalization problem</strong>）上面，问题输入了像这样的一个日期，这个日期实际上是阿波罗登月的日期，把它标准化成标准的形式，或者这样的日期。用一个序列的神经网络，即序列模型去标准化到这样的形式，这个日期实际上是威廉·莎士比亚的生日。一般认为是这个日期正如你之前联系中见到的，你可以训练一个神经网络，输入任何形式的日期，生成标准化的日期形式。其他可以做的有意思的事情是看看可视化的注意力权重（<strong>the visualizations of the attention weights</strong>）。这个一个机器翻译的例子，这里被画上了不同的颜色，不同注意力权重的大小，我不想在这上面花太多时间，但是你可以发现，对应的输入输出词，你会发现注意力权重，会变高，因此这显示了当它生成特定的输出词时通常会花注意力在输入的正确的词上面，包括学习花注意在哪。<br>在注意力模型中，使用反向传播时， 什么时候学习完成。</p><p>这就是注意力模型，在深度学习中真的是个非常强大的想法。在本周的编程练习中，我希望你可以享受自己应用它的过程。</p><h3 id="3-9语音识别（Speech-recognition）"><a href="#3-9语音识别（Speech-recognition）" class="headerlink" title="3.9语音识别（Speech recognition）"></a>3.9语音识别（Speech recognition）</h3><p>现今，最令人振奋的发展之一，就是<strong>seq2seq</strong>模型（<strong>sequence-to-sequence models</strong>）在语音识别方面准确性有了很大的提升。这门课程已经接近尾声，现在我想通过剩下几节视频，来告诉你们，<strong>seq2seq</strong>模型是如何应用于音频数据的（<strong>audio data</strong>），比如语音（<strong>the speech</strong>）。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/8da3e9cf049139a8e4a78503bd72e7fd.png" alt="8da3e9cf049139a8e4a78503bd72e7fd"><br>什么是语音视频问题呢？现在你有一个音频片段$x$（<strong>an audio clip,x</strong>），你的任务是自动地生成文本$y$。现在有一个音频片段，画出来是这样，该图的横轴是时间。一个麦克风的作用是测量出微小的气压变化，现在你之所以能听到我的声音，是因为你的耳朵能够探测到这些微小的气压变化，它可能是由你的扬声器或者耳机产生的，也就是像图上这样的音频片段，气压随着时间而变化。假如这个我说的音频片段的内容是：”<strong>the quick brown fox</strong>“(敏捷的棕色狐狸)，这时我们希望一个语音识别算法（<strong>a speech recognition algorithm</strong>），通过输入这段音频，然后输出音频的文本内容。考虑到人的耳朵并不会处理声音的原始波形，而是通过一种特殊的物理结构来测量这些，不同频率和强度的声波。音频数据的常见预处理步骤，就是运行这个原始的音频片段，然后生成一个声谱图（<strong>a spectrogram</strong>），就像这样。同样地，横轴是时间，纵轴是声音的频率（<strong>frequencies</strong>），而图中不同的颜色，显示了声波能量的大小（<strong>the amount of energy</strong>），也就是在不同的时间和频率上这些声音有多大。通过这样的声谱图，或者你可能还听过人们谈到过伪空白输出（<strong>the false blank outputs</strong>），也经常应用于预处理步骤，也就是在音频被输入到学习算法之前，而人耳所做的计算和这个预处理过程非常相似。语音识别方面，最令人振奋的趋势之一就是曾经有一段时间，语音识别系统是用音位（<strong>phonemes</strong>）来构建的，也就是人工设计的基本单元（<strong>hand-engineered basic units of cells</strong>），如果用音位来表示”<strong>the quick brown fox</strong>“，我这里稍微简化一些，”<strong>the</strong>“含有”<strong>th</strong>“和”<strong>e</strong>“的音，而”<strong>quick</strong>“有”<strong>k</strong>“ “<strong>w</strong>“ “<strong>i</strong>“ “<strong>k</strong>“的音，语音学家过去把这些音作为声音的基本单元写下来，把这些语音分解成这些基本的声音单元，而”<strong>brown</strong>“不是一个很正式的音位，因为它的音写起来比较复杂，不过语音学家（<strong>linguists</strong>）们认为用这些基本的音位单元（<strong>basic units of sound called phonemes</strong>）来表示音频（<strong>audio</strong>），是做语音识别最好的办法。不过在<strong>end-to-end</strong>模型中，我们发现这种音位表示法（<strong>phonemes representations</strong>）已经不再必要了，而是可以构建一个系统，通过向系统中输入音频片段（<strong>audio clip</strong>），然后直接输出音频的文本（<strong>a transcript</strong>），而不需要使用这种人工设计的表示方法。使这种方法成为可能的一件事就是用一个很大的数据集，所以语音识别的研究数据集可能长达300个小时，在学术界，甚至3000小时的文本音频数据集，都被认为是合理的大小。大量的研究，大量的论文所使用的数据集中，有几千种不同的声音，而且，最好的商业系统现在已经训练了超过1万个小时的数据，甚至10万个小时，并且它还会继续变得更大。在文本音频数据集中（<strong>Transcribe audio data sets</strong>）同时包含$x$和$y$，通过深度学习算法大大推进了语音识别的进程。那么，如何建立一个语音识别系统呢？</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/4130b85a0694549f02bdf60f7c47a3d7.png" alt="4130b85a0694549f02bdf60f7c47a3d7"><br>在上一节视频中，我们谈到了注意力模型，所以，一件你能做的事就是在横轴上，也就是在输入音频的不同时间帧上，你可以用一个注意力模型，来输出文本描述，如”<strong>the quick brown fox</strong>“，或者其他语音内容。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/8f409fc3980b0be00dca49bf4fac2659.png" alt="8f409fc3980b0be00dca49bf4fac2659"><br>还有一种效果也不错的方法，就是用<strong>CTC</strong>损失函数（<strong>CTC cost</strong>）来做语音识别。<strong>CTC</strong>就是<strong>Connectionist Temporal Classification</strong>，它是由<strong>Alex Graves</strong>、<strong>Santiago Fernandes</strong>, <strong>Faustino Gomez</strong>、和<strong>Jürgen Schmidhuber</strong>提出的。（<strong>Graves A, Gomez F. Connectionist temporal classification:labelling unsegmented sequence data with recurrent neural networks[C]// International Conference on Machine Learning. ACM, 2006:369-376.</strong>）</p><p>算法思想如下:</p><p>假设语音片段内容是某人说：”<strong>the quick brown fox</strong>“，这时我们使用一个新的网络，结构像这个样子，这里输入$x$和输出$y$的数量都是一样的，因为我在这里画的，只是一个简单的单向<strong>RNN</strong>结构。然而在实际中，它有可能是双向的<strong>LSTM</strong>结构，或者双向的<strong>GIU</strong>结构，并且通常是很深的模型。但注意一下这里时间步的数量，它非常地大。在语音识别中，通常输入的时间步数量（<strong>the number of input time steps</strong>）要比输出的时间步的数量（<strong>the number of output time steps</strong>）多出很多。举个例子，比如你有一段10秒的音频，并且特征（<strong>features</strong>）是100赫兹的，即每秒有100个样本，于是这段10秒的音频片段就会有1000个输入，就是简单地用100赫兹乘上10秒。所以有1000个输入，但可能你的输出就没有1000个字母了，或者说没有1000个字符。这时要怎么办呢？<strong>CTC</strong>损失函数允许<strong>RNN</strong>生成这样的输出：<strong>ttt</strong>，这是一个特殊的字符，叫做空白符，我们这里用下划线表示，这句话开头的音可表示为<strong>h_eee_ _ _</strong>，然后这里可能有个空格，我们用这个来表示空格，之后是<strong>_ _ _qqq__</strong>，这样的输出也被看做是正确的输出。下面这段输出对应的是”<strong>the q</strong>“。<strong>CTC损失函数的一个基本规则是将空白符之间的重复的字符折叠起来，再说清楚一些，我这里用下划线来表示这个特殊的空白符（a special blank character），它和空格（the space character）是不一样的</strong>。所以<strong>the</strong>和<strong>quick</strong>之间有一个空格符，所以我要输出一个空格，通过把用空白符所分割的重复的字符折叠起来，然后我们就可以把这段序列折叠成”<strong>the q</strong>“。这样一来你的神经网络因为有很多这种重复的字符，和很多插入在其中的空白符（<strong>blank characters</strong>），所以最后我们得到的文本会短上很多。于是这句”<strong>the quick brown fox</strong>“包括空格一共有19个字符，在这样的情况下，通过允许神经网络有重复的字符和插入空白符使得它能强制输出1000个字符，甚至你可以输出1000个$y$值来表示这段19个字符长的输出。这篇论文来自于<strong>Alex Grace</strong>以及刚才提到的那些人。我所参与的深度语音识别系统项目就使用这种思想来构建有效的语音识别系统。</p><p>希望这能给你一个粗略的理解，理解语音识别模型是如何工作的：注意力模型是如何工作的，以及<strong>CTC</strong>模型是如何工作的，以及这两种不同的构建这些系统的方法。现今，在生产技术中，构建一个有效语音识别系统，是一项相当重要的工作，并且它需要很大的数据集，下节视频我想做的是告诉你如何构建一个触发字检测系统（<strong>a rigger word detection system</strong>），其中的关键字检测系统（<strong>keyword detection system</strong>）将会更加简单，它可以通过一个更简洁的数量更合理的数据来完成。所以我们下节课再见。</p><h3 id="3-10触发字检测（Trigger-Word-Detection）"><a href="#3-10触发字检测（Trigger-Word-Detection）" class="headerlink" title="3.10触发字检测（Trigger Word Detection）"></a>3.10触发字检测（Trigger Word Detection）</h3><p>现在你已经学习了很多关于深度学习和序列模型的内容，于是我们可以真正去简便地描绘出一个触发字系统（<strong>a trigger word system</strong>），就像上节视频中你看到的那样。随着语音识别的发展，越来越多的设备可以通过你的声音来唤醒，这有时被叫做触发字检测系统（<strong>rigger word detection systems</strong>）。我们来看一看如何建立一个触发字系统。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/28443f8b28afa63adb6cf89ce19c4f6d.png" alt="28443f8b28afa63adb6cf89ce19c4f6d"><br>触发字系统的例子包括<strong>Amazon echo</strong>，它通过单词<strong>Alexa</strong>唤醒；还有百度<strong>DuerOS</strong>设备，通过”小度你好”来唤醒；苹果的<strong>Siri</strong>用<strong>Hey Siri</strong>来唤醒；<strong>Google Home</strong>使用<strong>Okay Google</strong>来唤醒，这就是触发字检测系统。假如你在卧室中，有一台<strong>Amazon echo</strong>，你可以在卧室中简单说一句: <strong>Alexa</strong>, 现在几点了?就能唤醒这个设备。它将会被单词”<strong>Alexa</strong>“唤醒，并回答你的询问。如果你能建立一个触发字检测系统，也许你就能让你的电脑通过你的声音来执行某些事，我有个朋友也在做一种用触发字来打开的特殊的灯，这是个很有趣的项目。但我想教会你的，是如何构建一个触发字检测系统。</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/f2da69f9fa6462c8e591e79db452f6c1.png" alt="f2da69f9fa6462c8e591e79db452f6c1"><br>有关于触发字检测系统的文献，还处于发展阶段。对于触发字检测，最好的算法是什么，目前还没有一个广泛的定论。我这里就简单向你介绍一个你能够使用的算法好了。现在有一个这样的<strong>RNN</strong>结构，<strong>我们要做的就是把一个音频片段（an audio clip）计算出它的声谱图特征（spectrogram features）得到特征向量</strong>$x^{&lt;1&gt;}$, $x^{&lt;2&gt;}$, $x^{&lt;3&gt;}$..，然后把它放到<strong>RNN</strong>中，最后要做的，就是定义我们的目标标签$y$。假如音频片段中的这一点是某人刚刚说完一个触发字，比如”<strong>Alexa</strong>“，或者”小度你好” 或者”<strong>Okay Google</strong>“，那么在这一点之前，你就可以在训练集中把目标标签都设为0，然后在这个点之后把目标标签设为1。假如在一段时间之后，触发字又被说了一次，比如是在这个点说的，那么就可以再次在这个点之后把目标标签设为1。这样的标签方案对于<strong>RNN</strong>来说是可行的，并且确实运行得非常不错。不过该算法一个明显的缺点就是它构建了一个很不平衡的训练集（<strong>a very imbalanced training set</strong>），0的数量比1多太多了。</p><p>这里还有一个解决方法，虽然听起来有点简单粗暴，但确实能使其变得更容易训练。比起只在一个时间步上去输出1，其实你可以在输出变回0之前，多次输出1，或说在固定的一段时间内输出多个1。这样的话，就稍微提高了1与0的比例，这确实有些简单粗暴。在音频片段中，触发字刚被说完之后，就把多个目标标签设为1，这里触发字又被说了一次。说完以后，又让<strong>RNN</strong>去输出1。在之后的编程练习中，你可以进行更多这样的操作，我想你应该会对自己学会了这么多东西而感到自豪。我们仅仅用了一张幻灯片来描述这种复杂的触发字检测系统。在这个基础上，希望你能够实现一个能有效地让你能够检测出触发字的算法，不过在编程练习中你可以看到更多的学习内容。这就是触发字检测，希望你能对自己感到自豪。因为你已经学了这么多深度学习的内容，现在你可以只用几分钟时间，就能用一张幻灯片来描述触发字能够实现它，并让它发挥作用。你甚至可能在你的家里用触发字系统做一些有趣的事情，比如打开或关闭电器，或者可以改造你的电脑，使得你或者其他人可以用触发字来操作它。</p></article><div class="post-donate"><div id="donate_board" class="donate_bar center"><a id="btn_donate" class="btn_donate" href="javascript:;" title="打赏"></a> <span class="donate_txt">↑<br> 欢迎投食,求鼓励，求支持！</span><br></div><div id="donate_guide" class="donate_bar center hidden"> <img src="/images/alipay.png" alt="支付宝打赏"> <img src="/images/wechatpay.png" alt="微信打赏"></div><script type="text/javascript">document.getElementById("btn_donate").onclick=function(){$("#donate_board").addClass("hidden"),$("#donate_guide").removeClass("hidden")}</script></div><div class="nexmoe-post-copyright"><i class="mdui-list-item-icon nexmoefont icon-info-circle"></i> <strong>本文作者：</strong>OneJane<br> <strong>本文链接：</strong><a href="https://onejane.github.io/2019/12/05/new_吴恩达深度学习笔记(第五课时)/" title="https://onejane.github.io/2019/12/05/new_吴恩达深度学习笔记(第五课时)/" target="_blank" rel="noopener">https://onejane.github.io/2019/12/05/new_吴恩达深度学习笔记(第五课时)/</a><br> <strong>版权声明：</strong>本文采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/cn/deed.zh" target="_blank">CC BY-NC-SA 3.0 CN</a> 协议进行许可</div><section class="nexmoe-comment"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.5.0/dist/gitalk.min.css"><div id="gitalk"></div><script src="https://cdn.jsdelivr.net/npm/gitalk@1.5.0/dist/gitalk.min.js"></script><script type="text/javascript">var gitalk=new Gitalk({clientID:"e677e59382e1c7a468fd",clientSecret:"717d041bc4ab749f069314862232cfb6ec8adc15",id:decodeURI(window.location.pathname),repo:"onejane.github.io",owner:"onejane",admin:"onejane"});gitalk.render("gitalk")</script></section></div></div></div><script src="https://cdn.jsdelivr.net/npm/mdui@0.4.3/dist/js/mdui.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/smoothscroll-for-websites@1.4.9/SmoothScroll.min.js"></script><script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.15.8/build/highlight.min.js"></script><script>hljs.initHighlightingOnLoad()</script><script src="/js/app.js?v=1578196807523"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@5.1.0/lazysizes.min.js"></script><div hidden><script type="text/javascript" src="https://js.users.51.la/20279757.js"></script></div></body><script src="https://cdn.jsdelivr.net/npm/meting@1.1.0/dist/Meting.min.js"></script></html>