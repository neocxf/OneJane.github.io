<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><title>scrapy爬取ssr链接 - OneJane</title><meta charset="UTF-8"><meta name="description" content="微服务,高可用,高并发,人工智能"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="msvalidate.01" content="396E9693347B4D18AAE96D9E75B9B686"><link rel="shortcut icon" href="/images/a.ico" type="image/png"><meta name="description" content="基于scrapy爬取ssr链接"><meta name="keywords" content="scrapy,selenium"><meta property="og:type" content="article"><meta property="og:title" content="scrapy爬取ssr链接"><meta property="og:url" content="https://onejane.github.io/2019/08/23/scrapy爬取ssr链接/index.html"><meta property="og:site_name" content="OneJane"><meta property="og:description" content="基于scrapy爬取ssr链接"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1566614730793.png"><meta property="og:updated_time" content="2019-08-28T02:55:06.213Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="scrapy爬取ssr链接"><meta name="twitter:description" content="基于scrapy爬取ssr链接"><meta name="twitter:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1566614730793.png"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdui@0.4.3/dist/css/mdui.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.15.8/styles/atom-one-dark.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1038733_0xvrvpg9c0r.css"><link rel="stylesheet" href="/css/style.css?v=1567548035302"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.js"></script></head><body class="mdui-drawer-body-left"><div id="nexmoe-background"><div class="nexmoe-bg" style="background-image:url(https://www.github.com/OneJane/blog/raw/master/小书匠/1566388885395.png)"></div><div class="mdui-appbar mdui-shadow-0"><div class="mdui-toolbar"> <a mdui-drawer="{target: '#drawer', swipe: true}" title="menu" class="mdui-btn mdui-btn-icon"><i class="mdui-icon material-icons">menu</i></a><div class="mdui-toolbar-spacer"></div> <a href="/" title="OneJane" class="mdui-btn mdui-btn-icon"><img src="https://www.github.com/OneJane/blog/raw/master/小书匠/1566388846021.png"></a></div></div></div><div id="nexmoe-header"><div class="nexmoe-drawer mdui-drawer" id="drawer"><div class="nexmoe-avatar mdui-ripple"> <a href="/" title="OneJane"><img src="https://www.github.com/OneJane/blog/raw/master/小书匠/1566388846021.png" alt="OneJane"></a></div><div class="nexmoe-count"><div><span>文章</span>35</div><div><span>标签</span>54</div><div><span>分类</span>11</div></div><ul class="nexmoe-list mdui-list" mdui-collapse="{accordion: true}"><a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/" title="回到首页"><i class="mdui-list-item-icon nexmoefont icon-home"></i><div class="mdui-list-item-content"> 回到首页</div></a><a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/about.html" title="关于博客"><i class="mdui-list-item-icon nexmoefont icon-info-circle"></i><div class="mdui-list-item-content"> 关于博客</div></a><a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/py.html" title="我的朋友"><i class="mdui-list-item-icon nexmoefont icon-unorderedlist"></i><div class="mdui-list-item-content"> 我的朋友</div></a></ul><aside id="nexmoe-sidebar"><div class="nexmoe-widget-wrap"><h3 class="nexmoe-widget-title">社交按钮</h3><div class="nexmoe-widget nexmoe-social"><a class="mdui-ripple" href="https://www.zhihu.com/people/codewj/activities" target="_blank" mdui-tooltip="{content: 'zhihu'}" style="color:#e76a8d;background-color:rgba(231,106,141,.15)"><i class="nexmoefont icon-zhihu"></i></a><a class="mdui-ripple" href="https://github.com/OneJane" target="_blank" mdui-tooltip="{content: 'GitHub'}" style="color:#191717;background-color:rgba(25,23,23,.15)"><i class="nexmoefont icon-github"></i></a></div></div><div class="nexmoe-widget-wrap"><h3 class="nexmoe-widget-title">文章分类</h3><div class="nexmoe-widget"><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/人工智能/">人工智能</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/博客/">博客</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/定时器/">定时器</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/持续集成/">持续集成</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据库/">数据库</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/注册中心/">注册中心</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/测试/">测试</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/系统/">系统</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/项目实战/">项目实战</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/高可用/">高可用</a><span class="category-list-count">4</span></li></ul></div></div><div class="nexmoe-widget-wrap"><h3 class="nexmoe-widget-title">标签云</h3><div class="nexmoe-widget tagcloud"> <a href="/tags/Gensim/" style="font-size:10px">Gensim</a> <a href="/tags/Hanlp/" style="font-size:10px">Hanlp</a> <a href="/tags/NLTK/" style="font-size:10px">NLTK</a> <a href="/tags/OpenCV/" style="font-size:13.33px">OpenCV</a> <a href="/tags/Stanford-NLP/" style="font-size:10px">Stanford NLP</a> <a href="/tags/Tensorflow/" style="font-size:16.67px">Tensorflow</a> <a href="/tags/ant-design/" style="font-size:10px">ant design</a> <a href="/tags/ant-design-pro/" style="font-size:11.67px">ant design pro</a> <a href="/tags/docker/" style="font-size:18.33px">docker</a> <a href="/tags/dubbo/" style="font-size:11.67px">dubbo</a> <a href="/tags/elasticsearch/" style="font-size:10px">elasticsearch</a> <a href="/tags/elastisearch/" style="font-size:10px">elastisearch</a> <a href="/tags/email/" style="font-size:10px">email</a> <a href="/tags/es6/" style="font-size:10px">es6</a> <a href="/tags/feign/" style="font-size:10px">feign</a> <a href="/tags/flask/" style="font-size:10px">flask</a> <a href="/tags/freemarker/" style="font-size:10px">freemarker</a> <a href="/tags/gateway/" style="font-size:10px">gateway</a> <a href="/tags/gitlab/" style="font-size:11.67px">gitlab</a> <a href="/tags/haproxy/" style="font-size:10px">haproxy</a> <a href="/tags/jenkins/" style="font-size:11.67px">jenkins</a> <a href="/tags/jmeter/" style="font-size:10px">jmeter</a> <a href="/tags/keepalived/" style="font-size:10px">keepalived</a> <a href="/tags/linux/" style="font-size:10px">linux</a> <a href="/tags/maven/" style="font-size:11.67px">maven</a> <a href="/tags/multi-druid/" style="font-size:10px">multi druid</a> <a href="/tags/mybatis/" style="font-size:10px">mybatis</a> <a href="/tags/mybatisplus/" style="font-size:10px">mybatisplus</a> <a href="/tags/mysql/" style="font-size:10px">mysql</a> <a href="/tags/nacos/" style="font-size:11.67px">nacos</a> <a href="/tags/nexmoe/" style="font-size:10px">nexmoe</a> <a href="/tags/nlp/" style="font-size:13.33px">nlp</a> <a href="/tags/numpy/" style="font-size:10px">numpy</a> <a href="/tags/partition/" style="font-size:10px">partition</a> <a href="/tags/pxc/" style="font-size:10px">pxc</a> <a href="/tags/rabbitmq/" style="font-size:10px">rabbitmq</a> <a href="/tags/react/" style="font-size:11.67px">react</a> <a href="/tags/redis/" style="font-size:11.67px">redis</a> <a href="/tags/redis-cluster/" style="font-size:10px">redis-cluster</a> <a href="/tags/replication/" style="font-size:11.67px">replication</a> <a href="/tags/rocketmq/" style="font-size:11.67px">rocketmq</a> <a href="/tags/scrapy/" style="font-size:10px">scrapy</a> <a href="/tags/selenium/" style="font-size:10px">selenium</a> <a href="/tags/sentinel/" style="font-size:15px">sentinel</a> <a href="/tags/session/" style="font-size:10px">session</a> <a href="/tags/skywalking/" style="font-size:11.67px">skywalking</a> <a href="/tags/spring-cloud-alibaba/" style="font-size:20px">spring cloud alibaba</a> <a href="/tags/springboot/" style="font-size:15px">springboot</a> <a href="/tags/swagger/" style="font-size:10px">swagger</a> <a href="/tags/tk-mybatis/" style="font-size:10px">tk mybatis</a> <a href="/tags/umi/" style="font-size:10px">umi</a> <a href="/tags/validate/" style="font-size:10px">validate</a> <a href="/tags/xxl-job/" style="font-size:11.67px">xxl-job</a> <a href="/tags/zookeeper/" style="font-size:10px">zookeeper</a></div></div><div class="nexmoe-widget-wrap"><h3 class="nexmoe-widget-title">文章归档</h3><div class="nexmoe-widget"><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">九月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li></ul></div></div></aside><div class="nexmoe-copyright"> &copy; 2019 OneJane</div></div></div><div id="nexmoe-content"><div class="nexmoe-primary"><div class="nexmoe-post"><div class="nexmoe-post-cover"> <img src="https://www.github.com/OneJane/blog/raw/master/小书匠/3d21604af4ece8292ae728500ce2c4f7_hd.jpg"><h1>scrapy爬取ssr链接</h1></div><div class="nexmoe-post-meta"><a><i class="nexmoefont icon-calendar-fill"></i> 2019年08月23日</a><a><i class="nexmoefont icon-areachart"></i> 3.1k 字</a><a><i class="nexmoefont icon-time-circle-fill"></i> 大概 18 分钟</a> <a class="nexmoefont icon-appstore-fill -link" href="/categories/爬虫/">爬虫</a> <a class="nexmoefont icon-tag-fill -link" href="/tags/scrapy/">scrapy</a> <a class="nexmoefont icon-tag-fill -link" href="/tags/selenium/">selenium</a></div><article><p><a href="https://onejane.github.io/">基于scrapy爬取ssr链接</a></p><a id="more"></a><h1 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h1><p><a href="https://www.python.org/ftp/python/3.5.0/python-3.5.0-amd64.exe" target="_blank" rel="noopener">python3.5</a></p><h2 id="虚拟环境virtualenv"><a href="#虚拟环境virtualenv" class="headerlink" title="虚拟环境virtualenv"></a>虚拟环境virtualenv</h2><pre><code>pip install virtualenv        提示pip版本太低
python -m pip install --upgrade pip
pip  install  -i  https://pypi.doubanio.com/simple/  --trusted-host pypi.doubanio.com  django    使用豆瓣源加速
pip uninstall django    卸载django 
virtualenv scrapytest   默认环境创建虚拟环境
cd scrapytest/Scripts &amp;&amp;  activate.bat &amp;&amp; python 进入3.5虚拟环境
virtualenv -p D:\Python27\python.exe scrapytest
cd scrapytest/Scripts &amp;&amp;  activate.bat &amp;&amp; python 进入2.7虚拟环境
deactivate.bat          退出虚拟环境

apt-get install python-virtualenv       安装虚拟环境
virtualenv py2 &amp;&amp; cd py2 &amp;&amp; cd bin &amp;&amp; source activate &amp;&amp; python 进入2.7虚拟环境
virtualenv -p /usr/bin/python3 py3 &amp;&amp; &amp;&amp; cd py3 &amp;&amp; cd bin &amp;&amp; source activate &amp;&amp; python  进入3.5虚拟环境</code></pre><h2 id="虚拟环境virtualenvwrapper"><a href="#虚拟环境virtualenvwrapper" class="headerlink" title="虚拟环境virtualenvwrapper"></a>虚拟环境virtualenvwrapper</h2><pre><code>pip install virtualenvwrapper
pip install virtualenvwrapper-win    解决workon不是内部指令
workon  列出所有虚拟环境
新建环境变量   WORKON_HOME=E:\envs
mkvirtualenv py3scrapy  新建并进入虚拟环境
deactivate          退出虚拟环境
workon py3scrapy        进入指定虚拟环境
    pip install -i https://pypi.douban.com/simple scrapy    安装scrapy源
    若缺少lxml出错https://www.lfd.uci.edu/~gohlke/pythonlibs/寻找对应版本的lxml的whl源
    python -m pip install --upgrade pip     更新pip
    pip install lxml-4.1.1-cp35-cp35m-win_amd64.whl
    若缺少Twisted出错http://www.lfd.uci.edu/~gohlke/pythonlibs/#lxml搜对应版本Twisted
    pip install Twisted‑17.9.0‑cp35‑cp35m‑win_amd64.whl
mkvirtualenv --python=D:\Python27\python.exe py2scrapy      一般不会出问题
    pip install -i https://pypi.douban.com/simple scrapy


pip install virtualenvwrapper    
    find / -name virualenvwrapper.sh
    vim ~/.bashrc
        export WORKON_HOME=$HOME/.virtualenvs
        source /home/wj/.local/bin/virtualenvwrapper.sh
    source ~/.bashrc    
mkvirtualenv py2scrapy          指向生成~/.virtualenv
deactivate          退出虚拟环境
mkdirtualenv --python=/usr/bin/python3 py3scrapy</code></pre><h1 id="项目实战"><a href="#项目实战" class="headerlink" title="项目实战"></a>项目实战</h1><h2 id="项目搭建"><a href="#项目搭建" class="headerlink" title="项目搭建"></a>项目搭建</h2><pre><code>pip install virtualenvwrapper-win
mkvirtualenv --python=F:\Python\Python35\python.exe ssr
pip install Twisted-17.9.0-cp35-cp35m-win_amd64.whl
pip install -i https://pypi.douban.com/simple/ scrapy
scrapy startproject ssr
cd ssr
scrapy genspider ssr https://freevpn-ss.tk/category/technology/ 
scrapy genspider --list
scrapy genspider -t crawl lagou www.lagou.com  使用crawl模板

pycharm--新建项目---Pure Python---Interpreter为E:\envs\ssr\Scripts\python.exe
pycharm--打开---ssr,修改settings--project Interpreter为D:\Envs\ss</code></pre><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1566614730793.png" alt="enter description here"></p><pre><code>pip list
pip install -i https://pypi.douban.com/simple pypiwin32 pillow requests redis fake-useragent
pip install mysqlclient-1.4.4-cp35-cp35m-win_amd64.whl

或者pip install -i https://pypi.douban.com/simple mysqlclient
出错apt-get install libmysqlclient-dev
或者yum install python-devel mysql-devel
scrapy crawl jobbole
修改settings.py     ROBOTSTXT_OBEY = False
scrapy shell http://blog.jobbole.com/       可以在脚本中调试xpath或者chrome浏览器右键copy xpath,chrome浏览器右键copy selector
scrapy shell -s USER_AGENT=&quot;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:51.0) Gecko/20100101 Firefox/51.0&quot; https://www.zhihu.com/question/56320032
pip freeze &gt; requirements.txt 生成依赖到文件
pip install -r requirements.txt 一键安装依赖</code></pre><h2 id="爬虫开发1"><a href="#爬虫开发1" class="headerlink" title="爬虫开发1"></a>爬虫开发1</h2><pre><code>scrapy shell https://freevpn-ss.tk/category/technology/  shell中查看节点
response.css(&quot;.posts-list .panel a::attr(href)&quot;).extract_first()
response.css(&quot;.posts-list .panel a img::attr(src)&quot;).extract_first()
response.xpath(&quot;//*[@id=&#39;container&#39;]/div/ul/li/article/a/img/@src&quot;).extract_first()</code></pre><p>启动类main.py</p><pre><code class="stylus">from scrapy.cmdline import execute

import sys
import os

sys.path.append(os.path.dirname(os.path.abspath(__file__)))
execute([&quot;scrapy&quot;, &quot;crawl&quot;, &quot;freevpn-ss.tk&quot;])</code></pre><p>基础配置ssr/settings.py</p><pre><code class="makefile">import os
BOT_NAME = &#39;ssr&#39;

SPIDER_MODULES = [&#39;ssr.spiders&#39;]
NEWSPIDER_MODULE = &#39;ssr.spiders&#39;


# Crawl responsibly by identifying yourself (and your website) on the user-agent
#USER_AGENT = &#39;ssr (+http://www.yourdomain.com)&#39;

# Obey robots.txt rules
ROBOTSTXT_OBEY = False

import sys
BASE_DIR = os.path.dirname(os.path.abspath(os.path.dirname(__file__)))
sys.path.insert(0, os.path.join(BASE_DIR, &#39;ssr&#39;))

MYSQL_HOST = &quot;127.0.0.1&quot;
MYSQL_DBNAME = &quot;scrapy&quot;
MYSQL_USER = &quot;root&quot;
MYSQL_PASSWORD = &quot;&quot;

ITEM_PIPELINES = {
    &#39;ssr.pipelines.MysqlTwistedPipline&#39;: 2,#连接池异步插入
    &#39;ssr.pipelines.JsonExporterPipleline&#39;: 1,#连接池异步插入
}</code></pre><p>ssr/pipelines.py</p><pre><code class="ruby">from scrapy.exporters import JsonItemExporter
from scrapy.pipelines.images import ImagesPipeline
import codecs
import json
import MySQLdb
import MySQLdb.cursors
from twisted.enterprise import adbapi

from ssr.utils.common import DateEncoder


class SsrPipeline(object):
    def process_item(self, item, spider):
        return item


class SsrImagePipeline(ImagesPipeline):
    def item_completed(self, results, item, info):
        if &quot;front_image_url&quot; in item:
            for ok, value in results:
                image_file_path = value[&quot;path&quot;]
            # 填充自定义路径
            item[&quot;front_image_path&quot;] = image_file_path

        return item


class JsonWithEncodingPipeline(object):

    # 自定义json文件的导出
    def __init__(self):
        self.file = codecs.open(&#39;article.json&#39;, &#39;w&#39;, encoding=&quot;utf-8&quot;)

    def process_item(self, item, spider):
        # 序列化，ensure_ascii利于中文,json没法序列化date格式，需要新写函数
        lines = json.dumps(dict(item), ensure_ascii=False, cls=DateEncoder) + &quot;\n&quot;
        self.file.write(lines)
        return item

    def spider_closed(self, spider):
        self.file.close()


class JsonExporterPipleline(object):
    # 调用scrapy提供的json export导出json文件
    def __init__(self):
        self.file = open(&#39;ssr.json&#39;, &#39;wb&#39;)
        self.exporter = JsonItemExporter(self.file, encoding=&quot;utf-8&quot;, ensure_ascii=False)
        self.exporter.start_exporting()

    def close_spider(self, spider):
        self.exporter.finish_exporting()
        self.file.close()

    def process_item(self, item, spider):
        self.exporter.export_item(item)
        return item


class MysqlPipeline(object):
    # 采用同步的机制写入mysql
    def __init__(self):
        self.conn = MySQLdb.connect(&#39;127.0.0.1&#39;, &#39;root&#39;, &#39;123456&#39;, &#39;scrapy&#39;, charset=&quot;utf8&quot;, use_unicode=True)
        self.cursor = self.conn.cursor()

    def process_item(self, item, spider):
        insert_sql = &quot;&quot;&quot;
            insert into ssr(url, ip,ssr, port,password,secret)
            VALUES (%s, %s, %s, %s, %s)
        &quot;&quot;&quot;
        self.cursor.execute(insert_sql, (item[&quot;url&quot;],item[&quot;ssr&quot;], item[&quot;ip&quot;], item[&quot;port&quot;], item[&quot;password&quot;], item[&quot;secret&quot;]))
        self.conn.commit()


class MysqlTwistedPipline(object):
    # 异步连接池插入数据库，不会阻塞
    def __init__(self, dbpool):
        self.dbpool = dbpool

    @classmethod
    def from_settings(cls, settings):# 初始化时即被调用静态方法
        dbparms = dict(
            host = settings[&quot;MYSQL_HOST&quot;],#setttings中定义
            db = settings[&quot;MYSQL_DBNAME&quot;],
            user = settings[&quot;MYSQL_USER&quot;],
            passwd = settings[&quot;MYSQL_PASSWORD&quot;],
            charset=&#39;utf8&#39;,
            cursorclass=MySQLdb.cursors.DictCursor,
            use_unicode=True,
        )
        dbpool = adbapi.ConnectionPool(&quot;MySQLdb&quot;, **dbparms)

        return cls(dbpool)

    def process_item(self, item, spider):
        #使用twisted将mysql插入变成异步执行
        query = self.dbpool.runInteraction(self.do_insert, item)
        query.addErrback(self.handle_error, item, spider) #处理异常

    def handle_error(self, failure, item, spider):
        #处理异步插入的异常
        print (failure)

    def do_insert(self, cursor, item):
        #执行具体的插入，不具体的如MysqlPipeline.process_item()
        #根据不同的item 构建不同的sql语句并插入到mysql中
        insert_sql, params = item.get_insert_sql()
        cursor.execute(insert_sql, params)</code></pre><p>实体类ssr/items.py</p><pre><code class="python">import scrapy
from scrapy.loader import ItemLoader
from scrapy.loader.processors import MapCompose, TakeFirst, Join
import re
import datetime
from w3lib.html import remove_tags


def date_convert(value):

    try:
        create_date = datetime.datetime.strptime(value, &quot;%Y/%m/%d&quot;).date()
    except Exception as e:
        create_date = datetime.datetime.now().date()

    return create_date


def get_nums(value):
    match_re = re.match(&quot;.*?(\d+).*&quot;, value)
    if match_re:
        nums = int(match_re.group(1))
    else:
        nums = 0
    return nums


def return_value(value):
    return value


class SsrItemLoader(ItemLoader):
    # 自定义itemloader
    default_output_processor = TakeFirst()



class SsrItem(scrapy.Item):
    url = scrapy.Field()
    ip = scrapy.Field(
        input_processor=MapCompose(return_value),#传递进来可以预处理
    )
    port = scrapy.Field()
    ssr = scrapy.Field()
    front_image_url = scrapy.Field()
    password = scrapy.Field()
    secret = scrapy.Field()


    def get_insert_sql(self):
        insert_sql = &quot;&quot;&quot;
            insert into ssr(url,ssr, ip, port, password,secret)
            VALUES (%s, %s,%s, %s, %s,%s) 
            ON DUPLICATE KEY UPDATE ssr=VALUES(ssr)
        &quot;&quot;&quot;
        params = (self[&quot;url&quot;],self[&quot;ssr&quot;], self[&quot;ip&quot;],self[&quot;port&quot;], self[&quot;password&quot;],self[&quot;secret&quot;])

        return insert_sql, params</code></pre><p>核心代码ssr/spiders/freevpn_ss_tk.py</p><pre><code class="python"># -*- coding: utf-8 -*-
import time
from datetime import datetime
from urllib import parse

import scrapy
from scrapy.http import Request

from ssr.items import SsrItemLoader, SsrItem


class FreevpnSsTkSpider(scrapy.Spider):
    name = &#39;freevpn-ss.tk&#39;
    # 必须一级域名
    allowed_domains = [&#39;freevpn-ss.tk&#39;]
    start_urls = [&#39;https://freevpn-ss.tk/category/technology/&#39;]

    custom_settings = {  # 优先并覆盖项目，避免被重定向
        &quot;COOKIES_ENABLED&quot;: False,  # 关闭cookies
        &quot;DOWNLOAD_DELAY&quot;: 1,
        &#39;DEFAULT_REQUEST_HEADERS&#39;: {
            &#39;Accept&#39;: &#39;application/json, text/javascript, */*; q=0.01&#39;,
            &#39;Accept-Encoding&#39;: &#39;gzip, deflate, br&#39;,
            &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.8&#39;,
            &#39;Connection&#39;: &#39;keep-alive&#39;,
            &#39;Cookie&#39;: &#39;&#39;,
            &#39;Host&#39;: &#39;freevpn-ss.tk&#39;,
            &#39;Origin&#39;: &#39;https://freevpn-ss.tk/&#39;,
            &#39;Referer&#39;: &#39;https://freevpn-ss.tk/&#39;,
            &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36&#39;,
        }
    }

    def parse(self, response):
        # post_nodes = response.css(&quot;.posts-list .panel&gt;a&quot;)
        # for post_node in post_nodes:
        #     image_url = post_node.css(&quot;img::attr(src)&quot;).extract_first(&quot;&quot;)
        #     post_url = post_node.css(&quot;::attr(href)&quot;).extract_first(&quot;&quot;)
        #     yield Request(url=parse.urljoin(response.url, post_url), meta={&quot;front_image_url&quot;: image_url},callback=self.parse_detail)  # response获取meta
        #
        # next_url = response.css(&quot;.next-page a::attr(href)&quot;).extract_first(&quot;&quot;)
        # if next_url:
        #     print(next_url)
        #     yield Request(url=parse.urljoin(response.url, next_url), callback=self.parse)
        post_node = response.css(&quot;.posts-list .panel&gt;a&quot;)[0]
        image_url = post_node.css(&quot;img::attr(src)&quot;).extract_first(&quot;&quot;)
        post_url = post_node.css(&quot;::attr(href)&quot;).extract_first(&quot;&quot;)
        yield Request(url=parse.urljoin(response.url, post_url), meta={&quot;front_image_url&quot;: image_url},
                      callback=self.parse_detail)  # response获取meta

    def parse_detail(self, response):
        # 通过item loader加载item
        front_image_url = response.meta.get(&quot;front_image_url&quot;, &quot;&quot;)  # 文章封面图
        ssr_nodes = response.css(&quot;table tbody tr&quot;)

        with open(datetime.now().strftime(&#39;%Y-%m-%d&#39;), &#39;a&#39;) as file_object:
            for ssr in ssr_nodes:
                item_loader = SsrItemLoader(item=SsrItem(), response=response)  # 默认ItemLoader是一个list，自定义TakeFirst()
                print(ssr.xpath(&quot;td[4]/text()&quot;).extract_first(&quot;&quot;))
                item_loader.add_value(&quot;url&quot;, response.url)
                item_loader.add_value(&quot;ssr&quot;, ssr.css(&quot;td:nth-child(1)&gt;a::attr(href)&quot;).extract_first(&quot;&quot;))
                item_loader.add_value(&quot;ip&quot;, ssr.css(&quot;td:nth-child(2)::text&quot;).extract_first(&quot;&quot;))
                item_loader.add_value(&quot;front_image_url&quot;, front_image_url)
                item_loader.add_value(&quot;port&quot;, ssr.xpath(&quot;td[3]/text()&quot;).extract_first(&quot;&quot;))
                item_loader.add_value(&quot;password&quot;, ssr.xpath(&quot;td[4]/text()&quot;).extract_first(&quot;&quot;))
                item_loader.add_value(&quot;secret&quot;, ssr.xpath(&quot;td[5]/text()&quot;).extract_first(&quot;&quot;))
                ssr_item = item_loader.load_item()
                file_object.write(ssr.css(&quot;td:nth-child(1)&gt;a::attr(href)&quot;).extract_first(&quot;&quot;)+&quot;\n&quot;)
                yield ssr_item  # 将传到piplines中</code></pre><h2 id="爬虫开发2"><a href="#爬虫开发2" class="headerlink" title="爬虫开发2"></a>爬虫开发2</h2><pre><code class="python"># -*- coding: utf-8 -*-
import time
from datetime import datetime
from urllib import parse

import scrapy
from scrapy.http import Request

from ssr.items import SsrItemLoader, SsrItem


class FanQiangSpider(scrapy.Spider):
    name = &#39;fanqiang.network&#39;
    # 必须一级域名
    allowed_domains = [&#39;fanqiang.network&#39;]
    start_urls = [&#39;https://fanqiang.network/免费ssr&#39;]

    def parse(self, response):
        post_nodes = response.css(&quot;.post-content table tbody tr&quot;)
        item_loader = SsrItemLoader(item=SsrItem(), response=response)
        with open(datetime.now().strftime(&#39;%Y-%m-%d&#39;), &#39;a&#39;) as file_object:
            for post_node in post_nodes:
                item_loader.add_value(&quot;url&quot;, response.url)
                item_loader.add_value(&quot;ssr&quot;, post_node.css(&quot;td:nth-child(1)&gt;a::attr(href)&quot;).extract_first(&quot;&quot;).replace(&quot;http://freevpn-ss.tk/&quot;, &quot;&quot;))
                item_loader.add_value(&quot;ip&quot;, post_node.css(&quot;td:nth-child(2)::text&quot;).extract_first(&quot;&quot;))
                item_loader.add_value(&quot;port&quot;, post_node.xpath(&quot;td[3]/text()&quot;).extract_first(&quot;&quot;))
                item_loader.add_value(&quot;password&quot;, post_node.xpath(&quot;td[4]/text()&quot;).extract_first(&quot;&quot;))
                item_loader.add_value(&quot;secret&quot;, post_node.xpath(&quot;td[5]/text()&quot;).extract_first(&quot;&quot;))
                ssr_item = item_loader.load_item()
                file_object.write(post_node.css(&quot;td:nth-child(1)&gt;a::attr(href)&quot;).extract_first(&quot;&quot;).replace(&quot;http://freevpn-ss.tk/&quot;, &quot;&quot;) + &quot;\n&quot;)
                yield ssr_item  # 将传到piplines中</code></pre><h1 id="多爬虫同时运行"><a href="#多爬虫同时运行" class="headerlink" title="多爬虫同时运行"></a>多爬虫同时运行</h1><p>settings.py</p><pre><code class="ini">COMMANDS_MODULE = &#39;ssr.commands&#39;</code></pre><p>ssr/commands/crawlall.py</p><pre><code>import os
from scrapy.commands import ScrapyCommand
from scrapy.utils.conf import arglist_to_dict
from scrapy.utils.python import without_none_values
from scrapy.exceptions import UsageError


class Command(ScrapyCommand):
    requires_project = True

    def syntax(self):
        return &quot;[options] &lt;spider&gt;&quot;

    def short_desc(self):
        return &quot;Run all spider&quot;

    def add_options(self, parser):
        ScrapyCommand.add_options(self, parser)
        parser.add_option(&quot;-a&quot;, dest=&quot;spargs&quot;, action=&quot;append&quot;, default=[], metavar=&quot;NAME=VALUE&quot;,
                          help=&quot;set spider argument (may be repeated)&quot;)
        parser.add_option(&quot;-o&quot;, &quot;--output&quot;, metavar=&quot;FILE&quot;,
                          help=&quot;dump scraped items into FILE (use - for stdout)&quot;)
        parser.add_option(&quot;-t&quot;, &quot;--output-format&quot;, metavar=&quot;FORMAT&quot;,
                          help=&quot;format to use for dumping items with -o&quot;)

    def process_options(self, args, opts):
        ScrapyCommand.process_options(self, args, opts)
        try:
            opts.spargs = arglist_to_dict(opts.spargs)
        except ValueError:
            raise UsageError(&quot;Invalid -a value, use -a NAME=VALUE&quot;, print_help=False)
        if opts.output:
            if opts.output == &#39;-&#39;:
                self.settings.set(&#39;FEED_URI&#39;, &#39;stdout:&#39;, priority=&#39;cmdline&#39;)
            else:
                self.settings.set(&#39;FEED_URI&#39;, opts.output, priority=&#39;cmdline&#39;)
            feed_exporters = without_none_values(
                self.settings.getwithbase(&#39;FEED_EXPORTERS&#39;))
            valid_output_formats = feed_exporters.keys()
            if not opts.output_format:
                opts.output_format = os.path.splitext(opts.output)[1].replace(&quot;.&quot;, &quot;&quot;)
            if opts.output_format not in valid_output_formats:
                raise UsageError(&quot;Unrecognized output format &#39;%s&#39;, set one&quot;
                                 &quot; using the &#39;-t&#39; switch or as a file extension&quot;
                                 &quot; from the supported list %s&quot; % (opts.output_format,
                                                                  tuple(valid_output_formats)))
            self.settings.set(&#39;FEED_FORMAT&#39;, opts.output_format, priority=&#39;cmdline&#39;)

    def run(self, args, opts):
        # 获取爬虫列表
        spd_loader_list = self.crawler_process.spider_loader.list()  # 获取所有的爬虫文件。
        print(spd_loader_list)
        # 遍历各爬虫
        for spname in spd_loader_list or args:
            self.crawler_process.crawl(spname, **opts.spargs)
            print(&#39;此时启动的爬虫为：&#39; + spname)
        self.crawler_process.start()</code></pre><p>main.py</p><pre><code class="xl">from scrapy import cmdline
from scrapy.cmdline import execute

import sys
import os

sys.path.append(os.path.dirname(os.path.abspath(__file__)))
# execute([&quot;scrapy&quot;, &quot;crawl&quot;, &quot;freevpn-ss.tk&quot;])
# execute([&quot;scrapy&quot;, &quot;crawl&quot;, &quot;fanqiang.network&quot;])
cmdline.execute(&quot;scrapy crawlall&quot;.split())</code></pre><h1 id="防反爬"><a href="#防反爬" class="headerlink" title="防反爬"></a>防反爬</h1><h2 id="随机ua"><a href="#随机ua" class="headerlink" title="随机ua"></a>随机ua</h2><p>pip install -i <a href="https://pypi.doubanio.com/simple/" target="_blank" rel="noopener">https://pypi.doubanio.com/simple/</a> –trusted-host pypi.doubanio.com scrapy-fake-useragent</p><pre><code class="vim">DOWNLOADER_MIDDLEWARES = {
    &#39;scrapy_fake_useragent.middleware.RandomUserAgentMiddleware&#39;: 1,
}</code></pre><blockquote><p>报错socket.timeout: timed out，查看F:/Anaconda3/Lib/site-packages/fake_useragent/settings.py</p></blockquote><pre><code>__version__ = &#39;0.1.11&#39;

DB = os.path.join(
    tempfile.gettempdir(),
    &#39;fake_useragent_{version}.json&#39;.format(
        version=__version__,
    ),
)

CACHE_SERVER = &#39;https://fake-useragent.herokuapp.com/browsers/{version}&#39;.format(
    version=__version__,
)

BROWSERS_STATS_PAGE = &#39;https://www.w3schools.com/browsers/default.asp&#39;

BROWSER_BASE_PAGE = &#39;http://useragentstring.com/pages/useragentstring.php?name={browser}&#39;  # noqa

BROWSERS_COUNT_LIMIT = 50

REPLACEMENTS = {
    &#39; &#39;: &#39;&#39;,
    &#39;_&#39;: &#39;&#39;,
}

SHORTCUTS = {
    &#39;internet explorer&#39;: &#39;internetexplorer&#39;,
    &#39;ie&#39;: &#39;internetexplorer&#39;,
    &#39;msie&#39;: &#39;internetexplorer&#39;,
    &#39;edge&#39;: &#39;internetexplorer&#39;,
    &#39;google&#39;: &#39;chrome&#39;,
    &#39;googlechrome&#39;: &#39;chrome&#39;,
    &#39;ff&#39;: &#39;firefox&#39;,
}

OVERRIDES = {
    &#39;Edge/IE&#39;: &#39;Internet Explorer&#39;,
    &#39;IE/Edge&#39;: &#39;Internet Explorer&#39;,
}

HTTP_TIMEOUT = 5

HTTP_RETRIES = 2

HTTP_DELAY = 0.1</code></pre><blockquote><p><a href="http://useragentstring.com/pages/useragentstring.php?name=Chrome" target="_blank" rel="noopener">http://useragentstring.com/pages/useragentstring.php?name=Chrome</a> 打开超时报错，其中CACHE_SERVER是存储了所有UserAgent的json数据，再次观察其中DB这个变量，结合fake_useragent\fake.py中的逻辑，判断这个变量应该是存储json数据的，所以大体逻辑应该是，首次初始化时，会自动爬取CACHE_SERVER中的json数据，然后将其存储到本地，所以我们直接将json存到指定路径下，再次初始化时，应该就不会报错</p></blockquote><pre><code class="tex">&gt;&gt;&gt; import tempfile
&gt;&gt;&gt; print(tempfile.gettempdir())
C:\Users\codewj\AppData\Local\Temp</code></pre><p>将CACHE_SERVER的json数据保存为fake_useragent_0.1.11.json,并放到目录C:\Users\codewj\AppData\Local\Temp中</p><pre><code>&gt;&gt;&gt; import fake_useragent
&gt;&gt;&gt; ua = fake_useragent.UserAgent()
&gt;&gt;&gt; ua.data_browsers[&#39;chrome&#39;][0]
&#39;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36&#39;</code></pre><blockquote><p>注：如果CACHE_SERVER不是<a href="https://fake-useragent.herokuapp.com/browsers/0.1.11，请更新一下库pip" target="_blank" rel="noopener">https://fake-useragent.herokuapp.com/browsers/0.1.11，请更新一下库pip</a> install –upgrade fake_useragent</p></blockquote></article><div class="post-donate"><div id="donate_board" class="donate_bar center"><a id="btn_donate" class="btn_donate" href="javascript:;" title="打赏"></a> <span class="donate_txt">↑<br> 欢迎投食,求鼓励，求支持！</span><br></div><div id="donate_guide" class="donate_bar center hidden"> <img src="/images/alipay.png" alt="支付宝打赏"> <img src="/images/wechatpay.png" alt="微信打赏"></div><script type="text/javascript">document.getElementById("btn_donate").onclick=function(){$("#donate_board").addClass("hidden"),$("#donate_guide").removeClass("hidden")}</script></div><div class="nexmoe-post-copyright"><i class="mdui-list-item-icon nexmoefont icon-info-circle"></i> <strong>本文作者：</strong>OneJane<br> <strong>本文链接：</strong><a href="https://onejane.github.io/2019/08/23/scrapy爬取ssr链接/" title="https://onejane.github.io/2019/08/23/scrapy爬取ssr链接/" target="_blank" rel="noopener">https://onejane.github.io/2019/08/23/scrapy爬取ssr链接/</a><br> <strong>版权声明：</strong>本文采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/cn/deed.zh" target="_blank">CC BY-NC-SA 3.0 CN</a> 协议进行许可</div><section class="nexmoe-comment"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.5.0/dist/gitalk.min.css"><div id="gitalk"></div><script src="https://cdn.jsdelivr.net/npm/gitalk@1.5.0/dist/gitalk.min.js"></script><script type="text/javascript">var gitalk=new Gitalk({clientID:"e677e59382e1c7a468fd",clientSecret:"717d041bc4ab749f069314862232cfb6ec8adc15",id:decodeURI(window.location.pathname),repo:"onejane.github.io",owner:"onejane",admin:"onejane"});gitalk.render("gitalk")</script></section></div></div></div><script src="https://cdn.jsdelivr.net/npm/mdui@0.4.3/dist/js/mdui.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/smoothscroll-for-websites@1.4.9/SmoothScroll.min.js"></script><script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.15.8/build/highlight.min.js"></script><script>hljs.initHighlightingOnLoad()</script><script src="/js/app.js?v=1567548035314"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@5.1.0/lazysizes.min.js"></script><div hidden><script type="text/javascript" src="https://js.users.51.la/20279757.js"></script></div></body><script src="https://cdn.jsdelivr.net/npm/meting@1.1.0/dist/Meting.min.js"></script></html>