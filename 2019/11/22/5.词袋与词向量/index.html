<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><title>词袋与词向量（5） - OneJane</title><meta charset="UTF-8"><meta name="description" content="微服务,高可用,高并发,人工智能"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="msvalidate.01" content="396E9693347B4D18AAE96D9E75B9B686"><link rel="shortcut icon" href="/images/a.ico" type="image/png"><meta name="description" content="词袋和词向量模型可以将文本数据如转换成计算机能够计算的数据。"><meta name="keywords" content="doc2vec,word2vec,gensim"><meta property="og:type" content="article"><meta property="og:title" content="词袋与词向量（5）"><meta property="og:url" content="https://onejane.github.io/2019/11/22/5.词袋与词向量/index.html"><meta property="og:site_name" content="OneJane"><meta property="og:description" content="词袋和词向量模型可以将文本数据如转换成计算机能够计算的数据。"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2019-11-22T13:34:17.736Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="词袋与词向量（5）"><meta name="twitter:description" content="词袋和词向量模型可以将文本数据如转换成计算机能够计算的数据。"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdui@0.4.3/dist/css/mdui.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.15.8/styles/atom-one-dark.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1038733_0xvrvpg9c0r.css"><link rel="stylesheet" href="/css/style.css?v=1578283207860"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.js"></script></head><body class="mdui-drawer-body-left"><div id="nexmoe-background"><div class="nexmoe-bg" style="background-image:url(https://www.github.com/OneJane/blog/raw/master/小书匠/1566388885395.png)"></div><div class="mdui-appbar mdui-shadow-0"><div class="mdui-toolbar"> <a mdui-drawer="{target: '#drawer', swipe: true}" title="menu" class="mdui-btn mdui-btn-icon"><i class="mdui-icon material-icons">menu</i></a><div class="mdui-toolbar-spacer"></div> <a href="/" title="OneJane" class="mdui-btn mdui-btn-icon"><img src="https://www.github.com/OneJane/blog/raw/master/小书匠/1566388846021.png"></a></div></div></div><div id="nexmoe-header"><div class="nexmoe-drawer mdui-drawer" id="drawer"><div class="nexmoe-avatar mdui-ripple"> <a href="/" title="OneJane"><img src="https://www.github.com/OneJane/blog/raw/master/小书匠/1566388846021.png" alt="OneJane"></a></div><div class="nexmoe-count"><div><span>文章</span>69</div><div><span>标签</span>83</div><div><span>分类</span>12</div></div><ul class="nexmoe-list mdui-list" mdui-collapse="{accordion: true}"><a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/" title="回到首页"><i class="mdui-list-item-icon nexmoefont icon-home"></i><div class="mdui-list-item-content"> 回到首页</div></a><a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/about.html" title="关于博客"><i class="mdui-list-item-icon nexmoefont icon-info-circle"></i><div class="mdui-list-item-content"> 关于博客</div></a><a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/py.html" title="我的朋友"><i class="mdui-list-item-icon nexmoefont icon-unorderedlist"></i><div class="mdui-list-item-content"> 我的朋友</div></a></ul><aside id="nexmoe-sidebar"><div class="nexmoe-widget-wrap"><h3 class="nexmoe-widget-title">社交按钮</h3><div class="nexmoe-widget nexmoe-social"><a class="mdui-ripple" href="https://github.com/OneJane" target="_blank" mdui-tooltip="{content: 'GitHub'}" style="color:#191717;background-color:rgba(25,23,23,.15)"><i class="nexmoefont icon-github"></i></a></div></div><div class="nexmoe-widget-wrap"><h3 class="nexmoe-widget-title">文章分类</h3><div class="nexmoe-widget"><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/人工智能/">人工智能</a><span class="category-list-count">16</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/博客/">博客</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/定时器/">定时器</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/持续集成/">持续集成</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据库/">数据库</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/注册中心/">注册中心</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/测试/">测试</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/系统/">系统</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/自然语言处理/">自然语言处理</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/项目实战/">项目实战</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/高可用/">高可用</a><span class="category-list-count">4</span></li></ul></div></div><div class="nexmoe-widget-wrap"><h3 class="nexmoe-widget-title">标签云</h3><div class="nexmoe-widget tagcloud"> <a href="/tags/Gensim/" style="font-size:10px">Gensim</a> <a href="/tags/Hanlp/" style="font-size:10px">Hanlp</a> <a href="/tags/NLTK/" style="font-size:10px">NLTK</a> <a href="/tags/OpenCV/" style="font-size:12.86px">OpenCV</a> <a href="/tags/Stanford-NLP/" style="font-size:10px">Stanford NLP</a> <a href="/tags/Tensorflow/" style="font-size:15.71px">Tensorflow</a> <a href="/tags/ant-design/" style="font-size:10px">ant design</a> <a href="/tags/ant-design-pro/" style="font-size:11.43px">ant design pro</a> <a href="/tags/auc/" style="font-size:10px">auc</a> <a href="/tags/bottle/" style="font-size:10px">bottle</a> <a href="/tags/chatterbot/" style="font-size:10px">chatterbot</a> <a href="/tags/cnn/" style="font-size:12.86px">cnn</a> <a href="/tags/crf/" style="font-size:12.86px">crf</a> <a href="/tags/doc2vec/" style="font-size:10px">doc2vec</a> <a href="/tags/docker/" style="font-size:17.14px">docker</a> <a href="/tags/dubbo/" style="font-size:11.43px">dubbo</a> <a href="/tags/elasticsearch/" style="font-size:10px">elasticsearch</a> <a href="/tags/elastisearch/" style="font-size:10px">elastisearch</a> <a href="/tags/email/" style="font-size:10px">email</a> <a href="/tags/es6/" style="font-size:10px">es6</a> <a href="/tags/feign/" style="font-size:10px">feign</a> <a href="/tags/flask/" style="font-size:11.43px">flask</a> <a href="/tags/folium/" style="font-size:10px">folium</a> <a href="/tags/freemarker/" style="font-size:10px">freemarker</a> <a href="/tags/function/" style="font-size:10px">function</a> <a href="/tags/gateway/" style="font-size:10px">gateway</a> <a href="/tags/gensim/" style="font-size:11.43px">gensim</a> <a href="/tags/gitlab/" style="font-size:11.43px">gitlab</a> <a href="/tags/gru/" style="font-size:11.43px">gru</a> <a href="/tags/hanlp/" style="font-size:11.43px">hanlp</a> <a href="/tags/haproxy/" style="font-size:10px">haproxy</a> <a href="/tags/hmm/" style="font-size:10px">hmm</a> <a href="/tags/jenkins/" style="font-size:11.43px">jenkins</a> <a href="/tags/jieba/" style="font-size:15.71px">jieba</a> <a href="/tags/jmeter/" style="font-size:10px">jmeter</a> <a href="/tags/keepalived/" style="font-size:10px">keepalived</a> <a href="/tags/lda/" style="font-size:11.43px">lda</a> <a href="/tags/linux/" style="font-size:10px">linux</a> <a href="/tags/lstm/" style="font-size:12.86px">lstm</a> <a href="/tags/maven/" style="font-size:11.43px">maven</a> <a href="/tags/multi-druid/" style="font-size:10px">multi druid</a> <a href="/tags/mybatis/" style="font-size:10px">mybatis</a> <a href="/tags/mybatisplus/" style="font-size:10px">mybatisplus</a> <a href="/tags/mysql/" style="font-size:10px">mysql</a> <a href="/tags/n-gram/" style="font-size:10px">n-gram</a> <a href="/tags/nacos/" style="font-size:11.43px">nacos</a> <a href="/tags/neo4j/" style="font-size:11.43px">neo4j</a> <a href="/tags/nexmoe/" style="font-size:10px">nexmoe</a> <a href="/tags/nlp/" style="font-size:20px">nlp</a> <a href="/tags/numpy/" style="font-size:10px">numpy</a> <a href="/tags/partition/" style="font-size:10px">partition</a> <a href="/tags/procedure/" style="font-size:10px">procedure</a> <a href="/tags/pxc/" style="font-size:10px">pxc</a> <a href="/tags/pyhanlp/" style="font-size:11.43px">pyhanlp</a> <a href="/tags/python/" style="font-size:10px">python</a> <a href="/tags/rabbitmq/" style="font-size:10px">rabbitmq</a> <a href="/tags/react/" style="font-size:11.43px">react</a> <a href="/tags/redis/" style="font-size:11.43px">redis</a> <a href="/tags/redis-cluster/" style="font-size:10px">redis-cluster</a> <a href="/tags/replication/" style="font-size:11.43px">replication</a> <a href="/tags/rnn/" style="font-size:10px">rnn</a> <a href="/tags/rocketmq/" style="font-size:11.43px">rocketmq</a> <a href="/tags/scrapy/" style="font-size:12.86px">scrapy</a> <a href="/tags/selenium/" style="font-size:12.86px">selenium</a> <a href="/tags/sentinel/" style="font-size:14.29px">sentinel</a> <a href="/tags/seq2seq/" style="font-size:10px">seq2seq</a> <a href="/tags/session/" style="font-size:10px">session</a> <a href="/tags/sklearn/" style="font-size:10px">sklearn</a> <a href="/tags/skywalking/" style="font-size:11.43px">skywalking</a> <a href="/tags/snownlp/" style="font-size:10px">snownlp</a> <a href="/tags/spring-cloud-alibaba/" style="font-size:18.57px">spring cloud alibaba</a> <a href="/tags/springboot/" style="font-size:14.29px">springboot</a> <a href="/tags/svm/" style="font-size:10px">svm</a> <a href="/tags/swagger/" style="font-size:10px">swagger</a> <a href="/tags/textrank/" style="font-size:10px">textrank</a> <a href="/tags/tf-idf/" style="font-size:12.86px">tf-idf</a> <a href="/tags/tk-mybatis/" style="font-size:10px">tk mybatis</a> <a href="/tags/umi/" style="font-size:10px">umi</a> <a href="/tags/validate/" style="font-size:10px">validate</a> <a href="/tags/word2vec/" style="font-size:10px">word2vec</a> <a href="/tags/wordcloud/" style="font-size:10px">wordcloud</a> <a href="/tags/xxl-job/" style="font-size:11.43px">xxl-job</a> <a href="/tags/zookeeper/" style="font-size:10px">zookeeper</a></div></div><div class="nexmoe-widget-wrap"><h3 class="nexmoe-widget-title">文章归档</h3><div class="nexmoe-widget"><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">十月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">九月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li></ul></div></div></aside><div class="nexmoe-copyright"> &copy; 2020 OneJane</div></div></div><div id="nexmoe-content"><div class="nexmoe-primary"><div class="nexmoe-post"><div class="nexmoe-post-cover"> <img src="https://www.github.com/OneJane/blog/raw/master/小书匠/5f549e01ba4ba668ee78d415b042a010_hd.jpg"><h1>词袋与词向量（5）</h1></div><div class="nexmoe-post-meta"><a><i class="nexmoefont icon-calendar-fill"></i> 2019年11月22日</a><a><i class="nexmoefont icon-areachart"></i> 2.8k 字</a><a><i class="nexmoefont icon-time-circle-fill"></i> 大概 12 分钟</a> <a class="nexmoefont icon-appstore-fill -link" href="/categories/自然语言处理/">自然语言处理</a> <a class="nexmoefont icon-tag-fill -link" href="/tags/doc2vec/">doc2vec</a> <a class="nexmoefont icon-tag-fill -link" href="/tags/gensim/">gensim</a> <a class="nexmoefont icon-tag-fill -link" href="/tags/word2vec/">word2vec</a></div><article><p>词袋和词向量模型可以将文本数据如转换成计算机能够计算的数据。</p><a id="more"></a><h1 id="词袋模型（Bag-of-Words-Model）"><a href="#词袋模型（Bag-of-Words-Model）" class="headerlink" title="词袋模型（Bag of Words Model）"></a>词袋模型（Bag of Words Model）</h1><p>词袋模型把文本（段落或者文档）被看作是无序的词汇集合，忽略语法甚至是单词的顺序，把每一个单词都进行统计，同时计算每个单词出现的次数，常常被用在文本分类中，如贝叶斯算法、LDA 和 LSA 等。</p><h2 id="实战词袋模型"><a href="#实战词袋模型" class="headerlink" title="实战词袋模型"></a>实战词袋模型</h2><pre><code class="makefile">import jieba
#定义停用词、标点符号
punctuation = [&quot;，&quot;,&quot;。&quot;, &quot;：&quot;, &quot;；&quot;, &quot;？&quot;]
#定义语料
content = [&quot;机器学习带动人工智能飞速的发展。&quot;,
           &quot;深度学习带动人工智能飞速的发展。&quot;,
           &quot;机器学习和深度学习带动人工智能飞速的发展。&quot;
          ]
#分词
segs_1 = [jieba.lcut(con) for con in content]
print(segs_1)          </code></pre><blockquote><p>输出： [[‘机器’, ‘学习’, ‘带动’, ‘人工智能’, ‘飞速’, ‘的’, ‘发展’, ‘。’], [‘深度’, ‘学习’, ‘带动’, ‘人工智能’, ‘飞速’, ‘的’, ‘发展’, ‘。’], [‘机器’, ‘学习’, ‘和’, ‘深度’, ‘学习’, ‘带动’, ‘人工智能’, ‘飞速’, ‘的’, ‘发展’, ‘。’]]</p></blockquote><pre><code class="livecodeserver"># 去标点符号
tokenized = []
for sentence in segs_1:
    words = []
    for word in sentence:
        if word not in punctuation:          
            words.append(word)
    tokenized.append(words)
print(tokenized)</code></pre><blockquote><p>输出：[[‘机器’, ‘学习’, ‘带动’, ‘人工智能’, ‘飞速’, ‘的’, ‘发展’], [‘深度’, ‘学习’, ‘带动’, ‘人工智能’, ‘飞速’, ‘的’, ‘发展’], [‘机器’, ‘学习’, ‘和’, ‘深度’, ‘学习’, ‘带动’, ‘人工智能’, ‘飞速’, ‘的’, ‘发展’]]</p></blockquote><pre><code class="applescript"># 把所有的分词结果放到一个袋子（List）里面，也就是取并集，再去重，获取对应的特征词。
#求并集
bag_of_words = [ x for item in segs_1 for x in item if x not in punctuation]
#去重
bag_of_words = list(set(bag_of_words))
print(bag_of_words)</code></pre><blockquote><p>输出： [‘飞速’, ‘的’, ‘深度’, ‘人工智能’, ‘发展’, ‘和’, ‘机器’, ‘学习’, ‘带动’]</p></blockquote><pre><code class="livecodeserver"># 以上面特征词的顺序，完成词袋化，得到词袋向量
bag_of_word2vec = []
for sentence in tokenized:
    tokens = [1 if token in sentence else 0 for token in bag_of_words ]
    bag_of_word2vec.append(tokens)</code></pre><blockquote><p>输出：[[1, 1, 0, 1, 1, 0, 1, 1, 1], [1, 1, 1, 1, 1, 0, 0, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]]</p></blockquote><h2 id="Gensim-构建词袋模型"><a href="#Gensim-构建词袋模型" class="headerlink" title="Gensim 构建词袋模型"></a>Gensim 构建词袋模型</h2><pre><code class="python">from gensim import corpora
import gensim
#tokenized是去标点之后的
dictionary = corpora.Dictionary(tokenized)
#保存词典
dictionary.save(&#39;deerwester.dict&#39;) 
print(dictionary)</code></pre><blockquote><p>输出： Dictionary(9 unique tokens: [‘人工智能’, ‘发展’, ‘学习’, ‘带动’, ‘机器’]…)</p></blockquote><pre><code class="vala">#查看词典和下标 id 的映射
print(dictionary.token2id)</code></pre><blockquote><p>输出： {‘人工智能’: 0, ‘发展’: 1, ‘学习’: 2, ‘带动’: 3, ‘机器’: 4, ‘的’: 5, ‘飞速’: 6, ‘深度’: 7, ‘和’: 8}</p></blockquote><pre><code class="stylus"># doc2bow()，作用只是计算每个不同单词的出现次数，将单词转换为其整数单词 id 并将结果作为稀疏向量返回。
corpus = [dictionary.doc2bow(sentence) for sentence in segs_1]
print(corpus )</code></pre><blockquote><p>输出： [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(0, 1), (1, 1), (2, 1), (3, 1), (5, 1), (6, 1), (7, 1)], [(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]]</p></blockquote><h1 id="词向量-（Word-Embedding）"><a href="#词向量-（Word-Embedding）" class="headerlink" title="词向量 （Word Embedding）"></a>词向量 （Word Embedding）</h1><p>词向量技术是将词语转化成为稠密向量。在自然语言处理应用中，词向量作为机器学习、深度学习模型的特征进行输入。因此，最终模型的效果很大程度上取决于词向量的效果。<br>在 Word2Vec 出现之前，自然语言处理经常把字词进行独热编码，也就是 One-Hot Encoder。</p><pre><code class="lsl">大数据 [0,0,0,0,0,0,0,1,0,……，0,0,0,0,0,0,0]
云计算[0,0,0,0,1,0,0,0,0,……，0,0,0,0,0,0,0]
机器学习[0,0,0,1,0,0,0,0,0,……，0,0,0,0,0,0,0]
人工智能[0,0,0,0,0,0,0,0,0,……，1,0,0,0,0,0,0]</code></pre><p>比如上面的例子中，大数据 、云计算、机器学习和人工智能各对应一个向量，向量中只有一个值为1，其余都为0。所以使用 One-Hot Encoder有以下问题：</p><p>第一，词语编码是随机的，向量之间相互独立，看不出词语之间可能存在的关联关系。<br>第二，向量维度的大小取决于语料库中词语的多少，如果语料包含的所有词语对应的向量合为一个矩阵的话，那这个矩阵过于稀疏，并且会造成维度灾难。<br>而解决这个问题的手段，就是使用向量表示（Vector Representations）。比如 Word2Vec 可以将 One-Hot Encoder 转化为低维度的连续值，也就是稠密向量，并且其中意思相近的词也将被映射到向量空间中相近的位置。经过降维，在二维空间中，相似的单词在空间中的距离也很接近。</p><p>这里简单给词向量一个定义，词向量就是要用某个固定维度的向量去表示单词。也就是说要把单词变成固定维度的向量，作为机器学习（Machine Learning）或深度学习模型的特征向量输入。</p><h2 id="词向量实战"><a href="#词向量实战" class="headerlink" title="词向量实战"></a>词向量实战</h2><h3 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h3><p>Word2Vec 主要包含两种模型：Skip-Gram 和 CBOW，值得一提的是，Word2Vec 词向量可以较好地表达不同词之间的相似和类比关系。</p><pre><code class="lsl">pip install gensim
from gensim.models import Word2Vec  
import jieba
#定义停用词、标点符号
punctuation = [&quot;,&quot;,&quot;。&quot;, &quot;:&quot;, &quot;;&quot;, &quot;.&quot;, &quot;&#39;&quot;, &#39;&quot;&#39;, &quot;’&quot;, &quot;?&quot;, &quot;/&quot;, &quot;-&quot;, &quot;+&quot;, &quot;&amp;&quot;, &quot;(&quot;, &quot;)&quot;]
sentences = [
&quot;长江是中国第一大河，干流全长6397公里（以沱沱河为源），一般称6300公里。流域总面积一百八十余万平方公里，年平均入海水量约九千六百余亿立方米。以干流长度和入海水量论，长江均居世界第三位。&quot;,
&quot;黄河，中国古代也称河，发源于中华人民共和国青海省巴颜喀拉山脉，流经青海、四川、甘肃、宁夏、内蒙古、陕西、山西、河南、山东9个省区，最后于山东省东营垦利县注入渤海。干流河道全长5464千米，仅次于长江，为中国第二长河。黄河还是世界第五长河。&quot;,
&quot;黄河,是中华民族的母亲河。作为中华文明的发祥地,维系炎黄子孙的血脉.是中华民族民族精神与民族情感的象征。&quot;,
&quot;黄河被称为中华文明的母亲河。公元前2000多年华夏族在黄河领域的中原地区形成、繁衍。&quot;,
&quot;在兰州的“黄河第一桥”内蒙古托克托县河口镇以上的黄河河段为黄河上游。&quot;,
&quot;黄河上游根据河道特性的不同，又可分为河源段、峡谷段和冲积平原三部分。 &quot;,
&quot;黄河,是中华民族的母亲河。&quot;
]
# 定义好语料，接下来进行分词，去标点符号
sentences = [jieba.lcut(sen) for sen in sentences]
tokenized = []
for sentence in sentences:
    words = []
    for word in sentence:
        if word not in punctuation:          
            words.append(word)
    tokenized.append(words)
# 模型训练    
model = Word2Vec(tokenized, sg=1, size=100,  window=5,  min_count=2,  negative=1, sample=0.001, hs=1, workers=4)    </code></pre><ul><li>sg=1 是 skip-gram 算法，对低频词敏感；默认 sg=0 为 CBOW 算法。</li><li>size 是输出词向量的维数，值太小会导致词映射因为冲突而影响结果，值太大则会耗内存并使算法计算变慢，一般值取为100到200之间。</li><li>window 是句子中当前词与目标词之间的最大距离，3表示在目标词前看3-b 个词，后面看 b 个词（b 在0-3之间随机）。</li><li>min_count 是对词进行过滤，频率小于 min-count 的单词则会被忽视，默认值为5。</li><li>negative 和 sample 可根据训练结果进行微调，sample 表示更高频率的词被随机下采样到所设置的阈值，默认值为 1e-3。</li><li>hs=1 表示层级 softmax 将会被使用，默认 hs=0 且 negative 不为0，则负采样将会被选择使用。</li></ul><pre><code class="stan"># 训练后的模型可以保存与加载
model.save(&#39;model&#39;)  #保存模型
model = Word2Vec.load(&#39;model&#39;)   #加载模型
# 模型训练好之后，接下来就可以使用模型，可以用来计算句子或者词的相似性、最大匹配程度等。
# 计算相似度
print(model.similarity(&#39;黄河&#39;, &#39;长江&#39;))
# 预测最接近的词，预测与黄河和母亲河最接近，而与长江不接近的词
print(model.most_similar(positive=[&#39;黄河&#39;, &#39;母亲河&#39;], negative=[&#39;长江&#39;]))</code></pre><blockquote><p>输出： [(‘是’, 0.14632007479667664), (‘以’, 0.14630728960037231), (‘长河’, 0.13878652453422546), (‘河道’, 0.13716217875480652), (‘在’, 0.11577725410461426), (‘全长’, 0.10969121754169464), (‘内蒙古’, 0.07590540498495102), (‘入海’, 0.06970417499542236), (‘民族’, 0.06064444035291672), (‘中华文明’, 0.057667165994644165)]</p></blockquote><p>Word2Vec 是一种将词变成词向量的工具。通俗点说，只有这样文本预料才转化为计算机能够计算的矩阵向量。</p><h3 id="Doc2Vec"><a href="#Doc2Vec" class="headerlink" title="Doc2Vec"></a>Doc2Vec</h3><p>在 Gensim 库中，Doc2Vec 与 Word2Vec 都极为相似。但两者在对输入数据的预处理上稍有不同，Doc2vec 接收一个由 LabeledSentence 对象组成的迭代器作为其构造函数的输入参数。其中，LabeledSentence 是 Gensim 内建的一个类，它接收两个 List 作为其初始化的参数：word list 和 label list。</p><p>Doc2Vec 也包括两种实现方式：DBOW（Distributed Bag of Words）和 DM （Distributed Memory）。DBOW 和 DM 的实现，二者在 gensim 库中的实现用的是同一个方法，该方法中参数 dm = 0 或者 dm=1 决定调用 DBOW 还是 DM。Doc2Vec 将文档语料通过一个固定长度的向量表达。</p><p>下面是 Gensim 中 Doc2Vec 模型的实战，我们把上述语料每一句话当做一个文本，添加上对应的标签。接下来，定义数据预处理类，作用是给每个文章添加对应的标签：</p><pre><code class="ruby">#定义数据预处理类，作用是给每个文章添加对应的标签
from gensim.models.doc2vec import Doc2Vec,LabeledSentence
doc_labels = [&quot;长江&quot;,&quot;黄河&quot;,&quot;黄河&quot;,&quot;黄河&quot;,&quot;黄河&quot;,&quot;黄河&quot;,&quot;黄河&quot;]
class LabeledLineSentence(object):
    def __init__(self, doc_list, labels_list):
       self.labels_list = labels_list
       self.doc_list = doc_list
    def __iter__(self):
        for idx, doc in enumerate(self.doc_list):
            yield LabeledSentence(words=doc,tags=[self.labels_list[idx]])

    model = Doc2Vec(documents,dm=1, size=100, window=8, min_count=5, workers=4)
    model.save(&#39;model&#39;)
    model = Doc2Vec.load(&#39;model&#39;) 
# 定义好了数据预处理函数，我们将 Word2Vec 中分词去标点后的数据，进行转换    
iter_data = LabeledLineSentence(tokenized, doc_labels)
# 开始定义模型参数，这里 dm=1，采用了 Gensim 中的 DM 实现
model = Doc2Vec(dm=1, size=100, window=8, min_count=5, workers=4)
model.build_vocab(iter_data)
# 训练模型， 设置迭代次数1000次，start_alpha 为开始学习率，end_alpha 与 start_alpha 线性递减。
model.train(iter_data,total_examples=model.corpus_count,epochs=1000,start_alpha=0.01,end_alpha =0.001)
# 进行一些预测 根据标签找最相似的，这里只有黄河和长江，所以结果为长江，并计算出了相似度
print(model.docvecs.most_similar(&#39;黄河&#39;))</code></pre><blockquote><p>输出： [(‘长江’, 0.25543850660324097)]</p></blockquote><pre><code>print(model.docvecs.similarity(&#39;黄河&#39;,&#39;长江&#39;))</code></pre><blockquote><p>输出： 0.25543848271351405</p></blockquote><p>最终影响模型准确率的因素有：文档的数量越多，文档的相似性越好，也就是基于大数据量的模型训练。在工业界，Word2Vec 和 Doc2Vec 常见的应用有：做相似词计算；相关词挖掘，在推荐系统中用在品牌、用户、商品挖掘中；上下文预测句子；机器翻译；作为特征输入其他模型等。</p><p>总结，本文只是简单的介绍了词袋和词向量模型的典型应用，对于两者的理论和其他词向量模型，比如 TextRank 、FastText 和 GloVe 等。</p></article><div class="post-donate"><div id="donate_board" class="donate_bar center"><a id="btn_donate" class="btn_donate" href="javascript:;" title="打赏"></a> <span class="donate_txt">↑<br> 欢迎投食,求鼓励，求支持！</span><br></div><div id="donate_guide" class="donate_bar center hidden"> <img src="/images/alipay.png" alt="支付宝打赏"> <img src="/images/wechatpay.png" alt="微信打赏"></div><script type="text/javascript">document.getElementById("btn_donate").onclick=function(){$("#donate_board").addClass("hidden"),$("#donate_guide").removeClass("hidden")}</script></div><div class="nexmoe-post-copyright"><i class="mdui-list-item-icon nexmoefont icon-info-circle"></i> <strong>本文作者：</strong>OneJane<br> <strong>本文链接：</strong><a href="https://onejane.github.io/2019/11/22/5.词袋与词向量/" title="https://onejane.github.io/2019/11/22/5.词袋与词向量/" target="_blank" rel="noopener">https://onejane.github.io/2019/11/22/5.词袋与词向量/</a><br> <strong>版权声明：</strong>本文采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/cn/deed.zh" target="_blank">CC BY-NC-SA 3.0 CN</a> 协议进行许可</div><section class="nexmoe-comment"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.5.0/dist/gitalk.min.css"><div id="gitalk"></div><script src="https://cdn.jsdelivr.net/npm/gitalk@1.5.0/dist/gitalk.min.js"></script><script type="text/javascript">var gitalk=new Gitalk({clientID:"e677e59382e1c7a468fd",clientSecret:"717d041bc4ab749f069314862232cfb6ec8adc15",id:decodeURI(window.location.pathname),repo:"onejane.github.io",owner:"onejane",admin:"onejane"});gitalk.render("gitalk")</script></section></div></div></div><script src="https://cdn.jsdelivr.net/npm/mdui@0.4.3/dist/js/mdui.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/smoothscroll-for-websites@1.4.9/SmoothScroll.min.js"></script><script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.15.8/build/highlight.min.js"></script><script>hljs.initHighlightingOnLoad()</script><script src="/js/app.js?v=1578283207874"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@5.1.0/lazysizes.min.js"></script><div hidden><script type="text/javascript" src="https://js.users.51.la/20279757.js"></script></div></body><script src="https://cdn.jsdelivr.net/npm/meting@1.1.0/dist/Meting.min.js"></script></html>