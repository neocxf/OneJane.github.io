<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><title>基于CRF的中文句法依存分析模型实现（17） - OneJane</title><meta charset="UTF-8"><meta name="description" content="微服务,高可用,高并发,人工智能"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="msvalidate.01" content="396E9693347B4D18AAE96D9E75B9B686"><link rel="shortcut icon" href="/images/a.ico" type="image/png"><meta name="description" content="句法分析是自然语言处理中的关键技术之一，其基本任务是确定句子的句法结构或者句子中词汇之间的依存关系。主要包括两方面的内容，一是确定语言的语法体系，即对语言中合法句子的语法结构给予形式化的定义；另一方面是句法分析技术，即根据给定的语法体系，自动推导出句子的句法结构，分析句子所包含的句法单位和这些句法单位之间的关系。"><meta name="keywords" content="crf"><meta property="og:type" content="article"><meta property="og:title" content="基于CRF的中文句法依存分析模型实现（17）"><meta property="og:url" content="https://onejane.github.io/2019/11/22/17.基于CRF的中文句法依存分析模型实现/index.html"><meta property="og:site_name" content="OneJane"><meta property="og:description" content="句法分析是自然语言处理中的关键技术之一，其基本任务是确定句子的句法结构或者句子中词汇之间的依存关系。主要包括两方面的内容，一是确定语言的语法体系，即对语言中合法句子的语法结构给予形式化的定义；另一方面是句法分析技术，即根据给定的语法体系，自动推导出句子的句法结构，分析句子所包含的句法单位和这些句法单位之间的关系。"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1574426380087.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1574426407110.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1574426456581.png"><meta property="og:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1574426645490.png"><meta property="og:updated_time" content="2019-11-22T13:18:20.715Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="基于CRF的中文句法依存分析模型实现（17）"><meta name="twitter:description" content="句法分析是自然语言处理中的关键技术之一，其基本任务是确定句子的句法结构或者句子中词汇之间的依存关系。主要包括两方面的内容，一是确定语言的语法体系，即对语言中合法句子的语法结构给予形式化的定义；另一方面是句法分析技术，即根据给定的语法体系，自动推导出句子的句法结构，分析句子所包含的句法单位和这些句法单位之间的关系。"><meta name="twitter:image" content="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1574426380087.png"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdui@0.4.3/dist/css/mdui.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.15.8/styles/atom-one-dark.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1038733_0xvrvpg9c0r.css"><link rel="stylesheet" href="/css/style.css?v=1577613607924"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.js"></script></head><body class="mdui-drawer-body-left"><div id="nexmoe-background"><div class="nexmoe-bg" style="background-image:url(https://www.github.com/OneJane/blog/raw/master/小书匠/1566388885395.png)"></div><div class="mdui-appbar mdui-shadow-0"><div class="mdui-toolbar"> <a mdui-drawer="{target: '#drawer', swipe: true}" title="menu" class="mdui-btn mdui-btn-icon"><i class="mdui-icon material-icons">menu</i></a><div class="mdui-toolbar-spacer"></div> <a href="/" title="OneJane" class="mdui-btn mdui-btn-icon"><img src="https://www.github.com/OneJane/blog/raw/master/小书匠/1566388846021.png"></a></div></div></div><div id="nexmoe-header"><div class="nexmoe-drawer mdui-drawer" id="drawer"><div class="nexmoe-avatar mdui-ripple"> <a href="/" title="OneJane"><img src="https://www.github.com/OneJane/blog/raw/master/小书匠/1566388846021.png" alt="OneJane"></a></div><div class="nexmoe-count"><div><span>文章</span>69</div><div><span>标签</span>83</div><div><span>分类</span>12</div></div><ul class="nexmoe-list mdui-list" mdui-collapse="{accordion: true}"><a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/" title="回到首页"><i class="mdui-list-item-icon nexmoefont icon-home"></i><div class="mdui-list-item-content"> 回到首页</div></a><a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/about.html" title="关于博客"><i class="mdui-list-item-icon nexmoefont icon-info-circle"></i><div class="mdui-list-item-content"> 关于博客</div></a><a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/py.html" title="我的朋友"><i class="mdui-list-item-icon nexmoefont icon-unorderedlist"></i><div class="mdui-list-item-content"> 我的朋友</div></a></ul><aside id="nexmoe-sidebar"><div class="nexmoe-widget-wrap"><h3 class="nexmoe-widget-title">社交按钮</h3><div class="nexmoe-widget nexmoe-social"><a class="mdui-ripple" href="https://github.com/OneJane" target="_blank" mdui-tooltip="{content: 'GitHub'}" style="color:#191717;background-color:rgba(25,23,23,.15)"><i class="nexmoefont icon-github"></i></a></div></div><div class="nexmoe-widget-wrap"><h3 class="nexmoe-widget-title">文章分类</h3><div class="nexmoe-widget"><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/人工智能/">人工智能</a><span class="category-list-count">16</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/博客/">博客</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/定时器/">定时器</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/持续集成/">持续集成</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据库/">数据库</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/注册中心/">注册中心</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/测试/">测试</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/系统/">系统</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/自然语言处理/">自然语言处理</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/项目实战/">项目实战</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/高可用/">高可用</a><span class="category-list-count">4</span></li></ul></div></div><div class="nexmoe-widget-wrap"><h3 class="nexmoe-widget-title">标签云</h3><div class="nexmoe-widget tagcloud"> <a href="/tags/Gensim/" style="font-size:10px">Gensim</a> <a href="/tags/Hanlp/" style="font-size:10px">Hanlp</a> <a href="/tags/NLTK/" style="font-size:10px">NLTK</a> <a href="/tags/OpenCV/" style="font-size:12.86px">OpenCV</a> <a href="/tags/Stanford-NLP/" style="font-size:10px">Stanford NLP</a> <a href="/tags/Tensorflow/" style="font-size:15.71px">Tensorflow</a> <a href="/tags/ant-design/" style="font-size:10px">ant design</a> <a href="/tags/ant-design-pro/" style="font-size:11.43px">ant design pro</a> <a href="/tags/auc/" style="font-size:10px">auc</a> <a href="/tags/bottle/" style="font-size:10px">bottle</a> <a href="/tags/chatterbot/" style="font-size:10px">chatterbot</a> <a href="/tags/cnn/" style="font-size:12.86px">cnn</a> <a href="/tags/crf/" style="font-size:12.86px">crf</a> <a href="/tags/doc2vec/" style="font-size:10px">doc2vec</a> <a href="/tags/docker/" style="font-size:17.14px">docker</a> <a href="/tags/dubbo/" style="font-size:11.43px">dubbo</a> <a href="/tags/elasticsearch/" style="font-size:10px">elasticsearch</a> <a href="/tags/elastisearch/" style="font-size:10px">elastisearch</a> <a href="/tags/email/" style="font-size:10px">email</a> <a href="/tags/es6/" style="font-size:10px">es6</a> <a href="/tags/feign/" style="font-size:10px">feign</a> <a href="/tags/flask/" style="font-size:11.43px">flask</a> <a href="/tags/folium/" style="font-size:10px">folium</a> <a href="/tags/freemarker/" style="font-size:10px">freemarker</a> <a href="/tags/function/" style="font-size:10px">function</a> <a href="/tags/gateway/" style="font-size:10px">gateway</a> <a href="/tags/gensim/" style="font-size:11.43px">gensim</a> <a href="/tags/gitlab/" style="font-size:11.43px">gitlab</a> <a href="/tags/gru/" style="font-size:11.43px">gru</a> <a href="/tags/hanlp/" style="font-size:11.43px">hanlp</a> <a href="/tags/haproxy/" style="font-size:10px">haproxy</a> <a href="/tags/hmm/" style="font-size:10px">hmm</a> <a href="/tags/jenkins/" style="font-size:11.43px">jenkins</a> <a href="/tags/jieba/" style="font-size:15.71px">jieba</a> <a href="/tags/jmeter/" style="font-size:10px">jmeter</a> <a href="/tags/keepalived/" style="font-size:10px">keepalived</a> <a href="/tags/lda/" style="font-size:11.43px">lda</a> <a href="/tags/linux/" style="font-size:10px">linux</a> <a href="/tags/lstm/" style="font-size:12.86px">lstm</a> <a href="/tags/maven/" style="font-size:11.43px">maven</a> <a href="/tags/multi-druid/" style="font-size:10px">multi druid</a> <a href="/tags/mybatis/" style="font-size:10px">mybatis</a> <a href="/tags/mybatisplus/" style="font-size:10px">mybatisplus</a> <a href="/tags/mysql/" style="font-size:10px">mysql</a> <a href="/tags/n-gram/" style="font-size:10px">n-gram</a> <a href="/tags/nacos/" style="font-size:11.43px">nacos</a> <a href="/tags/neo4j/" style="font-size:11.43px">neo4j</a> <a href="/tags/nexmoe/" style="font-size:10px">nexmoe</a> <a href="/tags/nlp/" style="font-size:20px">nlp</a> <a href="/tags/numpy/" style="font-size:10px">numpy</a> <a href="/tags/partition/" style="font-size:10px">partition</a> <a href="/tags/procedure/" style="font-size:10px">procedure</a> <a href="/tags/pxc/" style="font-size:10px">pxc</a> <a href="/tags/pyhanlp/" style="font-size:11.43px">pyhanlp</a> <a href="/tags/python/" style="font-size:10px">python</a> <a href="/tags/rabbitmq/" style="font-size:10px">rabbitmq</a> <a href="/tags/react/" style="font-size:11.43px">react</a> <a href="/tags/redis/" style="font-size:11.43px">redis</a> <a href="/tags/redis-cluster/" style="font-size:10px">redis-cluster</a> <a href="/tags/replication/" style="font-size:11.43px">replication</a> <a href="/tags/rnn/" style="font-size:10px">rnn</a> <a href="/tags/rocketmq/" style="font-size:11.43px">rocketmq</a> <a href="/tags/scrapy/" style="font-size:12.86px">scrapy</a> <a href="/tags/selenium/" style="font-size:12.86px">selenium</a> <a href="/tags/sentinel/" style="font-size:14.29px">sentinel</a> <a href="/tags/seq2seq/" style="font-size:10px">seq2seq</a> <a href="/tags/session/" style="font-size:10px">session</a> <a href="/tags/sklearn/" style="font-size:10px">sklearn</a> <a href="/tags/skywalking/" style="font-size:11.43px">skywalking</a> <a href="/tags/snownlp/" style="font-size:10px">snownlp</a> <a href="/tags/spring-cloud-alibaba/" style="font-size:18.57px">spring cloud alibaba</a> <a href="/tags/springboot/" style="font-size:14.29px">springboot</a> <a href="/tags/svm/" style="font-size:10px">svm</a> <a href="/tags/swagger/" style="font-size:10px">swagger</a> <a href="/tags/textrank/" style="font-size:10px">textrank</a> <a href="/tags/tf-idf/" style="font-size:12.86px">tf-idf</a> <a href="/tags/tk-mybatis/" style="font-size:10px">tk mybatis</a> <a href="/tags/umi/" style="font-size:10px">umi</a> <a href="/tags/validate/" style="font-size:10px">validate</a> <a href="/tags/word2vec/" style="font-size:10px">word2vec</a> <a href="/tags/wordcloud/" style="font-size:10px">wordcloud</a> <a href="/tags/xxl-job/" style="font-size:11.43px">xxl-job</a> <a href="/tags/zookeeper/" style="font-size:10px">zookeeper</a></div></div><div class="nexmoe-widget-wrap"><h3 class="nexmoe-widget-title">文章归档</h3><div class="nexmoe-widget"><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">十月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">九月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li></ul></div></div></aside><div class="nexmoe-copyright"> &copy; 2019 OneJane</div></div></div><div id="nexmoe-content"><div class="nexmoe-primary"><div class="nexmoe-post"><div class="nexmoe-post-cover"> <img src="https://www.github.com/OneJane/blog/raw/master/小书匠/v2-70bef3f757b3b9c179b845ddbb3a8785_hd.jpg"><h1>基于CRF的中文句法依存分析模型实现（17）</h1></div><div class="nexmoe-post-meta"><a><i class="nexmoefont icon-calendar-fill"></i> 2019年11月22日</a><a><i class="nexmoefont icon-areachart"></i> 3.3k 字</a><a><i class="nexmoefont icon-time-circle-fill"></i> 大概 15 分钟</a> <a class="nexmoefont icon-appstore-fill -link" href="/categories/自然语言处理/">自然语言处理</a> <a class="nexmoefont icon-tag-fill -link" href="/tags/crf/">crf</a></div><article><p>句法分析是自然语言处理中的关键技术之一，其基本任务是确定句子的句法结构或者句子中词汇之间的依存关系。主要包括两方面的内容，一是确定语言的语法体系，即对语言中合法句子的语法结构给予形式化的定义；另一方面是句法分析技术，即根据给定的语法体系，自动推导出句子的句法结构，分析句子所包含的句法单位和这些句法单位之间的关系。</p><a id="more"></a><p>依存关系本身是一个树结构，每一个词看成一个节点，依存关系就是一条有向边。本文主要通过清华大学的句法标注语料库，来实现基于 CRF 的中文句法依存分析模型。</p><h1 id="清华大学句法标注语料库"><a href="#清华大学句法标注语料库" class="headerlink" title="清华大学句法标注语料库"></a>清华大学句法标注语料库</h1><p>清华大学的句法标注语料，包括训练集（train.conll）和开发集合文件（dev.conll）。训练集大小 5.41M，共185541条数据。测试集大小为578kb，共19302条数据。</p><p>语料本身格式如下图所示：<br><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1574426380087.png" alt="enter description here"></p><p>通过上图，我们可以看出，每行语料包括有8个标签，分别是<br>ID、FROM、lEMMA、CPOSTAG、POSTAG、FEATS、HEAD、DEPREL。详细介绍如下图：</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1574426407110.png" alt="enter description here"></p><h1 id="模型的实现"><a href="#模型的实现" class="headerlink" title="模型的实现"></a>模型的实现</h1><p>通过上面对句法依存关键技术的定义，我们明白了，句法依存的基本任务是确定句子的句法结构或者句子中词汇之间的依存关系。同时，我们也对此次模型实现的语料有了基本了解。</p><p>有了这些基础内容，我们便可以开始着手开发了。</p><p>本模型的实现过程，我们将主要分为训练集和测试集数据预处理、语料特征生成、模型训练及预测三大部分来实现，最终将通过模型预测得到正确的预测结果。</p><p>本次实战演练，我们选择以下模型和软件：</p><ul><li>Sklearn_crfsuite</li><li>Python3.6</li><li>Jupyter Notebook</li></ul><h2 id="训练集和测试集数据预处理"><a href="#训练集和测试集数据预处理" class="headerlink" title="训练集和测试集数据预处理"></a>训练集和测试集数据预处理</h2><p>由于上述给定的语料，在模型中，我们不能直接使用，必须先经过预处理，把上述语料格式重新组织成具有词性、方向和距离的格式。</p><p>首先，我们通过一个 Python 脚本 <code>get_parser_train_test_input.py</code>，生成所需要的训练集和测试集，执行如下命令即可：</p><pre><code class="vim">cat train.conll | python get_parser_train_test_input.py &gt; train.data 
cat dev.conll | python get_parser_train_test_input.py &gt; dev.data </code></pre><p>上面的脚本通过 cat 命令和管道符把内容传递给脚本进行处理。这里需要注意的是，脚本需要在 Linux 环境下执行，且语料和脚本应放在同一目录下。</p><p><code>get_parser_train_test_input.py</code> 这一脚本的目的，就是重新组织语料，组织成可以使用 CRF算法的格式，具有词性、方向和距离的格式。我们认为，如果词 A 依赖词 B，A 就是孩子，B就是父亲。按照这种假设得到父亲节点的粗词性和详细词性，以及和依赖次之间的距离。</p><p>我们打开该脚本，看看它的代码，如下所示，重要的代码给出了注释。</p><pre><code class="livecodeserver">#coding=utf-8
&#39;&#39;&#39;词A依赖词B，A就是孩子，B就是父亲&#39;&#39;&#39;
import sys 

sentence = [&quot;Root&quot;]
def do_parse(sentence):
    if len(sentence) == 1:return 
    for line in sentence[1:]:
        line_arr = line.strip().split(&quot;\t&quot;)
        c_id = int(line_arr[0])
        f_id = int(line_arr[6])
        if f_id == 0:
            print(&quot;\t&quot;.join(line_arr[2:5])+&quot;\t&quot; + &quot;0_Root&quot;)
            continue
        f_post,f_detail_post = sentence[f_id].strip().split(&quot;\t&quot;)[3:5] #得到父亲节点的粗词性和详细词性
        c_edge_post = f_post #默认是依赖词的粗粒度词性，但是名词除外；名词取细粒度词性
        if f_post == &quot;n&quot;:
            c_edge_post = f_detail_post
        #计算是第几个出现这种词行
        diff = f_id - c_id #确定要走几步
        step = 1 if f_id &gt; c_id  else -1 #确定每一步方向
        same_post_num = 0 #中间每一步统计多少个一样的词性
        cmp_idx = 4 if f_post == &quot;n&quot; else 3  #根据是否是名词决定取的是粗or详细词性
        for i in range(0, abs(diff)):
            idx = c_id + (i+1)*step
            if sentence[idx].strip().split(&quot;\t&quot;)[cmp_idx] == c_edge_post:
                same_post_num += step

        print(&quot;\t&quot;.join(line_arr[2:5])+&quot;\t&quot; + &quot;%d_%s&quot;%(same_post_num, c_edge_post))
    print(&quot;&quot;)

for line in sys.stdin:
    line = line.strip()
    line_arr = line.split(&quot;\t&quot;)
    if  line == &quot;&quot; or line_arr[0] == &quot;1&quot;:
        do_parse(sentence)
        sentence = [&quot;Root&quot;]
    if line ==&quot;&quot;:continue 
    sentence.append(line)</code></pre><p>整个脚本按行读入，每行按 Tab 键分割，首先得到父亲节点的词性，然后根据词性是否是名词 n 进行判断，默认是依赖词的粗粒度词性，如果是名词取细粒度词性。</p><p>脚本处理完，数据集的格式如下：<br><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1574426456581.png" alt="enter description here"></p><p>根据依存文法，决定两个词之间依存关系的主要有两个因素：方向和距离。正如上图中第四列类别标签所示，该列可以定义为以下形式：</p><blockquote><p>[+|-]dPOS</p></blockquote><p>其中，<code>[+|-]</code> 表示中心词在句子中相对坐标轴的方向；POS 代表中心词具有的词性类别；d 表示与中心词词性相同的词的数量，即距离。</p><h2 id="语料特征生成"><a href="#语料特征生成" class="headerlink" title="语料特征生成"></a>语料特征生成</h2><p>语料特征提取，主要采用 N-gram 模型来完成。这里我们使用 3-gram完成提取，将词性与词语两两进行匹配，分别返回特征集合和标签集合，需要注意整个语料采用的是 UTF-8 编码格式。</p><p>整个编码过程中，我们首先需要引入需要的库，然后对语料进行读文件操作。语料采用 UTF-8 编码格式，以句子为单位，按 Tab 键作分割处理，从而实现句子3-gram 模型的特征提取。具体实现如下。</p><pre><code class="haskell">import sklearn_crfsuite
from sklearn_crfsuite import metrics
from sklearn.externals import joblib</code></pre><p>首先引入需要用到的库，如上面代码所示。其目的是使用模型 <code>sklearn_crfsuite .CRF</code>，metrics 用来进行模型性能测试，joblib用来保存和加载训练好的模型。</p><p>接着，定义包含特征处理方法的类，命名为 CorpusProcess，类结构定义如下：</p><pre><code class="python">class CorpusProcess(object):

def __init__(self):
    &quot;&quot;&quot;初始化&quot;&quot;&quot;
    pass

def read_corpus_from_file(self, file_path):
    &quot;&quot;&quot;读取语料&quot;&quot;&quot;
    pass

def write_corpus_to_file(self, data, file_path):
    &quot;&quot;&quot;写语料&quot;&quot;&quot;
    pass

def process_sentence(self,lines):
    &quot;&quot;&quot;处理句子&quot;&quot;&quot;
    pass   

def initialize(self):
    &quot;&quot;&quot;语料初始化&quot;&quot;&quot;
    pass   

def generator(self, train=True):
    &quot;&quot;&quot;特征生成器&quot;&quot;&quot;
    pass  

def extract_feature(self, sentences):
    &quot;&quot;&quot;提取特征&quot;&quot;&quot;
    pass</code></pre><p>下面介绍下 CorpusProcess 类中各个方法的具体实现。</p><p>第1步， 实现 init 构造函数，目的初始化预处理好的语料的路径：</p><pre><code class="python">def __init__(self):
        &quot;&quot;&quot;初始化&quot;&quot;&quot;
        self.train_process_path =  dir +  &quot;data//train.data&quot;   #预处理之后的训练集
        self.test_process_path =  dir +  &quot;data//dev.data&quot;  #预处理之后的测试集</code></pre><p>这里的路径可以自定义，这里的语料之前已经完成了预处理过程。<br>第2-3步，<code>read_corpus_from_file</code> 方法和 <code>write_corpus_to_file</code> 方法，分别定义了语料文件的读和写操作：</p><pre><code class="python">def read_corpus_from_file(self, file_path):
    &quot;&quot;&quot;读取语料&quot;&quot;&quot;
    f = open(file_path, &#39;r&#39;,encoding=&#39;utf-8&#39;)
    lines = f.readlines()
    f.close()
    return lines

def write_corpus_to_file(self, data, file_path):
    &quot;&quot;&quot;写语料&quot;&quot;&quot;
    f = open(file_path, &#39;w&#39;)
    f.write(str(data))
    f.close()</code></pre><p>这一步，主要用 open 函数来实现语料文件的读和写。</p><p>第4-5步，<code>process_sentence</code> 方法和 initialize 方法，用来处理句子和初始化语料，把语料按句子结构用 list存储起来，存储到内存中：</p><pre><code class="python">def process_sentence(self,lines):
        &quot;&quot;&quot;处理句子&quot;&quot;&quot;
        sentence = []
        for line in lines:
            if not line.strip():
                yield sentence
                sentence = []
            else:
                lines = line.strip().split(u&#39;\t&#39;)
                result = [line for line in lines]
                sentence.append(result)   

    def initialize(self):
        &quot;&quot;&quot;语料初始化&quot;&quot;&quot;
        train_lines = self.read_corpus_from_file(self.train_process_path)
        test_lines = self.read_corpus_from_file(self.test_process_path)
        self.train_sentences = [sentence for sentence in self.process_sentence(train_lines)]
        self.test_sentences = [sentence for sentence in self.process_sentence(test_lines)] </code></pre><p>这一步，通过 <code>process_sentence</code> 把句子收尾的空格去掉，然后通过 initialize 函数调用上面<code>read_corpus_from_file</code> 方法读取语料，分别加载训练集和测试集。</p><p>第6步，特征生成器，分别用来指定生成训练集或者测试集的特征集：</p><pre><code class="python">def generator(self, train=True):
    &quot;&quot;&quot;特征生成器&quot;&quot;&quot;
    if train: 
        sentences = self.train_sentences
    else: 
        sentences = self.test_sentences
    return self.extract_feature(sentences)</code></pre><p>这一步，对训练集和测试集分别处理，如果参数 train 为 True，则表示处理训练集，如果是 False，则表示处理测试集。<br>第7步，特征提取，简单的进行 <code>3-gram</code> 的抽取，将词性与词语两两进行匹配，分别返回特征集合和标签集合：</p><pre><code class="prolog">def extract_feature(self, sentences):
        &quot;&quot;&quot;提取特征&quot;&quot;&quot;
        features, tags = [], []
        for index in range(len(sentences)):
            feature_list, tag_list = [], []
            for i in range(len(sentences[index])):
                feature = {&quot;w0&quot;: sentences[index][i][0],
                           &quot;p0&quot;: sentences[index][i][1],
                           &quot;w-1&quot;: sentences[index][i-1][0] if i != 0 else &quot;BOS&quot;,
                           &quot;w+1&quot;: sentences[index][i+1][0] if i != len(sentences[index])-1 else &quot;EOS&quot;,
                           &quot;p-1&quot;: sentences[index][i-1][1] if i != 0 else &quot;un&quot;,
                           &quot;p+1&quot;: sentences[index][i+1][1] if i != len(sentences[index])-1 else &quot;un&quot;}
                feature[&quot;w-1:w0&quot;] = feature[&quot;w-1&quot;]+feature[&quot;w0&quot;]
                feature[&quot;w0:w+1&quot;] = feature[&quot;w0&quot;]+feature[&quot;w+1&quot;]
                feature[&quot;p-1:p0&quot;] = feature[&quot;p-1&quot;]+feature[&quot;p0&quot;]
                feature[&quot;p0:p+1&quot;] = feature[&quot;p0&quot;]+feature[&quot;p+1&quot;]
                feature[&quot;p-1:w0&quot;] = feature[&quot;p-1&quot;]+feature[&quot;w0&quot;]
                feature[&quot;w0:p+1&quot;] = feature[&quot;w0&quot;]+feature[&quot;p+1&quot;]
                feature_list.append(feature)
                tag_list.append(sentences[index][i][-1])
            features.append(feature_list)
            tags.append(tag_list)
        return features, tags    </code></pre><p>经过第6步，确定处理的是训练集还是测试集之后，通过 <code>extract_feature</code> 对句子进行特征抽取，使用 3-gram<br>模型，得到特征集合和标签集合的对应关系。</p><h2 id="模型训练及预测"><a href="#模型训练及预测" class="headerlink" title="模型训练及预测"></a>模型训练及预测</h2><p>在完成特征工程和特征提取之后，接下来，我们要进行模型训练和预测，要预定义模型需要的一些参数，并初始化模型对象，进而完成模型训练和预测，以及模型的保存与加载。</p><p>首先，我们定义模型 ModelParser 类，进行初始化参数、模型初始化，以及模型训练、预测、保存和加载，类的结构定义如下：</p><pre><code class="python">class ModelParser(object):

    def __init__(self):
        &quot;&quot;&quot;初始化参数&quot;&quot;&quot;
        pass

    def initialize_model(self):
        &quot;&quot;&quot;模型初始化&quot;&quot;&quot;
        pass

    def train(self):
        &quot;&quot;&quot;训练&quot;&quot;&quot;
        pass

    def predict(self, sentences):
        &quot;&quot;&quot;模型预测&quot;&quot;&quot;
        pass

    def load_model(self, name=&#39;model&#39;):
        &quot;&quot;&quot;加载模型 &quot;&quot;&quot;
        pass

    def save_model(self, name=&#39;model&#39;):
        &quot;&quot;&quot;保存模型&quot;&quot;&quot;
        pass</code></pre><p>接下来，我们分析 ModelParser 类中方法的具体实现。<br>第1步，init 方法实现算法模型参数和语料预处理 CorpusProcess 类的实例化和初始化：</p><pre><code class="ruby">def __init__(self):
        &quot;&quot;&quot;初始化参数&quot;&quot;&quot;
        self.algorithm = &quot;lbfgs&quot;
        self.c1 = 0.1
        self.c2 = 0.1
        self.max_iterations = 100
        self.model_path = &quot;model.pkl&quot;
        self.corpus = CorpusProcess()  #初始化CorpusProcess类
        self.corpus.initialize()  #语料预处理
        self.model = None</code></pre><p>这一步，init 方法初始化参数以及 CRF 模型的参数，算法选用 LBFGS，c1 和 c2分别为0.1，最大迭代次数100次。然后定义模型保存的文件名称，以及完成对 CorpusProcess 类 的初始化。</p><p>第2-3步，<code>initialize_model</code> 方法和 train 实现模型定义和训练：</p><pre><code class="python"> def initialize_model(self):
        &quot;&quot;&quot;模型初始化&quot;&quot;&quot;
        algorithm = self.algorithm
        c1 = float(self.c1)
        c2 = float(self.c2)
        max_iterations = int(self.max_iterations)
        self.model = sklearn_crfsuite.CRF(algorithm=algorithm, c1=c1, c2=c2,
                                          max_iterations=max_iterations, all_possible_transitions=True)

    def train(self):
        &quot;&quot;&quot;训练&quot;&quot;&quot;
        self.initialize_model()
        x_train, y_train = self.corpus.generator()
        self.model.fit(x_train, y_train)
        labels = list(self.model.classes_)
        x_test, y_test = self.corpus.generator(train=False)
        y_predict = self.model.predict(x_test)
        metrics.flat_f1_score(y_test, y_predict, average=&#39;weighted&#39;, labels=labels)
        sorted_labels = sorted(labels, key=lambda name: (name[1:], name[0]))
        print(metrics.flat_classification_report(y_test, y_predict, labels=sorted_labels, digits=3))
        self.save_model()</code></pre><p>这一步，<code>initialize_model</code> 方法实现 了 <code>sklearn_crfsuite.CRF</code> 模型的初始化。然后在 train 方法中，先通过fit 方法训练模型，再通过 <code>metrics.flat_f1_score</code> 对测试集进行 F1 性能测试，最后将模型保存。</p><p>第4-6步，分别实现模型预测、保存和加载方法</p><p>最后，实例化类，并进行模型训练：</p><pre><code class="gams">model = ModelParser()
model.train()</code></pre><p>对模型进行预测，预测数据输入格式为三维，表示完整的一句话：</p><blockquote><p>[[[‘坚决’, ‘a’, ‘ad’, ‘1_v’],</p></blockquote><blockquote><pre><code> [&#39;惩治&#39;, &#39;v&#39;, &#39;v&#39;, &#39;0_Root&#39;],</code></pre></blockquote><blockquote><pre><code> [&#39;贪污&#39;, &#39;v&#39;, &#39;v&#39;, &#39;1_v&#39;],</code></pre></blockquote><blockquote><pre><code> [&#39;贿赂&#39;, &#39;n&#39;, &#39;n&#39;, &#39;-1_v&#39;],</code></pre></blockquote><blockquote><pre><code> [&#39;等&#39;, &#39;u&#39;, &#39;udeng&#39;, &#39;-1_v&#39;],</code></pre></blockquote><blockquote><pre><code> [&#39;经济&#39;, &#39;n&#39;, &#39;n&#39;, &#39;1_v&#39;],

 [&#39;犯罪&#39;, &#39;v&#39;, &#39;vn&#39;, &#39;-2_v&#39;]]]</code></pre></blockquote><p>模型预测的结果如下图所示：</p><p><img src="https://www.github.com/OneJane/blog/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1574426645490.png" alt="enter description here"></p><p>预测的结果，和原始语料预处理得到的标签格式保持一致。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文通过清华大学的句法标注语料库，实现了基于 CRF 的中文句法依存分析模型。借此实例，相信大家对句法依存已有了一个完整客观的认识。</p></article><div class="post-donate"><div id="donate_board" class="donate_bar center"><a id="btn_donate" class="btn_donate" href="javascript:;" title="打赏"></a> <span class="donate_txt">↑<br> 欢迎投食,求鼓励，求支持！</span><br></div><div id="donate_guide" class="donate_bar center hidden"> <img src="/images/alipay.png" alt="支付宝打赏"> <img src="/images/wechatpay.png" alt="微信打赏"></div><script type="text/javascript">document.getElementById("btn_donate").onclick=function(){$("#donate_board").addClass("hidden"),$("#donate_guide").removeClass("hidden")}</script></div><div class="nexmoe-post-copyright"><i class="mdui-list-item-icon nexmoefont icon-info-circle"></i> <strong>本文作者：</strong>OneJane<br> <strong>本文链接：</strong><a href="https://onejane.github.io/2019/11/22/17.基于CRF的中文句法依存分析模型实现/" title="https://onejane.github.io/2019/11/22/17.基于CRF的中文句法依存分析模型实现/" target="_blank" rel="noopener">https://onejane.github.io/2019/11/22/17.基于CRF的中文句法依存分析模型实现/</a><br> <strong>版权声明：</strong>本文采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/cn/deed.zh" target="_blank">CC BY-NC-SA 3.0 CN</a> 协议进行许可</div><section class="nexmoe-comment"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.5.0/dist/gitalk.min.css"><div id="gitalk"></div><script src="https://cdn.jsdelivr.net/npm/gitalk@1.5.0/dist/gitalk.min.js"></script><script type="text/javascript">var gitalk=new Gitalk({clientID:"e677e59382e1c7a468fd",clientSecret:"717d041bc4ab749f069314862232cfb6ec8adc15",id:decodeURI(window.location.pathname),repo:"onejane.github.io",owner:"onejane",admin:"onejane"});gitalk.render("gitalk")</script></section></div></div></div><script src="https://cdn.jsdelivr.net/npm/mdui@0.4.3/dist/js/mdui.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/smoothscroll-for-websites@1.4.9/SmoothScroll.min.js"></script><script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.15.8/build/highlight.min.js"></script><script>hljs.initHighlightingOnLoad()</script><script src="/js/app.js?v=1577613607938"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@5.1.0/lazysizes.min.js"></script><div hidden><script type="text/javascript" src="https://js.users.51.la/20279757.js"></script></div></body><script src="https://cdn.jsdelivr.net/npm/meting@1.1.0/dist/Meting.min.js"></script></html>