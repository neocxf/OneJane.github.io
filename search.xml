<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[nlp基本概念及算法]]></title>
    <url>%2F2019%2F11%2F25%2Fnlp%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%8F%8A%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[整理的一些基本算法及概念 算法复杂度 分词最大前向匹配 最大后向匹配 依赖于词典，不能做词细分 局部最优（属于贪心算法） 效率不高（取决于max_length） 有歧义（不能考虑语义）unigram lmViterbi算法(DP算法)输入句子-&gt;生成所有可能的分割-&gt;利用语言模型，选择其中最好的，分词方法分两步进行（分割-&gt;计算unigram概率），时间复杂度很高编辑距离拼写纠错距离相似度欧式距离：d = |s1-s2|余弦相似度：d=s1s2/(∣s1∣∣s2∣) d(s1,s2)=0缺点: 只从出现频率来表示单词/句子，距离计算时，频次高的单词对结果影响较大，实际的语义分析时，并不是出现频率越高就越重要 BOW词袋模型假设我们不考虑文本中词与词之间的上下文关系，仅仅只考虑所有词的权重。而权重与词在文本中出现的频率有关。 one-hot表示单词 词典：[我们，去，爬山，今天，你们，昨天，跑步]按照单词在词典库中的顺序我们：(1,0,0,0,0,0,0)-&gt;7维=|词典|爬山：(0,0,1,0,0,0,0)跑步：(0,0,0,0,0,0,1)昨天：(0,0,0,0,0,1,0) boolean represention(不关心单词频率)表达句子 词典：[我们，又，去，爬山，今天，你们，昨天，跑步]按照单词在词典库中的顺序我们|今天|去|爬山：(1,0,1,1,1,0,0,0)-&gt;8维=|词典|你们|昨天|跑步：(0,0,0,0,0,1,1,1)你们|又|去|爬山|又|去|跑步：(0,1,1,1,0,1,0,1) count based represention(记录单词频率)表达句子 词典：[我们，又，去，爬山，今天，你们，昨天，跑步]按照单词在词典库中的顺序我们|今天|去|爬山：(1,0,1,1,1,0,0,0)-&gt;8维=|词典|你们|昨天|跑步：(0,0,0,0,0,1,1,1)你们|又|去|爬山|又|去|跑步：(0,2,2,1,0,1,0,1) tf-idf tf为词频，即一个词语在文档中的出现频率，假设一个词语在整个文档中出现了i次，而整个文档有N个词语，则tf的值为i/N.idf为逆向文件频率，假设整个文档有n篇文章，而一个词语在k篇文章中出现，则idf值为log(n/k)1.所有文档分词得到词典向量2.词典向量每个词对于当前句子进行词频统计，该词在句中出现n次，即tf3.所有文档总数m,词典向量中当前词出现在k个文档中，log(m/k)为idf4.针对词典向量生成tf-idf为m*log(m/k) 词典：[今天,上,NLP,课程,的,有,意思,数据,也]s1:今天|上|NLP|课程-&gt;[1log(3\2),1log(3\1),1log(3\1),1log(3\3),0,0,0,0,0]s2:今天|的|课程|有|意思-&gt;[1log(3\2),0,0,1log(3/3),1log(3\1),1log(3\2),1log(3\2),0,0]s3:数据|课程|也|有|意思-&gt;[0,0,0,1log(3\3),0,1log(3\2),1log(3\2),1log(3\1),1log(3\1)] distributed word 表示单词 我们:[0.4,0.6,0.8,0.1]爬山:[0.4,0.6,0.2,0.6]运动:[0.2,0.3,0.7,0.6]昨天:[0.5,0.9,0.1,0.3] 缺点：首先，它是一个词袋模型，不考虑词与词之间的顺序（文本中词的顺序信息也是很重要的）；其次，它假设词与词相互独立（在大多数情况下，词与词是相互影响的）；最后，它得到的特征是离散稀疏的。 文本语料库-&gt;预处理-&gt;文本处理集-&gt;one-hot向量输入word2vec-&gt;词向量（模型：CBOW和Skip-gram;；方法：负采样与层次softmax方法） Language Model语言模型：判断一句话是否通顺。 unigram bigram n-gram SmoothingLaplace Good-Turning]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>tf-idf</tag>
        <tag>hanlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊天机器人（13）]]></title>
    <url>%2F2019%2F11%2F22%2F13.%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%2F</url>
    <content type="text"><![CDATA[自动聊天机器人，也称为自动问答系统，由于所使用的场景不同，叫法也不一样。自动问答（Question Answering，QA）是指利用计算机自动回答用户所提出的问题以满足用户知识需求的任务。不同于现有搜索引擎，问答系统是信息服务的一种高级形式，系统返回用户的不再是基于关键词匹配排序的文档列表，而是精准的自然语言答案。近年来，随着人工智能的飞速发展，自动问答已经成为倍受关注且发展前景广泛的研究方向。 自动问答简介自动问答主要研究的内容和关键科学问题如下： 问句理解 ：给定用户问题，自动问答首先需要理解用户所提问题。用户问句的语义理解包含词法分析、句法分析、语义分析等多项关键技术，需要从文本的多个维度理解其中包含的语义内容。 文本信息抽取 ：自动问答系统需要在已有语料库、知识库或问答库中匹配相关的信息，并抽取出相应的答案。 知识推理 ：自动问答中，由于语料库、知识库和问答库本身的覆盖度有限，并不是所有问题都能直接找到答案。这就需要在已有的知识体系中，通过知识推理的手段获取这些隐含的答案。 纵观自动问答研究的发展态势和技术现状，以下研究方向或问题将可能成为未来整个领域和行业重点关注的方向：基于深度学习的端到端自动问答，多领域、多语言的自动问答，面向问答的深度推理，篇章阅读理解、对话等。 基于 Chatterbot 制作中文聊天机器人ChatterBot 是一个构建在 Python 上，基于一系列规则和机器学习算法完成的聊天机器人，具有结构清晰，可扩展性好，简单实用的特点。 Chatterbot 安装有两种方式： 使用 pip install chatterbot 安装； 直接在 Github Chatterbot 下载这个项目，通过 python setup.py install 安装，其中 examples 文件夹中包含几个例子，可以根据例子加深自己的理解。 安装过程如果出现错误，主要是需要安装这些依赖库： chatterbot-corpus&gt;=1.1,&lt;1.2 mathparse&gt;=0.1,&lt;0.2 nltk&gt;=3.2,&lt;4.0 pymongo&gt;=3.3,&lt;4.0 python-dateutil&gt;=2.6,&lt;2.7 python-twitter&gt;=3.0,&lt;4.0 sqlalchemy&gt;=1.2,&lt;1.3 pint&gt;=0.8.1 1. 手动设置一点语料，体验基于规则的聊天机器人回答。 from chatterbot import ChatBot from chatterbot.trainers import ListTrainer Chinese_bot = ChatBot(&quot;Training demo&quot;) #创建一个新的实例 Chinese_bot.set_trainer(ListTrainer) Chinese_bot.train([ &#39;亲，在吗？&#39;, &#39;亲，在呢&#39;, &#39;这件衣服的号码大小标准吗？&#39;, &#39;亲，标准呢，请放心下单吧。&#39;, &#39;有红色的吗？&#39;, &#39;有呢，目前有白红蓝3种色调。&#39;, ]) 下面进行测试： # 测试一下 question = &#39;亲，在吗&#39; print(question) response = Chinese_bot.get_response(question) print(response) print(&quot;\n&quot;) question = &#39;有红色的吗？&#39; print(question) response = Chinese_bot.get_response(question) print(response) 从得到的结果可以看出，这应该完全是基于规则的判断： 亲，在吗 亲，在呢 有红色的吗？ 有呢，目前有白红蓝3种色调。 2. 训练自己的语料。 本次使用的语料来自 QQ 群的聊天记录，导出的 QQ 聊天记录稍微处理一下即可使用，整个过程如下。 （1）首先载入语料，第二行代码主要是想把每句话后面的换行 \n 去掉。 lines = open(&quot;QQ.txt&quot;,&quot;r&quot;,encoding=&#39;gbk&#39;).readlines() sec = [ line.strip() for line in lines] （2）接下来就可以训练模型了，由于整个语料比较大，训练过程也比较耗时。 from chatterbot import ChatBot from chatterbot.trainers import ListTrainer Chinese_bot = ChatBot(&quot;Training&quot;) Chinese_bot.set_trainer(ListTrainer) Chinese_bot.train(sec) 这里需要注意，如果训练过程很慢，可以在第一步中加入如下代码，即只取前1000条进行训练： sec = sec[0:1000] （3）最后，对训练好的模型进行测试，可见训练数据是 QQ 群技术对话，也看得出程序员们都很努力，整体想的都是学习。 以上只是简单的 Chatterbot 演示，如果想看更好的应用，推荐看官方文档。 基于 Seq2Seq 制作中文聊天机器人序列数据处理模型，从N-gram 语言模型到 RNN 及其变种。这里我们讲另外一个基于深度学习的 Seq2Seq 模型。 从 RNN 结构说起，根据输出和输入序列不同数量 RNN ，可以有多种不同的结构，不同结构自然就有不同的引用场合。 One To One 结构，仅仅只是简单的给一个输入得到一个输出，此处并未体现序列的特征，例如图像分类场景。 One To Many 结构，给一个输入得到一系列输出，这种结构可用于生产图片描述的场景。 Many To One 结构，给一系列输入得到一个输出，这种结构可用于文本情感分析，对一些列的文本输入进行分类，看是消极还是积极情感。 Many To Many 结构，给一系列输入得到一系列输出，这种结构可用于翻译或聊天对话场景，将输入的文本转换成另外一系列文本。 同步 Many To Many 结构，它是经典的 RNN 结构，前一输入的状态会带到下一个状态中，而且每个输入都会对应一个输出，我们最熟悉的应用场景是字符预测，同样也可以用于视频分类，对视频的帧打标签。 在 Many To Many 的两种模型中，第四和第五种是有差异的，经典 RNN结构的输入和输出序列必须要等长，它的应用场景也比较有限。而第四种，输入和输出序列可以不等长，这种模型便是 Seq2Seq 模型，即 Sequence toSequence。它实现了从一个序列到另外一个序列的转换，比如 Google 曾用 Seq2Seq 模型加 Attention模型实现了翻译功能，类似的还可以实现聊天机器人对话模型。经典的 RNN 模型固定了输入序列和输出序列的大小，而 Seq2Seq 模型则突破了该限制。 Seq2Seq 属于 Encoder-Decoder 结构，这里看看常见的 Encoder-Decoder 结构。基本思想就是利用两个 RNN，一个RNN 作为 Encoder，另一个 RNN 作为 Decoder。Encoder负责将输入序列压缩成指定长度的向量，这个向量就可以看成是这个序列的语义，这个过程称为编码，如下图，获取语义向量最简单的方式就是直接将最后一个输入的隐状态作为语义向量。也可以对最后一个隐含状态做一个变换得到语义向量，还可以将输入序列的所有隐含状态做一个变换得到语义变量。 具体理论知识这里不再赘述，下面重点看看，如何通过 Keras 实现一个 LSTM_Seq2Seq 自动问答机器人。 1. 语料准备。 语料我们使用 Tab 键 \t 把问题和答案区分，每一对为一行。其中，语料为爬虫爬取的工程机械网站的问答。 2. 模型构建和训练。 第一步，引入需要的包： from keras.models import Model from keras.layers import Input, LSTM, Dense import numpy as np import pandas as pd 第二步，定义模型超参数、迭代次数、语料路径： #Batch size 的大小 batch_size = 32 # 迭代次数epochs epochs = 100 # 编码空间的维度Latent dimensionality latent_dim = 256 # 要训练的样本数 num_samples = 5000 #设置语料的路径 data_path = &#39;D://nlp//ch13//files.txt&#39; 第三步，把语料向量化： #把数据向量话 input_texts = [] target_texts = [] input_characters = set() target_characters = set() with open(data_path, &#39;r&#39;, encoding=&#39;utf-8&#39;) as f: lines = f.read().split(&#39;\n&#39;) for line in lines[: min(num_samples, len(lines) - 1)]: #print(line) input_text, target_text = line.split(&#39;\t&#39;) # We use &quot;tab&quot; as the &quot;start sequence&quot; character # for the targets, and &quot;\n&quot; as &quot;end sequence&quot; character. target_text = target_text[0:100] target_text = &#39;\t&#39; + target_text + &#39;\n&#39; input_texts.append(input_text) target_texts.append(target_text) for char in input_text: if char not in input_characters: input_characters.add(char) for char in target_text: if char not in target_characters: target_characters.add(char) input_characters = sorted(list(input_characters)) target_characters = sorted(list(target_characters)) num_encoder_tokens = len(input_characters) num_decoder_tokens = len(target_characters) max_encoder_seq_length = max([len(txt) for txt in input_texts]) max_decoder_seq_length = max([len(txt) for txt in target_texts]) print(&#39;Number of samples:&#39;, len(input_texts)) print(&#39;Number of unique input tokens:&#39;, num_encoder_tokens) print(&#39;Number of unique output tokens:&#39;, num_decoder_tokens) print(&#39;Max sequence length for inputs:&#39;, max_encoder_seq_length) print(&#39;Max sequence length for outputs:&#39;, max_decoder_seq_length) input_token_index = dict( [(char, i) for i, char in enumerate(input_characters)]) target_token_index = dict( [(char, i) for i, char in enumerate(target_characters)]) encoder_input_data = np.zeros( (len(input_texts), max_encoder_seq_length, num_encoder_tokens),dtype=&#39;float32&#39;) decoder_input_data = np.zeros( (len(input_texts), max_decoder_seq_length, num_decoder_tokens),dtype=&#39;float32&#39;) decoder_target_data = np.zeros( (len(input_texts), max_decoder_seq_length, num_decoder_tokens),dtype=&#39;float32&#39;) for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)): for t, char in enumerate(input_text): encoder_input_data[i, t, input_token_index[char]] = 1. for t, char in enumerate(target_text): # decoder_target_data is ahead of decoder_input_data by one timestep decoder_input_data[i, t, target_token_index[char]] = 1. if t &gt; 0: # decoder_target_data will be ahead by one timestep # and will not include the start character. decoder_target_data[i, t - 1, target_token_index[char]] = 1. 第四步，LSTM_Seq2Seq 模型定义、训练和保存： encoder_inputs = Input(shape=(None, num_encoder_tokens)) encoder = LSTM(latent_dim, return_state=True) encoder_outputs, state_h, state_c = encoder(encoder_inputs) # 输出 `encoder_outputs` encoder_states = [state_h, state_c] # 状态 `encoder_states` decoder_inputs = Input(shape=(None, num_decoder_tokens)) decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states) decoder_dense = Dense(num_decoder_tokens, activation=&#39;softmax&#39;) decoder_outputs = decoder_dense(decoder_outputs) # 定义模型 model = Model([encoder_inputs, decoder_inputs], decoder_outputs) # 训练 model.compile(optimizer=&#39;rmsprop&#39;, loss=&#39;categorical_crossentropy&#39;) model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=epochs, validation_split=0.2) # 保存模型 model.save(&#39;s2s.h5&#39;) 第五步，Seq2Seq 的 Encoder 操作： encoder_model = Model(encoder_inputs, encoder_states) decoder_state_input_h = Input(shape=(latent_dim,)) decoder_state_input_c = Input(shape=(latent_dim,)) decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c] decoder_outputs, state_h, state_c = decoder_lstm( decoder_inputs, initial_state=decoder_states_inputs) decoder_states = [state_h, state_c] decoder_outputs = decoder_dense(decoder_outputs) decoder_model = Model( [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states) 第六步，把索引和分词转成序列： reverse_input_char_index = dict( (i, char) for char, i in input_token_index.items()) reverse_target_char_index = dict( (i, char) for char, i in target_token_index.items()) 第七步，定义预测函数，先使用预模型预测，然后编码成汉字结果： def decode_sequence(input_seq): # Encode the input as state vectors. states_value = encoder_model.predict(input_seq) #print(states_value) # Generate empty target sequence of length 1. target_seq = np.zeros((1, 1, num_decoder_tokens)) # Populate the first character of target sequence with the start character. target_seq[0, 0, target_token_index[&#39;\t&#39;]] = 1. # Sampling loop for a batch of sequences # (to simplify, here we assume a batch of size 1). stop_condition = False decoded_sentence = &#39;&#39; while not stop_condition: output_tokens, h, c = decoder_model.predict( [target_seq] + states_value) # Sample a token sampled_token_index = np.argmax(output_tokens[0, -1, :]) sampled_char = reverse_target_char_index[sampled_token_index] decoded_sentence += sampled_char if (sampled_char == &#39;\n&#39; or len(decoded_sentence) &gt; max_decoder_seq_length): stop_condition = True # Update the target sequence (of length 1). target_seq = np.zeros((1, 1, num_decoder_tokens)) target_seq[0, 0, sampled_token_index] = 1. # 更新状态 states_value = [h, c] return decoded_sentence 3. 模型预测。首先，定义一个预测函数： def predict_ans(question): inseq = np.zeros((len(question), max_encoder_seq_length, num_encoder_tokens),dtype=&#39;float16&#39;) decoded_sentence = decode_sequence(inseq) return decoded_sentence 然后就可以预测了： print(&#39;Decoded sentence:&#39;, predict_ans(&quot;挖机履带掉了怎么装上去&quot;)) 总结本文我们首先基于 Chatterbot 制作了中文聊天机器人，并用 QQ 群对话语料自己尝试训练。然后通过 LSTM 和 Seq2Seq模型，根据爬取的语料，训练了一个自动问答的模型，通过以上两种方式，我们们对自动问答有了一个简单的入门。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>chatterbot</tag>
        <tag>seq2seq</tag>
        <tag>lstm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RNN-GRU-LSTM（9）]]></title>
    <url>%2F2019%2F11%2F22%2F9.RNN-GRU-LSTM%2F</url>
    <content type="text"><![CDATA[序列数据的处理，我们从语言模型 N-gram 模型说起，然后着重谈谈 RNN，并通过 RNN 的变种 LSTM 和 GRU 来实战文本分类。 首先，我们来思考下，当人工神经网络从浅层发展到深层；从全连接到卷积神经网络。在此过程中，人类在图片分类、语音识别等方面都取得了非常好的结果，那么我们为什么还需要循环神经网络呢？ 因为，上面提到的这些网络结构的层与层之间是全连接或部分连接的，但在每层之间的节点是无连接的，这样的网络结构并不能很好的处理序列数据。 语言模型 N-gram 模型通过前面的课程，我们了解到一般自然语言处理的传统方法是将句子处理为一个词袋模型（Bag-of-Words，BoW），而不考虑每个词的顺序，比如用朴素贝叶斯算法进行垃圾邮件识别或者文本分类。在中文里有时候这种方式没有问题，因为有些句子即使把词的顺序打乱，还是可以看懂这句话在说什么，比如： T：研究表明，汉字的顺序并不一定能影响阅读，比如当你看完这句话后。F：研表究明，汉字的序顺并不定一能影阅响读，比如当你看完这句话后。 但有时候不行，词的顺序打乱，句子意思就变得让人不可思议了，例如： T：我喜欢吃烧烤。F：烧烤喜欢吃我。 那么，有没有模型是考虑句子中词与词之间的顺序的呢？有，语言模型中的 N-gram 就是一种。 N-gram 模型是一种语言模型（Language Model，LM），是一个基于概率的判别模型，它的输入是一句话（词的顺序序列），输出是这句话的概率，即这些词的联合概率（JointProbability）。使用 N-gram语言模型思想，一般是需要知道当前词以及前面的词，因为一个句子中每个词的出现并不是独立的。比如，如果第一个词是“空气”，接下来的词是“很”，那么下一个词很大概率会是“新鲜”。类似于我们人的联想，N-gram模型知道的信息越多，得到的结果也越准确。 在前面课程中讲解的文本分类中，我们曾用到基于 sklearn 的词袋模型，尝试加入抽取 2-gram 和 3-gram的统计特征，把词库的量放大，获得更强的特征。 通过 ngram_range 参数来控制，代码如下： from sklearn.feature_extraction.text import CountVectorizer vec = CountVectorizer( analyzer=&#39;word&#39;, # tokenise by character ngrams ngram_range=(1,4), # use ngrams of size 1 and 2 max_features=20000, # keep the most common 1000 ngrams ) 因此，N-gram 模型，在自然语言处理中主要应用在如词性标注、垃圾短信分类、分词器、机器翻译和语音识别、语音识别等领域。 然而 N-gram 模型并不是完美的，它存在如下优缺点： 优点：包含了前 N-1 个词所能提供的全部信息，这些词对于当前词的出现概率具有很强的约束力； 缺点：需要很大规模的训练文本来确定模型的参数，当 N 很大时，模型的参数空间过大。所以常见的 N 值一般为1，2，3等。还有因数据稀疏而导致的数据平滑问题，解决方法主要是拉普拉斯平滑和内插与回溯。 所以，根据 N-gram 的优缺点，它的进化版 NNLM（Neural Network based Language Model）诞生了。 NNLM 由 Bengio 在2003年提出，它是一个很简单的模型，由四层组成，输入层、嵌入层、隐层和输出层，模型结构如下图（来自百度图片）： NNLM 接收的输入是长度为 N 的词序列，输出是下一个词的类别。首先，输入是词序列的 index 序列，例如词“我”在字典（大小为|V|）中的 index是10，词“是”的 index 是23， “小明”的 index 是65，则句子“我是小明”的 index 序列就是 10、23、65。嵌入层（Embedding）是一个大小为 |V|×K 的矩阵，从中取出第10、23、65行向量拼成 3×K 的矩阵就是 Embedding层的输出了。隐层接受拼接后的 Embedding 层输出作为输入，以 tanh 为激活函数，最后送入带 softmax 的输出层，输出概率。 NNLM 最大的缺点就是参数多，训练慢，要求输入定长 N 这一点很不灵活，同时不能利用完整的历史信息。 因此，针对 NNLM 存在的问题，Mikolov 在2010年提出了RNNLM，有兴趣可以阅读相关论文，其结构实际上是用RNN 代替 NNLM 里的隐层，这样做的好处，包括减少模型参数、提高训练速度、接受任意长度输入、利用完整的历史信息。同时，RNN 的引入意味着可以使用RNN 的其他变体，像 LSTM、BLSTM、GRU 等等，从而在序列建模上进行更多更丰富的优化。 以上，从词袋模型说起，引出语言模型 N-gram 以及其优化模型 NNLM 和 RNNLM，后续内容从 RNN 说起，来看看其变种 LSTM 和 GRU模型如何处理类似序列数据。 RNN 以及变种 LSTM 和 GRU 原理RNN 为序列数据而生RNN 称为循环神经网路，因为这种网络有“记忆性”，主要应用在自然语言处理（NLP）和语音领域。RNN具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。 理论上，RNN 能够对任何长度的序列数据进行处理，但由于该网络结构存在“梯度消失”问题，所以在实际应用中，解决梯度消失的方法有：梯度裁剪（ClippingGradient）和 LSTM（Long Short-Term Memory）。 下图是一个简单的 RNN 经典结构： RNN 包含输入单元（Input Units），输入集标记为{x0,x1,…,xt,xt…}\{x_0,x_1,…,x_t,x_t…\}{x0​,x1​,…,xt​,xt​…}；输出单元（Output Units）的输出集则被标记为{y0,y1,…,yt,…}\{y_0,y_1,…,y_t,…\}{y0​,y1​,…,yt​,…}；RNN还包含隐藏单元（Hidden Units），我们将其输出集标记为{h0,h1,…,ht,…}\{h_0,h_1,…,h_t,…\}{h0​,h1​,…,ht​,…}，这些隐藏单元完成了最为主要的工作。 LSTM 结构LSTM 在1997年由“Hochreiter &amp; Schmidhuber”提出，目前已经成为 RNN 中的标准形式，用来解决上面提到的 RNN模型存在“长期依赖”的问题。 LSTM 通过三个“门”结构来控制不同时刻的状态和输出。所谓的“门”结构就是使用了 Sigmoid激活函数的全连接神经网络和一个按位做乘法的操作，Sigmoid激活函数会输出一个0~1之间的数值，这个数值代表当前有多少信息能通过“门”，0表示任何信息都无法通过，1表示全部信息都可以通过。其中，“遗忘门”和“输入门”是LSTM 单元结构的核心。下面我们来详细分析下三种“门”结构。 遗忘门，用来让 LSTM“忘记”之前没有用的信息。它会根据当前时刻节点的输入 XtX_tXt​、上一时刻节点的状态 $C_{t -1 } $ 和上一时刻节点的输出 ht−1h_{t-1}ht−1​ 来决定哪些信息将被遗忘。 输入门，LSTM 来决定当前输入数据中哪些信息将被留下来。在 LSTM 使用遗忘门“忘记”部分信息后需要从当前的输入留下最新的记忆。输入门会根据当前时刻节点的输入 XtX_tXt​、上一时刻节点的状态 Ct−1C_{t-1}Ct−1​ 和上一时刻节点的输出 ht−1h_{t-1}ht−1​ 来决定哪些信息将进入当前时刻节点的状态 CtC_tCt​，模型需要记忆这个最新的信息。 输出门，LSTM 在得到最新节点状态 CtC_tCt​ 后，结合上一时刻节点的输出 ht−1h_{t-1}ht−1​ 和当前时刻节点的输入 XtX_tXt​ 来决定当前时刻节点的输出。 GRU 结构GRU（Gated Recurrent Unit）是2014年提出来的新的 RNN 架构，它是简化版的 LSTM。下面是 LSTM 和 GRU的结构比较图（来自于网络）： 在超参数均调优的前提下，据说效果和 LSTM 差不多，但是参数少了1/3，不容易过拟合。如果发现 LSTM 训练出来的模型过拟合比较严重，可以试试 GRU。 实战基于 Keras 的 LSTM 和 GRU 文本分类上面讲了那么多，但是 RNN 的知识还有很多，比如双向 RNN 等，这些需要自己去学习，下面，我们来实战一下基于 LSTM 和 GRU 的文本分类。 本次开发使用 Keras 来快速构建和训练模型，使用的数据集还是第06课使用的司法数据。 整个过程包括： 语料加载 分词和去停用词 数据预处理 使用 LSTM 分类 使用 GRU 分类 第一步，引入数据处理库，停用词和语料加载： #引入包 import random import jieba import pandas as pd #加载停用词 stopwords=pd.read_csv(&#39;stopwords.txt&#39;,index_col=False,quoting=3,sep=&quot;\t&quot;,names=[&#39;stopword&#39;], encoding=&#39;utf-8&#39;) stopwords=stopwords[&#39;stopword&#39;].values #加载语料 laogong_df = pd.read_csv(&#39;beilaogongda.csv&#39;, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) laopo_df = pd.read_csv(&#39;beilaogongda.csv&#39;, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) erzi_df = pd.read_csv(&#39;beierzida.csv&#39;, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) nver_df = pd.read_csv(&#39;beinverda.csv&#39;, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) #删除语料的nan行 laogong_df.dropna(inplace=True) laopo_df.dropna(inplace=True) erzi_df.dropna(inplace=True) nver_df.dropna(inplace=True) #转换 laogong = laogong_df.segment.values.tolist() laopo = laopo_df.segment.values.tolist() erzi = erzi_df.segment.values.tolist() nver = nver_df.segment.values.tolist() 第二步，分词和去停用词： #定义分词和打标签函数preprocess_text #参数content_lines即为上面转换的list #参数sentences是定义的空list，用来储存打标签之后的数据 #参数category 是类型标签 def preprocess_text(content_lines, sentences, category): for line in content_lines: try: segs=jieba.lcut(line) segs = [v for v in segs if not str(v).isdigit()]#去数字 segs = list(filter(lambda x:x.strip(), segs)) #去左右空格 segs = list(filter(lambda x:len(x)&gt;1, segs))#长度为1的字符 segs = list(filter(lambda x:x not in stopwords, segs)) #去掉停用词 sentences.append((&quot; &quot;.join(segs), category))# 打标签 except Exception: print(line) continue #调用函数、生成训练数据 sentences = [] preprocess_text(laogong, sentences,0) preprocess_text(laopo, sentences, 1) preprocess_text(erzi, sentences, 2) preprocess_text(nver, sentences, 3) 第三步，先打散数据，使数据分布均匀，然后获取特征和标签列表： #打散数据，生成更可靠的训练集 random.shuffle(sentences) #控制台输出前10条数据，观察一下 for sentence in sentences[:10]: print(sentence[0], sentence[1]) #所有特征和对应标签 all_texts = [ sentence[0] for sentence in sentences] all_labels = [ sentence[1] for sentence in sentences] 第四步，使用 LSTM 对数据进行分类： #引入需要的模块 from keras.preprocessing.text import Tokenizer from keras.preprocessing.sequence import pad_sequences from keras.utils import to_categorical from keras.layers import Dense, Input, Flatten, Dropout from keras.layers import LSTM, Embedding,GRU from keras.models import Sequential #预定义变量 MAX_SEQUENCE_LENGTH = 100 #最大序列长度 EMBEDDING_DIM = 200 #embdding 维度 VALIDATION_SPLIT = 0.16 #验证集比例 TEST_SPLIT = 0.2 #测试集比例 #keras的sequence模块文本序列填充 tokenizer = Tokenizer() tokenizer.fit_on_texts(all_texts) sequences = tokenizer.texts_to_sequences(all_texts) word_index = tokenizer.word_index print(&#39;Found %s unique tokens.&#39; % len(word_index)) data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) labels = to_categorical(np.asarray(all_labels)) print(&#39;Shape of data tensor:&#39;, data.shape) print(&#39;Shape of label tensor:&#39;, labels.shape) #数据切分 p1 = int(len(data)*(1-VALIDATION_SPLIT-TEST_SPLIT)) p2 = int(len(data)*(1-TEST_SPLIT)) x_train = data[:p1] y_train = labels[:p1] x_val = data[p1:p2] y_val = labels[p1:p2] x_test = data[p2:] y_test = labels[p2:] #LSTM训练模型 model = Sequential() model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)) model.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2)) model.add(Dropout(0.2)) model.add(Dense(64, activation=&#39;relu&#39;)) model.add(Dense(labels.shape[1], activation=&#39;softmax&#39;)) model.summary() #模型编译 model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;rmsprop&#39;, metrics=[&#39;acc&#39;]) print(model.metrics_names) model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10, batch_size=128) model.save(&#39;lstm.h5&#39;) #模型评估 print(model.evaluate(x_test, y_test)) 训练过程结果为： 第五步，使用 GRU 进行文本分类，上面就是完整的使用 LSTM 进行 文本分类，如果使用 GRU 只需要改变模型训练部分： model = Sequential() model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)) model.add(GRU(200, dropout=0.2, recurrent_dropout=0.2)) model.add(Dropout(0.2)) model.add(Dense(64, activation=&#39;relu&#39;)) model.add(Dense(labels.shape[1], activation=&#39;softmax&#39;)) model.summary() model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;rmsprop&#39;, metrics=[&#39;acc&#39;]) print(model.metrics_names) model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10, batch_size=128) model.save(&#39;lstm.h5&#39;) print(model.evaluate(x_test, y_test)) 训练过程结果： 总结本文从词袋模型谈起，旨在引出语言模型 N-gram 以及其优化模型 NNLM 和 RNNLM，并通过 RNN 以及其变种 LSTM 和 GRU模型，理解其如何处理类似序列数据的原理，并实战基于 LSTM 和 GRU 的中文文本分类。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>gru</tag>
        <tag>lstm</tag>
        <tag>n-gram</tag>
        <tag>rnn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于ML的中文短文本聚类（7）]]></title>
    <url>%2F2019%2F11%2F22%2F7.%E5%9F%BA%E4%BA%8EML%E7%9A%84%E4%B8%AD%E6%96%87%E7%9F%AD%E6%96%87%E6%9C%AC%E8%81%9A%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[文本聚类是将一个个文档由原有的自然语言文字信息转化成数学信息，以高维空间点的形式展现出来，通过计算哪些点距离比较近，从而将那些点聚成一个簇，簇的中心叫做簇心。一个好的聚类要保证簇内点的距离尽量的近，但簇与簇之间的点要尽量的远。 文本聚类如下图，以 K、M、N 三个点分别为聚类的簇心，将结果聚为三类，使得簇内点的距离尽量的近，但簇与簇之间的点尽量的远。 文本无监督聚类步骤： 语料加载 分词 去停用词 抽取词向量特征 实战 TF-IDF 的中文文本 K-means 聚类 实战 word2Vec 的中文文本 K-means 聚类 语料加载进行语料加载，在这之前，引入所需要的 Python 依赖包，并将全部语料和停用词字典读入内存中 #依赖库，有随机数库、jieba 分词、pandas 库 import random import jieba import pandas as pd import numpy as np from sklearn.feature_extraction.text import TfidfTransformer from sklearn.feature_extraction.text import TfidfVectorizer import matplotlib.pyplot as plt from sklearn.decomposition import PCA from sklearn.cluster import KMeans import gensim from gensim.models import Word2Vec from sklearn.preprocessing import scale import multiprocessing #加载停用词 stopwords.txt topwords=pd.read_csv(&#39;stopwords.txt&#39;,index_col=False,quoting=3,sep=&quot;\t&quot;,names=[&#39;stopword&#39;], encoding=&#39;utf-8&#39;) topwords=stopwords[&#39;stopword&#39;].values # 加载语料，语料是4个已经分好类的 csv 文件，直接用 pandas 加载即可，加载之后可以首先删除 nan 行，并提取要分词的 content 列转换为 list 列表 #加载语料 laogong_df = pd.read_csv(&#39;beilaogongda.csv&#39;, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) laopo_df = pd.read_csv(&#39;beilaogongda.csv&#39;, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) erzi_df = pd.read_csv(&#39;beierzida.csv&#39;, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) nver_df = pd.read_csv(&#39;beinverda.csv&#39;, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) #删除语料的nan行 laogong_df.dropna(inplace=True) laopo_df.dropna(inplace=True) erzi_df.dropna(inplace=True) nver_df.dropna(inplace=True) #转换 laogong = laogong_df.segment.values.tolist() laopo = laopo_df.segment.values.tolist() erzi = erzi_df.segment.values.tolist() nver = nver_df.segment.values.tolist() 分词和去停用词定义分词、去停用词的函数，函数包含两个参数：content_lines 参数为语料列表；sentences 参数为预先定义的 list，用来存储分词后的结果 def preprocess_text(content_lines, sentences): for line in content_lines: try: segs=jieba.lcut(line) segs = [v for v in segs if not str(v).isdigit()]#去数字 segs = list(filter(lambda x:x.strip(), segs)) #去左右空格 segs = list(filter(lambda x:len(x)&gt;1, segs)) #长度为1的字符 segs = list(filter(lambda x:x not in stopwords, segs)) #去掉停用词 sentences.append(&quot; &quot;.join(segs)) except Exception: print(line) continue # 调用函数、生成训练数据，根据我提供的司法语料数据，分为报警人被老公打，报警人被老婆打，报警人被儿子打，报警人被女儿打 sentences = [] preprocess_text(laogong, sentences) preprocess_text(laopo, sentences) preprocess_text(erzi, sentences) preprocess_text(nver, sentences) # 将得到的数据集打散，生成更可靠的训练集分布，避免同类数据分布不均匀 random.shuffle(sentences) # 我们控制台输出前10条数据，观察一下（因为上面进行了随机打散，你看到的前10条可能不一样） for sentence in sentences[:10]: print(sentenc) 得到的结果聚类和分类是不同的，这里没有标签 抽取词向量特征抽取特征，将文本中的词语转换为词频矩阵，统计每个词语的 tf-idf 权值，获得词在对应文本中的 tf-idf 权重 #将文本中的词语转换为词频矩阵 矩阵元素a[i][j] 表示j词在i类文本下的词频 vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5) #统计每个词语的tf-idf权值 transformer = TfidfTransformer() # 第一个fit_transform是计算tf-idf 第二个fit_transform是将文本转为词频矩阵 tfidf = transformer.fit_transform(vectorizer.fit_transform(sentences)) # 获取词袋模型中的所有词语 word = vectorizer.get_feature_names() # 将tf-idf矩阵抽取出来，元素w[i][j]表示j词在i类文本中的tf-idf权重 weight = tfidf.toarray() #查看特征大小 print (&#39;Features length: &#39; + str(len(word)))TF-IDF 的中文文本 K-means 聚类使用 k-means++ 来初始化模型，当然也可以选择随机初始化，即 init=”random”，然后通过 PCA 降维把上面的权重 weight 降到10维，进行聚类模型训练 numClass=4 #聚类分几簇 clf = KMeans(n_clusters=numClass, max_iter=10000, init=&quot;k-means++&quot;, tol=1e-6) #这里也可以选择随机初始化init=&quot;random&quot; pca = PCA(n_components=10) # 降维 TnewData = pca.fit_transform(weight) # 载入N维 s = clf.fit(TnewData)第二步，定义聚类结果可视化函数 plot_cluster(result,newData,numClass)，该函数包含3个参数，其中 result 表示聚类拟合的结果集；newData 表示权重 weight 降维的结果，这里需要降维到2维，即平面可视化；numClass 表示聚类分为几簇，绘制代码第一部分绘制结果 newData，第二部分绘制聚类的中心点 def plot_cluster(result,newData,numClass): plt.figure(2) Lab = [[] for i in range(numClass)] index = 0 for labi in result: Lab[labi].append(index) index += 1 color = [&#39;oy&#39;, &#39;ob&#39;, &#39;og&#39;, &#39;cs&#39;, &#39;ms&#39;, &#39;bs&#39;, &#39;ks&#39;, &#39;ys&#39;, &#39;yv&#39;, &#39;mv&#39;, &#39;bv&#39;, &#39;kv&#39;, &#39;gv&#39;, &#39;y^&#39;, &#39;m^&#39;, &#39;b^&#39;, &#39;k^&#39;, &#39;g^&#39;] * 3 for i in range(numClass): x1 = [] y1 = [] for ind1 in newData[Lab[i]]: # print ind1 try: y1.append(ind1[1]) x1.append(ind1[0]) except: pass plt.plot(x1, y1, color[i]) #绘制初始中心点 x1 = [] y1 = [] for ind1 in clf.cluster_centers_: try: y1.append(ind1[1]) x1.append(ind1[0]) except: pass plt.plot(x1, y1, &quot;rv&quot;) #绘制中心 plt.show() 对数据降维到2维，然后获得结果，最后绘制聚类结果图 pca = PCA(n_components=2) # 输出两维 newData = pca.fit_transform(weight) # 载入N维 result = list(clf.predict(TnewData)) plot_cluster(result,newData,numClass) 得到的聚类结果图，4个中心点和4个簇，我们看到结果还比较好，簇的边界很清楚上面演示的可视化过程，降维使用了 PCA，我们还可以试试 TSNE，两者同为降维工具，主要区别在于，所在的包不同因为原理不同，导致 TSNE 保留下的属性信息，更具代表性，也即最能体现样本间的差异，但是 TSNE 运行极慢，PCA 则相对较快，下面看看 TSNE 运行的可视化结果 from sklearn.decomposition import PCA from sklearn.manifold import TSNE ts =TSNE(2) newData = ts.fit_transform(weight) result = list(clf.predict(TnewData)) plot_cluster(result,newData,numClass) 得到的可视化结果，为一个中心点，不同簇落在围绕中心点的不同半径之内，得到的可视化结果，为一个中心点，不同簇落在围绕中心点的不同半径之内，我们看到在这里结果并不是很好为了更好的表达和获取更具有代表性的信息，在展示（可视化）高维数据时，更为一般的处理，常常先用 PCA 进行降维，再使用 TSNE from sklearn.manifold import TSNE newData = PCA(n_components=4).fit_transform(weight) # 载入N维 newData =TSNE(2).fit_transform(newData) result = list(clf.predict(TnewData)) plot_cluster(result,newData,numClass) 得到的可视化结果，不同簇落在围绕中心点的不同半径之内从优化和提高模型准确率来说，主要有两方面可以尝试： 特征向量的构建，除了词袋模型，可以考虑使用 word2vec 和 doc2vec 等； 模型上可以采用基于密度的 DBSCAN、层次聚类等算法。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>jieba</tag>
        <tag>tf-idf</tag>
        <tag>gensim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关键字提取（3）]]></title>
    <url>%2F2019%2F11%2F22%2F3.%E5%85%B3%E9%94%AE%E5%AD%97%E6%8F%90%E5%8F%96%2F</url>
    <content type="text"><![CDATA[关键词提取就是从文本里面把跟这篇文章意义最相关的一些词语抽取出来。关键词在文献检索、自动文摘、文本聚类/分类等方面有着重要的应用，它不仅是进行这些工作不可或缺的基础和前提，也是互联网上信息建库的一项重要工作。 简介关键词抽取从方法来说主要有两种： 第一种是关键词分配：就是给定一个已有的关键词库，对于新来的文档从该词库里面匹配几个词语作为这篇文档的关键词。 第二种是关键词提取：针对新文档，通过算法分析，提取文档中一些词语作为该文档的关键词。 目前大多数应用领域的关键词抽取算法都是基于后者实现的，从逻辑上说，后者比前者在实际应用中更准确。 下面介绍一些关于关键词抽取的常用和经典的算法实现。 基于 TF-IDF 算法进行关键词提取在信息检索理论中，TF-IDF 是 Term Frequency - Inverse Document Frequency 的简写。TF-IDF 是一种数值统计，用于反映一个词对于语料中某篇文档的重要性。在信息检索和文本挖掘领域，它经常用于因子加权。TF-IDF 的主要思想就是：如果某个词在一篇文档中出现的频率高，也即 TF 高；并且在语料库中其他文档中很少出现，即 DF 低，也即 IDF 高，则认为这个词具有很好的类别区分能力。 TF 为词频（Term Frequency），表示词 t 在文档 d 中出现的频率，计算公式：IDF 为逆文档频率（Inverse Document Frequency），表示语料库中包含词 t 的文档的数目的倒数，计算公式：TF-IDF 在实际中主要是将二者相乘，也即 TF * IDF， 计算公式：因此，TF-IDF 倾向于过滤掉常见的词语，保留重要的词语。例如，某一特定文件内的高频率词语，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的 TF-IDF。好在 jieba 已经实现了基于 TF-IDF 算法的关键词抽取，通过命令 import jieba.analyse 引入，函数参数解释如下： jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=()) sentence：待提取的文本语料； topK：返回 TF/IDF 权重最大的关键词个数，默认值为 20； withWeight：是否需要返回关键词权重值，默认值为 False； allowPOS：仅包括指定词性的词，默认值为空，即不筛选 import jieba.analyse sentence = &quot;人工智能（Artificial Intelligence），英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，可以设想，未来人工智能带来的科技产品，将会是人类智慧的“容器”。人工智能可以对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。人工智能是一门极富挑战性的科学，从事这项工作的人必须懂得计算机知识，心理学和哲学。人工智能是包括十分广泛的科学，它由不同的领域组成，如机器学习，计算机视觉等等，总的说来，人工智能研究的一个主要目标是使机器能够胜任一些通常需要人类智能才能完成的复杂工作。但不同的时代、不同的人对这种“复杂工作”的理解是不同的。2017年12月，人工智能入选“2017年度中国媒体十大流行语”。&quot; keywords = &quot; &quot;.join(jieba.analyse.extract_tags(sentence , topK=20, withWeight=False, allowPOS=())) print(keywords) 输出：人工智能 智能 2017 机器 不同 人类 科学 模拟 一门 技术 计算机 研究 工作 Artificial Intelligence AI 图像识别 12 复杂 流行语 keywords =(jieba.analyse.extract_tags(sentence , topK=10, withWeight=True, allowPOS=([&#39;n&#39;,&#39;v&#39;]))) print(keywords) 输出：[(‘人工智能’, 0.9750542675762887), (‘智能’, 0.5167124540885567), (‘机器’, 0.20540911929525774), (‘人类’, 0.17414426566082475), (‘科学’, 0.17250169374402063), (‘模拟’, 0.15723537382948452), (‘技术’, 0.14596259315164947), (‘计算机’, 0.14030483362639176), (‘图像识别’, 0.12324502580309278), (‘流行语’, 0.11242211730309279)] 基于 TextRank 算法进行关键词提取TextRank 是由 PageRank 改进而来，核心思想将文本中的词看作图中的节点，通过边相互连接，不同的节点会有不同的权重，权重高的节点可以作为关键词。这里给出 TextRank 的公式：节点 i 的权重取决于节点 i 的邻居节点中 i-j 这条边的权重 / j 的所有出度的边的权重 * 节点 j 的权重，将这些邻居节点计算的权重相加，再乘上一定的阻尼系数，就是节点 i 的权重，阻尼系数 d 一般取 0.85。 TextRank 用于关键词提取的算法如下： （1）把给定的文本 T 按照完整句子进行分割，即:（2）对于每个句子，进行分词和词性标注处理，并过滤掉停用词，只保留指定词性的单词，如名词、动词、形容词，其中 ti,j 是保留后的候选关键词。（3）构建候选关键词图 G = (V,E)，其中 V 为节点集，由（2）生成的候选关键词组成，然后采用共现关系（Co-Occurrence）构造任两点之间的边，两个节点之间存在边仅当它们对应的词汇在长度为 K 的窗口中共现，K 表示窗口大小，即最多共现 K 个单词。 （4）根据 TextRank 的公式，迭代传播各节点的权重，直至收敛。 （5）对节点权重进行倒序排序，从而得到最重要的 T 个单词，作为候选关键词。 （6）由（5）得到最重要的 T 个单词，在原始文本中进行标记，若形成相邻词组，则组合成多词关键词。 同样 jieba 已经实现了基于 TextRank 算法的关键词抽取，通过命令 import jieba.analyse 引用，函数参数解释如下： jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=(&#39;ns&#39;, &#39;n&#39;, &#39;vn&#39;, &#39;v&#39;)) 直接使用，接口参数同 TF-IDF 相同，注意默认过滤词性。接下来，我们继续看例子，语料继续使用上例中的句子。 result = &quot; &quot;.join(jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=(&#39;ns&#39;, &#39;n&#39;, &#39;vn&#39;, &#39;v&#39;))) print(result) 输出：智能 人工智能 机器 人类 研究 技术 模拟 包括 科学 工作 领域 理论 计算机 年度 需要 语言 相似 方式 做出 心理学 修改词性 result = &quot; &quot;.join(jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=(&#39;n&#39;,&#39;v&#39;))) print(result) 输出： 智能 人工智能 机器 人类 技术 模拟 包括 科学 理论 计算机 领域 年度 需要 心理学 信息 语言 识别 带来 过程 延伸 基于 LDA 主题模型进行关键词提取通过 Gensim 库完成基于 LDA 的关键字提取。整个过程的步骤为：文件加载 -&gt; jieba 分词 -&gt; 去停用词 -&gt; 构建词袋模型 -&gt; LDA 模型训练 -&gt; 结果可视化。 数据处理训练#引入库文件 import jieba.analyse as analyse import jieba import pandas as pd from gensim import corpora, models, similarities import gensim import numpy as np import matplotlib.pyplot as plt %matplotlib inline #设置文件路径 dir = &quot;./&quot; file_desc = &quot;&quot;.join([dir,&#39;car.csv&#39;]) stop_words = &quot;&quot;.join([dir,&#39;stopwords.txt&#39;]) #定义停用词 stopwords=pd.read_csv(stop_words,index_col=False,quoting=3,sep=&quot;\t&quot;,names=[&#39;stopword&#39;], encoding=&#39;utf-8&#39;) stopwords=stopwords[&#39;stopword&#39;].values #加载语料 df = pd.read_csv(file_desc, encoding=&#39;gbk&#39;) #删除nan行 df.dropna(inplace=True) lines=df.content.values.tolist() #开始分词 sentences=[] for line in lines: try: segs=jieba.lcut(line) segs = [v for v in segs if not str(v).isdigit()]#去数字 segs = list(filter(lambda x:x.strip(), segs)) #去左右空格 segs = list(filter(lambda x:x not in stopwords, segs)) #去掉停用词 sentences.append(segs) except Exception: print(line) continue #构建词袋模型 dictionary = corpora.Dictionary(sentences) corpus = [dictionary.doc2bow(sentence) for sentence in sentences] #lda模型，num_topics是主题的个数，这里定义了5个 lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10) #我们查一下第1号分类，其中最常出现的5个词是： print(lda.print_topic(1, topn=5)) #我们打印所有5个主题，每个主题显示8个词 for topic in lda.print_topics(num_topics=10, num_words=8): print(topic[1]) 结果可视化#显示中文matplotlib plt.rcParams[&#39;font.sans-serif&#39;] = [u&#39;SimHei&#39;] plt.rcParams[&#39;axes.unicode_minus&#39;] = False # 在可视化部分，我们首先画出了九个主题的7个词的概率分布图 num_show_term = 8 # 每个主题下显示几个词 num_topics = 10 for i, k in enumerate(range(num_topics)): ax = plt.subplot(2, 5, i+1) item_dis_all = lda.get_topic_terms(topicid=k) item_dis = np.array(item_dis_all[:num_show_term]) ax.plot(range(num_show_term), item_dis[:, 1], &#39;b*&#39;) item_word_id = item_dis[:, 0].astype(np.int) word = [dictionary.id2token[i] for i in item_word_id] ax.set_ylabel(u&quot;概率&quot;) for j in range(num_show_term): ax.text(j, item_dis[j, 1], word[j], bbox=dict(facecolor=&#39;green&#39;,alpha=0.1)) plt.suptitle(u&#39;9个主题及其7个主要词的概率&#39;, fontsize=18) plt.show() 基于 pyhanlp 进行关键词提取 HanLP 来完成关键字提取，内部采用 TextRankKeyword 实现 from pyhanlp import * result = HanLP.extractKeyword(sentence, 20) print(result) 输出：[人工智能, 智能, 领域, 人类, 研究, 不同, 工作, 包括, 模拟, 新的, 机器, 计算机, 门, 科学, 应用, 系统, 理论, 技术, 入选, 复杂]]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>tf-idf</tag>
        <tag>pyhanlp</tag>
        <tag>textrank</tag>
        <tag>lda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HMM和CRF（8）]]></title>
    <url>%2F2019%2F11%2F22%2F8.HMM%E5%92%8CCRF%2F</url>
    <content type="text"><![CDATA[HMM（隐马尔可夫模型）和 CRF（条件随机场）算法常常被用于分词、句法分析、命名实体识别、词性标注等。由于两者之间有很大的共同点，所以在很多应用上往往是重叠的，但在命名实体、句法分析等领域 CRF 似乎更胜一筹。 在机器学习中，生成式模型和判别式模型都用于有监督学习，有监督学习的任务就是从数据中学习一个模型（也叫分类器），应用这一模型，对给定的输入 X 预测相应的输出 Y。这个模型的一般形式为：决策函数 Y=f(X) 或者条件概率分布 P(Y|X)。 首先，简单从贝叶斯定理说起，若记 P(A)、P(B) 分别表示事件 A 和事件 B 发生的概率，则 P(A|B) 表示事件 B 发生的情况下事件 A 发生的概率；P(AB)表示事件 A 和事件 B 同时发生的概率。 生成式模型：估计的是联合概率分布，P(Y, X)=P(Y|X)*P(X)，由联合概率密度分布 P(X,Y)，然后求出条件概率分布 P(Y|X) 作为预测的模型，即生成模型公式为：P(Y|X)= P(X,Y)/ P(X)。基本思想是首先建立样本的联合概率密度模型 P(X,Y)，然后再得到后验概率 P(Y|X)，再利用它进行分类，其主要关心的是给定输入 X 产生输出 Y 的生成关系。 判别式模型：估计的是条件概率分布， P(Y|X)，是给定观测变量 X 和目标变量 Y 的条件模型。由数据直接学习决策函数 Y=f(X) 或者条件概率分布 P(Y|X) 作为预测的模型，其主要关心的是对于给定的输入 X，应该预测什么样的输出 Y。 HMM 使用隐含变量生成可观测状态，其生成概率有标注集统计得到，是一个生成模型。其他常见的生成式模型有：Gaussian、 Naive Bayes、Mixtures of multinomials 等。 CRF 就像一个反向的隐马尔可夫模型（HMM），通过可观测状态判别隐含变量，其概率亦通过标注集统计得来，是一个判别模型。其他常见的判别式模型有：K 近邻法、感知机、决策树、逻辑斯谛回归模型、最大熵模型、支持向量机、提升方法等。 基于 HMM 训练自己的 Python 中文分词器HMM 模型是由一个“五元组”组成的集合： StatusSet：状态值集合，状态值集合为 (B, M, E, S)，其中 B 为词的首个字，M 为词中间的字，E 为词语中最后一个字，S 为单个字，B、M、E、S 每个状态代表的是该字在词语中的位置。 举个例子，对“中国的人工智能发展进入高潮阶段”，分词可以标注为：“中B国E的S人B工E智B能E发B展E进B入E高B潮E阶B段E”，最后的分词结果为：[‘中国’, ‘的’, ‘人工’, ‘智能’, ‘发展’, ‘进入’, ‘高潮’, ‘阶段’]。 ObservedSet：观察值集合，观察值集合就是所有语料的汉字，甚至包括标点符号所组成的集合。 TransProbMatrix：转移概率矩阵，状态转移概率矩阵的含义就是从状态 X 转移到状态 Y 的概率，是一个4×4的矩阵，即 {B,E,M,S}×{B,E,M,S}。 EmitProbMatrix：发射概率矩阵，发射概率矩阵的每个元素都是一个条件概率，代表 P(Observed[i]|Status[j]) 概率。 InitStatus：初始状态分布，初始状态概率分布表示句子的第一个字属于 {B,E,M,S} 这四种状态的概率。 将 HMM 应用在分词上，要解决的问题是：参数（ObservedSet、TransProbMatrix、EmitRobMatrix、InitStatus）已知的情况下，求解状态值序列。解决这个问题的最有名的方法是 Viterbi 算法。 本次训练使用的预料 syj_trainCorpus_utf8.txt ,整个语料大小 264M，包含1116903条数据，UTF-8 编码，词与词之间用空格隔开，用来训练分词模型。语料格式，用空格隔开的： 如果 继续 听任 资产阶级 自由化 的 思潮 泛滥 ， 党 就 失去 了 凝聚力 和 战斗力 ， 怎么 能 成为 全国 人民 的 领导 核心 ？ 中国 又 会 成为 一盘散沙 ， 那 还有 什么 希望 ？预定义# 用来模型保存 import pickle import json # 定义 HMM 中的状态，初始化概率，以及中文停顿词 STATES = {&#39;B&#39;, &#39;M&#39;, &#39;E&#39;, &#39;S&#39;} EPS = 0.0001 #定义停顿标点 seg_stop_words = {&quot; &quot;,&quot;，&quot;,&quot;。&quot;,&quot;“&quot;,&quot;”&quot;,&#39;“&#39;, &quot;？&quot;, &quot;！&quot;, &quot;：&quot;, &quot;《&quot;, &quot;》&quot;, &quot;、&quot;, &quot;；&quot;, &quot;·&quot;, &quot;‘ &quot;, &quot;’&quot;, &quot;──&quot;, &quot;,&quot;, &quot;.&quot;, &quot;?&quot;, &quot;!&quot;, &quot;`&quot;, &quot;~&quot;, &quot;@&quot;, &quot;#&quot;, &quot;$&quot;, &quot;%&quot;, &quot;^&quot;, &quot;&amp;&quot;, &quot;*&quot;, &quot;(&quot;, &quot;)&quot;, &quot;-&quot;, &quot;_&quot;, &quot;+&quot;, &quot;=&quot;, &quot;[&quot;, &quot;]&quot;, &quot;{&quot;, &quot;}&quot;, &#39;&quot;&#39;, &quot;&#39;&quot;, &quot;&lt;&quot;, &quot;&gt;&quot;, &quot;\\&quot;, &quot;|&quot; &quot;\r&quot;, &quot;\n&quot;,&quot;\t&quot;} 编码实现将 HMM 模型封装为独立的类 HMM_Model class HMM_Model: def __init__(self): pass #初始化 def setup(self): pass #模型保存 def save(self, filename, code): pass #模型加载 def load(self, filename, code): pass #模型训练 def do_train(self, observes, states): pass #HMM计算 def get_prob(self): pass #模型预测 def do_predict(self, sequence): pass 第一个方法 __init__()是一种特殊的方法，被称为类的构造函数或初始化方法，当创建了这个类的实例时就会调用该方法，其中定义了数据结构和初始变量，实现如下： def __init__(self): self.trans_mat = {} self.emit_mat = {} self.init_vec = {} self.state_count = {} self.states = {} self.inited = False 其中的数据结构定义： trans_mat：状态转移矩阵，trans_mat[state1][state2] 表示训练集中由 state1 转移到 state2 的次数。 emit_mat：观测矩阵，emit_mat[state][char] 表示训练集中单字 char 被标注为 state 的次数。 init_vec：初始状态分布向量，init_vec[state] 表示状态 state 在训练集中出现的次数。 state_count：状态统计向量，state_count[state]表示状态 state 出现的次数。 word_set：词集合，包含所有单词。第二个方法 setup()，初始化第一个方法中的数据结构，具体实现如下：#初始化数据结构 def setup(self): for state in self.states: # build trans_mat self.trans_mat[state] = {} for target in self.states: self.trans_mat[state][target] = 0.0 self.emit_mat[state] = {} self.init_vec[state] = 0 self.state_count[state] = 0 self.inited = True 第三个方法 save()，用来保存训练好的模型，filename 指定模型名称，默认模型名称为 hmm.json，这里提供两种格式的保存类型，JSON 或者pickle 格式，通过参数 code 来决定，code 的值为 code=&#39;json&#39; 或者 code = &#39;pickle&#39;，默认为code=&#39;json&#39;，具体实现如下：#模型保存 def save(self, filename=&quot;hmm.json&quot;, code=&#39;json&#39;): fw = open(filename, &#39;w&#39;, encoding=&#39;utf-8&#39;) data = { &quot;trans_mat&quot;: self.trans_mat, &quot;emit_mat&quot;: self.emit_mat, &quot;init_vec&quot;: self.init_vec, &quot;state_count&quot;: self.state_count } if code == &quot;json&quot;: txt = json.dumps(data) txt = txt.encode(&#39;utf-8&#39;).decode(&#39;unicode-escape&#39;) fw.write(txt) elif code == &quot;pickle&quot;: pickle.dump(data, fw) fw.close() 第四个方法 load()，与第三个 save() 方法对应，用来加载模型，filename 指定模型名称，默认模型名称为hmm.json，这里提供两种格式的保存类型，JSON 或者 pickle 格式，通过参数 code 来决定，code 的值为 code=&#39;json&#39;或者 code = &#39;pickle&#39;，默认为 code=&#39;json&#39;，具体实现如下： #模型加载 def load(self, filename=&quot;hmm.json&quot;, code=&quot;json&quot;): fr = open(filename, &#39;r&#39;, encoding=&#39;utf-8&#39;) if code == &quot;json&quot;: txt = fr.read() model = json.loads(txt) elif code == &quot;pickle&quot;: model = pickle.load(fr) self.trans_mat = model[&quot;trans_mat&quot;] self.emit_mat = model[&quot;emit_mat&quot;] self.init_vec = model[&quot;init_vec&quot;] self.state_count = model[&quot;state_count&quot;] self.inited = True fr.close() 第五个方法 do_train()，用来训练模型，因为使用的标注数据集， 因此可以使用更简单的监督学习算法，训练函数输入观测序列和状态序列进行训练，依次更新各矩阵数据。类中维护的模型参数均为频数而非频率，这样的设计使得模型可以进行在线训练，使得模型随时都可以接受新的训练数据继续训练，不会丢失前次训练的结果。具体实现如下： #模型训练 def do_train(self, observes, states): if not self.inited: self.setup() for i in range(len(states)): if i == 0: self.init_vec[states[0]] += 1 self.state_count[states[0]] += 1 else: self.trans_mat[states[i - 1]][states[i]] += 1 self.state_count[states[i]] += 1 if observes[i] not in self.emit_mat[states[i]]: self.emit_mat[states[i]][observes[i]] = 1 else: self.emit_mat[states[i]][observes[i]] += 1 第六个方法 get_prob()，在进行预测前，需将数据结构的频数转换为频率，具体实现如下： #频数转频率 def get_prob(self): init_vec = {} trans_mat = {} emit_mat = {} default = max(self.state_count.values()) for key in self.init_vec: if self.state_count[key] != 0: init_vec[key] = float(self.init_vec[key]) / self.state_count[key] else: init_vec[key] = float(self.init_vec[key]) / default for key1 in self.trans_mat: trans_mat[key1] = {} for key2 in self.trans_mat[key1]: if self.state_count[key1] != 0: trans_mat[key1][key2] = float(self.trans_mat[key1][key2]) / self.state_count[key1] else: trans_mat[key1][key2] = float(self.trans_mat[key1][key2]) / default for key1 in self.emit_mat: emit_mat[key1] = {} for key2 in self.emit_mat[key1]: if self.state_count[key1] != 0: emit_mat[key1][key2] = float(self.emit_mat[key1][key2]) / self.state_count[key1] else: emit_mat[key1][key2] = float(self.emit_mat[key1][key2]) / default return init_vec, trans_mat, emit_mat 第七个方法 do_predict()，预测采用 Viterbi 算法求得最优路径， 具体实现如下： #模型预测 def do_predict(self, sequence): tab = [{}] path = {} init_vec, trans_mat, emit_mat = self.get_prob() # 初始化 for state in self.states: tab[0][state] = init_vec[state] * emit_mat[state].get(sequence[0], EPS) path[state] = [state] # 创建动态搜索表 for t in range(1, len(sequence)): tab.append({}) new_path = {} for state1 in self.states: items = [] for state2 in self.states: if tab[t - 1][state2] == 0: continue prob = tab[t - 1][state2] * trans_mat[state2].get(state1, EPS) * emit_mat[state1].get(sequence[t], EPS) items.append((prob, state2)) best = max(items) tab[t][state1] = best[0] new_path[state1] = path[best[1]] + [state1] path = new_path # 搜索最有路径 prob, state = max([(tab[len(sequence) - 1][state], state) for state in self.states]) return path[state] 上面实现了类 HMM_Model 的7个方法，接下来我们来实现分词器，这里先定义两个函数，这两个函数是独立的，不在类中。 定义一个工具函数，对输入的训练语料中的每个词进行标注，因为训练数据是空格隔开的，可以进行转态标注，该方法用在训练数据的标注，具体实现如下： def get_tags(src): tags = [] if len(src) == 1: tags = [&#39;S&#39;] elif len(src) == 2: tags = [&#39;B&#39;, &#39;E&#39;] else: m_num = len(src) - 2 tags.append(&#39;B&#39;) tags.extend([&#39;M&#39;] * m_num) tags.append(&#39;E&#39;) return tags 定义一个工具函数,根据预测得到的标注序列将输入的句子分割为词语列表，也就是预测得到的状态序列，解析成一个 list 列表进行返回，具体实现如下： def cut_sent(src, tags): word_list = [] start = -1 started = False if len(tags) != len(src): return None if tags[-1] not in {&#39;S&#39;, &#39;E&#39;}: if tags[-2] in {&#39;S&#39;, &#39;E&#39;}: tags[-1] = &#39;S&#39; else: tags[-1] = &#39;E&#39; for i in range(len(tags)): if tags[i] == &#39;S&#39;: if started: started = False word_list.append(src[start:i]) word_list.append(src[i]) elif tags[i] == &#39;B&#39;: if started: word_list.append(src[start:i]) start = i started = True elif tags[i] == &#39;E&#39;: started = False word = src[start:i+1] word_list.append(word) elif tags[i] == &#39;M&#39;: continue return word_list 最后，我们来定义分词器类 HMMSoyoger，继承 HMM_Model 类并实现中文分词器训练、分词功能，先给出 HMMSoyoger 类的结构定义： class HMMSoyoger(HMM_Model): def __init__(self, *args, **kwargs): pass #加载训练数据 def read_txt(self, filename): pass #模型训练函数 def train(self): pass #模型分词预测 def lcut(self, sentence): pass 第一个方法 init()，构造函数，定义了初始化变量，具体实现如下： def __init__(self, *args, **kwargs): super(HMMSoyoger, self).__init__(*args, **kwargs) self.states = STATES self.data = None 第二个方法 read_txt()，加载训练语料，读入文件为 txt，并且 UTF-8 编码，防止中文出现乱码，具体实现如下： #加载语料 def read_txt(self, filename): self.data = open(filename, &#39;r&#39;, encoding=&quot;utf-8&quot;) 第三个方法 train()，根据单词生成观测序列和状态序列，并通过父类的 do_train() 方法进行训练，具体实现如下： def train(self): if not self.inited: self.setup() for line in self.data: line = line.strip() if not line: continue #观测序列 observes = [] for i in range(len(line)): if line[i] == &quot; &quot;: continue observes.append(line[i]) #状态序列 words = line.split(&quot; &quot;) states = [] for word in words: if word in seg_stop_words: continue states.extend(get_tags(word)) #开始训练 if(len(observes) &gt;= len(states)): self.do_train(observes, states) else: pass 第四个方法 lcut()，模型训练好之后，通过该方法进行分词测试，具体实现如下： def lcut(self, sentence): try: tags = self.do_predict(sentence) return cut_sent(sentence, tags) except: return sentence 通过上面两个类和两个方法，就完成了基于 HMM 的中文分词器编码，下面我们来进行模型训练和测试。 训练模型首先实例化 HMMSoyoger 类，然后通过 read_txt() 方法加载语料，再通过 train()进行在线训练，如果训练语料比较大，可能需要等待一点时间，具体实现如下： soyoger = HMMSoyoger() soyoger.read_txt(&quot;syj_trainCorpus_utf8.txt&quot;) soyoger.train() 模型测试模型训练完成之后，我们就可以进行测试： soyoger.lcut(&quot;中国的人工智能发展进入高潮阶段。&quot;) 得到结果为： [‘中国’, ‘的’, ‘人工’, ‘智能’, ‘发展’, ‘进入’, ‘高潮’, ‘阶段’, ‘。’] soyoger.lcut(&quot;中文自然语言处理是人工智能技术的一个重要分支。&quot;) 得到结果为： [‘中文’, ‘自然’, ‘语言’, ‘处理’, ‘是人’, ‘工智’, ‘能技’, ‘术的’, ‘一个’, ‘重要’, ‘分支’, ‘。’] 可见，最后的结果还是不错的，如果想取得更好的结果，可自行制备更大更丰富的训练数据集。 基于 CRF 的开源中文分词工具 Genius 实践Genius 是一个基于 CRF 的开源中文分词工具，采用了 Wapiti 做训练与序列标注，支持 Python 2.x、Python 3.x。 安装（1）下载源码 在 Github 上下载源码地址，解压源码，然后通过 python setup.py install 安装。 （2）Pypi 安装 通过执行命令：easy_install genius 或者 pip install genius 安装。 分词首先引入 Genius，然后对 text 文本进行分词。 import genius text = u&quot;&quot;&quot;中文自然语言处理是人工智能技术的一个重要分支。&quot;&quot;&quot; seg_list = genius.seg_text( text, use_combine=True, use_pinyin_segment=True, use_tagging=True, use_break=True ) print(&#39; &#39;.join([word.text for word in seg_list]) 其中，genius.seg_text 函数接受5个参数，其中 text 是必填参数： text 第一个参数为需要分词的字。 use_break 代表对分词结构进行打断处理，默认值 True。 use_combine 代表是否使用字典进行词合并，默认值 False。 use_tagging 代表是否进行词性标注，默认值 True。 use_pinyin_segment 代表是否对拼音进行分词处理，默认值 True。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>crf</tag>
        <tag>hmm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4j从入门到构建一个简单知识图谱（20）]]></title>
    <url>%2F2019%2F11%2F22%2F20.Neo4j%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%2F</url>
    <content type="text"><![CDATA[Neo4j 对于大多数人来说，可能是比较陌生的。其实，Neo4j 是一个图形数据库，就像传统的关系数据库中的 Oracel 和MySQL一样，用来持久化数据。Neo4j 是最近几年发展起来的新技术，属于 NoSQL 数据库中的一种。 本文主要从 Neo4j 为什么被用来做知识图谱，Neo4j 的简单安装，在 Neo4j 浏览器中创建节点和关系，Neo4j 的 Python 接口操作以及用Neo4j 构建一个简单的农业知识图谱五个方面来讲。 Neo4j 为什么被用来做知识图谱从第19课《知识挖掘与知识图谱概述》中，我们已经明白，知识图谱是一种基于图的数据结构，由节点和边组成。其中节点即实体，由一个全局唯一的 ID标示，关系（也称属性）用于连接两个节点。通俗地讲，知识图谱就是把所有不同种类的信息连接在一起而得到一个关系网络，提供了从“关系”的角度去分析问题的能力。 而 Neo4j 作为一种经过特别优化的图形数据库，有以下优势： 数据存储 ：不像传统数据库整条记录来存储数据，Neo4j 以图的结构存储，可以存储图的节点、属性和边。属性、节点都是分开存储的，属性与节点的关系构成边，这将大大有助于提高数据库的性能。 数据读写 ：在 Neo4j 中，存储节点时使用了 Index-free Adjacency 技术，即每个节点都有指向其邻居节点的指针，可以让我们在时间复杂度为 O(1) 的情况下找到邻居节点。另外，按照官方的说法，在 Neo4j 中边是最重要的，是 First-class Entities，所以单独存储，更有利于在图遍历时提高速度，也可以很方便地以任何方向进行遍历。 资源丰富 ：Neo4j 作为较早的一批图形数据库之一，其文档和各种技术博客较多。 同类对比 ：Flockdb 安装过程中依赖太多，安装复杂；Orientdb，Arangodb 与 Neo4j 做对比，从易用性来说都差不多，但是从稳定性来说，neo4j 是最好的。 综合上述以及因素，我认为 Neo4j 是做知识图谱比较简单、灵活、易用的图形数据库。 Neo4j 的简单安装Neo4j 是基于 Java 的图形数据库，运行 Neo4j 需要启动 JVM 进程，因此必须安装 Java SE 的 JDK。从 Oracle官方网站下载 Java SEJDK，选择版本JDK8 以上版本即可。 下面简单介绍下 Neo4j 在 Linux 和 Windows的安装过程。首先去官网下载对应版本。解压之后，Neo4j 应用程序有如下主要的目录结构： bin 目录：用于存储 Neo4j 的可执行程序； conf 目录：用于控制 Neo4j 启动的配置文件； data 目录：用于存储核心数据库文件； plugins 目录：用于存储 Neo4j 的插件。 Linux 系统下的安装通过 tar 解压命令解压到一个目录下： tar -xzvf neo4j-community-3.3.1-unix.tar.gz 然后进入 Neo4j 解压目录： cd /usr/local/neo4j/neo4j-community-3.1.0 通过启动命令，可以实现启动、控制台、停止服务： bin/neo4j start/console/stop（启动/控制台/停止） 通过 cypher-shell 命令，可以进入命令行： bin/cypher-shell Windows 系统下的安装启动 DOS 命令行窗口，切换到解压目录 bin 下，以管理员身份运行命令，分别为启动服务、停止服务、重启服务和查询服务的状态： bin\neo4j start bin\neo4j stop bin\neo4j restart bin\neo4j status 把 Neo4j 安装为服务（Windows Services），可通过以下命令： bin\neo4j install-service bin\neo4j uninstall-service Neo4j 的配置文档存储在 conf 目录下，Neo4j 通过配置文件 neo4j.conf控制服务器的工作。默认情况下，不需要进行任意配置，就可以启动服务器。 下面我们在 Windows 环境下启动 Neo4j： Neo4j 服务器具有一个集成的浏览器，在一个运行的服务器实例上访问： http://localhost:7474/，打开浏览器，显示启动页面： 默认的 Host 是 bolt://localhost:7687，默认的用户是 neo4j，其默认的密码是 neo4j，第一次成功登录到 Neo4j服务器之后，需要重置密码。访问 Graph Database 需要输入身份验证，Host 是 Bolt 协议标识的主机。登录成功后界面： 到此为止，我们就完成了 Neo4j 的基本安装过程，更详细的参数配置，可以参考官方文档。 在 Neo4j 浏览器中创建节点和关系下面，我们简单编写 Cypher 命令，Cypher 命令可以通过 Neo4j教程学习，在浏览器中通过 Neo4j 创建两个节点和两个关系。 在 $ 命令行中，编写 Cypher 脚本代码，点击 Play 按钮完成创建，依次执行下面的语句： CREATE (n:Person { name: &#39;Andres&#39;, title: &#39;Developer&#39; }) return n; 作用是创建一个 Person，并包含属性名字和职称。 下面这条语句也创建了一个 Person 对象，属性中只是名字和职称不一样。 CREATE (n:Person { name: &#39;Vic&#39;, title: &#39;Developer&#39; }) return n; 紧接着，通过下面两行命令进行两个 Person 的关系匹配： match(n:Person{name:&quot;Vic&quot;}),(m:Person{name:&quot;Andres&quot;}) create (n)-[r:Friend]-&gt;(m) return r; match(n:Person{name:&quot;Vic&quot;}),(m:Person{name:&quot;Andres&quot;}) create (n)&lt;-[r:Friend]-(m) return r; 最后，在创建完两个节点和关系之后，查看数据库中的图形： match(n) return n; 如下图，返回两个 Person 节点，以及其关系网，两个 Person 之间组成 Friend 关系： Neo4j 的 Python 操作既然 Neo4j 作为一个图库数据库，那我们在项目中使用的时候，必然不能通过上面那种方式完成任务，一般都要通过代码来完成数据的持久化操作。其中，对于Java 编程者来说，可通过 Spring Data Neo4j 达到这一目的。 而对于 Python 开发者来说，Py2neo 库也可以完成对 Neo4j 的操作，操作过程如下。 首先 安装 Py2neo。Py2neo 的安装过程非常简单，在命令行通过下面命令即可安装成功。 pip install py2neo 安装好之后，我们来看一下简单的图关系构建，看下面代码： from py2neo.data import Node, Relationship a = Node(&quot;Person&quot;, name=&quot;Alice&quot;) b = Node(&quot;Person&quot;, name=&quot;Bob&quot;) ab = Relationship(a, &quot;KNOWS&quot;, b) 第一行代码，首先引入 Node 和 Relationship 对象，紧接着，创建 a 和 b 节点对象，最后一行匹配 a 和 b之间的工作雇佣关系。接着来看看 ab 对象的内容是什么： print(ab) 通过 print 打印出 ab 的内容： (Alice)-[:KNOWS {}]-&gt;(Bob) 通过这样，就完成了 Alice 和 Bob 之间的工作关系，如果有多组关系将构建成 Person 之间的一个关系网。 了解更多 Py2neo 的使用方法，建议查看官方文档。 用 Neo4j 构建一个简单的农业知识图谱我们来看一个基于开源语料的简单农业知识图谱，由于过程比较繁杂，数据和知识图谱数据预处理过程这里不再赘述，下面，我们重点看基于 Neo4j来创建知识图谱的过程。 整个过程主要包含以下步骤： 环境准备 语料准备 语料加载 知识图谱查询展示 Neo4j 环境准备。根据上面对 Neo4j 环境的介绍，这里默认你已经搭建好 Neo4j 的环境，并能正常访问，如果没有环境，请自行搭建好 Neo4j 的可用环境。 数据语料介绍。本次提供的语料是已经处理好的数据，包含6个 csv 文件，文件内容和描述如下。 attributes.csv：文件大小 2M，内容是通过互动百科页面得到的部分实体的属性，包含字段：Entity、AttributeName、Attribute，分别表示实体、属性名称、属性值。文件前5行结构如下： Entity,AttributeName,Attribute 密度板,别名,纤维板 葡萄蔓枯病,主要为害部位,枝蔓 坎德拉,性别,男 坎德拉,国籍,法国 坎德拉,场上位置,后卫 hudong_pedia.csv：文件大小 94.6M，内容是已经爬好的农业实体的百科页面的结构化数据，包含字段：title、url、image、openTypeList、detail、baseInfoKeyList、baseInfoValueList，分别表示名称、百科 URL 地址、图片、分类类型、详情、关键字、依据来源。文件前2行结构如下： &quot;title&quot;,&quot;url&quot;,&quot;image&quot;,&quot;openTypeList&quot;,&quot;detail&quot;,&quot;baseInfoKeyList&quot;,&quot;baseInfoValueList&quot; &quot;菊糖&quot;,&quot;http://www.baike.com/wiki/菊糖&quot;,&quot;http://a0.att.hudong.com/72/85/20200000013920144736851207227_s.jpg&quot;,&quot;健康科学##分子生物学##化学品##有机物##科学##自然科学##药品##药学名词##药物中文名称列表&quot;,&quot;[药理作用] 诊断试剂 人体内不含菊糖，静注后，不被机体分解、结合、利用和破坏，经肾小球滤过，通过测定血中和尿中的菊糖含量，可以准确计算肾小球的滤过率。菊糖广泛存在于植物组织中,约有3.6万种植物中含有菊糖,尤其是菊芋、菊苣块根中含有丰富的菊糖[6,8]。菊芋(Jerusalem artichoke)又名洋姜,多年生草本植物,在我国栽种广泛,其适应性广、耐贫瘠、产量高、易种植,一般亩产菊芋块茎为2 000～4 000 kg,菊芋块茎除水分外,还含有15%～20%的菊糖,是加工生产菊糖及其制品的良好原料。&quot;,&quot;中文名：&quot;,&quot;菊糖&quot; &quot;密度板&quot;,&quot;http://www.baike.com/wiki/密度板&quot;,&quot;http://a0.att.hudong.com/64/31/20200000013920144728317993941_s.jpg&quot;,&quot;居家##巧克力包装##应用科学##建筑材料##珠宝盒##礼品盒##科学##糖果盒##红酒盒##装修##装饰材料##隔断##首饰盒&quot;,&quot;密度板（英文：Medium Density Fiberboard (MDF)）也称纤维板，是以木质纤维或其他植物纤维为原料，施加脲醛树脂或其他适用的胶粘剂制成的人造板材。按其密度的不同，分为高密度板、中密度板、低密度板。密度板由于质软耐冲击，也容易再加工，在国外是制作家私的一种良好材料，但由于国家关于高密度板的标准比国际标准低数倍，所以，密度板在中国的使用质量还有待提高。&quot;,&quot;中文名：##全称：##别名：##主要材料：##分类：##优点：&quot;,&quot;密度板##中密度板纤维板##纤维板##以木质纤维或其他植物纤维##高密度板、中密度板、低密度板##表面光滑平整、材质细密性能稳定&quot; hudong_pedia2.csv：文件大小 41M，内容结构和 hudong_pedia.csv 文件保持一致，只是增加数据量，作为 hudong_pedia.csv 数据的补充。 new_node.csv：文件大小 2.28M，内容是节点名称和标签，包含字段：title、lable，分别表示节点名称、标签，文件前5行结构如下： title,lable 药物治疗,newNode 膳食纤维,newNode Boven Merwede,newNode 亚美尼亚苏维埃百科全书,newNode wikidata_relation.csv：文件大小 1.83M，内容是实体和关系，包含字段 HudongItem1、relation、HudongItem2，分别表示实体1、关系、实体2，文件前5行结构如下： HudongItem1,relation,HudongItem2 菊糖,instance of,化合物 菊糖,instance of,多糖 瓦尔,instance of,河流 菊糖,subclass of,食物 瓦尔,origin of the watercourse,莱茵河 wikidata_relation2.csv：大小 7.18M，内容结构和 wikidata_relation.csv 一致，作为 wikidata_relation.csv 数据的补充。 语料加载。语料加载，利用 Neo4j 的 LOAD CSV WITH HEADERS FROM... 功能进行加载，具体操作过程如下。 首先，依次执行以下命令： // 将hudong_pedia.csv 导入 LOAD CSV WITH HEADERS FROM &quot;file:///hudong_pedia.csv&quot; AS line CREATE (p:HudongItem{title:line.title,image:line.image,detail:line.detail,url:line.url,openTypeList:line.openTypeList,baseInfoKeyList:line.baseInfoKeyList,baseInfoValueList:line.baseInfoValueList}) 执行成功之后，控制台显示成功： 上面这张图，表示数据加载成功，并显示加载的数据条数和耗费的时间。 // 新增了hudong_pedia2.csv LOAD CSV WITH HEADERS FROM &quot;file:///hudong_pedia2.csv&quot; AS line CREATE (p:HudongItem{title:line.title,image:line.image,detail:line.detail,url:line.url,openTypeList:line.openTypeList,baseInfoKeyList:line.baseInfoKeyList,baseInfoValueList:line.baseInfoValueList}) // 创建索引 CREATE CONSTRAINT ON (c:HudongItem) ASSERT c.title IS UNIQUE 以上命令的意思是，将 hudong_pedia.csv 和 hudong_pedia2.csv 导入 Neo4j 作为结点，然后对 titile属性添加 UNIQUE（唯一约束/索引）。 注意： 如果导入的时候出现 Neo4j JVM 内存溢出错误，可以在导入前，先把 Neo4j 下的 conf/neo4j.conf 中的dbms.memory.heap.initial_size 和 dbms.memory.heap.max_size调大点。导入完成后再把值改回去即可。 下面继续执行数据导入命令： // 导入新的节点 LOAD CSV WITH HEADERS FROM &quot;file:///new_node.csv&quot; AS line CREATE (:NewNode { title: line.title }) //添加索引 CREATE CONSTRAINT ON (c:NewNode) ASSERT c.title IS UNIQUE //导入hudongItem和新加入节点之间的关系 LOAD CSV WITH HEADERS FROM &quot;file:///wikidata_relation2.csv&quot; AS line MATCH (entity1:HudongItem{title:line.HudongItem}) , (entity2:NewNode{title:line.NewNode}) CREATE (entity1)-[:RELATION { type: line.relation }]-&gt;(entity2) LOAD CSV WITH HEADERS FROM &quot;file:///wikidata_relation.csv&quot; AS line MATCH (entity1:HudongItem{title:line.HudongItem1}) , (entity2:HudongItem{title:line.HudongItem2}) CREATE (entity1)-[:RELATION { type: line.relation }]-&gt;(entity2) 执行完这些命令后，我们导入 new_node.csv 新节点，并对 titile 属性添加 UNIQUE（唯一约束/索引），导入wikidata_relation.csv 和 wikidata_relation2.csv，并给节点之间创建关系。 紧接着，继续导入实体属性，并创建实体之间的关系： LOAD CSV WITH HEADERS FROM &quot;file:///attributes.csv&quot; AS line MATCH (entity1:HudongItem{title:line.Entity}), (entity2:HudongItem{title:line.Attribute}) CREATE (entity1)-[:RELATION { type: line.AttributeName }]-&gt;(entity2); LOAD CSV WITH HEADERS FROM &quot;file:///attributes.csv&quot; AS line MATCH (entity1:HudongItem{title:line.Entity}), (entity2:NewNode{title:line.Attribute}) CREATE (entity1)-[:RELATION { type: line.AttributeName }]-&gt;(entity2); LOAD CSV WITH HEADERS FROM &quot;file:///attributes.csv&quot; AS line MATCH (entity1:NewNode{title:line.Entity}), (entity2:NewNode{title:line.Attribute}) CREATE (entity1)-[:RELATION { type: line.AttributeName }]-&gt;(entity2); LOAD CSV WITH HEADERS FROM &quot;file:///attributes.csv&quot; AS line MATCH (entity1:NewNode{title:line.Entity}), (entity2:HudongItem{title:line.Attribute}) CREATE (entity1)-[:RELATION { type: line.AttributeName }]-&gt;(entity2) 这里注意，建索引的时候带了 label，因此只有使用 label 时才会使用索引，这里我们的实体有两个 label，所以一共做 2*2=4次。当然也可以建立全局索引，即对于不同的 label 使用同一个索引。 以上过程，我们就完成了语料加载，并创建了实体之间的关系和属性匹配，下面我们来看看 Neo4j 图谱关系展示。 知识图谱查询展示最后通过 cypher 语句查询来看看农业图谱展示。 首先，展示 HudongItem 实体，执行如下命令： MATCH (n:HudongItem) RETURN n LIMIT 25 对 HudongItem 实体进行查询，返回结果的25条数据，结果如下图： 接着，展示 NewNode 实体，执行如下命令： MATCH (n:NewNode) RETURN n LIMIT 25 对 NewNode 实体进行查询，返回结果的25条数据，结果如下图： 之后，展示 RELATION 直接的关系，执行如下命令： MATCH p=()-[r:RELATION]-&gt;() RETURN p LIMIT 25 展示实体属性关系，结果如下图： 总结本节内容到此结束，回顾下整篇文章，主要讲了以下内容： 解释了 Neo4j 被用来做知识图谱的原因； Neo4j 的简单安装以及在 Neo4j 浏览器中创建节点和关系； Neo4j 的 Python 接口操作及使用； 从五个方面讲解了如何使用 Neo4j 构建一个简单的农业知识图谱。 最后，强调一句，知识图谱未来会通过自然语言处理技术和搜索技术结合应用会越来越广，工业界所出的地位也会越来越重要。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于情感词典的文本情感分析（12）]]></title>
    <url>%2F2019%2F11%2F22%2F12.%E5%9F%BA%E4%BA%8E%E6%83%85%E6%84%9F%E8%AF%8D%E5%85%B8%E7%9A%84%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[目前情感分析在中文自然语言处理中比较火热，很多场景下，我们都需要用到情感分析。比如，做金融产品量化交易，需要根据爬取的舆论数据来分析政策和舆论对股市或者基金期货的态度；电商交易，根据买家的评论数据，来分析商品的预售率等等。 下面我们通过以下几点来介绍中文自然语言处理情感分析： 中文情感分析方法简介； SnowNLP 快速进行评论数据情感分析； 基于标注好的情感词典来计算情感值； pytreebank 绘制情感树； 股吧数据情感分类。 中文情感分析方法简介情感倾向可认为是主体对某一客体主观存在的内心喜恶，内在评价的一种倾向。它由两个方面来衡量：一个情感倾向方向，一个是情感倾向度。 目前，情感倾向分析的方法主要分为两类：一种是基于情感词典的方法；一种是基于机器学习的方法，如基于大规模语料库的机器学习。前者需要用到标注好的情感词典；后者则需要大量的人工标注的语料作为训练集，通过提取文本特征，构建分类器来实现情感的分类。 文本情感分析的分析粒度可以是词语、句子、段落或篇章。 段落篇章级情感分析主要是针对某个主题或事件进行情感倾向判断，一般需要构建对应事件的情感词典，如电影评论的分析，需要构建电影行业自己的情感词典，这样效果会比通用情感词典更好；也可以通过人工标注大量电影评论来构建分类器。句子级的情感分析大多通过计算句子里包含的所有情感词的值来得到。 篇章级的情感分析，也可以通过聚合篇章中所有的句子的情感倾向来计算得出。因此，针对句子级的情感倾向分析，既能解决短文本的情感分析，同时也是篇章级文本情感分析的基础。 中文情感分析的一些难点，比如句子是由词语根据一定的语言规则构成的，应该把句子中词语的依存关系纳入到句子情感的计算过程中去，不同的依存关系，进行情感倾向计算是不一样的。文档的情感，根据句子对文档的重要程度赋予不同权重，调整其对文档情感的贡献程度等。 SnowNLP 快速进行评论数据情感分析如果有人问，有没有比较快速简单的方法能判断一句话的情感倾向，那么 SnowNLP 库就是答案。 SnowNLP 主要可以进行中文分词、词性标注、情感分析、文本分类、转换拼音、繁体转简体、提取文本关键词、提取摘要、分割句子、文本相似等。 需要注意的是，用 SnowNLP进行情感分析，官网指出进行电商评论的准确率较高，其实是因为它的语料库主要是电商评论数据，但是可以自己构建相关领域语料库，替换单一的电商评论语料，准确率也挺不错的。 1. SnowNLP 安装。 （1） 使用 pip 安装： pip install snownlp==0.11.1 （2）使用 Github 源码安装。 首先，下载 SnowNLP 的 Github源码并解压，在解压目录，通过下面命令安装： python setup.py install 以上方式，二选一安装完成之后，就可以引入 SnowNLP 库使用了。 from snownlp import SnowNLP 2. 评论语料获取情感值。 首先，SnowNLP 对情感的测试值为0到1，值越大，说明情感倾向越积极。下面我们通过 SnowNLP 测试在京东上找的好评、中评、差评的结果。 首先，引入 SnowNLP 库： from snownlp import SnowNLP （1） 测试一条京东的好评数据： SnowNLP(u&#39;本本已收到，体验还是很好，功能方面我不了解，只看外观还是很不错很薄，很轻，也有质感。&#39;).sentiments 得到的情感值很高，说明买家对商品比较认可，情感值为： 0.999950702449061 （2）测试一条京东的中评数据： SnowNLP(u&#39;屏幕分辨率一般，送了个极丑的鼠标。&#39;).sentiments 得到的情感值一般，说明买家对商品看法一般，甚至不喜欢，情感值为： 0.03251402883400323 （3）测试一条京东的差评数据： SnowNLP(u&#39;很差的一次购物体验，细节做得极差了，还有发热有点严重啊，散热不行，用起来就是烫得厉害，很垃圾！！！&#39;).sentiments 得到的情感值一般，说明买家对商品不认可，存在退货嫌疑，情感值为： 0.0036849517156107847 以上就完成了简单快速的情感值计算，对评论数据是不是很好用呀！！！ 使用 SnowNLP 来计算情感值，官方推荐的是电商评论数据计算准确度比较高，难道非评论数据就不能使用 SnowNLP 来计算情感值了吗？当然不是，虽然SnowNLP 默认提供的模型是用评论数据训练的，但是它还支持我们根据现有数据训练自己的模型。 首先我们来看看自定义训练模型的 源码 Sentiment 类 ，代码定义如下： class Sentiment(object): def __init__(self): self.classifier = Bayes() def save(self, fname, iszip=True): self.classifier.save(fname, iszip) def load(self, fname=data_path, iszip=True): self.classifier.load(fname, iszip) def handle(self, doc): words = seg.seg(doc) words = normal.filter_stop(words) return words def train(self, neg_docs, pos_docs): data = [] for sent in neg_docs: data.append([self.handle(sent), &#39;neg&#39;]) for sent in pos_docs: data.append([self.handle(sent), &#39;pos&#39;]) self.classifier.train(data) def classify(self, sent): ret, prob = self.classifier.classify(self.handle(sent)) if ret == &#39;pos&#39;: return prob return 1-prob 通过源代码，我们可以看到，可以使用 train方法训练数据，并使用 save 方法和 load 方法保存与加载模型。下面训练自己的模型，训练集pos.txt 和 neg.txt 分别表示积极和消极情感语句，两个 TXT 文本中每行表示一句语料。 下面代码进行自定义模型训练和保存： from snownlp import sentiment sentiment.train(&#39;neg.txt&#39;, &#39;pos.txt&#39;) sentiment.save(&#39;sentiment.marshal&#39;) 基于标注好的情感词典来计算情感值这里我们使用一个行业标准的情感词典——玻森情感词典，来自定义计算一句话、或者一段文字的情感值。 整个过程如下： 加载玻森情感词典； jieba 分词； 获取句子得分。 首先引入包： import pandas as pd import jieba 接下来加载情感词典： df = pd.read_table(&quot;bosonnlp//BosonNLP_sentiment_score.txt&quot;,sep= &quot; &quot;,names=[&#39;key&#39;,&#39;score&#39;]) 查看一下情感词典前5行： 将词 key 和对应得分 score 转成2个 list 列表，目的是找到词 key 的时候，能对应获取到 score 值： key = df[&#39;key&#39;].values.tolist() score = df[&#39;score&#39;].values.tolist() 定义分词和统计得分函数： def getscore(line): segs = jieba.lcut(line) #分词 score_list = [score[key.index(x)] for x in segs if(x in key)] return sum(score_list) #计算得分 最后来进行结果测试： line = &quot;今天天气很好，我很开心&quot; print(round(getscore(line),2)) line = &quot;今天下雨，心情也受到影响。&quot; print(round(getscore(line),2)) 获得的情感得分保留2位小数： 5.26 -0.96 pytreebank 绘制情感树1. 安装 pytreebank。 在 Github 上下载 pytreebank源码，解压之后，进入解压目录命令行，执行命令： python setup.py install 最后通过引入命令，判断是否安装成功： import pytreebank 提示，如果在 Windows 下安装之后，报错误： UnicodeDecodeError: &#39;gbk&#39; codec can&#39;t decode byte 0x92 in position 24783: illegal multibyte sequence 这是由于编码问题引起的，可以在安装目录下报错的文件中报错的代码地方加个 encoding=&#39;utf-8&#39; 编码： import_tag( &quot;script&quot;, contents=format_replacements(open(scriptname,encoding=&#39;utf-8&#39;).read(), replacements), type=&quot;text/javascript&quot; ) 2. 绘制情感树。首先引入 pytreebank 包： import pytreebank 然后，加载用来可视化的 JavaScript 和 CSS 脚本： pytreebank.LabeledTree.inject_visualization_javascript() 绘制情感树，把句子首先进行组合再绘制图形： line = &#39;(4 (0 你) (3 (2 是) (3 (3 (3 谁) (2 的)) (2 谁))))&#39; pytreebank.create_tree_from_string(line).display() 得到的情感树如下： 股吧数据情感分类但在7月15日之前，随着中美贸易战不断升级，中兴股价又上演了一场“跌跌不休”的惨状，我以中美贸易战背景下中兴通讯在股吧解禁前一段时间的评论数据，来进行情感数据人工打标签和分类。其中，把消极、中性 、积极分别用0、1、2来表示。 整个文本分类流程主要包括以下6个步骤： 中文语料； 分词； 复杂规则； 特征向量； 算法建模； 情感分析。 本次分类算法采用 CNN，首先引入需要的包： import pandas as pd import numpy as np import jieba import random import keras from keras.preprocessing import sequence from keras.models import Sequential from keras.layers import Dense, Dropout, Activation from keras.layers import Embedding from keras.layers import Conv1D, GlobalMaxPooling1D from keras.datasets import imdb from keras.models import model_from_json from keras.utils import np_utils import matplotlib.pyplot as plt 继续引入停用词和语料文件： dir = &quot;D://ProgramData//PythonWorkSpace//chat//chat8//&quot; stopwords=pd.read_csv(dir +&quot;stopwords.txt&quot;,index_col=False,quoting=3,sep=&quot;\t&quot;,names=[&#39;stopword&#39;], encoding=&#39;utf-8&#39;) stopwords=stopwords[&#39;stopword&#39;].values df_data1 = pd.read_csv(dir+&quot;data1.csv&quot;,encoding=&#39;utf-8&#39;) df_data1.head() 下图展示数据的前5行：接着进行数据预处理，把消极、中性、积极分别为0、1、2的预料分别拿出来： #把内容有缺失值的删除 df_data1.dropna(inplace=True) #抽取文本数据和标签 data_1 = df_data1.loc[:,[&#39;content&#39;,&#39;label&#39;]] #把消极 中性 积极分别为0、1、2的预料分别拿出来 data_label_0 = data_1.loc[data_1[&#39;label&#39;] ==0,:] data_label_1 = data_1.loc[data_1[&#39;label&#39;] ==1,:] data_label_2 = data_1.loc[data_1[&#39;label&#39;] ==2,:] 接下来，定义中文分词函数： #定义分词函数 def preprocess_text(content_lines, sentences, category): for line in content_lines: try: segs=jieba.lcut(line) segs = filter(lambda x:len(x)&gt;1, segs) segs = [v for v in segs if not str(v).isdigit()]#去数字 segs = list(filter(lambda x:x.strip(), segs)) #去左右空格 segs = filter(lambda x:x not in stopwords, segs) temp = &quot; &quot;.join(segs) if(len(temp)&gt;1): sentences.append((temp, category)) except Exception: print(line) continue 生成训练的分词数据，并进行打散，使其分布均匀： #获取数据 data_label_0_content = data_label_0[&#39;content&#39;].values.tolist() data_label_1_content = data_label_1[&#39;content&#39;].values.tolist() data_label_2_content = data_label_2[&#39;content&#39;].values.tolist() #生成训练数据 sentences = [] preprocess_text(data_label_0_content, sentences, 0) preprocess_text(data_label_1_content, sentences, 1) preprocess_text(data_label_2_content, sentences,2) #我们打乱一下顺序，生成更可靠的训练集 random.shuffle(sentences) 对数据集进行切分，按照训练集合测试集7:3的比例： #所以把原数据集分成训练集的测试集，咱们用sklearn自带的分割函数。 from sklearn.model_selection import train_test_split x, y = zip(*sentences) x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3,random_state=1234) 然后，对特征构造词向量： #抽取特征，我们对文本抽取词袋模型特征 from sklearn.feature_extraction.text import CountVectorizer vec = CountVectorizer( analyzer=&#39;word&#39;, #tokenise by character ngrams max_features=4000, #keep the most common 1000 ngrams ) vec.fit(x_train) 定义模型参数： # 设置参数 max_features = 5001 maxlen = 100 batch_size = 32 embedding_dims = 50 filters = 250 kernel_size = 3 hidden_dims = 250 epochs = 10 nclasses = 3 输入特征转成 Array 和标签处理，打印训练集和测试集的 shape： x_train = vec.transform(x_train) x_test = vec.transform(x_test) x_train = x_train.toarray() x_test = x_test.toarray() y_train = np_utils.to_categorical(y_train,nclasses) y_test = np_utils.to_categorical(y_test,nclasses) x_train = sequence.pad_sequences(x_train, maxlen=maxlen) x_test = sequence.pad_sequences(x_test, maxlen=maxlen) print(&#39;x_train shape:&#39;, x_train.shape) print(&#39;x_test shape:&#39;, x_test.shape) 定义一个绘制 Loss 曲线的类： class LossHistory(keras.callbacks.Callback): def on_train_begin(self, logs={}): self.losses = {&#39;batch&#39;:[], &#39;epoch&#39;:[]} self.accuracy = {&#39;batch&#39;:[], &#39;epoch&#39;:[]} self.val_loss = {&#39;batch&#39;:[], &#39;epoch&#39;:[]} self.val_acc = {&#39;batch&#39;:[], &#39;epoch&#39;:[]} def on_batch_end(self, batch, logs={}): self.losses[&#39;batch&#39;].append(logs.get(&#39;loss&#39;)) self.accuracy[&#39;batch&#39;].append(logs.get(&#39;acc&#39;)) self.val_loss[&#39;batch&#39;].append(logs.get(&#39;val_loss&#39;)) self.val_acc[&#39;batch&#39;].append(logs.get(&#39;val_acc&#39;)) def on_epoch_end(self, batch, logs={}): self.losses[&#39;epoch&#39;].append(logs.get(&#39;loss&#39;)) self.accuracy[&#39;epoch&#39;].append(logs.get(&#39;acc&#39;)) self.val_loss[&#39;epoch&#39;].append(logs.get(&#39;val_loss&#39;)) self.val_acc[&#39;epoch&#39;].append(logs.get(&#39;val_acc&#39;)) def loss_plot(self, loss_type): iters = range(len(self.losses[loss_type])) plt.figure() # acc plt.plot(iters, self.accuracy[loss_type], &#39;r&#39;, label=&#39;train acc&#39;) # loss plt.plot(iters, self.losses[loss_type], &#39;g&#39;, label=&#39;train loss&#39;) if loss_type == &#39;epoch&#39;: # val_acc plt.plot(iters, self.val_acc[loss_type], &#39;b&#39;, label=&#39;val acc&#39;) # val_loss plt.plot(iters, self.val_loss[loss_type], &#39;k&#39;, label=&#39;val loss&#39;) plt.grid(True) plt.xlabel(loss_type) plt.ylabel(&#39;acc-loss&#39;) plt.legend(loc=&quot;upper right&quot;) plt.show() 然后，初始化上面类的对象，并作为模型的回调函数输入，训练模型： history = LossHistory() print(&#39;Build model...&#39;) model = Sequential() model.add(Embedding(max_features, embedding_dims, input_length=maxlen)) model.add(Dropout(0.5)) model.add(Conv1D(filters, kernel_size, padding=&#39;valid&#39;, activation=&#39;relu&#39;, strides=1)) model.add(GlobalMaxPooling1D()) model.add(Dense(hidden_dims)) model.add(Dropout(0.5)) model.add(Activation(&#39;relu&#39;)) model.add(Dense(nclasses)) model.add(Activation(&#39;softmax&#39;)) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test),callbacks=[history]) 得到的模型迭代次数为10轮的训练过程： 最后绘制 Loss 图像： 关于本次分类，这里重点讨论的一个知识点就是数据分布不均匀的情况，我们都知道，本次贸易战中兴公司受影响很大，导致整个股票价格处于下跌趋势，所以整个舆论上，大多数评论都是消极的态度，导致数据分布极不均匀。 那数据分布不均匀一般怎么处理呢？从以下几个方面考虑： 数据采样，包括上采样、下采样和综合采样； 改变分类算法，在传统分类算法的基础上对不同类别采取不同的加权方式，使得模型更看重少数类； 采用合理的性能评价指标； 代价敏感。 总结，本文通过第三方、基于词典等方式计算中文文本情感值，以及通过情感树来进行可视化，然而这些内容只是情感分析的入门知识，情感分析还涉及句法依存等，最后通过一个CNN 分类模型，提供一种有监督的情感分类思路。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>cnn</tag>
        <tag>snownlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nlp基础（1）]]></title>
    <url>%2F2019%2F11%2F22%2F1.nlp%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[目前，随着大数据、云计算对关系型数据处理技术趋向稳定成熟，各大互联网公司对关系数据的整合也已经落地成熟，笔者预测未来数据领域的挑战将主要集中在半结构化和非结构化数据的整合，NLP 技术对个人发展越来越重要，尤其在中文文本上挑战更大。 技术知识点 获取语料 语料是构成语料库的基本单元。所以，人们简单地用文本作为替代，并把文本中的上下文关系作为现实世界中语言的上下文关系的替代品。我们把一个文本集合称为语料库（Corpus），当有几个这样的文本集合的时候，我们称之为语料库集合(Corpora) 已有语料很多业务部门、公司等组织随着业务发展都会积累有大量的纸质或者电子文本资料。那么，对于这些资料，在允许的条件下我们稍加整合，把纸质的文本全部电子化就可以作为我们的语料库。 网络抓取获取国内外标准开放数据集，比如国内的中文汉语有搜狗语料、人民日报语料。国外的因为大都是英文或者外文，这里暂时用不到。也可以选择通过爬虫自己去抓取一些数据，然后来进行后续内容。 语料预处理在一个完整的中文自然语言处理工程应用中，语料预处理大概会占到整个50%-70%的工作量，所以开发人员大部分时间就在进行语料预处理。下面通过数据洗清、分词、词性标注、去停用词四个大的方面来完成语料的预处理工作。 语料清洗数据清洗，顾名思义就是在语料中找到我们感兴趣的东西，把不感兴趣的、视为噪音的内容清洗删除，包括对于原始文本提取标题、摘要、正文等信息，对于爬取的网页内容，去除广告、标签、HTML、JS 等代码和注释等。常见的数据清洗方式有：人工去重、对齐、删除和标注等，或者规则提取内容、正则表达式匹配、根据词性和命名实体提取、编写脚本或者代码批处理等。 分词中文语料数据为一批短文本或者长文本，比如：句子，文章摘要，段落或者整篇文章组成的一个集合。一般句子、段落之间的字、词语是连续的，有一定含义。而进行文本挖掘分析时，我们希望文本处理的最小单位粒度是词或者词语，所以这个时候就需要分词来将文本全部进行分词。 常见的分词算法有：基于字符串匹配的分词方法、基于理解的分词方法、基于统计的分词方法和基于规则的分词方法。 词性标注词性标注，就是给每个词或者词语打词类标签，如形容词、动词、名词等。这样做可以让文本在后面的处理中融入更多有用的语言信息。词性标注是一个经典的序列标注问题，不过对于有些中文自然语言处理来说，词性标注不是非必需的。比如，常见的文本分类就不用关心词性问题，但是类似情感分析、知识推理却是需要的，下图是常见的中文词性整理。 常见的词性标注方法可以分为基于规则和基于统计的方法。其中基于统计的方法，如基于最大熵的词性标注、基于统计最大概率输出词性和基于 HMM 的词性标注。 去停用词停用词一般指对文本特征没有任何贡献作用的字词，比如标点符号、语气、人称等一些词。所以在一般性的文本处理中，分词之后，接下来一步就是去停用词。但是对于中文来说，去停用词操作不是一成不变的，停用词词典是根据具体场景来决定的，比如在情感分析中，语气词、感叹号是应该保留的，因为他们对表示语气程度、感情色彩有一定的贡献和意义。 特征工程做完语料预处理之后，接下来需要考虑如何把分词之后的字和词语表示成计算机能够计算的类型。显然，如果要计算我们至少需要把中文分词的字符串转换成数字，确切的说应该是数学中的向量。有两种常用的表示模型分别是词袋模型和词向量。 词袋模型（Bag of Word, BOW)，即不考虑词语原本在句子中的顺序，直接将每一个词语或者符号统一放置在一个集合（如 list），然后按照计数的方式对出现的次数进行统计。统计词频这只是最基本的方式，TF-IDF 是词袋模型的一个经典用法。 词向量是将字、词语转换成向量矩阵的计算模型。目前为止最常用的词表示方法是 One-hot，这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。还有 Google 团队的 Word2Vec，其主要包含两个模型：跳字模型（Skip-Gram）和连续词袋模型（Continuous Bag of Words，简称 CBOW），以及两种高效训练的方法：负采样（Negative Sampling）和层序 Softmax（Hierarchical Softmax）。值得一提的是，Word2Vec 词向量可以较好地表达不同词之间的相似和类比关系。除此之外，还有一些词向量的表示方式，如 Doc2Vec、WordRank 和 FastText 等。 特征选择同数据挖掘一样，在文本挖掘相关问题中，特征工程也是必不可少的。在一个实际问题中，构造好的特征向量，是要选择合适的、表达能力强的特征。文本特征一般都是词语，具有语义信息，使用特征选择能够找出一个特征子集，其仍然可以保留语义信息；但通过特征提取找到的特征子空间，将会丢失部分语义信息。所以特征选择是一个很有挑战的过程，更多的依赖于经验和专业知识，并且有很多现成的算法来进行特征的选择。目前，常见的特征选择方法主要有 DF、 MI、 IG、 CHI、WLLR、WFO 六种。 模型训练在特征向量选择好之后，接下来要做的事情当然就是训练模型，对于不同的应用需求，我们使用不同的模型，传统的有监督和无监督等机器学习模型， 如 KNN、SVM、Naive Bayes、决策树、GBDT、K-means 等模型；深度学习模型比如 CNN、RNN、LSTM、 Seq2Seq、FastText、TextCNN 等。这些模型在后续的分类、聚类、神经序列、情感分析等示例中都会用到，这里不再赘述。下面是在模型训练时需要注意的几个点。 注意过拟合、欠拟合问题，不断提高模型的泛化能力。 过拟合：模型学习能力太强，以至于把噪声数据的特征也学习到了，导致模型泛化能力下降，在训练集上表现很好，但是在测试集上表现很差。 常见的解决方法有： 增大数据的训练量； 增加正则化项，如 L1 正则和 L2 正则； 特征选取不合理，人工筛选特征和使用特征选择算法； 采用 Dropout 方法等。 欠拟合：就是模型不能够很好地拟合数据，表现在模型过于简单。 常见的解决方法有： 添加其他特征项； 增加模型复杂度，比如神经网络加更多的层、线性模型通过添加多项式使模型泛化能力更强； 减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。 对于神经网络，注意梯度消失和梯度爆炸问题。 评价指标训练好的模型，上线之前要对模型进行必要的评估，目的让模型对语料具备较好的泛化能力。具体有以下这些指标可以参考。 错误率、精度、准确率、精确度、召回率、F1 衡量。 错误率：是分类错误的样本数占样本总数的比例。对样例集 D，分类错误率计算公式如下： 精度：是分类正确的样本数占样本总数的比例。这里的分类正确的样本数指的不仅是正例分类正确的个数还有反例分类正确的个数。对样例集 D，精度计算公式如下： 对于二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为真正例（True Positive）、假正例（False Positive）、真反例（True Negative)、假反例（False Negative）四种情形，令 TP、FP、TN、FN 分别表示其对应的样例数，则显然有 TP+FP++TN+FN=样例总数。分类结果的“混淆矩阵”（Confusion Matrix）如下： 准确率，缩写表示用 P。准确率是针对我们预测结果而言的，它表示的是预测为正的样例中有多少是真正的正样例。定义公式如下： 精确度，缩写表示用 A。精确度则是分类正确的样本数占样本总数的比例。Accuracy 反应了分类器对整个样本的判定能力（即能将正的判定为正的，负的判定为负的）。定义公式如下： 召回率，缩写表示用 R。召回率是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确。定义公式如下： F1 衡量，表达出对查准率/查全率的不同偏好。定义公式如下： ROC 曲线、AUC 曲线。 ROC 全称是“受试者工作特征”（Receiver Operating Characteristic）曲线。我们根据模型的预测结果，把阈值从0变到最大，即刚开始是把每个样本作为正例进行预测，随着阈值的增大，学习器预测正样例数越来越少，直到最后没有一个样本是正样例。在这一过程中，每次计算出两个重要量的值，分别以它们为横、纵坐标作图，就得到了 ROC 曲线。 ROC 曲线的纵轴是“真正例率”（True Positive Rate, 简称 TPR)，横轴是“假正例率”（False Positive Rate,简称FPR），两者分别定义为： ROC 曲线的意义有以下几点： ROC 曲线能很容易的查出任意阈值对模型的泛化性能影响；有助于选择最佳的阈值；可以对不同的模型比较性能，在同一坐标中，靠近左上角的 ROC 曲所代表的学习器准确性最高。如果两条 ROC 曲线没有相交，我们可以根据哪条曲线最靠近左上角哪条曲线代表的学习器性能就最好。但是实际任务中，情况很复杂，若两个模型的 ROC 曲线发生交叉，则难以一般性的断言两者孰优孰劣。此时如果一定要进行比较，则比较合理的判断依据是比较 ROC 曲线下的面积，即AUC（Area Under ROC Curve）。 AUC 就是 ROC 曲线下的面积，衡量学习器优劣的一种性能指标。AUC 是衡量二分类模型优劣的一种评价指标，表示预测的正例排在负例前面的概率。 前面我们所讲的都是针对二分类问题，那么如果实际需要在多分类问题中用 ROC 曲线的话，一般性的转化为多个“一对多”的问题。即把其中一个当作正例，其余当作负例来看待，画出多个 ROC 曲线。 模型上线应用模型线上应用，目前主流的应用方式就是提供服务或者将模型持久化。 第一就是线下训练模型，然后将模型做线上部署，发布成接口服务以供业务系统使用。 第二种就是在线训练，在线训练完成之后把模型 pickle 持久化，然后在线服务接口模板通过读取 pickle 而改变接口服务。 模型重构随着时间和变化，可能需要对模型做一定的重构，包括根据业务不同侧重点对上面提到的一至七步骤也进行调整，重新训练模型进行上线。 jieba安装pip install jieba git clone https://github.com/fxsjy/jieba.gitpython setup.py install 分词算法 基于统计词典，构造前缀词典，基于前缀词典对句子进行切分，得到所有切分可能，根据切分位置，构造一个有向无环图（DAG）； 基于DAG图，采用动态规划计算最大概率路径（最有可能的分词结果），根据最大概率路径分词； 对于新词(词库中没有的词），采用有汉字成词能力的 HMM 模型进行切分。 api参数 jieba.cut 方法接受三个输入参数: 需要分词的字符串；cut_all 参数用来控制是否采用全模式；HMM 参数用来控制是否使用 HMM 模型。 jieba.cut_for_search 方法接受两个参数：需要分词的字符串；是否使用 HMM 模型。该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细 精确分词精确模式试图将句子最精确地切开 import jieba content = &quot;现如今，机器学习和深度学习带动人工智能飞速的发展，并在图片处理、语音识别领域取得巨大成功。&quot; segs_1 = jieba.cut(content, cut_all=False) print(&quot;/&quot;.join(segs_1)) 输出：现如今/，/机器/学习/和/深度/学习/带动/人工智能/飞速/的/发展/，/并/在/图片/处理/、/语音/识别/领域/取得/巨大成功/。 全模式把句子中所有的可能是词语的都扫描出来，速度非常快，但不能解决歧义 segs_3 = jieba.cut(content, cut_all=True) print(&quot;/&quot;.join(segs_3)) 输出： 现如今/如今///机器/学习/和/深度/学习/带动/动人/人工/人工智能/智能/飞速/的/发展///并/在/图片/处理///语音/识别/领域/取得/巨大/巨大成功/大成/成功// 搜索引擎模式在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。 segs_4 = jieba.cut_for_search(content) print(&quot;/&quot;.join(segs_4)) 输出： 如今/现如今/，/机器/学习/和/深度/学习/带动/人工/智能/人工智能/飞速/的/发展/，/并/在/图片/处理/、/语音/识别/领域/取得/巨大/大成/成功/巨大成功/。 lcut生成 list segs_5 = jieba.lcut(content) print(segs_5) 输出： [‘现如今’, ‘，’, ‘机器’, ‘学习’, ‘和’, ‘深度’, ‘学习’, ‘带动’, ‘人工智能’, ‘飞速’, ‘的’, ‘发展’, ‘，’, ‘并’, ‘在’, ‘图片’, ‘处理’, ‘、’, ‘语音’, ‘识别’, ‘领域’, ‘取得’, ‘巨大成功’, ‘。’] 获取词性jieba.posseg 模块实现词性标注 import jieba.posseg as psg print([(x.word, x.flag) for x in psg.lcut(content)]) 输出：[(‘现如今’, ‘t’), (‘，’, ‘x’), (‘机器’, ‘n’), (‘学习’, ‘v’), (‘和’, ‘c’), (‘深度’, ‘ns’), (‘学习’, ‘v’), (‘带动’, ‘v’), (‘人工智能’, ‘n’), (‘飞速’, ‘n’), (‘的’, ‘uj’), (‘发展’, ‘vn’), (‘，’, ‘x’), (‘并’, ‘c’), (‘在’, ‘p’), (‘图片’, ‘n’), (‘处理’, ‘v’), (‘、’, ‘x’), (‘语音’, ‘n’), (‘识别’, ‘v’), (‘领域’, ‘n’), (‘取得’, ‘v’), (‘巨大成功’, ‘nr’), (‘。’, ‘x’)] 并行分词为文本按行分隔后，分配到多个 Python 进程并行分词，最后归并结果,默认分词器 jieba.dt 和 jieba.posseg.dt暂不支持 Windows。 jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数 。 jieba.disable_parallel() # 关闭并行分词模式 。 Counter获取分词结果中词列表的 top n from collections import Counter top5 = Counter(segs_5).most_common(5) print(top5) 输出：[(‘，’, 2), (‘学习’, 2), (‘现如今’, 1), (‘机器’, 1), (‘和’, 1)] 自定义添加词和字典txt = &quot;铁甲网是中国最大的工程机械交易平台。&quot; jieba.add_word(&quot;铁甲网&quot;) print(jieba.lcut(txt)) jieba.load_userdict(&#39;user_dict.txt&#39;) # 添加词典 print(jieba.lcut(txt)) 输出：[‘铁甲网’, ‘是’, ‘中国’, ‘最大’, ‘的’, ‘工程机械’, ‘交易平台’, ‘。’] hanlp安装 pip install pyhanlp 如报错building ‘_jpype’ extensionerror: Microsoft Visual C++ 14.0 is required，则conda install -c conda-forge jpype1;pip install pyhanlp 如ValueError: 配置错误: 数据包/pyhanlp/static\data 不存在，请修改配置文件中的root，则到https://github.com/hankcs/HanLP/releases下载data-for-1.7.2.zip下载后放入f:/anaconda3/envs/learn/lib/site-packages/pyhanlp/static\data 安装jdk环境 hanlp segment 交互分词模式 hanlp serve 内置http服务器 http://localhost:8765 分词from pyhanlp import * content = &quot;现如今，机器学习和深度学习带动人工智能飞速的发展，并在图片处理、语音识别领域取得巨大成功。&quot; print(HanLP.segment(content)) 输出：[现如今/t, ，/w, 机器学习/gi, 和/cc, 深度/n, 学习/v, 带动/v, 人工智能/n, 飞速/d, 的/ude1, 发展/vn, ，/w, 并/cc, 在/p, 图片/n, 处理/vn, 、/w, 语音/n, 识别/vn, 领域/n, 取得/v, 巨大/a, 成功/a, 。/w] 自定义词典分词txt = &quot;铁甲网是中国最大的工程机械交易平台。&quot; CustomDictionary.add(&quot;铁甲网&quot;) CustomDictionary.insert(&quot;工程机械&quot;, &quot;nz 1024&quot;) CustomDictionary.add(&quot;交易平台&quot;, &quot;nz 1024 n 1&quot;) print(HanLP.segment(txt)) 输出： [铁甲网/nz, 是/vshi, 中国/ns, 最大/gm, 的/ude1, 工程机械/nz, 交易平台/nz, 。/w] 以上两种工具可以做关键词提取、自动摘要、依存句法分析、情感分析等]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>jieba</tag>
        <tag>hanlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[词袋与词向量（5）]]></title>
    <url>%2F2019%2F11%2F22%2F5.%E8%AF%8D%E8%A2%8B%E4%B8%8E%E8%AF%8D%E5%90%91%E9%87%8F%2F</url>
    <content type="text"><![CDATA[词袋和词向量模型可以将文本数据如转换成计算机能够计算的数据。 词袋模型（Bag of Words Model）词袋模型把文本（段落或者文档）被看作是无序的词汇集合，忽略语法甚至是单词的顺序，把每一个单词都进行统计，同时计算每个单词出现的次数，常常被用在文本分类中，如贝叶斯算法、LDA 和 LSA 等。 实战词袋模型import jieba #定义停用词、标点符号 punctuation = [&quot;，&quot;,&quot;。&quot;, &quot;：&quot;, &quot;；&quot;, &quot;？&quot;] #定义语料 content = [&quot;机器学习带动人工智能飞速的发展。&quot;, &quot;深度学习带动人工智能飞速的发展。&quot;, &quot;机器学习和深度学习带动人工智能飞速的发展。&quot; ] #分词 segs_1 = [jieba.lcut(con) for con in content] print(segs_1) 输出： [[‘机器’, ‘学习’, ‘带动’, ‘人工智能’, ‘飞速’, ‘的’, ‘发展’, ‘。’], [‘深度’, ‘学习’, ‘带动’, ‘人工智能’, ‘飞速’, ‘的’, ‘发展’, ‘。’], [‘机器’, ‘学习’, ‘和’, ‘深度’, ‘学习’, ‘带动’, ‘人工智能’, ‘飞速’, ‘的’, ‘发展’, ‘。’]] # 去标点符号 tokenized = [] for sentence in segs_1: words = [] for word in sentence: if word not in punctuation: words.append(word) tokenized.append(words) print(tokenized) 输出：[[‘机器’, ‘学习’, ‘带动’, ‘人工智能’, ‘飞速’, ‘的’, ‘发展’], [‘深度’, ‘学习’, ‘带动’, ‘人工智能’, ‘飞速’, ‘的’, ‘发展’], [‘机器’, ‘学习’, ‘和’, ‘深度’, ‘学习’, ‘带动’, ‘人工智能’, ‘飞速’, ‘的’, ‘发展’]] # 把所有的分词结果放到一个袋子（List）里面，也就是取并集，再去重，获取对应的特征词。 #求并集 bag_of_words = [ x for item in segs_1 for x in item if x not in punctuation] #去重 bag_of_words = list(set(bag_of_words)) print(bag_of_words) 输出： [‘飞速’, ‘的’, ‘深度’, ‘人工智能’, ‘发展’, ‘和’, ‘机器’, ‘学习’, ‘带动’] # 以上面特征词的顺序，完成词袋化，得到词袋向量 bag_of_word2vec = [] for sentence in tokenized: tokens = [1 if token in sentence else 0 for token in bag_of_words ] bag_of_word2vec.append(tokens) 输出：[[1, 1, 0, 1, 1, 0, 1, 1, 1], [1, 1, 1, 1, 1, 0, 0, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]] Gensim 构建词袋模型from gensim import corpora import gensim #tokenized是去标点之后的 dictionary = corpora.Dictionary(tokenized) #保存词典 dictionary.save(&#39;deerwester.dict&#39;) print(dictionary) 输出： Dictionary(9 unique tokens: [‘人工智能’, ‘发展’, ‘学习’, ‘带动’, ‘机器’]…) #查看词典和下标 id 的映射 print(dictionary.token2id) 输出： {‘人工智能’: 0, ‘发展’: 1, ‘学习’: 2, ‘带动’: 3, ‘机器’: 4, ‘的’: 5, ‘飞速’: 6, ‘深度’: 7, ‘和’: 8} # doc2bow()，作用只是计算每个不同单词的出现次数，将单词转换为其整数单词 id 并将结果作为稀疏向量返回。 corpus = [dictionary.doc2bow(sentence) for sentence in segs_1] print(corpus ) 输出： [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(0, 1), (1, 1), (2, 1), (3, 1), (5, 1), (6, 1), (7, 1)], [(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]] 词向量 （Word Embedding）词向量技术是将词语转化成为稠密向量。在自然语言处理应用中，词向量作为机器学习、深度学习模型的特征进行输入。因此，最终模型的效果很大程度上取决于词向量的效果。在 Word2Vec 出现之前，自然语言处理经常把字词进行独热编码，也就是 One-Hot Encoder。 大数据 [0,0,0,0,0,0,0,1,0,……，0,0,0,0,0,0,0] 云计算[0,0,0,0,1,0,0,0,0,……，0,0,0,0,0,0,0] 机器学习[0,0,0,1,0,0,0,0,0,……，0,0,0,0,0,0,0] 人工智能[0,0,0,0,0,0,0,0,0,……，1,0,0,0,0,0,0] 比如上面的例子中，大数据 、云计算、机器学习和人工智能各对应一个向量，向量中只有一个值为1，其余都为0。所以使用 One-Hot Encoder有以下问题： 第一，词语编码是随机的，向量之间相互独立，看不出词语之间可能存在的关联关系。第二，向量维度的大小取决于语料库中词语的多少，如果语料包含的所有词语对应的向量合为一个矩阵的话，那这个矩阵过于稀疏，并且会造成维度灾难。而解决这个问题的手段，就是使用向量表示（Vector Representations）。比如 Word2Vec 可以将 One-Hot Encoder 转化为低维度的连续值，也就是稠密向量，并且其中意思相近的词也将被映射到向量空间中相近的位置。经过降维，在二维空间中，相似的单词在空间中的距离也很接近。 这里简单给词向量一个定义，词向量就是要用某个固定维度的向量去表示单词。也就是说要把单词变成固定维度的向量，作为机器学习（Machine Learning）或深度学习模型的特征向量输入。 词向量实战Word2VecWord2Vec 主要包含两种模型：Skip-Gram 和 CBOW，值得一提的是，Word2Vec 词向量可以较好地表达不同词之间的相似和类比关系。 pip install gensim from gensim.models import Word2Vec import jieba #定义停用词、标点符号 punctuation = [&quot;,&quot;,&quot;。&quot;, &quot;:&quot;, &quot;;&quot;, &quot;.&quot;, &quot;&#39;&quot;, &#39;&quot;&#39;, &quot;’&quot;, &quot;?&quot;, &quot;/&quot;, &quot;-&quot;, &quot;+&quot;, &quot;&amp;&quot;, &quot;(&quot;, &quot;)&quot;] sentences = [ &quot;长江是中国第一大河，干流全长6397公里（以沱沱河为源），一般称6300公里。流域总面积一百八十余万平方公里，年平均入海水量约九千六百余亿立方米。以干流长度和入海水量论，长江均居世界第三位。&quot;, &quot;黄河，中国古代也称河，发源于中华人民共和国青海省巴颜喀拉山脉，流经青海、四川、甘肃、宁夏、内蒙古、陕西、山西、河南、山东9个省区，最后于山东省东营垦利县注入渤海。干流河道全长5464千米，仅次于长江，为中国第二长河。黄河还是世界第五长河。&quot;, &quot;黄河,是中华民族的母亲河。作为中华文明的发祥地,维系炎黄子孙的血脉.是中华民族民族精神与民族情感的象征。&quot;, &quot;黄河被称为中华文明的母亲河。公元前2000多年华夏族在黄河领域的中原地区形成、繁衍。&quot;, &quot;在兰州的“黄河第一桥”内蒙古托克托县河口镇以上的黄河河段为黄河上游。&quot;, &quot;黄河上游根据河道特性的不同，又可分为河源段、峡谷段和冲积平原三部分。 &quot;, &quot;黄河,是中华民族的母亲河。&quot; ] # 定义好语料，接下来进行分词，去标点符号 sentences = [jieba.lcut(sen) for sen in sentences] tokenized = [] for sentence in sentences: words = [] for word in sentence: if word not in punctuation: words.append(word) tokenized.append(words) # 模型训练 model = Word2Vec(tokenized, sg=1, size=100, window=5, min_count=2, negative=1, sample=0.001, hs=1, workers=4) sg=1 是 skip-gram 算法，对低频词敏感；默认 sg=0 为 CBOW 算法。 size 是输出词向量的维数，值太小会导致词映射因为冲突而影响结果，值太大则会耗内存并使算法计算变慢，一般值取为100到200之间。 window 是句子中当前词与目标词之间的最大距离，3表示在目标词前看3-b 个词，后面看 b 个词（b 在0-3之间随机）。 min_count 是对词进行过滤，频率小于 min-count 的单词则会被忽视，默认值为5。 negative 和 sample 可根据训练结果进行微调，sample 表示更高频率的词被随机下采样到所设置的阈值，默认值为 1e-3。 hs=1 表示层级 softmax 将会被使用，默认 hs=0 且 negative 不为0，则负采样将会被选择使用。 # 训练后的模型可以保存与加载 model.save(&#39;model&#39;) #保存模型 model = Word2Vec.load(&#39;model&#39;) #加载模型 # 模型训练好之后，接下来就可以使用模型，可以用来计算句子或者词的相似性、最大匹配程度等。 # 计算相似度 print(model.similarity(&#39;黄河&#39;, &#39;长江&#39;)) # 预测最接近的词，预测与黄河和母亲河最接近，而与长江不接近的词 print(model.most_similar(positive=[&#39;黄河&#39;, &#39;母亲河&#39;], negative=[&#39;长江&#39;])) 输出： [(‘是’, 0.14632007479667664), (‘以’, 0.14630728960037231), (‘长河’, 0.13878652453422546), (‘河道’, 0.13716217875480652), (‘在’, 0.11577725410461426), (‘全长’, 0.10969121754169464), (‘内蒙古’, 0.07590540498495102), (‘入海’, 0.06970417499542236), (‘民族’, 0.06064444035291672), (‘中华文明’, 0.057667165994644165)] Word2Vec 是一种将词变成词向量的工具。通俗点说，只有这样文本预料才转化为计算机能够计算的矩阵向量。 Doc2Vec在 Gensim 库中，Doc2Vec 与 Word2Vec 都极为相似。但两者在对输入数据的预处理上稍有不同，Doc2vec 接收一个由 LabeledSentence 对象组成的迭代器作为其构造函数的输入参数。其中，LabeledSentence 是 Gensim 内建的一个类，它接收两个 List 作为其初始化的参数：word list 和 label list。 Doc2Vec 也包括两种实现方式：DBOW（Distributed Bag of Words）和 DM （Distributed Memory）。DBOW 和 DM 的实现，二者在 gensim 库中的实现用的是同一个方法，该方法中参数 dm = 0 或者 dm=1 决定调用 DBOW 还是 DM。Doc2Vec 将文档语料通过一个固定长度的向量表达。 下面是 Gensim 中 Doc2Vec 模型的实战，我们把上述语料每一句话当做一个文本，添加上对应的标签。接下来，定义数据预处理类，作用是给每个文章添加对应的标签： #定义数据预处理类，作用是给每个文章添加对应的标签 from gensim.models.doc2vec import Doc2Vec,LabeledSentence doc_labels = [&quot;长江&quot;,&quot;黄河&quot;,&quot;黄河&quot;,&quot;黄河&quot;,&quot;黄河&quot;,&quot;黄河&quot;,&quot;黄河&quot;] class LabeledLineSentence(object): def __init__(self, doc_list, labels_list): self.labels_list = labels_list self.doc_list = doc_list def __iter__(self): for idx, doc in enumerate(self.doc_list): yield LabeledSentence(words=doc,tags=[self.labels_list[idx]]) model = Doc2Vec(documents,dm=1, size=100, window=8, min_count=5, workers=4) model.save(&#39;model&#39;) model = Doc2Vec.load(&#39;model&#39;) # 定义好了数据预处理函数，我们将 Word2Vec 中分词去标点后的数据，进行转换 iter_data = LabeledLineSentence(tokenized, doc_labels) # 开始定义模型参数，这里 dm=1，采用了 Gensim 中的 DM 实现 model = Doc2Vec(dm=1, size=100, window=8, min_count=5, workers=4) model.build_vocab(iter_data) # 训练模型， 设置迭代次数1000次，start_alpha 为开始学习率，end_alpha 与 start_alpha 线性递减。 model.train(iter_data,total_examples=model.corpus_count,epochs=1000,start_alpha=0.01,end_alpha =0.001) # 进行一些预测 根据标签找最相似的，这里只有黄河和长江，所以结果为长江，并计算出了相似度 print(model.docvecs.most_similar(&#39;黄河&#39;)) 输出： [(‘长江’, 0.25543850660324097)] print(model.docvecs.similarity(&#39;黄河&#39;,&#39;长江&#39;)) 输出： 0.25543848271351405 最终影响模型准确率的因素有：文档的数量越多，文档的相似性越好，也就是基于大数据量的模型训练。在工业界，Word2Vec 和 Doc2Vec 常见的应用有：做相似词计算；相关词挖掘，在推荐系统中用在品牌、用户、商品挖掘中；上下文预测句子；机器翻译；作为特征输入其他模型等。 总结，本文只是简单的介绍了词袋和词向量模型的典型应用，对于两者的理论和其他词向量模型，比如 TextRank 、FastText 和 GloVe 等。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>doc2vec</tag>
        <tag>word2vec</tag>
        <tag>gensim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于CNN的电影推荐系统（10）]]></title>
    <url>%2F2019%2F11%2F22%2F10.%E5%9F%BA%E4%BA%8ECNN%E7%9A%84%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[本文从深度学习卷积神经网络入手，基于 Github 的开源项目来完成 MovieLens 数据集的电影推荐系统。 什么是推荐系统呢？什么是推荐系统呢？首先我们来看看几个常见的推荐场景。 如果你经常通过豆瓣电影评分来找电影，你会发现下图所示的推荐： 如果你喜欢购物，根据你的选择和购物行为，平台会给你推荐相似商品： 在互联网的很多场景下都可以看到推荐的影子。因为推荐可以帮助用户和商家满足不同的需求： 对用户而言：找到感兴趣的东西，帮助发现新鲜、有趣的事物。 对商家而言：提供个性化服务，提高信任度和粘性，增加营收。 常见的推荐系统主要包含两个方面的内容，基于用户的推荐系统（UserCF）和基于物品的推荐系统（ItemCF）。两者的区别在于，UserCF给用户推荐那些和他有共同兴趣爱好的用户喜欢的商品，而 ItemCF 给用户推荐那些和他之前喜欢的商品类似的商品。这两种方式都会遭遇冷启动问题。 下面是 UserCF 和 ItemCF 的对比： CNN 是如何应用在文本处理上的？提到卷积神经网络（CNN），相信大部分人首先想到的是图像分类，比如 MNIST 手写体识别，CAFRI10 图像分类。CNN已经在图像识别方面取得了较大的成果，随着近几年的不断发展，在文本处理领域，基于文本挖掘的文本卷积神经网络被证明是有效的。 首先，来看看 CNN 是如何应用到 NLP 中的，下面是一个简单的过程图： 和图像像素处理不一样，自然语言通常是一段文字，那么在特征矩阵中，矩阵的每一个行向量（比如 word2vec 或者 doc2vec）代表一个Token，包括词或者字符。如果一段文字包含有 n 个词，每个词有 m 维的词向量，那么我们可以构造出一个 n*m 的词向量矩阵，在 NLP处理过程中，让过滤器宽度和矩阵宽度保持一致整行滑动。 动手实战基于 CNN 的电影推荐系统将 CNN 的技术应用到自然语言处理中并与电影推荐相结合，来训练一个基于文本的卷积神经网络，实现电影个性化推荐系统。 首先感谢作者 chengstone 的分享，源码请访问下面网址： Github 在验证了 CNN 应用在自然语言处理上是有效的之后，从推荐系统的个性化推荐入手，在文本上，把 CNN成果应用到电影的个性化推荐上。并在特征工程中，对训练集和测试集做了相应的特征处理，其中有部分字段是类型性变量，特征工程上可以采用 one-hot编码，但是对于 UserID、MovieID 这样非常稀疏的变量，如果使用 one-hot，那么数据的维度会急剧膨胀，对于这份数据集来说是不合适的。 具体算法设计如下： 1. 定义用户嵌入矩阵。 用户的特征矩阵主要是通过用户信息嵌入网络来生成的，在预处理数据的时候，我们将UserID、MovieID、性别、年龄、职业特征全部转成了数字类型，然后把这个数字当作嵌入矩阵的索引，在网络的第一层就使用嵌入层，这样数据输入的维度保持在（N，32）和（N，16）。然后进行全连接层，转成（N，128）的大小，再进行全连接层，转成（N，200）的大小，这样最后输出的用户特征维度相对比较高，也保证了能把每个用户所带有的特征充分携带并通过特征表达。 具体流程如下： 2. 生成用户特征。 生成用户特征是在用户嵌入矩阵网络输出结果的基础上，通过2层全连接层实现的。第一个全连接层把特征矩阵转成（N，128）的大小，再进行第二次全连接层，转成（N，200）的大小，这样最后输出的用户特征维度相对比较高，也保证了能把每个用户所带有的特征充分携带并通过特征表达。 具体流程如下： 3. 定义电影 ID 嵌入矩阵。 通过电影 ID 和电影类型分别生成电影 ID 和电影类型特征，电影类型的多个嵌入向量做加和输出。电影 ID的实现过程和上面一样，但是对于电影类型的处理相较于上面，稍微复杂一点。因为电影类型有重叠性，一个电影可以属于多个类别，当把电影类型从嵌入矩阵索引出来之后是一个（N，32）形状的矩阵，因为有多个类别，这里采用的处理方式是矩阵求和，把类别加上去，变成（1，32）形状，这样使得电影的类别信息不会丢失。 具体流程如下： 4. 文本卷积神经网络设计。 文本卷积神经网络和单纯的 CNN网络结构有点不同，因为自然语言通常是一段文字与图片像素组成的矩阵是不一样的。在电影文本特征矩阵中，矩阵的每一个行构成的行向量代表一个Token，包括词或者字符。如果一段文字有 n 个词，每个词有 m 维的词向量，那么我们可以构造出一个 n*m 的矩阵。而且 NLP处理过程中，会有多个不同大小的过滤器串行执行，且过滤器宽度和矩阵宽度保持一致，是整行滑动。在执行完卷积操作之后采用了 ReLU激活函数，然后采用最大池化操作，最后通过全连接并 Dropout 操作和 Softmax输出。这里电影名称的处理比较特殊，并没有采用循环神经网络，而采用的是文本在 CNN 网络上的应用。 对于电影数据集，我们对电影名称做 CNN处理，其大致流程，从嵌入矩阵中得到电影名对应的各个单词的嵌入向量，由于电影名称比较特殊一点，名称长度有一定限制，这里过滤器大小使用时，就选择2、3、4、5长度。然后对文本嵌入层使用滑动2、3、4、5个单词尺寸的卷积核做卷积和最大池化，然后Dropout 操作，全连接层输出。 具体流程如下： 具体过程描述： （1）首先输入一个 32*32 的矩阵； （2）第一次卷积核大小为 2*2，得到 31*31 的矩阵，然后通过 [1,14,1,1] 的 max-pooling 操作，得到的矩阵为18*31； （3）第二次卷积核大小为 3*3，得到 16*29的矩阵，然后通过[1,13,1,1] 的 max-pooling 操作，得到的矩阵为4*29； （4）第三次卷积核大小 4*4，得到 1*26 的矩阵，然后通过 [1,12,1,1] 的 max-pooling 操作，得到的矩阵为1*26； （5）第四次卷积核大小 5*5，得到 1*22 的矩阵，然后通过 [1,11,1,1] 的 max-pooling 操作，得到的矩阵为1*22； （6）最后通过 Dropout 和全连接层，len(window_sizes) * filter_num =32，得到 1*32的矩阵。 5. 电影各层做一个全连接层。 将上面几步生成的特征向量，通过2个全连接层连接在一起，第一个全连接层是电影 ID 特征和电影类型特征先全连接，之后再和 CNN生成的电影名称特征全连接，生成最后的特征集。 具体流程如下： 6. 完整的基于 CNN 的电影推荐流程。 把以上实现的模块组合成整个算法，将网络模型作为回归问题进行训练，得到训练好的用户特征矩阵和电影特征矩阵进行推荐。 基于 CNN 的电影推荐系统代码调参过程在训练过程中，我们需要对算法预先设置一些超参数，这里给出的最终的设置结果： # 设置迭代次数 num_epochs = 5 # 设置BatchSize大小 batch_size = 256 #设置dropout保留比例 dropout_keep = 0.5 # 设置学习率 learning_rate = 0.0001 # 设置每轮显示的batches大小 show_every_n_batches = 20 首先对数据集进行划分，按照 4:1 的比例划分为训练集和测试集，下面给出的是算法模型最终训练集合测试集使用的划分结果： #将数据集分成训练集和测试集，随机种子不固定 train_X,test_X, train_y, test_y = train_test_split(features, targets_values, test_size = 0.3, random_state = 0) 接下来是具体模型训练过程。训练过程，要不断调参，根据经验调参粒度可以选择从粗到细分阶段进行。 调参过程对比： （1）第一步，先固定，learning_rate=0.01 和 num_epochs=10，测试 batch_size=128 对迭代时间和Loss 的影响； （2）第二步，先固定，learning_rate=0.01 和 num_epochs=10，测试 batch_size=256 对迭代时间和Loss 的影响； （3）第三步，先固定，learning_rate=0.01 和 num_epochs=10，测试 batch_size=512 对迭代时间和Loss 的影响； （4）第四步，先固定，learning_rate=0.01 和 num_epochs=5，测试 batch_size=128 对迭代时间和Loss 的影响； （5）第五步，先固定，learning_rate=0.01 和 num_epochs=5，测试 batch_size=256 对迭代时间和Loss 的影响； （6）第六步，先固定，learning_rate=0.01 和 num_epochs=5，测试 batch_size=512 对迭代时间和Loss 的影响； （7）第七步，先固定，batch_size=256 和 num_epochs=5，测试 learning_rate=0.001 对 Loss的影响； （8）第八步，先固定，batch_size=256 和 num_epochs=5，测试 learning_rate=0.0005 对 Loss的影响； （9）第九步，先固定，batch_size=256 和 num_epochs=5，测试 learning_rate=0.0001 对 Loss的影响； （10）第十步，先固定，batch_size=256 和 num_epochs=5，测试 learning_rate=0.00005 对Loss 的影响。 得到的调参结果对比表如下： 通过上面（1）-（6）步调参比较，在 learning_rate、batch_size 相同的情况下，num_epochs对于训练时间影响较大；而在 learning_rate、num_epochs 相同情况下，batch_size 对 Loss的影响较大，batch_size 选择512，Loss 有抖动情况，权衡之下，最终确定后续调参固定采用batch_size=256、num_epochs=5 的超参数值，后续（7）-（10）步，随着 learning_rate 逐渐减小，发现Loss 是先逐渐减小，而在 learning_rate=0.00005 时反而增大，最终选择出学习率为 learning_rate=0.0001的超参数值。 基于 CNN 的电影推荐系统电影推荐在上面，完成模型训练验证之后，实际来进行推荐电影，这里使用生产的用户特征矩阵和电影特征矩阵做电影推荐，主要有三种方式的推荐。 1. 推荐同类型的电影。 思路是：计算当前看的电影特征向量与整个电影特征矩阵的余弦相似度，取相似度最大的 top_k 个，这里加了些随机选择在里面，保证每次的推荐稍稍有些不同。 def recommend_same_type_movie(movie_id_val, top_k = 20): loaded_graph = tf.Graph() # with tf.Session(graph=loaded_graph) as sess: # # Load saved model loader = tf.train.import_meta_graph(load_dir + &#39;.meta&#39;) loader.restore(sess, load_dir) norm_movie_matrics = tf.sqrt(tf.reduce_sum(tf.square(movie_matrics), 1, keep_dims=True)) normalized_movie_matrics = movie_matrics / norm_movie_matrics #推荐同类型的电影 probs_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200]) probs_similarity = tf.matmul(probs_embeddings, tf.transpose(normalized_movie_matrics)) sim = (probs_similarity.eval()) print(&quot;您看的电影是：{}&quot;.format(movies_orig[movieid2idx[movie_id_val]])) print(&quot;以下是给您的推荐：&quot;) p = np.squeeze(sim) p[np.argsort(p)[:-top_k]] = 0 p = p / np.sum(p) results = set() while len(results) != 5: c = np.random.choice(3883, 1, p=p)[0] results.add(c) for val in (results): print(val) print(movies_orig[val]) return result 2. 推荐您喜欢的电影。 思路是：使用用户特征向量与电影特征矩阵计算所有电影的评分，取评分最高的 top_k 个，同样加了些随机选择部分。 def recommend_your_favorite_movie(user_id_val, top_k = 10): loaded_graph = tf.Graph() # with tf.Session(graph=loaded_graph) as sess: # # Load saved model loader = tf.train.import_meta_graph(load_dir + &#39;.meta&#39;) loader.restore(sess, load_dir) #推荐您喜欢的电影 probs_embeddings = (users_matrics[user_id_val-1]).reshape([1, 200]) probs_similarity = tf.matmul(probs_embeddings, tf.transpose(movie_matrics)) sim = (probs_similarity.eval()) print(&quot;以下是给您的推荐：&quot;) p = np.squeeze(sim) p[np.argsort(p)[:-top_k]] = 0 p = p / np.sum(p) results = set() while len(results) != 5: c = np.random.choice(3883, 1, p=p)[0] results.add(c) for val in (results): print(val) print(movies_orig[val]) return results 3. 看过这个电影的人还看了（喜欢）哪些电影。 （1）首先选出喜欢某个电影的 top_k 个人，得到这几个人的用户特征向量； （2）然后计算这几个人对所有电影的评分 ； （3）选择每个人评分最高的电影作为推荐； （4）同样加入了随机选择。 def recommend_other_favorite_movie(movie_id_val, top_k = 20): loaded_graph = tf.Graph() # with tf.Session(graph=loaded_graph) as sess: # # Load saved model loader = tf.train.import_meta_graph(load_dir + &#39;.meta&#39;) loader.restore(sess, load_dir) probs_movie_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200]) probs_user_favorite_similarity = tf.matmul(probs_movie_embeddings, tf.transpose(users_matrics)) favorite_user_id = np.argsort(probs_user_favorite_similarity.eval())[0][-top_k:] print(&quot;您看的电影是：{}&quot;.format(movies_orig[movieid2idx[movie_id_val]])) print(&quot;喜欢看这个电影的人是：{}&quot;.format(users_orig[favorite_user_id-1])) probs_users_embeddings = (users_matrics[favorite_user_id-1]).reshape([-1, 200]) probs_similarity = tf.matmul(probs_users_embeddings, tf.transpose(movie_matrics)) sim = (probs_similarity.eval()) p = np.argmax(sim, 1) print(&quot;喜欢看这个电影的人还喜欢看：&quot;) results = set() while len(results) != 5: c = p[random.randrange(top_k)] results.add(c) for val in (results): print(val) print(movies_orig[val]) return results 基于 CNN 的电影推荐系统不足这里讨论一下基于上述方法所带来的不足： 由于一个新的用户在刚开始的时候并没有任何行为记录，所以系统会出现冷启动的问题； 由于神经网络是一个黑盒子过程，我们并不清楚在反向传播的过程中的具体细节，也不知道每一个卷积层抽取的特征细节，所以此算法缺乏一定的可解释性； 一般来说，在工业界，用户的数据量是海量的，而卷积神经网络又要耗费大量的计算资源，所以进行集群计算是非常重要的。但是由于本课程所做实验环境有限，还是在单机上运行，所以后期可以考虑在服务器集群上全量跑数据，这样获得的结果也更准确。 总结上面通过 Github 上一个开源的项目，梳理了CNN 在文本推荐上的应用，并通过模型训练调参，给出一般的模型调参思路，最后建议大家自己把源码下载下来跑跑模型，效果更好。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>cnn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动手实战中文句法依存分析（16）]]></title>
    <url>%2F2019%2F11%2F22%2F16.%E5%8A%A8%E6%89%8B%E5%AE%9E%E6%88%98%E4%B8%AD%E6%96%87%E5%8F%A5%E6%B3%95%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[句法分析被用在很多场景中，比如搜索引擎用户日志分析和关键词识别，比如信息抽取、自动问答、机器翻译等其他自然语言处理相关的任务。 语法体系句法分析需要遵循某一语法体系，根据该体系的语法确定语法树的表示形式，我们看下面这个句子： 西门子将努力参与中国的三峡工程建设。 用可视化的工具 Stanford Parser来看看句法分析的整个过程： 短语结构树由终节点、非终结点以及短语标记三部分组成。句子分裂的语法规则为若干终结点构成一个短语，作为非终结点参与下一次规约，直至结束。如下图： 句法分析技术依存句法分析依存句法依存句法（Dependency Parsing， DP）通过分析语言单位内成分之间的依存关系揭示其句法结构。 直观来讲，依存句法的目的在于分析识别句子中的“主谓宾”、“定状补”这些语法成分，并分析各成分之间的关系。 依存句法的结构没有非终结点，词与词之间直接发生依存关系，构成一个依存对，其中一个是核心词，也叫支配词，另一个叫修饰词，也叫从属词。 依存关系用一个有向弧表示，叫做依存弧。依存弧的方向为由从属词指向支配词，当然反过来也是可以的，按个人习惯统一表示即可。 例如，下面这个句子： 国务院总理李克强调研上海外高桥时提出，支持上海积极探索新机制。 依存句法的分析结果见下（利用哈工大 LTP）： 从分析结果中我们可以看到，句子的核心谓词为“提出”，主语是“李克强”，提出的宾语是“支持上海……”，“调研……时”是“提出”的（时间）状语，“李克强”的修饰语是“国务院总理”，“支持”的宾语是“探索新机制”。 有了上面的依存句法分析结果，我们就可以比较容易的看到，“提出者”是“李克强”，而不是“上海”或“外高桥”，即使它们都是名词，而且距离“提出”更近。 依存关系依存句法通过分析语言单位内成分之前的依存关系解释其句法结构，主张句子中核心动词是支配其他成分的中心成分。而它本身却不受其他任何成分的支配，所有受支配成分都以某种关系从属于支配者。 在20世纪70年代，Robinson 提出依存句法中关于依存关系的四条公理，在处理中文信息的研究中，中国学者提出了依存关系的第五条公理，分别如下： 一个句子中只有一个成分是独立的； 句子的其他成分都从属于某一成分； 任何一个成分都不能依存于两个或两个以上的成分； 如果成分 A 直接从属成分 B，而成分 C 在句子中位于 A 和 B 之间，那么，成分 C 或者从属于 A，或者从属于 B，或者从属于 A 和 B 之间的某一成分； 中心成分左右两边的其他成分相互不发生关系。 句子成分之间相互支配与被支配、依存与被依存的现象，普遍存在于汉语的词汇（合成语）、短语、单句、段落、篇章等能够独立运用和表达的语言之中，这一特点体现了依存关系的普遍性。依存句法分析可以反映出句子各成分之间的语义修饰关系，它可以获得长距离的搭配信息，并与句子成分的物理位置无关。 依存句法分析标注关系（共14种）及含义如下表所示： 语义依存分析语义依存分析（Semantic Dependency Parsing，SDP），分析句子各个语言单位之间的语义关联，并将语义关联以依存结构呈现。使用语义依存刻画句子语义，好处在于不需要去抽象词汇本身，而是通过词汇所承受的语义框架来描述该词汇，而论元的数目相对词汇来说数量总是少了很多。 语义依存分析目标是跨越句子表层句法结构的束缚，直接获取深层的语义信息。例如以下三个句子，用不同的表达方式表达了同一个语义信息，即张三实施了一个吃的动作，吃的动作是对苹果实施的。 语义依存分析不受句法结构的影响，将具有直接语义关联的语言单元直接连接依存弧并标记上相应的语义关系。这也是语义依存分析与依存句法分析的重要区别。 语义依存关系分为三类，分别是主要语义角色，每一种语义角色对应存在一个嵌套关系和反关系；事件关系，描述两个事件间的关系；语义依附标记，标记说话者语气等依附性信息。 Pyhanlp 实战依存句法最后，我们通过 Pyhanlp 库实现依存句法的实战练习。这个过程中，我们选用 Dependency Viewer 工具进行可视化展示。可视化时， txt文档需要采用 UTF-8 编码。 首先，引入包，然后可直接进行分析： from pyhanlp import * sentence = &quot;徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。&quot; print(HanLP.parseDependency(sentence)) 得到的结果： 然后，我们将结果保存在 txt 文件中： f = open(&quot;D://result.txt&quot;,&#39;a+&#39;) print((HanLP.parseDependency(sentence )),file = f) 最后，通过 Dependency Viewer 工具进行可视化，如果出现乱码，记得把 txt 文档保存为 UTF-8 式即可，得到的可视化结果如下图所示： 总结本文，首先为大家介绍了语法体系，以及如何根据语法体系确定一个句子的语法树，为后面的句法分析奠定基础。 接着，介绍了依存句法，它的目的是通过分析语言单位内成分之间的依存关系揭示其句法结构，随之讲解了依存句法中的五大依存关系。 最后，进一步介绍了区别于依存句法的语义依存，其目的是分析句子各个语言单位之间的语义关联，并将语义关联以依存结构呈现。 文章结尾，通过 Pyhanlp 实战以及可视化，带大家进一步加深对中文依存句法的了解。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[中文自然语言处理的应用、现状和未来（21）]]></title>
    <url>%2F2019%2F11%2F22%2F21.%E4%B8%AD%E6%96%87%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%9A%84%E5%BA%94%E7%94%A8%E3%80%81%E7%8E%B0%E7%8A%B6%E5%92%8C%E6%9C%AA%E6%9D%A5%2F</url>
    <content type="text"><![CDATA[自然语言理解和自然语言生成是自然语言处理的两大内核，机器翻译是自然语言理解方面最早的研究工作。自然语言处理的主要任务是：研究表示语言能力和语言应用的模型，建立和实现计算框架并提出相应的方法不断地完善模型，根据这样的语言模型设计有效地实现自然语言通信的计算机系统，并研讨关于系统的评测技术，最终实现用自然语言与计算机进行通信。目前，具有一定自然语言处理能力的典型应用包括计算机信息检索系统、多语种翻译系统等。 微软创始人比尔·盖茨曾经表示，“语言理解是人工智能领域皇冠上的明珠”。 语言是逻辑思维和交流的工具，宇宙万物中，只有人类才具有这种高级功能。要实现人与计算机间采用自然语言通信，必须使计算机同时具备自然语言理解和自然语言生成两大功能。 因此，NLP作为人工智能的一个子领域，其主要目的就包括两个方面：自然语言理解，让计算机理解自然语言文本的意义；自然语言生成，让计算机能以自然语言文本来表达给定的意图、思想等。自然语言是人类智慧的结晶，自然语言处理是人工智能中最为困难的问题之一，而对自然语言处理的研究也是充满魅力和挑战的。 NLP 领域发展现状如何？近年来，自然语言处理处于快速发展阶段。各种词表、语义语法词典、语料库等数据资源的日益丰富，词语切分、词性标注、句法分析等技术的快速进步，各种新理论、新方法、新模型的出现推动了自然语言处理研究的繁荣。互联网与移动互联网和世界经济社会一体化的潮流对自然语言处理技术的迫切需求，为自然语言处理研究发展提供了强大的市场动力。 我国直到上世纪80年代中期才开始较大规模和较系统的自然语言处理研究，尽管较国际水平尚有较大差距，但已经有了比较稳定的研究内容，包括语料库、知识库等数据资源建设，词语切分、句法分析等基础技术，以及信息检索、机器翻译等应用技术。 当前国内外出现了一批基于 NLP 技术的应用系统，例如 IBM 的 Watson 在电视问答节目中战胜人类冠军；苹果公司的 Siri个人助理被大众广为测试；谷歌、微软、百度等公司纷纷发布个人智能助理；科大讯飞牵头研发高考机器人……但相比于性能趋于饱和的计算机视觉和语音识别技术，自然语言处理因技术难度太大、应用场景太复杂，研究成果还未达到足够的高度。 自然语言处理中句子级分析技术目前，自然语言处理的对象有词、句子、篇章和段落、文本等，但是大多归根到底在句子的处理上，自然语言处理中的自然语言句子级分析技术，可以大致分为词法分析、句法分析、语义分析三个层面。 第一层面的词法分析包括汉语分词和词性标注两部分。和大部分西方语言不同，汉语书面语词语之间没有明显的空格标记，文本中的句子以字串的形式出现。因此汉语自然语言处理的首要工作就是要将输入的字串切分为单独的词语，然后在此基础上进行其他更高级的分析，这一步骤称为分词。 除了分词，词性标注也通常认为是词法分析的一部分。给定一个切好词的句子，词性标注的目的是为每一个词赋予一个类别，这个类别称为词性标记，比如，名词（Noun）、动词（Verb）、形容词（Adjective）等。一般来说，属于相同词性的词，在句法中承担类似的角色。 第二个层面的句法分析是对输入的文本句子进行分析以得到句子的句法结构的处理过程。对句法结构进行分析，一方面是语言理解的自身需求，句法分析是语言理解的重要一环，另一方面也为其它自然语言处理任务提供支持。例如句法驱动的统计机器翻译需要对源语言或目标语言（或者同时两种语言）进行句法分析；语义分析通常以句法分析的输出结果作为输入以便获得更多的指示信息。 根据句法结构表示形式的不同，最常见的句法分析任务可以分为以下三种： 短语结构句法分析，该任务也被称作成分句法分析，作用是识别出句子中的短语结构以及短语之间的层次句法关系； 依存句法分析，作用是识别句子中词汇与词汇之间的相互依存关系； 深层文法句法分析，即利用深层文法，例如词汇化树邻接文法、词汇功能文法、组合范畴文法等，对句子进行深层的句法以及语义分析。 上述几种句法分析任务比较而言，依存句法分析属于浅层句法分析。其实现过程相对简单，比较适合在多语言环境下的应用，但是依存句法分析所能提供的信息也相对较少。深层文法句法分析可以提供丰富的句法和语义信息，但是采用的文法相对复杂，分析器的运行复杂度也较高，这使得深层句法分析当前不适合处理大规模数据。短语结构句法分析介于依存句法分析和深层文法句法分析之间。 第三个层面是语义分析。语义分析的最终目的是理解句子表达的真实语义。但是，语义应该采用什么表示形式一直困扰着研究者们，至今这个问题也没有一个统一的答案。 语义角色标注是目前比较成熟的浅层语义分析技术。基于逻辑表达的语义分析也得到学术界的长期关注。出于机器学习模型复杂度、效率的考虑，自然语言处理系统通常采用级联的方式，即分词、词性标注、句法分析、语义分析分别训练模型。实际使用时，给定输入句子，逐一使用各个模块进行分析，最终得到所有结果。 深度学习背景下的自然语言处理近年来，随着研究工作的深入，研究者们开始从传统机器学习转向深度学习。2006年开始，有人利用深层神经网络在大规模无标注语料上无监督的为每个词学到了一个分布式表示，形式上把每个单词表示成一个固定维数的向量，当作词的底层特征。在此特征基础上，完成了词性标注、命名实体识别和语义角色标注等多个任务，后来有人利用递归神经网络完成了句法分析、情感分析和句子表示等多个任务，这也为语言表示提供了新的思路。 面向自然语言处理的深度学习研究工作，目前尚处于起步阶段，尽管已有的深度学习算法模型如循环神经网络、递归神经网络和卷积神经网络等已经有较为显著的应用，但还没有重大突破。围绕适合自然语言处理领域的深度学习模型构建等研究应该有着非常广阔的空间。 在当前已有的深度学习模型研究中，难点是在模型构建过程中参数的优化调整方面。主要有深度网络层数、正则化问题及网络学习速率等，可能的解决方案比如有采用多核机提升网络训练速度，针对不同应用场合，选择合适的优化算法等。 自然语言处理未来的研究方向纵观自然语言处理技术研究发展的态势和现状，以下研究方向或问题将可能成为自然语言处理未来研究必须攻克的堡垒： 词法和句法分析方面：包括多粒度分词、新词发现、词性标注等； 语义分析方面：包括词义消歧、非规范文本的语义分析。其中，非规范划化文本主要指社交平台上比较口语化、弱规范甚至不规范的短文本，因其数据量巨大和实时性而具有研究和应用价值，被广泛用于舆情监控、情感分析和突发事件发现等任务； 语言认知模型方面：比如使用深度神经网络处理自然语言，建立更有效、可解释的语言计算模型，例如，词嵌入的发现。还有目前词的表示是通过大量的语料库学习得到的，如何通过基于少量样本来发现新词、低频词也急需探索； 知识图谱方面：如何构建能够融合符号逻辑和表示学习的大规模高精度的知识图谱； 文本分类与聚类方面：通过有监督、半监督和无监督学习，能够准确进行分类和聚类。当下大多数语料都是没有标签的，未来在无监督或者半监督方面更有需求； 信息抽取方面：对于多源异构信息，如何准确进行关系、事件的抽取等。信息抽取主要从面向开放域的可扩展信息抽取技术、自学习与自适应和自演化的信息抽取系统以及面向多源异构数据的信息融合技术方向发展； 情感分析方面：包括基于上下文感知的情感分析、跨领域跨语言情感分析、基于深度学习的端到端情感分析、情感解释、反讽分析、立场分析等； 自动文摘方面：如何表达要点信息？如何评估信息单元的重要性？这些都要随着语义分析、篇章理解、深度学习等技术快速发展； 信息检索方面：包括意图搜索、语义搜索等，都将有可能出现在各种场景的垂直领域，将以知识化推理为检索运行方式，以自然语言多媒体交互为手段的智能化搜索与推荐技术； 自动问答方面：包括深度推理问答、多轮问答等各种形式的自动问答系统； 机器翻译方面：包括面向小数据的机器翻译、非规范文本的机器翻译和篇章级机器翻译等。 总结本文，从 NLP 的概念出发，首先指出了自然语言处理的两大内核：自然语言理解和自然语言生成；然后简单介绍了国内外 NLP研究发展现状；紧接着重点介绍了最常用、应用最广的自然语言处理中句子级分析技术，最后在深度学习背景下，指出了自然语言处理未来可能遇到的挑战和重点研究方向，为后期的学习提供指导和帮助。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[基于LSTM轻松生成各种古诗（11）]]></title>
    <url>%2F2019%2F11%2F22%2F11.%E5%9F%BA%E4%BA%8ELSTM%E8%BD%BB%E6%9D%BE%E7%94%9F%E6%88%90%E5%90%84%E7%A7%8D%E5%8F%A4%E8%AF%97%2F</url>
    <content type="text"><![CDATA[目前循环神经网络（RNN）已经广泛用于自然语言处理中，可以处理大量的序列数据，可以说是最强大的神经网络模型之一。人们已经给 RNN找到了越来越多的事情做，比如画画和写诗，微软的小冰都已经出版了一本诗集了。 而其实训练一个能写诗的神经网络并不难，下面我们就介绍如何简单快捷地建立一个会写诗的网络模型。 本次开发环境如下： Python 3.6 Keras 环境 Jupyter Notebook 整个过程分为以下步骤完成： 语料准备 语料预处理 模型参数配置 构建模型 训练模型 模型作诗 绘制模型网络结构图 下面一步步来构建和训练一个会写诗的模型。 第一 ，语料准备。一共四万多首古诗，每行一首诗，标题在预处理的时候已经去掉了。 第二 ，文件预处理。首先，机器并不懂每个中文汉字代表的是什么，所以要将文字转换为机器能理解的形式，这里我们采用 One-Hot的形式，这样诗句中的每个字都能用向量来表示，下面定义函数 preprocess_file() 来处理。 puncs = [&#39;]&#39;, &#39;[&#39;, &#39;（&#39;, &#39;）&#39;, &#39;{&#39;, &#39;}&#39;, &#39;：&#39;, &#39;《&#39;, &#39;》&#39;] def preprocess_file(Config): # 语料文本内容 files_content = &#39;&#39; with open(Config.poetry_file, &#39;r&#39;, encoding=&#39;utf-8&#39;) as f: for line in f: # 每行的末尾加上&quot;]&quot;符号代表一首诗结束 for char in puncs: line = line.replace(char, &quot;&quot;) files_content += line.strip() + &quot;]&quot; words = sorted(list(files_content)) words.remove(&#39;]&#39;) counted_words = {} for word in words: if word in counted_words: counted_words[word] += 1 else: counted_words[word] = 1 # 去掉低频的字 erase = [] for key in counted_words: if counted_words[key] &lt;= 2: erase.append(key) for key in erase: del counted_words[key] del counted_words[&#39;]&#39;] wordPairs = sorted(counted_words.items(), key=lambda x: -x[1]) words, _ = zip(*wordPairs) # word到id的映射 word2num = dict((c, i + 1) for i, c in enumerate(words)) num2word = dict((i, c) for i, c in enumerate(words)) word2numF = lambda x: word2num.get(x, 0) return word2numF, num2word, words, files_content 在每行末尾加上 ]符号是为了标识这首诗已经结束了。我们给模型学习的方法是，给定前六个字，生成第七个字，所以在后面生成训练数据的时候，会以6的跨度，1的步长截取文字，生成语料。如果出现了] 符号，说明 ] 符号之前的语句和之后的语句是两首诗里面的内容，两首诗之间是没有关联关系的，所以我们后面会舍弃掉包含 ] 符号的训练数据。 第三 ，模型参数配置。预先定义模型参数和加载语料以及模型保存名称，通过类 Config 实现。 class Config(object): poetry_file = &#39;poetry.txt&#39; weight_file = &#39;poetry_model.h5&#39; # 根据前六个字预测第七个字 max_len = 6 batch_size = 512 learning_rate = 0.001 第四 ，构建模型，通过 PoetryModel 类实现，类的代码结构如下： class PoetryModel(object): def __init__(self, config): pass def build_model(self): pass def sample(self, preds, temperature=1.0): pass def generate_sample_result(self, epoch, logs): pass def predict(self, text): pass def data_generator(self): pass def train(self): pass 类中定义的方法具体实现功能如下： （1）init 函数定义，通过加载 Config 配置信息，进行语料预处理和模型加载，如果模型文件存在则直接加载模型，否则开始训练。 def __init__(self, config): self.model = None self.do_train = True self.loaded_model = False self.config = config # 文件预处理 self.word2numF, self.num2word, self.words, self.files_content = preprocess_file(self.config) if os.path.exists(self.config.weight_file): self.model = load_model(self.config.weight_file) self.model.summary() else: self.train() self.do_train = False self.loaded_model = True （2）build_model 函数主要用 Keras 来构建网络模型，这里使用 LSTM 的 GRU 来实现，当然直接使用 LSTM 也没问题。 def build_model(self): &#39;&#39;&#39;建立模型&#39;&#39;&#39; input_tensor = Input(shape=(self.config.max_len,)) embedd = Embedding(len(self.num2word)+1, 300, input_length=self.config.max_len)(input_tensor) lstm = Bidirectional(GRU(128, return_sequences=True))(embedd) dropout = Dropout(0.6)(lstm) lstm = Bidirectional(GRU(128, return_sequences=True))(embedd) dropout = Dropout(0.6)(lstm) flatten = Flatten()(lstm) dense = Dense(len(self.words), activation=&#39;softmax&#39;)(flatten) self.model = Model(inputs=input_tensor, outputs=dense) optimizer = Adam(lr=self.config.learning_rate) self.model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=optimizer, metrics=[&#39;accuracy&#39;]) （3）sample 函数，在训练过程的每个 epoch 迭代中采样。 def sample(self, preds, temperature=1.0): &#39;&#39;&#39; 当temperature=1.0时，模型输出正常 当temperature=0.5时，模型输出比较open 当temperature=1.5时，模型输出比较保守 在训练的过程中可以看到temperature不同，结果也不同 &#39;&#39;&#39; preds = np.asarray(preds).astype(&#39;float64&#39;) preds = np.log(preds) / temperature exp_preds = np.exp(preds) preds = exp_preds / np.sum(exp_preds) probas = np.random.multinomial(1, preds, 1) return np.argmax(probas) （4）训练过程中，每个 epoch 打印出当前的学习情况。 def generate_sample_result(self, epoch, logs): print(&quot;\n==================Epoch {}=====================&quot;.format(epoch)) for diversity in [0.5, 1.0, 1.5]: print(&quot;------------Diversity {}--------------&quot;.format(diversity)) start_index = random.randint(0, len(self.files_content) - self.config.max_len - 1) generated = &#39;&#39; sentence = self.files_content[start_index: start_index + self.config.max_len] generated += sentence for i in range(20): x_pred = np.zeros((1, self.config.max_len)) for t, char in enumerate(sentence[-6:]): x_pred[0, t] = self.word2numF(char) preds = self.model.predict(x_pred, verbose=0)[0] next_index = self.sample(preds, diversity) next_char = self.num2word[next_index] generated += next_char sentence = sentence + next_char print(sentence) （5）predict 函数，用于根据给定的提示，来进行预测。 根据给出的文字，生成诗句，如果给的 text 不到四个字，则随机补全。 def predict(self, text): if not self.loaded_model: return with open(self.config.poetry_file, &#39;r&#39;, encoding=&#39;utf-8&#39;) as f: file_list = f.readlines() random_line = random.choice(file_list) # 如果给的text不到四个字，则随机补全 if not text or len(text) != 4: for _ in range(4 - len(text)): random_str_index = random.randrange(0, len(self.words)) text += self.num2word.get(random_str_index) if self.num2word.get(random_str_index) not in [&#39;,&#39;, &#39;。&#39;, &#39;，&#39;] else self.num2word.get( random_str_index + 1) seed = random_line[-(self.config.max_len):-1] res = &#39;&#39; seed = &#39;c&#39; + seed for c in text: seed = seed[1:] + c for j in range(5): x_pred = np.zeros((1, self.config.max_len)) for t, char in enumerate(seed): x_pred[0, t] = self.word2numF(char) preds = self.model.predict(x_pred, verbose=0)[0] next_index = self.sample(preds, 1.0) next_char = self.num2word[next_index] seed = seed[1:] + next_char res += seed return res （6） data_generator 函数，用于生成数据，提供给模型训练时使用。 def data_generator(self): i = 0 while 1: x = self.files_content[i: i + self.config.max_len] y = self.files_content[i + self.config.max_len] puncs = [&#39;]&#39;, &#39;[&#39;, &#39;（&#39;, &#39;）&#39;, &#39;{&#39;, &#39;}&#39;, &#39;：&#39;, &#39;《&#39;, &#39;》&#39;, &#39;:&#39;] if len([i for i in puncs if i in x]) != 0: i += 1 continue if len([i for i in puncs if i in y]) != 0: i += 1 continue y_vec = np.zeros( shape=(1, len(self.words)), dtype=np.bool ) y_vec[0, self.word2numF(y)] = 1.0 x_vec = np.zeros( shape=(1, self.config.max_len), dtype=np.int32 ) for t, char in enumerate(x): x_vec[0, t] = self.word2numF(char) yield x_vec, y_vec i += 1 （7）train 函数，用来进行模型训练，其中迭代次数 number_of_epoch ，是根据训练语料长度除以 batch_size计算的，如果在调试中，想用更小一点的 number_of_epoch ，可以自定义大小，把 train 函数的第一行代码注释即可。 def train(self): #number_of_epoch = len(self.files_content) // self.config.batch_size number_of_epoch = 10 if not self.model: self.build_model() self.model.summary() self.model.fit_generator( generator=self.data_generator(), verbose=True, steps_per_epoch=self.config.batch_size, epochs=number_of_epoch, callbacks=[ keras.callbacks.ModelCheckpoint(self.config.weight_file, save_weights_only=False), LambdaCallback(on_epoch_end=self.generate_sample_result) ] ) 第五 ，整个模型构建好以后，接下来进行模型训练。 model = PoetryModel(Config) 训练过程中的第1-2轮迭代： 训练过程中的第9-10轮迭代： 虽然训练过程写出的诗句不怎么能看得懂，但是可以看到模型从一开始标点符号都不会用 ，到最后写出了有一点点模样的诗句，能看到模型变得越来越聪明了。 第六 ，模型作诗，模型迭代10次之后的测试，首先输入几个字，模型根据输入的提示，做出诗句。 text = input(&quot;text:&quot;) sentence = model.predict(text) print(sentence) 比如输入：小雨，模型做出的诗句为： 输入：text：小雨 结果：小妃侯里守。雨封即客寥。俘剪舟过槽。傲老槟冬绛。 第七 ，绘制网络结构图。 模型结构绘图，采用 Keras自带的功能实现： plot_model(model.model, to_file=&#39;model.png&#39;) 得到的模型结构图如下： 本节使用 LSTM 的变形 GRU 训练出一个能作诗的模型，当然大家可以替换训练语料为歌词或者小说，让机器人自动创作不同风格的歌曲或者小说。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>gru</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据可视化（4）]]></title>
    <url>%2F2019%2F11%2F22%2F4.%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[文本可视化依赖于自然语言处理，因此词袋模型、命名实体识别、关键词抽取、主题分析、情感分析等是较常用的文本分析技术。文本分析的过程主要包括特征提取，通过分词、抽取、归一化等操作提取出文本词汇级的内容，利用特征构建向量空间模型并进行降维，以便将其呈现在低维空间，或者利用主题模型处理特征，最终以灵活有效的形式表示这些处理过的数据，以便进行可视化呈现 介绍文本可视化类型，除了包含常规的图表类，如柱状图、饼图、折线图等的表现形式，在文本领域用的比较多的可视化类型有： 基于文本内容的可视化基于文本内容的可视化研究包括基于词频的可视化和基于词汇分布的可视化，常用的有词云、分布图和 Document Cards 等。 基于文本关系的可视化。基于文本关系的可视化研究文本内外关系，帮助人们理解文本内容和发现规律。常用的可视化形式有树状图、节点连接的网络图、力导向图、叠式图和 Word Tree 等。 基于多层面信息的可视化基于多层面信息的可视化主要研究如何结合信息的多个方面帮助用户从更深层次理解文本数据，发现其内在规律。其中，包含时间信息和地理坐标的文本可视化近年来受到越来越多的关注。常用的有地理热力图、ThemeRiver、SparkClouds、TextFlow 和基于矩阵视图的情感分析可视化等。 词云*第一种是默认的样式** wordcloud=WordCloud(font_path=simhei,background_color=&quot;white&quot;,max_font_size=80) word_frequence = {x[0]:x[1] for x in words_stat.head(1000).values} wordcloud=wordcloud.fit_words(word_frequence) #**第二种是自定义图片** text = &quot; &quot;.join(words_stat[&#39;segment&#39;].head(100).astype(str)) abel_mask = imread(r&quot;china.jpg&quot;) #这里设置了一张中国地图 wordcloud2 = WordCloud(background_color=&#39;white&#39;, # 设置背景颜色 mask = abel_mask, # 设置背景图片 max_words = 3000, # 设置最大现实的字数 font_path = simhei, # 设置字体格式 width=2048, height=1024, scale=4.0, max_font_size= 300, # 字体最大值 random_state=42).generate(text) # 根据图片生成词云颜色 image_colors = ImageColorGenerator(abel_mask) wordcloud2.recolor(color_func=image_colors) # 以下代码显示图片 plt.imshow(wordcloud2) plt.axis(&quot;off&quot;) plt.show() wordcloud2.to_file(r&#39;wordcloud_2.jpg&#39;) #保存结果 关系图关系图法，是指用连线图来表示事物相互关系的一种方法。最常见的关系图是数据库里的 E-R 图，表示实体、关系、属性三者之间的关系。在文本可视化里面，关系图也经常被用来表示有相互关系、原因与结果和目的与手段等复杂关系 安装 Matplotlib、NetworkX； 解决 Matplotlib 无法写中文问题。 NetworkX 绘制关系图的数据组织结构，节点和边都是 list 格式，边的 list 里面是成对的节点 classes= df[&#39;class&#39;].values.tolist() classrooms=df[&#39;classroom&#39;].values.tolist() nodes = list(set(classes + classrooms)) weights = [(df.loc[index,&#39;class&#39;],df.loc[index,&#39;classroom&#39;])for index in df.index] weights = list(set(weights)) # 设置matplotlib正常显示中文 plt.rcParams[&#39;font.sans-serif&#39;]=[&#39;SimHei&#39;] # 用黑体显示中文 plt.rcParams[&#39;axes.unicode_minus&#39;]=False colors = [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;, &#39;yellow&#39;] #有向图 DG = nx.DiGraph() #一次性添加多节点，输入的格式为列表 DG.add_nodes_from(nodes) #添加边，数据格式为列表 DG.add_edges_from(weights) #作图，设置节点名显示,节点大小，节点颜色 nx.draw(DG,with_labels=True, node_size=1000, node_color = colors) plt.show() 地理热力图地理热力图，是以特殊高亮的形式显示用户的地理位置，借助热力图，可以直观地观察到用户的总体情况和偏好。 安装 Folium； 将地理名词通过百度转换成经纬度。 在通过分词得到城市名称后，将地理名词通过百度转换成经纬度。首先注册密钥，使用百度 Web 服务 API 下的 Geocoding API 接口来获取你所需要地址的经纬度坐标，并转化为 JSON 结构的数据（个人接口，百度每天限制调用6000次），接下来定义经纬度获取函数： #经纬度转换 def getlnglat(address): url = &#39;http://api.map.baidu.com/geocoder/v2/&#39; output = &#39;json&#39; ak = &#39;sqGDDvCDEZPSz24bt4b0BpKLnMk1dv6d&#39; add = quote(address) #由于本文城市变量为中文，为防止乱码，先用quote进行编码 uri = url + &#39;?&#39; + &#39;address=&#39; + add + &#39;&amp;output=&#39; + output + &#39;&amp;ak=&#39; + ak req = urlopen(uri) res = req.read().decode() #将其他编码的字符串解码成unicode temp = json.loads(res) #对json数据进行解析 return temp 输出：北京,116.39564503787867,39.92998577808024,840成都,104.06792346330406,30.679942845419564,291重庆,106.53063501341296,29.54460610888615,261昆明,102.71460113878045,25.049153100453157,238潍坊,119.14263382297052,36.71611487305138,214济南,117.02496706629023,36.68278472716141,212 使用 Folium 库进行热力图绘制地图lat = np.array(cities[&quot;lat&quot;][0:num]) # 获取维度之维度值 lon = np.array(cities[&quot;lng&quot;][0:num]) # 获取经度值 pop = np.array(cities[&quot;count&quot;][0:num],dtype=float) # 获取人口数，转化为numpy浮点型 data1 = [[lat[i],lon[i],pop[i]] for i in range(num)] #将数据制作成[lats,lons,weights]的形式 map_osm = folium.Map(location=[35,110],zoom_start=5) #绘制Map，开始缩放程度是5倍 HeatMap(data1).add_to(map_osm) # 将热力图添加到前面建立的map里 file_path = dir + &quot;heatmap.html&quot; map_osm.save(file_path) 可视化技术栈 第一个是百度的 Echarts，基于 Canvas，适合刚入门的新手，遵循了数据可视化的一些经典范式，只要把数据组织好，就可以轻松得到很漂亮的图表； 第二个推荐 D3.js，基于 SVG 方便自己定制，D3 V4 支持 Canvas+SVG，D3.js 比 Echarts 稍微难点，适合有一定开发经验的人； 第三个 three.js，是一个基于 WebGL 的 3D 图形的框架，可以让用户通过 JavaScript 搭建 WebGL 项目]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>wordcloud</tag>
        <tag>folium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于CRF的中文命名实体识别模型实现（15）]]></title>
    <url>%2F2019%2F11%2F22%2F15.%E5%9F%BA%E4%BA%8ECRF%E7%9A%84%E4%B8%AD%E6%96%87%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[命名实体识别在越来越多的场景下被应用，如自动问答、知识图谱等。非结构化的文本内容有很多丰富的信息，但找到相关的知识始终是一个具有挑战性的任务，命名实体识别也不例外。 前面我们用隐马尔可夫模型（HMM）自己尝试训练过一个分词器，其实 HMM也可以用来训练命名实体识别器，但在本文，我们讲另外一个算法——条件随机场（CRF），来训练一个命名实体识别器。 浅析条件随机场（CRF）条件随机场（Conditional Random Fields，简称CRF）是给定一组输入序列条件下另一组输出序列的条件概率分布模型，在自然语言处理中得到了广泛应用。 首先，我们来看看什么是随机场。“随机场”的名字取的很玄乎，其实理解起来不难。随机场是由若干个位置组成的整体，当按照某种分布给每一个位置随机赋予一个值之后，其全体就叫做随机场。 还是举词性标注的例子。假如我们有一个十个词形成的句子需要做词性标注。这十个词每个词的词性可以在我们已知的词性集合（名词，动词……)中去选择。当我们为每个词选择完词性后，这就形成了一个随机场。 了解了随机场，我们再来看看马尔科夫随机场。马尔科夫随机场是随机场的特例，它假设随机场中某一个位置的赋值仅仅与和它相邻的位置的赋值有关，和与其不相邻的位置的赋值无关。 继续举十个词的句子词性标注的例子。如果我们假设所有词的词性只和它相邻的词的词性有关时，这个随机场就特化成一个马尔科夫随机场。比如第三个词的词性除了与自己本身的位置有关外，还只与第二个词和第四个词的词性有关。 理解了马尔科夫随机场，再理解 CRF 就容易了。CRF 是马尔科夫随机场的特例，它假设马尔科夫随机场中只有 X 和 Y 两种变量，X 一般是给定的，而 Y一般是在给定 X 的条件下我们的输出。这样马尔科夫随机场就特化成了条件随机场。 在我们十个词的句子词性标注的例子中，X 是词，Y 是词性。因此，如果我们假设它是一个马尔科夫随机场，那么它也就是一个 CRF。 对于 CRF，我们给出准确的数学语言描述：设 X 与 Y 是随机变量，P(Y|X) 是给定 X 时 Y 的条件概率分布，若随机变量 Y构成的是一个马尔科夫随机场，则称条件概率分布 P(Y|X) 是条件随机场。 基于 CRF 的中文命名实体识别模型实现在常规的命名实体识别中，通用场景下最常提取的是时间、人物、地点及组织机构名，因此本模型也将提取以上四种实体。 1.开发环境。 本次开发所选用的环境为： Sklearn_crfsuite Python 3.6 Jupyter Notebook 2.数据预处理。 本模型使用人民日报1998年标注数据，进行预处理。语料库词性标记中，对应的实体词依次为 t、nr、ns、nt。对语料需要做以下处理： 将语料全角字符统一转为半角； 合并语料库分开标注的姓和名，例如：温/nr 家宝/nr； 合并语料库中括号中的大粒度词，例如：[国家/n 环保局/n]nt； 合并语料库分开标注的时间，例如：（/w 一九九七年/t 十二月/t 三十一日/t ）/w。 首先引入需要用到的库： import re import sklearn_crfsuite from sklearn_crfsuite import metrics from sklearn.externals import joblib 数据预处理，定义 CorpusProcess 类，我们还是先给出类实现框架： class CorpusProcess(object): def __init__(self): &quot;&quot;&quot;初始化&quot;&quot;&quot; pass def read_corpus_from_file(self, file_path): &quot;&quot;&quot;读取语料&quot;&quot;&quot; pass def write_corpus_to_file(self, data, file_path): &quot;&quot;&quot;写语料&quot;&quot;&quot; pass def q_to_b(self,q_str): &quot;&quot;&quot;全角转半角&quot;&quot;&quot; pass def b_to_q(self,b_str): &quot;&quot;&quot;半角转全角&quot;&quot;&quot; pass def pre_process(self): &quot;&quot;&quot;语料预处理 &quot;&quot;&quot; pass def process_k(self, words): &quot;&quot;&quot;处理大粒度分词,合并语料库中括号中的大粒度分词,类似：[国家/n 环保局/n]nt &quot;&quot;&quot; pass def process_nr(self, words): &quot;&quot;&quot; 处理姓名，合并语料库分开标注的姓和名，类似：温/nr 家宝/nr&quot;&quot;&quot; pass def process_t(self, words): &quot;&quot;&quot;处理时间,合并语料库分开标注的时间词，类似： （/w 一九九七年/t 十二月/t 三十一日/t ）/w &quot;&quot;&quot; pass def pos_to_tag(self, p): &quot;&quot;&quot;由词性提取标签&quot;&quot;&quot; pass def tag_perform(self, tag, index): &quot;&quot;&quot;标签使用BIO模式&quot;&quot;&quot; pass def pos_perform(self, pos): &quot;&quot;&quot;去除词性携带的标签先验知识&quot;&quot;&quot; pass def initialize(self): &quot;&quot;&quot;初始化 &quot;&quot;&quot; pass def init_sequence(self, words_list): &quot;&quot;&quot;初始化字序列、词性序列、标记序列 &quot;&quot;&quot; pass def extract_feature(self, word_grams): &quot;&quot;&quot;特征选取&quot;&quot;&quot; pass def segment_by_window(self, words_list=None, window=3): &quot;&quot;&quot;窗口切分&quot;&quot;&quot; pass def generator(self): &quot;&quot;&quot;训练数据&quot;&quot;&quot; pass 由于整个代码实现过程较长，我这里给出重点步骤，最后会在 Github 上连同语料代码一同给出 ，下面是关键过程实现。 对语料中的句子、词性，实体分类标记进行区分。标签采用“BIO”体系，即实体的第一个字为 B_*，其余字为 I_*，非实体字统一标记为O。大部分情况下，标签体系越复杂，准确度也越高，但这里采用简单的 BIO 体系也能达到相当不错的效果。这里模型采用 tri-gram形式，所以在字符列中，要在句子前后加上占位符。 def init_sequence(self, words_list): &quot;&quot;&quot;初始化字序列、词性序列、标记序列 &quot;&quot;&quot; words_seq = [[word.split(u&#39;/&#39;)[0] for word in words] for words in words_list] pos_seq = [[word.split(u&#39;/&#39;)[1] for word in words] for words in words_list] tag_seq = [[self.pos_to_tag(p) for p in pos] for pos in pos_seq] self.pos_seq = [[[pos_seq[index][i] for _ in range(len(words_seq[index][i]))] for i in range(len(pos_seq[index]))] for index in range(len(pos_seq))] self.tag_seq = [[[self.tag_perform(tag_seq[index][i], w) for w in range(len(words_seq[index][i]))] for i in range(len(tag_seq[index]))] for index in range(len(tag_seq))] self.pos_seq = [[u&#39;un&#39;]+[self.pos_perform(p) for pos in pos_seq for p in pos]+[u&#39;un&#39;] for pos_seq in self.pos_seq] self.tag_seq = [[t for tag in tag_seq for t in tag] for tag_seq in self.tag_seq] self.word_seq = [[u&#39;&lt;BOS&gt;&#39;]+[w for word in word_seq for w in word]+[u&#39;&lt;EOS&gt;&#39;] for word_seq in words_seq] 处理好语料之后，紧接着进行模型定义和训练，定义 CRF_NER 类，我们还是采用先给出类实现框架，再具体讲解其实现： class CRF_NER(object): def __init__(self): &quot;&quot;&quot;初始化参数&quot;&quot;&quot; pass def initialize_model(self): &quot;&quot;&quot;初始化&quot;&quot;&quot; pass def train(self): &quot;&quot;&quot;训练&quot;&quot;&quot; pass def predict(self, sentence): &quot;&quot;&quot;预测&quot;&quot;&quot; pass def load_model(self): &quot;&quot;&quot;加载模型 &quot;&quot;&quot; pass def save_model(self): &quot;&quot;&quot;保存模型&quot;&quot;&quot; pass 在 CRF_NER 类中，分别完成了语料预处理和模型训练、保存、预测功能，具体实现如下。 第一步，init 函数实现了模型参数定义和 CorpusProcess 的实例化和语料预处理： def __init__(self): &quot;&quot;&quot;初始化参数&quot;&quot;&quot; self.algorithm = &quot;lbfgs&quot; self.c1 =&quot;0.1&quot; self.c2 = &quot;0.1&quot; self.max_iterations = 100 #迭代次数 self.model_path = dir + &quot;model.pkl&quot; self.corpus = CorpusProcess() #Corpus 实例 self.corpus.pre_process() #语料预处理 self.corpus.initialize() #初始化语料 self.model = None 第二步，给出模型定义，了解 sklearn_crfsuite.CRF 详情可查该[文档](https://sklearn-crfsuite.readthedocs.io/en/latest/api.html#sklearn_crfsuite.CRF)。 def initialize_model(self): &quot;&quot;&quot;初始化&quot;&quot;&quot; algorithm = self.algorithm c1 = float(self.c1) c2 = float(self.c2) max_iterations = int(self.max_iterations) self.model = sklearn_crfsuite.CRF(algorithm=algorithm, c1=c1, c2=c2, max_iterations=max_iterations, all_possible_transitions=True) 第三步，模型训练和保存，分为训练集和测试集： def train(self): &quot;&quot;&quot;训练&quot;&quot;&quot; self.initialize_model() x, y = self.corpus.generator() x_train, y_train = x[500:], y[500:] x_test, y_test = x[:500], y[:500] self.model.fit(x_train, y_train) labels = list(self.model.classes_) labels.remove(&#39;O&#39;) y_predict = self.model.predict(x_test) metrics.flat_f1_score(y_test, y_predict, average=&#39;weighted&#39;, labels=labels) sorted_labels = sorted(labels, key=lambda name: (name[1:], name[0])) print(metrics.flat_classification_report(y_test, y_predict, labels=sorted_labels, digits=3)) self.save_model() 第四至第六步中 predict、load_model、save_model 方法的实现，大家可以在文末给出的地址中查看源码，这里就不堆代码了。 最后，我们来看看模型训练和预测的过程和结果： ner = CRF_NER() model = ner.train() 经过模型训练，得到的准确率和召回率如下： 进行模型预测，其结果还不错，如下： 基于 CRF的中文命名实体识别模型实现先讲到这儿，项目源码和涉及到的语料，大家可以到：Github上查看。 总结本文浅析了条件随机场，并使用 sklearn_crfsuite.CRF模型，对人民日报1998年标注数据进行了模型训练和预测，以帮助大家加强对条件随机场的理解。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>crf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中文命名实体提取（14）]]></title>
    <url>%2F2019%2F11%2F22%2F14.%E4%B8%AD%E6%96%87%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E6%8F%90%E5%8F%96%2F</url>
    <content type="text"><![CDATA[命名实体识别（NamedEntitiesRecognition，NER）是自然语言处理的一个基础任务。其目的是识别语料中人名、地名、组织机构名等命名实体，比如，[2015年中国国家海洋局对124个国际海底地理实体的命名]https://baike.baidu.com/item/2015%E5%B9%B4%E4%B8%AD%E5%9B%BD%E5%91%BD%E5%90%8D%E7%9A%84124%E4%B8%AA%E5%9B%BD%E9%99%85%E6%B5%B7%E5%BA%95%E5%9C%B0%E7%90%86%E5%AE%9E%E4%BD%93%E5%90%8D%E7%A7%B0%E4%BF%A1%E6%81%AF/18705238)。 由于命名实体数量不断增加，通常不可能在词典中穷尽列出，且其构成方法具有各自的一些规律性，因而，通常把对这些词的识别从词汇形态处理（如汉语切分）任务中独立处理，称为命名实体识别。 命名实体识别技术是信息抽取、信息检索、机器翻译、问答系统等多种自然语言处理技术必不可少的组成部分。 常见的命名实体识别方法综述命名实体是命名实体识别的研究主体，一般包括三大类（实体类、时间类和数字类）和七小类（人名、地名、机构名、时间、日期、货币和百分比）命名实体。评判一个命名实体是否被正确识别包括两个方面：实体的边界是否正确和实体的类型是否标注正确。 命名实体识别的主要技术方法分为：基于规则和词典的方法、基于统计的方法、二者混合的方法等。 1.基于规则和词典的方法。 基于规则的方法多采用语言学专家手工构造规则模板，选用特征包括统计信息、标点符号、关键字、指示词和方向词、位置词（如尾字）、中心词等方法，以模式和字符串相匹配为主要手段，这类系统大多依赖于知识库和词典的建立。基于规则和词典的方法是命名实体识别中最早使用的方法，一般而言，当提取的规则能比较精确地反映语言现象时，基于规则的方法性能要优于基于统计的方法。但是这些规则往往依赖于具体语言、领域和文本风格，编制过程耗时且难以涵盖所有的语言现象，特别容易产生错误，系统可移植性不好，对于不同的系统需要语言学专家重新书写规则。基于规则的方法的另外一个缺点是代价太大，存在系统建设周期长、移植性差而且需要建立不同领域知识库作为辅助以提高系统识别能力等问题。 2.基于统计的方法。 基于统计机器学习的方法主要包括隐马尔可夫模型（HiddenMarkovMode，HMM）、最大熵（MaxmiumEntropy，ME）、支持向量机（SupportVectorMachine，SVM）、条件随机场（ConditionalRandom Fields，CRF）等。 在基于统计的这四种学习方法中，最大熵模型结构紧凑，具有较好的通用性，主要缺点是训练时间长复杂性高，有时甚至导致训练代价难以承受，另外由于需要明确的归一化计算，导致开销比较大。而条件随机场为命名实体识别提供了一个特征灵活、全局最优的标注框架，但同时存在收敛速度慢、训练时间长的问题。一般说来，最大熵和支持向量机在正确率上要比隐马尔可夫模型高一些，但隐马尔可夫模型在训练和识别时的速度要快一些，主要是由于在利用Viterbi算法求解命名实体类别序列时的效率较高。隐马尔可夫模型更适用于一些对实时性有要求以及像信息检索这样需要处理大量文本的应用，如短文本命名实体识别。 基于统计的方法对特征选取的要求较高，需要从文本中选择对该项任务有影响的各种特征，并将这些特征加入到特征向量中。依据特定命名实体识别所面临的主要困难和所表现出的特性，考虑选择能有效反映该类实体特性的特征集合。主要做法是通过对训练语料所包含的语言信息进行统计和分析，从训练语料中挖掘出特征。有关特征可以分为具体的单词特征、上下文特征、词典及词性特征、停用词特征、核心词特征以及语义特征等。 基于统计的方法对语料库的依赖也比较大，而可以用来建设和评估命名实体识别系统的大规模通用语料库又比较少。 3.混合方法。 自然语言处理并不完全是一个随机过程，单独使用基于统计的方法使状态搜索空间非常庞大，必须借助规则知识提前进行过滤修剪处理。目前几乎没有单纯使用统计模型而不使用规则知识的命名实体识别系统，在很多情况下是使用混合方法： 统计学习方法之间或内部层叠融合。 规则、词典和机器学习方法之间的融合，其核心是融合方法技术。在基于统计的学习方法中引入部分规则，将机器学习和人工知识结合起来。 将各类模型、算法结合起来，将前一级模型的结果作为下一级的训练数据，并用这些训练数据对模型进行训练，得到下一级模型。 命名实体识别的一般流程如下图所示，一般的命名实体流程主要分为四个步骤： 对需要进行提取的文本语料进行分词； 获取需要识别的领域标签，并对分词结果进行标签标注； 对标签标注的分词进行抽取； 将抽取的分词组成需要的领域的命名实体。 动手实战命名实体识别下面通过jieba 分词包和 pyhanlp 来实战命名实体识别和提取。 1.jieba 进行命名实体识别和提取。 第一步，引入 jieba 包： import jieba import jieba.analyse import jieba.posseg as posg 第二步，使用 jieba 进行词性切分，allowPOS 指定允许的词性，这里选择名词 n 和地名 ns： sentence=u&#39;&#39;&#39;上线三年就成功上市,拼多多上演了互联网企业的上市奇迹,却也放大平台上存在的诸多问题，拼多多在美国上市。&#39;&#39;&#39; kw=jieba.analyse.extract_tags(sentence,topK=10,withWeight=True,allowPOS=(&#39;n&#39;,&#39;ns&#39;)) for item in kw: print(item[0],item[1]) 在这里，我们可以得到打印出来的结果： 上市 1.437080435586 上线 0.820694551317 奇迹 0.775434839431 互联网 0.712189275429 平台 0.6244340485550001 企业 0.422177218495 美国 0.415659623166 问题 0.39635135730800003 可以看得出，上市和上线应该是动词，这里给出的结果不是很准确。接下来，我们使用 textrank 算法来试试： kw=jieba.analyse.textrank(sentence,topK=20,withWeight=True,allowPOS=(&#39;ns&#39;,&#39;n&#39;)) for item in kw: print(item[0],item[1]) 这次得到的结果如下，可见，两次给出的结果还是不一样的。 上市 1.0 奇迹 0.572687398431635 企业 0.5710407272273452 互联网 0.5692560484441649 上线 0.23481844682115297 美国 0.23481844682115297 2.pyhanlp 进行命名实体识别和提取。 第一步，引入pyhanlp包： from pyhanlp import * 第二步，进行词性切分： sentence=u&#39;&#39;&#39;上线三年就成功上市,拼多多上演了互联网企业的上市奇迹,却也放大平台上存在的诸多问题，拼多多在美国上市。&#39;&#39;&#39; analyzer = PerceptronLexicalAnalyzer() segs = analyzer.analyze(sentence) arr = str(segs).split(&quot; &quot;) 第三步，定义一个函数，从得到的结果中，根据词性获取指定词性的词： def get_result(arr): re_list = [] ner = [&#39;n&#39;,&#39;ns&#39;] for x in arr: temp = x.split(&quot;/&quot;) if(temp[1] in ner): re_list.append(temp[0]) return re_list 第四步，我们获取结果： result = get_result(arr) print(result) 得到的结果如下，可见比 jieba 更准确： [&#39;互联网&#39;, &#39;企业&#39;, &#39;奇迹&#39;, &#39;平台&#39;, &#39;问题&#39;, &#39;美国&#39;] 总结本文对命名实体识别的方法进行了总结，并给出一般的处理流程，最后通过简单的 jieba 分词和 pyhanlp分词根据词性获取实体对象，后续大家也可以尝试通过哈工大和斯坦福的包来处理，下篇我们通过条件随机场 CRF 来训练一个命名实体识别模型。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>jieba</tag>
        <tag>pyhanlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于CRF的中文句法依存分析模型实现（17）]]></title>
    <url>%2F2019%2F11%2F22%2F17.%E5%9F%BA%E4%BA%8ECRF%E7%9A%84%E4%B8%AD%E6%96%87%E5%8F%A5%E6%B3%95%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[句法分析是自然语言处理中的关键技术之一，其基本任务是确定句子的句法结构或者句子中词汇之间的依存关系。主要包括两方面的内容，一是确定语言的语法体系，即对语言中合法句子的语法结构给予形式化的定义；另一方面是句法分析技术，即根据给定的语法体系，自动推导出句子的句法结构，分析句子所包含的句法单位和这些句法单位之间的关系。 依存关系本身是一个树结构，每一个词看成一个节点，依存关系就是一条有向边。本文主要通过清华大学的句法标注语料库，来实现基于 CRF 的中文句法依存分析模型。 清华大学句法标注语料库清华大学的句法标注语料，包括训练集（train.conll）和开发集合文件（dev.conll）。训练集大小 5.41M，共185541条数据。测试集大小为578kb，共19302条数据。 语料本身格式如下图所示： 通过上图，我们可以看出，每行语料包括有8个标签，分别是ID、FROM、lEMMA、CPOSTAG、POSTAG、FEATS、HEAD、DEPREL。详细介绍如下图： 模型的实现通过上面对句法依存关键技术的定义，我们明白了，句法依存的基本任务是确定句子的句法结构或者句子中词汇之间的依存关系。同时，我们也对此次模型实现的语料有了基本了解。 有了这些基础内容，我们便可以开始着手开发了。 本模型的实现过程，我们将主要分为训练集和测试集数据预处理、语料特征生成、模型训练及预测三大部分来实现，最终将通过模型预测得到正确的预测结果。 本次实战演练，我们选择以下模型和软件： Sklearn_crfsuite Python3.6 Jupyter Notebook 训练集和测试集数据预处理由于上述给定的语料，在模型中，我们不能直接使用，必须先经过预处理，把上述语料格式重新组织成具有词性、方向和距离的格式。 首先，我们通过一个 Python 脚本 get_parser_train_test_input.py，生成所需要的训练集和测试集，执行如下命令即可： cat train.conll | python get_parser_train_test_input.py &gt; train.data cat dev.conll | python get_parser_train_test_input.py &gt; dev.data 上面的脚本通过 cat 命令和管道符把内容传递给脚本进行处理。这里需要注意的是，脚本需要在 Linux 环境下执行，且语料和脚本应放在同一目录下。 get_parser_train_test_input.py 这一脚本的目的，就是重新组织语料，组织成可以使用 CRF算法的格式，具有词性、方向和距离的格式。我们认为，如果词 A 依赖词 B，A 就是孩子，B就是父亲。按照这种假设得到父亲节点的粗词性和详细词性，以及和依赖次之间的距离。 我们打开该脚本，看看它的代码，如下所示，重要的代码给出了注释。 #coding=utf-8 &#39;&#39;&#39;词A依赖词B，A就是孩子，B就是父亲&#39;&#39;&#39; import sys sentence = [&quot;Root&quot;] def do_parse(sentence): if len(sentence) == 1:return for line in sentence[1:]: line_arr = line.strip().split(&quot;\t&quot;) c_id = int(line_arr[0]) f_id = int(line_arr[6]) if f_id == 0: print(&quot;\t&quot;.join(line_arr[2:5])+&quot;\t&quot; + &quot;0_Root&quot;) continue f_post,f_detail_post = sentence[f_id].strip().split(&quot;\t&quot;)[3:5] #得到父亲节点的粗词性和详细词性 c_edge_post = f_post #默认是依赖词的粗粒度词性，但是名词除外；名词取细粒度词性 if f_post == &quot;n&quot;: c_edge_post = f_detail_post #计算是第几个出现这种词行 diff = f_id - c_id #确定要走几步 step = 1 if f_id &gt; c_id else -1 #确定每一步方向 same_post_num = 0 #中间每一步统计多少个一样的词性 cmp_idx = 4 if f_post == &quot;n&quot; else 3 #根据是否是名词决定取的是粗or详细词性 for i in range(0, abs(diff)): idx = c_id + (i+1)*step if sentence[idx].strip().split(&quot;\t&quot;)[cmp_idx] == c_edge_post: same_post_num += step print(&quot;\t&quot;.join(line_arr[2:5])+&quot;\t&quot; + &quot;%d_%s&quot;%(same_post_num, c_edge_post)) print(&quot;&quot;) for line in sys.stdin: line = line.strip() line_arr = line.split(&quot;\t&quot;) if line == &quot;&quot; or line_arr[0] == &quot;1&quot;: do_parse(sentence) sentence = [&quot;Root&quot;] if line ==&quot;&quot;:continue sentence.append(line) 整个脚本按行读入，每行按 Tab 键分割，首先得到父亲节点的词性，然后根据词性是否是名词 n 进行判断，默认是依赖词的粗粒度词性，如果是名词取细粒度词性。 脚本处理完，数据集的格式如下： 根据依存文法，决定两个词之间依存关系的主要有两个因素：方向和距离。正如上图中第四列类别标签所示，该列可以定义为以下形式： [+|-]dPOS 其中，[+|-] 表示中心词在句子中相对坐标轴的方向；POS 代表中心词具有的词性类别；d 表示与中心词词性相同的词的数量，即距离。 语料特征生成语料特征提取，主要采用 N-gram 模型来完成。这里我们使用 3-gram完成提取，将词性与词语两两进行匹配，分别返回特征集合和标签集合，需要注意整个语料采用的是 UTF-8 编码格式。 整个编码过程中，我们首先需要引入需要的库，然后对语料进行读文件操作。语料采用 UTF-8 编码格式，以句子为单位，按 Tab 键作分割处理，从而实现句子3-gram 模型的特征提取。具体实现如下。 import sklearn_crfsuite from sklearn_crfsuite import metrics from sklearn.externals import joblib 首先引入需要用到的库，如上面代码所示。其目的是使用模型 sklearn_crfsuite .CRF，metrics 用来进行模型性能测试，joblib用来保存和加载训练好的模型。 接着，定义包含特征处理方法的类，命名为 CorpusProcess，类结构定义如下： class CorpusProcess(object): def __init__(self): &quot;&quot;&quot;初始化&quot;&quot;&quot; pass def read_corpus_from_file(self, file_path): &quot;&quot;&quot;读取语料&quot;&quot;&quot; pass def write_corpus_to_file(self, data, file_path): &quot;&quot;&quot;写语料&quot;&quot;&quot; pass def process_sentence(self,lines): &quot;&quot;&quot;处理句子&quot;&quot;&quot; pass def initialize(self): &quot;&quot;&quot;语料初始化&quot;&quot;&quot; pass def generator(self, train=True): &quot;&quot;&quot;特征生成器&quot;&quot;&quot; pass def extract_feature(self, sentences): &quot;&quot;&quot;提取特征&quot;&quot;&quot; pass 下面介绍下 CorpusProcess 类中各个方法的具体实现。 第1步， 实现 init 构造函数，目的初始化预处理好的语料的路径： def __init__(self): &quot;&quot;&quot;初始化&quot;&quot;&quot; self.train_process_path = dir + &quot;data//train.data&quot; #预处理之后的训练集 self.test_process_path = dir + &quot;data//dev.data&quot; #预处理之后的测试集 这里的路径可以自定义，这里的语料之前已经完成了预处理过程。第2-3步，read_corpus_from_file 方法和 write_corpus_to_file 方法，分别定义了语料文件的读和写操作： def read_corpus_from_file(self, file_path): &quot;&quot;&quot;读取语料&quot;&quot;&quot; f = open(file_path, &#39;r&#39;,encoding=&#39;utf-8&#39;) lines = f.readlines() f.close() return lines def write_corpus_to_file(self, data, file_path): &quot;&quot;&quot;写语料&quot;&quot;&quot; f = open(file_path, &#39;w&#39;) f.write(str(data)) f.close() 这一步，主要用 open 函数来实现语料文件的读和写。 第4-5步，process_sentence 方法和 initialize 方法，用来处理句子和初始化语料，把语料按句子结构用 list存储起来，存储到内存中： def process_sentence(self,lines): &quot;&quot;&quot;处理句子&quot;&quot;&quot; sentence = [] for line in lines: if not line.strip(): yield sentence sentence = [] else: lines = line.strip().split(u&#39;\t&#39;) result = [line for line in lines] sentence.append(result) def initialize(self): &quot;&quot;&quot;语料初始化&quot;&quot;&quot; train_lines = self.read_corpus_from_file(self.train_process_path) test_lines = self.read_corpus_from_file(self.test_process_path) self.train_sentences = [sentence for sentence in self.process_sentence(train_lines)] self.test_sentences = [sentence for sentence in self.process_sentence(test_lines)] 这一步，通过 process_sentence 把句子收尾的空格去掉，然后通过 initialize 函数调用上面read_corpus_from_file 方法读取语料，分别加载训练集和测试集。 第6步，特征生成器，分别用来指定生成训练集或者测试集的特征集： def generator(self, train=True): &quot;&quot;&quot;特征生成器&quot;&quot;&quot; if train: sentences = self.train_sentences else: sentences = self.test_sentences return self.extract_feature(sentences) 这一步，对训练集和测试集分别处理，如果参数 train 为 True，则表示处理训练集，如果是 False，则表示处理测试集。第7步，特征提取，简单的进行 3-gram 的抽取，将词性与词语两两进行匹配，分别返回特征集合和标签集合： def extract_feature(self, sentences): &quot;&quot;&quot;提取特征&quot;&quot;&quot; features, tags = [], [] for index in range(len(sentences)): feature_list, tag_list = [], [] for i in range(len(sentences[index])): feature = {&quot;w0&quot;: sentences[index][i][0], &quot;p0&quot;: sentences[index][i][1], &quot;w-1&quot;: sentences[index][i-1][0] if i != 0 else &quot;BOS&quot;, &quot;w+1&quot;: sentences[index][i+1][0] if i != len(sentences[index])-1 else &quot;EOS&quot;, &quot;p-1&quot;: sentences[index][i-1][1] if i != 0 else &quot;un&quot;, &quot;p+1&quot;: sentences[index][i+1][1] if i != len(sentences[index])-1 else &quot;un&quot;} feature[&quot;w-1:w0&quot;] = feature[&quot;w-1&quot;]+feature[&quot;w0&quot;] feature[&quot;w0:w+1&quot;] = feature[&quot;w0&quot;]+feature[&quot;w+1&quot;] feature[&quot;p-1:p0&quot;] = feature[&quot;p-1&quot;]+feature[&quot;p0&quot;] feature[&quot;p0:p+1&quot;] = feature[&quot;p0&quot;]+feature[&quot;p+1&quot;] feature[&quot;p-1:w0&quot;] = feature[&quot;p-1&quot;]+feature[&quot;w0&quot;] feature[&quot;w0:p+1&quot;] = feature[&quot;w0&quot;]+feature[&quot;p+1&quot;] feature_list.append(feature) tag_list.append(sentences[index][i][-1]) features.append(feature_list) tags.append(tag_list) return features, tags 经过第6步，确定处理的是训练集还是测试集之后，通过 extract_feature 对句子进行特征抽取，使用 3-gram模型，得到特征集合和标签集合的对应关系。 模型训练及预测在完成特征工程和特征提取之后，接下来，我们要进行模型训练和预测，要预定义模型需要的一些参数，并初始化模型对象，进而完成模型训练和预测，以及模型的保存与加载。 首先，我们定义模型 ModelParser 类，进行初始化参数、模型初始化，以及模型训练、预测、保存和加载，类的结构定义如下： class ModelParser(object): def __init__(self): &quot;&quot;&quot;初始化参数&quot;&quot;&quot; pass def initialize_model(self): &quot;&quot;&quot;模型初始化&quot;&quot;&quot; pass def train(self): &quot;&quot;&quot;训练&quot;&quot;&quot; pass def predict(self, sentences): &quot;&quot;&quot;模型预测&quot;&quot;&quot; pass def load_model(self, name=&#39;model&#39;): &quot;&quot;&quot;加载模型 &quot;&quot;&quot; pass def save_model(self, name=&#39;model&#39;): &quot;&quot;&quot;保存模型&quot;&quot;&quot; pass 接下来，我们分析 ModelParser 类中方法的具体实现。第1步，init 方法实现算法模型参数和语料预处理 CorpusProcess 类的实例化和初始化： def __init__(self): &quot;&quot;&quot;初始化参数&quot;&quot;&quot; self.algorithm = &quot;lbfgs&quot; self.c1 = 0.1 self.c2 = 0.1 self.max_iterations = 100 self.model_path = &quot;model.pkl&quot; self.corpus = CorpusProcess() #初始化CorpusProcess类 self.corpus.initialize() #语料预处理 self.model = None 这一步，init 方法初始化参数以及 CRF 模型的参数，算法选用 LBFGS，c1 和 c2分别为0.1，最大迭代次数100次。然后定义模型保存的文件名称，以及完成对 CorpusProcess 类 的初始化。 第2-3步，initialize_model 方法和 train 实现模型定义和训练： def initialize_model(self): &quot;&quot;&quot;模型初始化&quot;&quot;&quot; algorithm = self.algorithm c1 = float(self.c1) c2 = float(self.c2) max_iterations = int(self.max_iterations) self.model = sklearn_crfsuite.CRF(algorithm=algorithm, c1=c1, c2=c2, max_iterations=max_iterations, all_possible_transitions=True) def train(self): &quot;&quot;&quot;训练&quot;&quot;&quot; self.initialize_model() x_train, y_train = self.corpus.generator() self.model.fit(x_train, y_train) labels = list(self.model.classes_) x_test, y_test = self.corpus.generator(train=False) y_predict = self.model.predict(x_test) metrics.flat_f1_score(y_test, y_predict, average=&#39;weighted&#39;, labels=labels) sorted_labels = sorted(labels, key=lambda name: (name[1:], name[0])) print(metrics.flat_classification_report(y_test, y_predict, labels=sorted_labels, digits=3)) self.save_model() 这一步，initialize_model 方法实现 了 sklearn_crfsuite.CRF 模型的初始化。然后在 train 方法中，先通过fit 方法训练模型，再通过 metrics.flat_f1_score 对测试集进行 F1 性能测试，最后将模型保存。 第4-6步，分别实现模型预测、保存和加载方法 最后，实例化类，并进行模型训练： model = ModelParser() model.train() 对模型进行预测，预测数据输入格式为三维，表示完整的一句话： [[[‘坚决’, ‘a’, ‘ad’, ‘1_v’], [&#39;惩治&#39;, &#39;v&#39;, &#39;v&#39;, &#39;0_Root&#39;], [&#39;贪污&#39;, &#39;v&#39;, &#39;v&#39;, &#39;1_v&#39;], [&#39;贿赂&#39;, &#39;n&#39;, &#39;n&#39;, &#39;-1_v&#39;], [&#39;等&#39;, &#39;u&#39;, &#39;udeng&#39;, &#39;-1_v&#39;], [&#39;经济&#39;, &#39;n&#39;, &#39;n&#39;, &#39;1_v&#39;], [&#39;犯罪&#39;, &#39;v&#39;, &#39;vn&#39;, &#39;-2_v&#39;]]] 模型预测的结果如下图所示： 预测的结果，和原始语料预处理得到的标签格式保持一致。 总结本文通过清华大学的句法标注语料库，实现了基于 CRF 的中文句法依存分析模型。借此实例，相信大家对句法依存已有了一个完整客观的认识。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>crf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型部署上线的几种服务发布方式（18）]]></title>
    <url>%2F2019%2F11%2F22%2F18.%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E4%B8%8A%E7%BA%BF%E7%9A%84%E5%87%A0%E7%A7%8D%E6%9C%8D%E5%8A%A1%E5%8F%91%E5%B8%83%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[在前面所有的模型训练和预测中，我们训练好的模型都是直接通过控制台或者 Jupyter Notebook来进行预测和交互的，在一个系统或者项目中使用这种方式显然不可能，那在 Web 应用中如何使用我们训练好的模型呢？ 本文将通过以下四个方面对该问题进行讲解： 微服务架构简介； 模型的持久化与加载方式； Flask 和 Bottle 微服务框架； Tensorflow Serving 模型部署和服务。 微服务架构简介微服务是指开发一个单个小型的但有业务功能的服务，每个服务都有自己的处理和轻量通讯机制，可以部署在单个或多个服务器上。微服务也指一种松耦合的、有一定的有界上下文的面向服务架构。也就是说，如果每个服务都要同时修改，那么它们就不是微服务，因为它们紧耦合在一起；如果你需要掌握一个服务太多的上下文场景使用条件，那么它就是一个有上下文边界的服务，这个定义来自DDD 领域驱动设计。 相对于单体架构和 SOA，它的主要特点是组件化、松耦合、自治、去中心化，体现在以下几个方面： 一组小的服务：服务粒度要小，而每个服务是针对一个单一职责的业务能力的封装，专注做好一件事情； 独立部署运行和扩展：每个服务能够独立被部署并运行在一个进程内。这种运行和部署方式能够赋予系统灵活的代码组织方式和发布节奏，使得快速交付和应对变化成为可能。 独立开发和演化：技术选型灵活，不受遗留系统技术约束。合适的业务问题选择合适的技术可以独立演化。服务与服务之间采取与语言无关的 API 进行集成。相对单体架构，微服务架构是更面向业务创新的一种架构模式。 独立团队和自治：团队对服务的整个生命周期负责，工作在独立的上下文中，自己决策自己治理，而不需要统一的指挥中心。团队和团队之间通过松散的社区部落进行衔接。 由此，我们可以看到整个微服务的思想，与我们现在面对信息爆炸、知识爆炸做事情的思路是相通的：通过解耦我们所做的事情，分而治之以减少不必要的损耗，使得整个复杂的系统和组织能够快速地应对变化。 我们为什么采用微服务呢？ “让我们的系统尽可能快地响应变化” ——Rebecca Parson 下面是一个简单的微服务模型架构设计： 模型的持久化与加载方式开发过 J2EE应用的人应该对持久化的概念很清楚。通俗得讲，就是临时数据（比如内存中的数据，是不能永久保存的）持久化为持久数据（比如持久化至数据库中，能够长久保存）。 那我们训练好的模型一般都是存储在内存中，这个时候就需要用到持久化方式，在 Python 中，常用的模型持久化方式有三种，并且都是以文件的方式持久化。 1.JSON（JavaScript Object Notation）格式。 JSON 是一种轻量级的数据交换格式，易于人们阅读和编写。使用 JSON 函数需要导入 JSON 库： import json 它拥有两个格式处理函数： json.dumps：将 Python 对象编码成 JSON 字符串； json.loads：将已编码的 JSON 字符串解码为 Python 对象。 下面看一个例子。 首先我们创建一个 List 对象 data，然后把 data 编码成 JSON 字符串保存在 data.json 文件中，之后再读取 data.json文件中的字符串解码成 Python 对象，代码如下： 2. pickle 模块 pickle 提供了一个简单的持久化功能。可以将对象以文件的形式存放在磁盘上。pickle 模块只能在 Python 中使用，Python中几乎所有的数据类型（列表、字典、集合、类等）都可以用 pickle 来序列化。pickle 序列化后的数据，可读性差，人一般无法识别。 使用的时候需要引入库： import pickle 它有以下两个方法： pickle.dump(obj, file[, protocol])：序列化对象，并将结果数据流写入到文件对象中。参数 protocol 是序列化模式，默认值为0，表示以文本的形式序列化。protocol 的值还可以是1或2，表示以二进制的形式序列化。 pickle.load(file)：反序列化对象。将文件中的数据解析为一个 Python 对象。 我们继续延用上面的例子。实现的不同点在于，这次文件打开时用了 with...as... 语法，使用 pickle 保存结果，文件保存为data.pkl，代码如下。3. sklearn 中的 joblib 模块。使用 joblib，首先需要引入包： from sklearn.externals import joblib 使用方法如下，基本和 JSON、pickle一样，这里不再详细讲解。第17课中，进行模型保存时使用的就是这种方式，可以看代码，回顾一下。 joblib.dump(model, model_path) #模型保存 joblib.load(model_path) #模型加载 Flask 和 Bottle 微服务框架通过上面，我们对微服务和 Python 中三种模型持久化和加载方式有了基本了解。下面我们看看，Python 中如何把模型发布成一个微服务的。 这里给出两个微服务框架 Bottle 和Flask。 Bottle 是一个非常小巧但高效的微型 Python Web 框架，它被设计为仅仅只有一个文件的 Python 模块，并且除 Python标准库外，它不依赖于任何第三方模块。 Bottle 本身主要包含以下四个模块，依靠它们便可快速开发微 Web 服务： 路由（Routing）：将请求映射到函数，可以创建十分优雅的 URL； 模板（Templates）：可以快速构建 Python 内置模板引擎，同时还支持 Mako、Jinja2、Cheetah 等第三方模板引擎； 工具集（Utilites）：用于快速读取 form 数据，上传文件，访问 Cookies，Headers 或者其它 HTTP 相关的 metadata； 服务器（Server）：内置 HTTP 开发服务器，并且支持 paste、fapws3、 bjoern、Google App Engine、Cherrypy 或者其它任何 WSGI HTTP 服务器。 Flask 也是一个 Python 编写的 Web 微框架，可以让我们使用 Python 语言快速实现一个网站或 Web 服务。并使用方式和 Bottle相似，Flask 依赖 Jinja2 模板和 Werkzeug WSGI 服务。Werkzeug 本质是 Socket 服务端，其用于接收 HTTP请求并对请求进行预处理，然后触发 Flask 框架，开发人员基于 Flask框架提供的功能对请求进行相应的处理，并返回给用户，如果返回给用户的内容比较复杂时，需要借助 Jinja2模板来实现对模板的处理，即将模板和数据进行渲染，将渲染后的字符串返回给用户浏览器。 Bottle 和 Flask 在使用上相似，而且 Flask 的文档资料更全，发布的服务更稳定，因此下面重点以 Flask为例，来说明模型的微服务发布过程。 如果大家想进一步了解这两个框架，可以参考说明文档。 1.安装。 对 Bottle 和 Flask 进行安装，分别执行如下命令即可安装成功： pip install bottle pip install Flask 安装好之后，分别进入需要的包就可以写微服务程序了。这两个框架在使用时，用法、语法结构都差不多，网上 Flask 的中文资料相对多一些，所以这里用 Flask来举例。 2. 第一个最小的 Flask 应用。 第一个最小的 Flask 应用看起来会是这样: from flask import Flask app = Flask(__name__) @app.route(&#39;/&#39;) def hello_world(): return &#39;Hello World!&#39; if __name__ == &#39;__main__&#39;: app.run() 把它保存为 hello.py（或是类似的），然后用 Python 解释器来运行： python hello.py 或者直接在 Jupyter Notebook 里面执行，都没有问题。服务启动将在控制台打印如下消息： Running on http://127.0.0.1:5000/ 意思就是，可以通过 localhost 和 5000 端口，在浏览器访问： 这时我们就得到了服务在浏览器上的返回结果，于是也成功构建了与浏览器交互的服务。 如果要修改服务对应的 IP 地址和端口怎么办？只需要修改这行代码，即可修改 IP 地址和端口： app.run(host=&#39;192.168.31.19&#39;,port=8088) 3. Flask 发布一个预测模型。 首先，我们这里使用第17课保存的模型“model.pkl”。如果不使用浏览器，常规的控制台交互，我们这样就可以实现： from sklearn.externals import joblib model_path = &quot;D://达人课//中文自然语言处理入门实战课程//ch18//model.pkl&quot; model = joblib.load(model_path) sen =[[[&#39;坚决&#39;, &#39;a&#39;, &#39;ad&#39;, &#39;1_v&#39;], [&#39;惩治&#39;, &#39;v&#39;, &#39;v&#39;, &#39;0_Root&#39;], [&#39;贪污&#39;, &#39;v&#39;, &#39;v&#39;, &#39;1_v&#39;], [&#39;贿赂&#39;, &#39;n&#39;, &#39;n&#39;, &#39;-1_v&#39;], [&#39;等&#39;, &#39;u&#39;, &#39;udeng&#39;, &#39;-1_v&#39;], [&#39;经济&#39;, &#39;n&#39;, &#39;n&#39;, &#39;1_v&#39;], [&#39;犯罪&#39;, &#39;v&#39;, &#39;vn&#39;, &#39;-2_v&#39;]]] print(model.predict(sen)) 如果你现在有个需求，要求你的模型和浏览器进行交互，那 Flask 就可以实现。在第一个最小的 Flask 应用基础上，我们增加模型预测接口，这里注意： 启动之前把 IP 地址修改为自己本机的地址或者服务器工作站所在的 IP地址。 完整的代码如下，首先在启动之前先把模型预加载到内存中，然后重新定义 predict 函数，接受一个参数 sen： from sklearn.externals import joblib from flask import Flask,request app = Flask(__name__) @app.route(&#39;/&#39;) def hello_world(): return &#39;Hello World!&#39; @app.route(&#39;/predict/&lt;sen&gt;&#39;) def predict(sen): result = model.predict(sen) return str(result) if __name__ == &#39;__main__&#39;: model_path = &quot;D://ch18//model.pkl&quot; model = joblib.load(model_path) app.run(host=&#39;192.168.31.19&#39;) 启动 Flask 服务之后，在浏览器地址中输入： http://192.168.31.19:5000/predict/[[[&#39;坚决&#39;, ‘a’, ‘ad’, ‘1 v’], [‘惩治’, ‘v’,’v’, ‘0 Root’], [‘贪污’, ‘v’, ‘v’, ‘1 v’], [‘贿赂’, ‘n’, ‘n’, ‘-1 v’], [‘等’,’u’, ‘udeng’, ‘-1 v’], [‘经济’, ‘n’, ‘n’, ‘1 v’], [‘犯罪’, ‘v’, ‘vn’, ‘-2_v’]]] 得到预测结果，这样就完成了微服务的发布，并实现了模型和前端浏览器的交互。 Tensorflow Serving 模型部署和服务TensorFlow Serving 是一个用于机器学习模型 Serving 的高性能开源库。它可以将训练好的机器学习模型部署到线上，使用 gRPC作为接口接受外部调用。更加让人眼前一亮的是，它支持模型热更新与自动模型版本管理。这意味着一旦部署 TensorFlow Serving后，你再也不需要为线上服务操心，只需要关心你的线下模型训练。 同样，TensorFlow Serving 可以将模型部署在移动端，如安卓或者 iOS 系统的 App 应用上。关于 Tensorflow Serving模型部署和服务，这里不在列举示例，直接参考文末的推荐阅读。 总结本节对微服务架构做了简单介绍，并介绍了三种机器学习模型持久化和加载的方式，接着介绍了 Python 的两个轻量级微服务框架 Bottle 和Flask。随后，我们通过 Flask 制作了一个简单的微服务预测接口，实现模型的预测和浏览器交互功能，最后简单介绍了 TensorFlow Servin模型的部署和服务功能。 学完上述内容，读者可轻易实现自己训练的模型和 Web 应用的结合，提供微服务接口，实现模型上线应用。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>flask</tag>
        <tag>bottle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[知识挖掘与知识图谱概述（19）]]></title>
    <url>%2F2019%2F11%2F22%2F19.%E7%9F%A5%E8%AF%86%E6%8C%96%E6%8E%98%E4%B8%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[搜索技术日新月异，如今它不再是搜索框中输入几个单词那么简单了。不仅输入方式多样化，并且还要在非常短的时间内给出一个精准而又全面的答案。目前，谷歌给出的解决方案就是——知识图谱（KnowledgeGraph）。 知识图谱能做什么？知识图谱想做的，就是在不同数据（来自现实世界）之间建立联系，从而带给我们更有意义的搜索结果。 比如，在上图中，用 Google搜索自然语言处理，右侧会显示研究领域和相关概念。点击这些知识点，又可以深入了解；再比如，搜索一个人名时，右侧会给出此人的生平、背景、居住位置、作品等信息。 这就是知识图谱，它不再是单一的信息，而是一个多元的信息网络。 知识图谱的源头知识图谱的雏形好几年前就已出现，一家名为 Metaweb的小公司，将现实世界中实体（人或事）的各种数据信息存储在系统中，并在数据之间建立起联系，从而发展出有别于传统关键词搜索的技术。 谷歌认为这一系统很有发展潜力，于2010年收购了 Metaweb。那时 Metawab 已经存储了1200万个节点（ReferencePoint，相当于一个词条或者一个页面），谷歌收购后的两年中，大大加速这一进程，现已有超过5.7亿个节点并在它们之间建了180亿个有效连接（这可是一个相当大的数字，维基百科英文版也才有大约400万个节点）。 知识图谱的通用表示方法本质上，知识图谱是一种揭示实体之间关系的语义网络 ，可以对现实世界的事物及其相互关系进行形式化地描述 。现在的知识图谱己被用来泛指各种大规模的知识库 。 三元组是知识图谱的一种通用表示方式，即 ${G=(E，R，S)}$，其中 $E={e_1，e_2，…，e_{|E|}}$ 是知识库中的实体集合，共包含$|E|$ 种不同实体，$R={r_1，r_2，…,r_{|E|}}$ 是知识库中的关系集合，共包含 $|R|$ 种不同关系，$S \subseteqE×R×E$ 代表知识库中的三元组集合。 三元组的基本形式主要包括实体 A、关系、实体 B和概念、属性、属性值等，实体是知识图谱中的最基本元素，不同的实体间存在不同的关系。概念主要指集合、类别、对象类型、事物的种类，例如人物、地理等；属性主要指对象可能具有的属性、特征、特性、特点以及参数，例如国籍、生日等；属性值主要指对象指定属性的值，例如中国、1988—09—08等。每个实体（概念的外延）可用一个全局唯一确定的ID 来标识，每个属性—属性值对可用来刻画实体的内在特性，而关系可用来连接两个实体，刻画它们之间的关联。 如下图是实体 A 与实体 B 组成的一个简单三元组形式。 知识图谱的架构知识图谱的架构主要包括自身的逻辑结构以及体系架构，分别说明如下。 1. 知识图谱的逻辑结构。 知识图谱在逻辑上可分为模式层与数据层两个层次，数据层主要是由一系列的事实组成，而知识将以事实为单位进行存储。如果用（实体 A，关系，实体B）、（实体、属性，属性值）这样的三元组来表达事实，可选择图数据库作为存储介质，例如开源的 Neo4j、Twitter 的 FlockDB、Sones 的GraphDB等。模式层构建在数据层之上，主要是通过本体库来规范数据层的一系列事实表达。本体是结构化知识库的概念模板，通过本体库而形成的知识库不仅层次结构较强，并且冗余程度较小。 2. 知识图谱的体系架构。 知识图谱的体系架构是指其构建模式结构，如图下图所示。 知识图谱主要有自顶向下与自底向上两种构建方式。自顶向下指的是先为知识图谱定义好本体与数据模式，再将实体加入到知识库。该构建方式需要利用一些现有的结构化知识库作为其基础知识库，例如Freebase项目就是采用这种方式，它的绝大部分数据是从维基百科中得到的。自底向上指的是从一些开放链接数据中提取出实体，选择其中置信度较高的加入到知识库，再构建顶层的本体模式。目前，大多数知识图谱都采用自底向上的方式进行构建，其中最典型就是Google 的 Knowledge Vault。 知识图谱的关键技术大规模知识库的构建与应用需要多种智能信息处理技术的支持。这就涉及到当下异常火爆的人工智能中的自然语言处理（NLP）技术。 所谓自然语言，就是我们平时所说的话（包括语音或文字），但这些话计算机如何能“理解”？过程很复杂，下面是其中的几个关键步骤。 1. 知识抽取。 知识抽取技术，可以从一些公开的半结构化、非结构化的数据中提取出实体、关系、属性等知识要素。 知识抽取主要包含实体抽取、关系抽取、属性抽取等，涉及到的 NLP 技术有命名实体识别、句法依存、实体关系识别等。 2. 知识表示。 知识表示形成的综合向量对知识库的构建、推理、融合以及应用均具有重要的意义。 基于三元组的知识表示形式受到了人们广泛的认可，但是其在计算效率、数据稀疏性等方面却面临着诸多问题。近年来，以深度学习为代表的表示学习技术取得了重要的进展，可以将实体的语义信息表示为稠密低维实值向量，进而在低维空间中高效计算实体、关系及其之间的复杂语义关联。 知识表示学习主要包含的 NLP 技术有语义相似度计算、复杂关系模型，知识代表模型如距离模型、双线性模型、神经张量模型、矩阵分解模型、翻译模型等。 3.知识融合。 由于知识图谱中的知识来源广泛，存在知识质量良莠不齐、来自不同数据源的知识重复、知识间的关联不够明确等问题，所以必须要进行知识的融合。知识融合是高层次的知识组织，使来自不同知识源的知识在同一框架规范下进行异构数据整合、消歧、加工、推理验证、更新等步骤，达到数据、信息、方法、经验以及人的思想的融合，形成高质量的知识库。 在知识融合过程中，实体对齐、知识加工是两个重要的过程。 4.知识推理。 知识推理则是在已有的知识库基础上进一步挖掘隐含的知识，从而丰富、扩展知识库。在推理的过程中，往往需要关联规则的支持。由于实体、实体属性以及关系的多样性，人们很难穷举所有的推理规则，一些较为复杂的推理规则往往是手动总结的。对于推理规则的挖掘，主要还是依赖于实体以及关系间的丰富情况。知识推理的对象可以是实体、实体的属性、实体间的关系、本体库中概念的层次结构等。 知识推理方法主要可分为基于逻辑的推理与基于图的推理两种类别。 大规模开放知识库互联网的发展为知识工程提供了新的机遇。从一定程度上看，是互联网的出现帮助突破了传统知识工程在知识获取方面的瓶颈。从1998年 Tim Berners Lee提出语义网至今，涌现出大量以互联网资源为基础的新一代知识库。这类知识库的构建方法可以分为三类：互联网众包、专家协作和互联网挖掘，如下图所示： 下面介绍几个知名的中文知识图谱资源： OpenKG.CN：中文开放知识图谱联盟旨在通过建设开放的社区来促进中文知识图谱数据的开放与互联，促进中文知识图谱工具的标准化和技术普及。 Zhishi.me ：Zhishi.me 是中文常识知识图谱。主要通过从开放的百科数据中抽取结构化数据，已融合了百度百科，互动百科以及维基百科中的中文数据。 CN-DBPeidia：CN-DBpedia 是由复旦大学知识工场实验室研发并维护的大规模通用领域结构化百科。 cnSchema.org: cnSchema.org 是一个基于社区维护的开放的知识图谱 Schema 标准。cnSchema 的词汇集包括了上千种概念分类、数据类型、属性和关系等常用概念定义，以支持知识图谱数据的通用性、复用性和流动性。 知识图谱的典型应用知识图谱为互联网上海量、异构、动态的大数据表达、组织、管理以及利用提供了一种更为有效的方式，使得网络的智能化水平更高，更加接近于人类的认知思维。 基于大规模开放知识库或知识图谱的应用，目前尚处在持续不断的发展与探索的阶段。下面列出了一些国内外比较出色的应用。 1. 语义检索。 谷歌公司通过建立 Google KnowledgeGraph，实现了对知识的体系化组织与展示，试图从用户搜索意图感知、以及查询扩展的角度，直接提供给用户想要的知识。 2. 智能问答。 IBM 公司通过搭建知识图谱，并通过自然语言处理和机器学习等技术，开发出了 Watson系统。在2011年2月的美国问答节目《Jeopardy!》上，Watson 战胜了这一节目的两位冠军选手，可与1996年同样来自 IBM的“深蓝”战胜国际象棋大师卡斯帕罗夫产生的影响相提并论，被认为是人工智能历史上的一个里程碑。 3. 领域专家快速生成。 构建面向特定领域、特定主题的大规模知识库是实现对某一领域深度分析和计算的重要基础，OpenKN通过实现端到端的开放知识库构建工具集，实现了在给定部分种子（Seed）的情况下，从无到有的生成领域知识库，进而形成领域专家。 4. 行业生态深度分析与预测。 利用开放大数据可以帮助企业发现潜伏在数据中的威胁，将结构化网络日志、文本数据、开源和第三方数据整合进一个单一的环境，屏蔽可疑的信号与噪声，有效保护用户网络，可在信用卡欺诈行为识别、医疗行业疾病预测、电商商品推荐、强化组织数据安全、不一致性验证、异常分析、金融量化交易、法律分析服务等多方面提供有价值的服务。 知识图谱的前景与挑战在关注到知识图谱在自然语言处理、人工智能等领域展现巨大潜力的同时，也不难发现知识图谱中的知识获取、知识表示、知识推理等技术依然面临着一些困难与挑战，在未来的一段时间内，知识图谱将是大数据智能的前沿研究问题，有很多重要的开放性问题亟待学术界和产业界协力解决。我们认为，未来知识图谱研究有以下几个重要挑战： 知识类型与表示。知识图谱主要采用（实体1、关系、实体2）三元组的形式来表示知识，这种方法可以较好地表示很多事实性知识。然而，人类知识类型多样，面对很多复杂知识，三元组就束手无策了。例如，人们的购物记录信息、新闻事件等，包含大量实体及其之间的复杂关系，更不用说人类大量的涉及主观感受、主观情感和模糊的知识了。 知识获取。如何从互联网大数据萃取知识，是构建知识图谱的重要问题。目前已经提出各种知识获取方案，并已成功抽取大量有用的知识。但在抽取知识的准确率、覆盖率和效率等方面，都仍不如人意，有极大的提升空间。 知识融合。来自不同数据的抽取知识可能存在大量噪音和冗余，或者使用了不同的语言。如何将这些知识有机融合起来，建立更大规模的知识图谱，是实现大数据智能的必由之路。 知识应用。目前大规模知识图谱的应用场景和方式还比较有限，如何有效实现知识图谱的应用，利用知识图谱实现深度知识推理，提高大规模知识图谱计算效率，需要人们不断锐意发掘用户需求，探索更重要的应用场景，提出新的应用算法。 总结本文对知识图谱的起源、定义、架构、大规模知识库、应用以及未来挑战等内容，进行了全面阐述。 知识抽取、知识表示、知识融合以及知识推理为构建知识图谱的四大核心技术，本文就当前产业界的需求介绍了它在智能搜索、深度问答、社交网络以及一些垂直行业中的实际应用。此外，还总结了目前知识图谱面临的主要挑战，并对其未来的研究方向进行了展望。 知识图谱的重要性不仅在于它是一个拥有强大语义处理能力与开放互联能力的知识库，并且还是一把开启智能机器大脑的钥匙，能够打开 Web3.0时代的知识宝库，为相关学科领域开启新的发展方向。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于ML的中文短文本分类（6）]]></title>
    <url>%2F2019%2F11%2F22%2F6.%E5%9F%BA%E4%BA%8EML%E7%9A%84%E4%B8%AD%E6%96%87%E7%9F%AD%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[对每一条输入数据，判断事情的主体是谁 语料加载 分词 去停用词 抽取词向量特征 分别进行算法建模和模型训练 评估、计算 AUC 值 模型对比语料加载 import random import jieba import pandas as pd #加载停用词 stopwords=pd.read_csv(&#39;stopwords.txt&#39;,index_col=False,quoting=3,sep=&quot;\t&quot;,names=[&#39;stopword&#39;], encoding=&#39;utf-8&#39;) stopwords=stopwords[&#39;stopword&#39;].values #加载语料 laogong_df = pd.read_csv(&#39;beilaogongda.csv&#39;, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) laopo_df = pd.read_csv(&#39;beilaogongda.csv&#39;, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) erzi_df = pd.read_csv(&#39;beierzida.csv&#39;, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) nver_df = pd.read_csv(&#39;beinverda.csv&#39;, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) #删除语料的nan行 laogong_df.dropna(inplace=True) laopo_df.dropna(inplace=True) erzi_df.dropna(inplace=True) nver_df.dropna(inplace=True) #提取要分词的 content 列转换为 list 列表 laogong = laogong_df.segment.values.tolist() laopo = laopo_df.segment.values.tolist() erzi = erzi_df.segment.values.tolist() nver = nver_df.segment.values.tolist() 分词和去停用词#定义分词、去停用词和批量打标签的函数 #参数content_lines即为语料列表 上面转换的list #参数sentences是先定义的 list，用来存储分词并打标签后的结果 #参数category 是类型标签 def preprocess_text(content_lines, sentences, category): for line in content_lines: try: segs=jieba.lcut(line) segs = [v for v in segs if not str(v).isdigit()]#去数字 segs = list(filter(lambda x:x.strip(), segs)) #去左右空格 segs = list(filter(lambda x:len(x)&gt;1, segs)) #长度为1的字符 segs = list(filter(lambda x:x not in stopwords, segs)) #去掉停用词 sentences.append((&quot; &quot;.join(segs), category))# 打标签 except Exception: print(line) continue # 调用函数、生成训练数据，根据我提供的司法语料数据，分为报警人被老公打，报警人被老婆打，报警人被儿子打，报警人被女儿打，标签分别为0、1、2、3 sentences = [] preprocess_text(laogong, sentences,0) preprocess_text(laopo, sentences, 1) preprocess_text(erzi, sentences, 2) preprocess_text(nver, sentences, 3) # 将得到的数据集打散，生成更可靠的训练集分布，避免同类数据分布不均匀 random.shuffle(sentences) # 控制台输出前10条数据 for sentence in sentences[:10]: print(sentence[0], sentence[1]) #下标0是词列表，1是标签 抽取词向量特征# 抽取特征，我们定义文本抽取词袋模型特征 from sklearn.feature_extraction.text import CountVectorizer vec = CountVectorizer( analyzer=&#39;word&#39;, # tokenise by character ngrams max_features=4000, # keep the most common 1000 ngrams ) # 把语料数据切分，用 sk-learn 对数据切分，分成训练集和测试集 from sklearn.model_selection import train_test_split x, y = zip(*sentences) x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1256) # 把训练数据转换为词袋模型 vec.fit(x_train) 分别进行算法建模和模型训练# 定义朴素贝叶斯模型，然后对训练集进行模型训练，直接使用 sklearn 中的 MultinomialNB from sklearn.naive_bayes import MultinomialNB classifier = MultinomialNB() classifier.fit(vec.transform(x_train), y_train)评估、计算 AUC 值# 评分为 0.647331786543。 print(classifier.score(vec.transform(x_test), y_test)) # 测试集的预测 pre = classifier.predict(vec.transform(x_test)) 模型对比改变特征向量模型和训练模型对结果有什么变化。 ## 改变特征向量模型:把特征做得更强一点，尝试加入抽取 2-gram 和 3-gram 的统计特征，把词库的量放大一点 from sklearn.feature_extraction.text import CountVectorizer vec = CountVectorizer( analyzer=&#39;word&#39;, # tokenise by character ngrams ngram_range=(1,4), # use ngrams of size 1 and 2 max_features=20000, # keep the most common 1000 ngrams ) vec.fit(x_train) #用朴素贝叶斯算法进行模型训练 classifier = MultinomialNB() classifier.fit(vec.transform(x_train), y_train) #对结果进行评分 结果评分为：0.649651972158 print(classifier.score(vec.transform(x_test), y_test)) ## SVM 训练 from sklearn.svm import SVC svm = SVC(kernel=&#39;linear&#39;) svm.fit(vec.transform(x_train), y_train) print(svm.score(vec.transform(x_test), y_test)) ## 使用决策树、随机森林、XGBoost、神经网络 import xgboost as xgb from sklearn.model_selection import StratifiedKFold import numpy as np # xgb矩阵赋值 xgb_train = xgb.DMatrix(vec.transform(x_train), label=y_train) xgb_test = xgb.DMatrix(vec.transform(x_test)) 在 XGBoost 中，下面主要是调参指标，可以根据参数进行调参 params = { &#39;booster&#39;: &#39;gbtree&#39;, #使用gbtree &#39;objective&#39;: &#39;multi:softmax&#39;, # 多分类的问题、 # &#39;objective&#39;: &#39;multi:softprob&#39;, # 多分类概率 #&#39;objective&#39;: &#39;binary:logistic&#39;, #二分类 &#39;eval_metric&#39;: &#39;merror&#39;, #logloss &#39;num_class&#39;: 4, # 类别数，与 multisoftmax 并用 &#39;gamma&#39;: 0.1, # 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。 &#39;max_depth&#39;: 8, # 构建树的深度，越大越容易过拟合 &#39;alpha&#39;: 0, # L1正则化系数 &#39;lambda&#39;: 10, # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。 &#39;subsample&#39;: 0.7, # 随机采样训练样本 &#39;colsample_bytree&#39;: 0.5, # 生成树时进行的列采样 &#39;min_child_weight&#39;: 3, # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言 # 假设 h 在 0.01 附近，min_child_weight 为 1 叶子节点中最少需要包含 100 个样本。 &#39;silent&#39;: 0, # 设置成1则没有运行信息输出，最好是设置为0. &#39;eta&#39;: 0.03, # 如同学习率 &#39;seed&#39;: 1000, &#39;nthread&#39;: -1, # cpu 线程数 &#39;missing&#39;: 1 }]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>jieba</tag>
        <tag>auc</tag>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql使用实战]]></title>
    <url>%2F2019%2F10%2F29%2Fmysql%2F</url>
    <content type="text"><![CDATA[mysql使用实战 中文转拼音模糊查询CREATE TABLE IF NOT EXISTS `t_base_pinyin` ( `pin_yin_` VARCHAR (1000) CHARACTER SET gbk NOT NULL, `code_` INT (11) NOT NULL, PRIMARY KEY (`code_`) ) ENGINE = INNODB DEFAULT CHARSET = latin1; INSERT INTO t_base_pinyin (pin_yin_, code_) VALUES (&quot;a&quot;, 20319), (&quot;ai&quot;, 20317), (&quot;an&quot;, 20304), (&quot;ang&quot;, 20295), (&quot;ao&quot;, 20292), (&quot;ba&quot;, 20283), (&quot;bai&quot;, 20265), (&quot;ban&quot;, 20257), (&quot;bang&quot;, 20242), (&quot;bao&quot;, 20230), (&quot;bei&quot;, 20051), (&quot;ben&quot;, 20036), (&quot;beng&quot;, 20032), (&quot;bi&quot;, 20026), (&quot;bian&quot;, 20002), (&quot;biao&quot;, 19990), (&quot;bie&quot;, 19986), (&quot;bin&quot;, 19982), (&quot;bing&quot;, 19976), (&quot;bo&quot;, 19805), (&quot;bu&quot;, 19784), (&quot;ca&quot;, 19775), (&quot;cai&quot;, 19774), (&quot;can&quot;, 19763), (&quot;cang&quot;, 19756), (&quot;cao&quot;, 19751), (&quot;ce&quot;, 19746), (&quot;ceng&quot;, 19741), (&quot;cha&quot;, 19739), (&quot;chai&quot;, 19728), (&quot;chan&quot;, 19725), (&quot;chang&quot;, 19715), (&quot;chao&quot;, 19540), (&quot;che&quot;, 19531), (&quot;chen&quot;, 19525), (&quot;cheng&quot;, 19515), (&quot;chi&quot;, 19500), (&quot;chong&quot;, 19484), (&quot;chou&quot;, 19479), (&quot;chu&quot;, 19467), (&quot;chuai&quot;, 19289), (&quot;chuan&quot;, 19288), (&quot;chuang&quot;, 19281), (&quot;chui&quot;, 19275), (&quot;chun&quot;, 19270), (&quot;chuo&quot;, 19263), (&quot;ci&quot;, 19261), (&quot;cong&quot;, 19249), (&quot;cou&quot;, 19243), (&quot;cu&quot;, 19242), (&quot;cuan&quot;, 19238), (&quot;cui&quot;, 19235), (&quot;cun&quot;, 19227), (&quot;cuo&quot;, 19224), (&quot;da&quot;, 19218), (&quot;dai&quot;, 19212), (&quot;dan&quot;, 19038), (&quot;dang&quot;, 19023), (&quot;dao&quot;, 19018), (&quot;de&quot;, 19006), (&quot;deng&quot;, 19003), (&quot;di&quot;, 18996), (&quot;dian&quot;, 18977), (&quot;diao&quot;, 18961), (&quot;die&quot;, 18952), (&quot;ding&quot;, 18783), (&quot;diu&quot;, 18774), (&quot;dong&quot;, 18773), (&quot;dou&quot;, 18763), (&quot;du&quot;, 18756), (&quot;duan&quot;, 18741), (&quot;dui&quot;, 18735), (&quot;dun&quot;, 18731), (&quot;duo&quot;, 18722), (&quot;e&quot;, 18710), (&quot;en&quot;, 18697), (&quot;er&quot;, 18696), (&quot;fa&quot;, 18526), (&quot;fan&quot;, 18518), (&quot;fang&quot;, 18501), (&quot;fei&quot;, 18490), (&quot;fen&quot;, 18478), (&quot;feng&quot;, 18463), (&quot;fo&quot;, 18448), (&quot;fou&quot;, 18447), (&quot;fu&quot;, 18446), (&quot;ga&quot;, 18239), (&quot;gai&quot;, 18237), (&quot;gan&quot;, 18231), (&quot;gang&quot;, 18220), (&quot;gao&quot;, 18211), (&quot;ge&quot;, 18201), (&quot;gei&quot;, 18184), (&quot;gen&quot;, 18183), (&quot;geng&quot;, 18181), (&quot;gong&quot;, 18012), (&quot;gou&quot;, 17997), (&quot;gu&quot;, 17988), (&quot;gua&quot;, 17970), (&quot;guai&quot;, 17964), (&quot;guan&quot;, 17961), (&quot;guang&quot;, 17950), (&quot;gui&quot;, 17947), (&quot;gun&quot;, 17931), (&quot;guo&quot;, 17928), (&quot;ha&quot;, 17922), (&quot;hai&quot;, 17759), (&quot;han&quot;, 17752), (&quot;hang&quot;, 17733), (&quot;hao&quot;, 17730), (&quot;he&quot;, 17721), (&quot;hei&quot;, 17703), (&quot;hen&quot;, 17701), (&quot;heng&quot;, 17697), (&quot;hong&quot;, 17692), (&quot;hou&quot;, 17683), (&quot;hu&quot;, 17676), (&quot;hua&quot;, 17496), (&quot;huai&quot;, 17487), (&quot;huan&quot;, 17482), (&quot;huang&quot;, 17468), (&quot;hui&quot;, 17454), (&quot;hun&quot;, 17433), (&quot;huo&quot;, 17427), (&quot;ji&quot;, 17417), (&quot;jia&quot;, 17202), (&quot;jian&quot;, 17185), (&quot;jiang&quot;, 16983), (&quot;jiao&quot;, 16970), (&quot;jie&quot;, 16942), (&quot;jin&quot;, 16915), (&quot;jing&quot;, 16733), (&quot;jiong&quot;, 16708), (&quot;jiu&quot;, 16706), (&quot;ju&quot;, 16689), (&quot;juan&quot;, 16664), (&quot;jue&quot;, 16657), (&quot;jun&quot;, 16647), (&quot;ka&quot;, 16474), (&quot;kai&quot;, 16470), (&quot;kan&quot;, 16465), (&quot;kang&quot;, 16459), (&quot;kao&quot;, 16452), (&quot;ke&quot;, 16448), (&quot;ken&quot;, 16433), (&quot;keng&quot;, 16429), (&quot;kong&quot;, 16427), (&quot;kou&quot;, 16423), (&quot;ku&quot;, 16419), (&quot;kua&quot;, 16412), (&quot;kuai&quot;, 16407), (&quot;kuan&quot;, 16403), (&quot;kuang&quot;, 16401), (&quot;kui&quot;, 16393), (&quot;kun&quot;, 16220), (&quot;kuo&quot;, 16216), (&quot;la&quot;, 16212), (&quot;lai&quot;, 16205), (&quot;lan&quot;, 16202), (&quot;lang&quot;, 16187), (&quot;lao&quot;, 16180), (&quot;le&quot;, 16171), (&quot;lei&quot;, 16169), (&quot;leng&quot;, 16158), (&quot;li&quot;, 16155), (&quot;lia&quot;, 15959), (&quot;lian&quot;, 15958), (&quot;liang&quot;, 15944), (&quot;liao&quot;, 15933), (&quot;lie&quot;, 15920), (&quot;lin&quot;, 15915), (&quot;ling&quot;, 15903), (&quot;liu&quot;, 15889), (&quot;long&quot;, 15878), (&quot;lou&quot;, 15707), (&quot;lu&quot;, 15701), (&quot;lv&quot;, 15681), (&quot;luan&quot;, 15667), (&quot;lue&quot;, 15661), (&quot;lun&quot;, 15659), (&quot;luo&quot;, 15652), (&quot;ma&quot;, 15640), (&quot;mai&quot;, 15631), (&quot;man&quot;, 15625), (&quot;mang&quot;, 15454), (&quot;mao&quot;, 15448), (&quot;me&quot;, 15436), (&quot;mei&quot;, 15435), (&quot;men&quot;, 15419), (&quot;meng&quot;, 15416), (&quot;mi&quot;, 15408), (&quot;mian&quot;, 15394), (&quot;miao&quot;, 15385), (&quot;mie&quot;, 15377), (&quot;min&quot;, 15375), (&quot;ming&quot;, 15369), (&quot;miu&quot;, 15363), (&quot;mo&quot;, 15362), (&quot;mou&quot;, 15183), (&quot;mu&quot;, 15180), (&quot;na&quot;, 15165), (&quot;nai&quot;, 15158), (&quot;nan&quot;, 15153), (&quot;nang&quot;, 15150), (&quot;nao&quot;, 15149), (&quot;ne&quot;, 15144), (&quot;nei&quot;, 15143), (&quot;nen&quot;, 15141), (&quot;neng&quot;, 15140), (&quot;ni&quot;, 15139), (&quot;nian&quot;, 15128), (&quot;niang&quot;, 15121), (&quot;niao&quot;, 15119), (&quot;nie&quot;, 15117), (&quot;nin&quot;, 15110), (&quot;ning&quot;, 15109), (&quot;niu&quot;, 14941), (&quot;nong&quot;, 14937), (&quot;nu&quot;, 14933), (&quot;nv&quot;, 14930), (&quot;nuan&quot;, 14929), (&quot;nue&quot;, 14928), (&quot;nuo&quot;, 14926), (&quot;o&quot;, 14922), (&quot;ou&quot;, 14921), (&quot;pa&quot;, 14914), (&quot;pai&quot;, 14908), (&quot;pan&quot;, 14902), (&quot;pang&quot;, 14894), (&quot;pao&quot;, 14889), (&quot;pei&quot;, 14882), (&quot;pen&quot;, 14873), (&quot;peng&quot;, 14871), (&quot;pi&quot;, 14857), (&quot;pian&quot;, 14678), (&quot;piao&quot;, 14674), (&quot;pie&quot;, 14670), (&quot;pin&quot;, 14668), (&quot;ping&quot;, 14663), (&quot;po&quot;, 14654), (&quot;pu&quot;, 14645), (&quot;qi&quot;, 14630), (&quot;qia&quot;, 14594), (&quot;qian&quot;, 14429), (&quot;qiang&quot;, 14407), (&quot;qiao&quot;, 14399), (&quot;qie&quot;, 14384), (&quot;qin&quot;, 14379), (&quot;qing&quot;, 14368), (&quot;qiong&quot;, 14355), (&quot;qiu&quot;, 14353), (&quot;qu&quot;, 14345), (&quot;quan&quot;, 14170), (&quot;que&quot;, 14159), (&quot;qun&quot;, 14151), (&quot;ran&quot;, 14149), (&quot;rang&quot;, 14145), (&quot;rao&quot;, 14140), (&quot;re&quot;, 14137), (&quot;ren&quot;, 14135), (&quot;reng&quot;, 14125), (&quot;ri&quot;, 14123), (&quot;rong&quot;, 14122), (&quot;rou&quot;, 14112), (&quot;ru&quot;, 14109), (&quot;ruan&quot;, 14099), (&quot;rui&quot;, 14097), (&quot;run&quot;, 14094), (&quot;ruo&quot;, 14092), (&quot;sa&quot;, 14090), (&quot;sai&quot;, 14087), (&quot;san&quot;, 14083), (&quot;sang&quot;, 13917), (&quot;sao&quot;, 13914), (&quot;se&quot;, 13910), (&quot;sen&quot;, 13907), (&quot;seng&quot;, 13906), (&quot;sha&quot;, 13905), (&quot;shai&quot;, 13896), (&quot;shan&quot;, 13894), (&quot;shang&quot;, 13878), (&quot;shao&quot;, 13870), (&quot;she&quot;, 13859), (&quot;shen&quot;, 13847), (&quot;sheng&quot;, 13831), (&quot;shi&quot;, 13658), (&quot;shou&quot;, 13611), (&quot;shu&quot;, 13601), (&quot;shua&quot;, 13406), (&quot;shuai&quot;, 13404), (&quot;shuan&quot;, 13400), (&quot;shuang&quot;, 13398), (&quot;shui&quot;, 13395), (&quot;shun&quot;, 13391), (&quot;shuo&quot;, 13387), (&quot;si&quot;, 13383), (&quot;song&quot;, 13367), (&quot;sou&quot;, 13359), (&quot;su&quot;, 13356), (&quot;suan&quot;, 13343), (&quot;sui&quot;, 13340), (&quot;sun&quot;, 13329), (&quot;suo&quot;, 13326), (&quot;ta&quot;, 13318), (&quot;tai&quot;, 13147), (&quot;tan&quot;, 13138), (&quot;tang&quot;, 13120), (&quot;tao&quot;, 13107), (&quot;te&quot;, 13096), (&quot;teng&quot;, 13095), (&quot;ti&quot;, 13091), (&quot;tian&quot;, 13076), (&quot;tiao&quot;, 13068), (&quot;tie&quot;, 13063), (&quot;ting&quot;, 13060), (&quot;tong&quot;, 12888), (&quot;tou&quot;, 12875), (&quot;tu&quot;, 12871), (&quot;tuan&quot;, 12860), (&quot;tui&quot;, 12858), (&quot;tun&quot;, 12852), (&quot;tuo&quot;, 12849), (&quot;wa&quot;, 12838), (&quot;wai&quot;, 12831), (&quot;wan&quot;, 12829), (&quot;wang&quot;, 12812), (&quot;wei&quot;, 12802), (&quot;wen&quot;, 12607), (&quot;weng&quot;, 12597), (&quot;wo&quot;, 12594), (&quot;wu&quot;, 12585), (&quot;xi&quot;, 12556), (&quot;xia&quot;, 12359), (&quot;xian&quot;, 12346), (&quot;xiang&quot;, 12320), (&quot;xiao&quot;, 12300), (&quot;xie&quot;, 12120), (&quot;xin&quot;, 12099), (&quot;xing&quot;, 12089), (&quot;xiong&quot;, 12074), (&quot;xiu&quot;, 12067), (&quot;xu&quot;, 12058), (&quot;xuan&quot;, 12039), (&quot;xue&quot;, 11867), (&quot;xun&quot;, 11861), (&quot;ya&quot;, 11847), (&quot;yan&quot;, 11831), (&quot;yang&quot;, 11798), (&quot;yao&quot;, 11781), (&quot;ye&quot;, 11604), (&quot;yi&quot;, 11589), (&quot;yin&quot;, 11536), (&quot;ying&quot;, 11358), (&quot;yo&quot;, 11340), (&quot;yong&quot;, 11339), (&quot;you&quot;, 11324), (&quot;yu&quot;, 11303), (&quot;yuan&quot;, 11097), (&quot;yue&quot;, 11077), (&quot;yun&quot;, 11067), (&quot;za&quot;, 11055), (&quot;zai&quot;, 11052), (&quot;zan&quot;, 11045), (&quot;zang&quot;, 11041), (&quot;zao&quot;, 11038), (&quot;ze&quot;, 11024), (&quot;zei&quot;, 11020), (&quot;zen&quot;, 11019), (&quot;zeng&quot;, 11018), (&quot;zha&quot;, 11014), (&quot;zhai&quot;, 10838), (&quot;zhan&quot;, 10832), (&quot;zhang&quot;, 10815), (&quot;zhao&quot;, 10800), (&quot;zhe&quot;, 10790), (&quot;zhen&quot;, 10780), (&quot;zheng&quot;, 10764), (&quot;zhi&quot;, 10587), (&quot;zhong&quot;, 10544), (&quot;zhou&quot;, 10533), (&quot;zhu&quot;, 10519), (&quot;zhua&quot;, 10331), (&quot;zhuai&quot;, 10329), (&quot;zhuan&quot;, 10328), (&quot;zhuang&quot;, 10322), (&quot;zhui&quot;, 10315), (&quot;zhun&quot;, 10309), (&quot;zhuo&quot;, 10307), (&quot;zi&quot;, 10296), (&quot;zong&quot;, 10281), (&quot;zou&quot;, 10274), (&quot;zu&quot;, 10270), (&quot;zuan&quot;, 10262), (&quot;zui&quot;, 10260), (&quot;zun&quot;, 10256), (&quot;zuo&quot;, 10254); DROP FUNCTION IF EXISTS to_pinyin; DELIMITER $ CREATE FUNCTION to_pinyin(NAME text CHARSET gbk) RETURNS text CHARSET gbk BEGIN DECLARE mycode INT; DECLARE tmp_lcode VARCHAR(2) CHARSET gbk; DECLARE lcode INT; DECLARE tmp_rcode VARCHAR(2) CHARSET gbk; DECLARE rcode INT; DECLARE mypy text CHARSET gbk DEFAULT &#39;&#39;; DECLARE lp INT; SET mycode = 0; SET lp = 1; SET NAME = HEX(NAME); WHILE lp &lt; LENGTH(NAME) DO SET tmp_lcode = SUBSTRING(NAME, lp, 2); SET lcode = CAST(ASCII(UNHEX(tmp_lcode)) AS UNSIGNED); SET tmp_rcode = SUBSTRING(NAME, lp + 2, 2); SET rcode = CAST(ASCII(UNHEX(tmp_rcode)) AS UNSIGNED); IF lcode &gt; 128 THEN SET mycode =65536 - lcode * 256 - rcode ; SELECT CONCAT(mypy,pin_yin_) INTO mypy FROM t_base_pinyin WHERE CODE_ &gt;= ABS(mycode) ORDER BY CODE_ ASC LIMIT 1; SET lp = lp + 4; ELSE SET mypy = CONCAT(mypy,CHAR(CAST(ASCII(UNHEX(SUBSTRING(NAME, lp, 2))) AS UNSIGNED))); SET lp = lp + 2; END IF; END WHILE; RETURN LOWER(mypy); END; $ DELIMITER ; CREATE VIEW v_pinyin AS SELECT u.jjbh,u.CJLB,u.SFDD,u.CJXXDD,u.BJNR,u.CLJGNR,u.CJDWMC,u.JJRQSJ ，to_pinyin (u.CLJGNR) AS CLJGNR_PINYIN,to_pinyin (u.BJNR) AS BJNR_PINYIN，u.JJRQSJ FROM p_answer_handle_alarm u 抽取文本中数字/字母/中文DELIMITER $$ DROP FUNCTION IF EXISTS `Num_char_extract`$$ CREATE FUNCTION `Num_char_extract`(Varstring VARCHAR(100)CHARSET utf8, flag INT) RETURNS VARCHAR(50) CHARSET utf8 BEGIN DECLARE len INT DEFAULT 0; DECLARE Tmp VARCHAR(100) DEFAULT &#39;&#39;; SET len=CHAR_LENGTH(Varstring); IF flag = 0 THEN WHILE len &gt; 0 DO IF MID(Varstring,len,1)REGEXP&#39;[0-9]&#39; THEN SET Tmp=CONCAT(Tmp,MID(Varstring,len,1)); END IF; SET len = len - 1; END WHILE; ELSEIF flag=1 THEN WHILE len &gt; 0 DO IF (MID(Varstring,len,1)REGEXP &#39;[a-zA-Z]&#39;) THEN SET Tmp=CONCAT(Tmp,MID(Varstring,len,1)); END IF; SET len = len - 1; END WHILE; ELSEIF flag=2 THEN WHILE len &gt; 0 DO IF ( (MID(Varstring,len,1)REGEXP&#39;[0-9]&#39;) OR (MID(Varstring,len,1)REGEXP &#39;[a-zA-Z]&#39;) ) THEN SET Tmp=CONCAT(Tmp,MID(Varstring,len,1)); END IF; SET len = len - 1; END WHILE; ELSEIF flag=3 THEN WHILE len &gt; 0 DO IF NOT (MID(Varstring,len,1)REGEXP &#39;^[u0391-uFFE5]&#39;) THEN SET Tmp=CONCAT(Tmp,MID(Varstring,len,1)); END IF; SET len = len - 1; END WHILE; ELSE SET Tmp = &#39;Error: The second paramter should be in (0,1,2,3)&#39;; RETURN Tmp; END IF; RETURN REVERSE(Tmp); END$$ DELIMITER ; 中文转首字母查询CREATE FUNCTION `fristPinyin`(P_NAME VARCHAR(255)) RETURNS varchar(255) CHARSET utf8 BEGIN DECLARE V_RETURN VARCHAR(255); SET V_RETURN = ELT(INTERVAL(CONV(HEX(left(CONVERT(P_NAME USING gbk),1)),16,10), 0xB0A1,0xB0C5,0xB2C1,0xB4EE,0xB6EA,0xB7A2,0xB8C1,0xB9FE,0xBBF7, 0xBFA6,0xC0AC,0xC2E8,0xC4C3,0xC5B6,0xC5BE,0xC6DA,0xC8BB, 0xC8F6,0xCBFA,0xCDDA,0xCEF4,0xD1B9,0xD4D1), &#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;,&#39;E&#39;,&#39;F&#39;,&#39;G&#39;,&#39;H&#39;,&#39;J&#39;,&#39;K&#39;,&#39;L&#39;,&#39;M&#39;,&#39;N&#39;,&#39;O&#39;,&#39;P&#39;,&#39;Q&#39;,&#39;R&#39;,&#39;S&#39;,&#39;T&#39;,&#39;W&#39;,&#39;X&#39;,&#39;Y&#39;,&#39;Z&#39;); RETURN V_RETURN; END; CREATE FUNCTION `pinyin`(P_NAME VARCHAR(255)) RETURNS varchar(255) CHARSET utf8 BEGIN DECLARE V_COMPARE VARCHAR(255); DECLARE V_RETURN VARCHAR(255); DECLARE I INT; SET I = 1; SET V_RETURN = &#39;&#39;; while I &lt; LENGTH(P_NAME) do SET V_COMPARE = SUBSTR(P_NAME, I, 1); IF (V_COMPARE != &#39;&#39;) THEN #SET V_RETURN = CONCAT(V_RETURN, &#39;,&#39;, V_COMPARE); SET V_RETURN = CONCAT(V_RETURN, fristPinyin(V_COMPARE)); #SET V_RETURN = fristPinyin(V_COMPARE); END IF; SET I = I + 1; end while; IF (ISNULL(V_RETURN) or V_RETURN = &#39;&#39;) THEN SET V_RETURN = P_NAME; END IF; RETURN V_RETURN; END; select upper(pinyin(company_name)) as cn,company_name from job where upper(pinyin(company_name)) like &#39;%pa%&#39;; select pinyin(&quot;金石&quot;) 联表查询在A不在B表select * from (select DISTINCT(company_name) from job j1 where is_checked = ‘1’) j2 where j2.company_name not in(select DISTINCT(job_company_name) from standard_business s1)]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>function</tag>
        <tag>procedure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy集成selenium爬取boss直聘]]></title>
    <url>%2F2019%2F10%2F17%2Fscrapy%E9%9B%86%E6%88%90selenium%E7%88%AC%E5%8F%96boss%E7%9B%B4%E8%81%98%2F</url>
    <content type="text"><![CDATA[scrapy集成selenium爬取boss直聘 核心代码class BossSpider(scrapy.Spider): name = &#39;boss&#39; allowed_domains = [&#39;zhipin.com&#39;] start_urls = [&#39;https://www.zhipin.com/&#39;] nodes = [] def parse(self, response): driver = None chrome_options = webdriver.ChromeOptions() # proxy_url = get_random_proxy() # print(proxy_url + &quot;代理服务器正在爬取&quot;) # chrome_options.add_argument(&#39;--proxy-server=https://&#39; + proxy_url.strip()) prefs = { &#39;profile.default_content_setting_values&#39;: { &#39;images&#39;: 1, # 加载图片 &quot;User-Agent&quot;: UserAgent().random, # 更换UA } } chrome_options.add_experimental_option(&quot;prefs&quot;, prefs) chrome_options.add_experimental_option(&#39;excludeSwitches&#39;, [&#39;enable-automation&#39;]) if platform.system() == &quot;Windows&quot;: driver = webdriver.Chrome(&#39;chromedriver.exe&#39;, chrome_options=chrome_options) elif platform.system() == &quot;Linux&quot;: chrome_options.add_argument(&quot;--headless&quot;) chrome_options.add_argument(&#39;--disable-gpu&#39;) chrome_options.add_argument(&#39;--no-sandbox&#39;) driver = webdriver.Chrome( executable_path=&quot;/usr/bin/chromedriver&quot;, chrome_options=chrome_options) # driver.set_window_size(500, 200) data = [&quot;游戏&quot;, &quot;期货&quot;, &quot;贷款&quot;] for kw in data: url = &quot;https://www.zhipin.com/c101190400/?query={}&quot;.format(kw) driver.get(url) time.sleep(2) # 获取信息 last_url = driver.current_url source = etree.HTML(driver.page_source) links = source.xpath(&quot;//div[@class=&#39;job-primary&#39;]/div[@class=&#39;info-primary&#39;]//a/@href&quot;) global nodes nodes = list(map(lambda x: &quot;https://www.zhipin.com{}&quot;.format(x), links)) while len(source.xpath(&#39;//div[@class=&quot;page&quot;]/a[@class=&quot;next&quot; and @ka=&quot;page-next&quot;]&#39;)) == 1: next_page = driver.find_element_by_xpath( &#39;//div[@class=&quot;page&quot;]/a[@class=&quot;next&quot; and @ka=&quot;page-next&quot;]&#39;) WebDriverWait(driver, 10).until(expected_conditions.element_to_be_clickable( (By.XPATH, &#39;//div[@class=&quot;page&quot;]/a[@class=&quot;next&quot; and @ka=&quot;page-next&quot;]&#39;))) current_url = driver.current_url while last_url == current_url: self.loop_try(next_page) last_url = driver.current_url print(driver.current_url) source = etree.HTML(driver.page_source) new_links = source.xpath(&quot;//div[@class=&#39;job-primary&#39;]/div[@class=&#39;info-primary&#39;]//a/@href&quot;) new_nodes = list(map(lambda x: &quot;https://www.zhipin.com{}&quot;.format(x), new_links)) nodes.extend(new_nodes) yield Request(url=&quot;https://www.zhipin.com&quot;, callback=self.parse_detail, meta={&#39;params&#39;: (nodes, driver, kw)},dont_filter=True) def parse_detail(self,response): nodes, driver, kw = response.meta.get(&quot;params&quot;) for node in nodes: print(node) driver.execute_script(&quot;window.open(&#39;%s&#39;)&quot; % node) time.sleep(2) driver.switch_to.window(driver.window_handles[1]) WebDriverWait(driver, timeout=10).until( EC.presence_of_element_located((By.XPATH, &quot;//div[@class=&#39;detail-content&#39;]&quot;)) ) html = etree.HTML(driver.page_source) driver.close() driver.switch_to.window(driver.window_handles[0]) item = JobItem() item[&#39;recruitment_position&#39;] = html.xpath( &quot;//div[@class=&#39;job-primary detail-box&#39;]/div[@class=&#39;info-primary&#39;]/div[@class=&#39;name&#39;]/h1/text()&quot;)[0] item[&#39;salary&#39;] = html.xpath( &quot;//div[@class=&#39;job-primary detail-box&#39;]/div[@class=&#39;info-primary&#39;]/div[@class=&#39;name&#39;]/span/text()&quot;)[ 0] item[&#39;keyword&#39;] = kw item[&#39;url&#39;] = node item[&#39;source&#39;] = &quot;BOSS直聘&quot; item[&#39;update_date&#39;] = html.xpath(&#39;//div[@class=&quot;sider-company&quot;]/p[last()]/text()&#39;)[0] item[&#39;company_name&#39;] = html.xpath(&#39;//a[@ka=&quot;job-detail-company_custompage&quot;]&#39;)[0].attrib.get(&#39;title&#39;).strip().replace(&quot;\n招聘&quot;,&quot;&quot;) # item[&#39;company_name&#39;] = html.xpath(&#39;//div[@class=&quot;level-list&quot;]/preceding-sibling::div[1]/text()&#39;)[0] item[&#39;work_experience&#39;] = html.xpath(&#39;//*[@class=&quot;job-primary detail-box&quot;]/div[2]/p/text()&#39;)[1] item[&#39;education_background&#39;] = html.xpath(&#39;//*[@class=&quot;job-primary detail-box&quot;]/div[2]/p/text()&#39;)[2] item[&#39;job_requirements&#39;] = &quot;&quot;.join( html.xpath(&#39;//div[@class=&quot;detail-content&quot;]/div[@class=&quot;job-sec&quot;]/div[@class=&quot;text&quot;]/text()&#39;)) item[&#39;company_info&#39;] = &quot;&quot;.join( html.xpath(&#39;//div[@class=&quot;job-sec company-info&quot;]//div[@class=&quot;text&quot;]/text()&#39;)) item[&#39;company_address&#39;] = html.xpath(&#39;//*[@class=&quot;location-address&quot;]/text()&#39;)[0] item[&#39;company_welfare&#39;] = &quot;,&quot;.join(html.xpath( &#39;//div[@class=&quot;job-banner&quot;]/div[@class=&quot;job-primary detail-box&quot;]/div[@class=&quot;info-primary&quot;]/div[@class=&quot;tag-container&quot;]/div[@class=&quot;job-tags&quot;]/text()&#39;)) item[&#39;id&#39;] = get_md5(node) item[&#39;crawl_date&#39;] = datetime.now().strftime(&quot;%Y-%m-%d&quot;) yield item def loop_try(self,next_page): try: next_page.click() except: self.loop_try(next_page)]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
        <tag>selenium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python基础]]></title>
    <url>%2F2019%2F10%2F09%2Fpython%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[python基础 比心import time import os import math [([(time.sleep(a), print(&quot;\033[91m&quot;+i,end=&quot;&quot;,flush=True)) for i in (&#39;\n&#39;.join([&#39;&#39;.join([(&#39; I love U&#39;[(x-y)%9]if((x*0.05)**2+(y*0.1)**2-1)**3-(x*0.05)**2*(y*0.1)**3&lt;=0 else&#39; &#39;)for x in range(-30,30)])for y in range(15,-15,-1)]))] ,time.sleep(1/math.log(ai+3)), os.system(&#39;clear&#39;) ) for (ai,a) in enumerate([0.001,*[ 0.00001 ]*99])]![enter description here](https://www.github.com/OneJane/blog/raw/master/小书匠/1570610415940.png]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于selenium爬取智联招聘及国家企业信用信息公示系统]]></title>
    <url>%2F2019%2F09%2F18%2F%E5%9F%BA%E4%BA%8Eselenium%E7%88%AC%E5%8F%96%E6%99%BA%E8%81%94%E6%8B%9B%E8%81%98%E5%8F%8A%E5%9B%BD%E5%AE%B6%E4%BC%81%E4%B8%9A%E4%BF%A1%E7%94%A8%E4%BF%A1%E6%81%AF%E5%85%AC%E7%A4%BA%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[突破加密混淆的js文件，IP封锁，验证码识别（滑动和语序点击并存），useragent检查，多重url拼接cookie 智联招聘 通过获取链接返回的json数据拿到新的页面，selenium进行解析 class ZhilianSpider(scrapy.Spider): name = &#39;zhilian&#39; allowed_domains = [&#39;zhaopin.com&#39;] start_urls = [&#39;https://sou.zhaopin.com/&#39;] driver = None chrome_options = webdriver.ChromeOptions() # proxy_url = get_random_proxy() # print(proxy_url + &quot;代理服务器正在爬取&quot;) # chrome_options.add_argument(&#39;--proxy-server=https://&#39; + proxy_url.strip()) prefs = { &#39;profile.default_content_setting_values&#39;: { &#39;images&#39;: 1, # 不加载图片 &quot;User-Agent&quot;: UserAgent().random, # 更换UA } } chrome_options.add_experimental_option(&quot;prefs&quot;, prefs) if platform.system() == &quot;Windows&quot;: driver = webdriver.Chrome(&#39;chromedriver.exe&#39;, chrome_options=chrome_options) elif platform.system() == &quot;Linux&quot;: chrome_options.add_argument(&quot;--headless&quot;) chrome_options.add_argument(&#39;--disable-gpu&#39;) chrome_options.add_argument(&#39;--no-sandbox&#39;) driver = webdriver.Chrome( executable_path=&quot;/usr/bin/chromedriver&quot;, chrome_options=chrome_options) wait = WebDriverWait(driver, 15) def start_requests(self): data = [&quot;游戏&quot;, &quot;期货&quot;, &quot;贷款&quot;] for kw in data: yield Request( url=&quot;https://fe-api.zhaopin.com/c/i/sou?start=0&amp;pageSize=90&amp;cityId=639&amp;salary=0,0&amp;workExperience=-1&amp;education=-1&amp;companyType=-1&amp;employmentType=-1&amp;jobWelfareTag=-1&amp;kw=&quot; + kw + &quot;&amp;kt=3&quot;, meta={&quot;kw&quot;: kw}, callback=self.parse_pages) # response获取meta def parse_pages(self, response): numtotal = json.loads(response.text)[&quot;data&quot;][&quot;count&quot;] kw = response.meta.get(&quot;kw&quot;, &quot;游戏&quot;) for i in range(0, numtotal // 90 + 1): url = &quot;https://fe-api.zhaopin.com/c/i/sou?start=&quot; + str( 90 * i) + &quot;&amp;pageSize=90&amp;cityId=639&amp;salary=0,0&amp;workExperience=-1&amp;education=-1&amp;companyType=-1&amp;employmentType=-1&amp;jobWelfareTag=-1&amp;kw=&quot; + kw + &quot;&amp;kt=3&quot; yield Request( url=url, meta={&quot;kw&quot;: kw}, callback=self.parse) # response获取meta def parse(self, response): job_list = json.loads(response.text)[&quot;data&quot;][&quot;results&quot;] for job in job_list: yield Request(url=job[&quot;positionURL&quot;], callback=self.parse_detail, meta={&#39;cookiejar&#39;: &#39;chrome&#39;, &#39;kw&#39;: response.meta.get(&quot;kw&quot;, &quot;&quot;)}) def parse_detail(self, response): print(response.url) self.driver.get(response.url) self.driver.refresh() time.sleep(2) self.driver.implicitly_wait(20) dom = etree.HTML(self.driver.page_source) item = JobItem() item[&#39;recruitment_position&#39;] = null_if(dom.xpath(&#39;//*[@class=&quot;summary-plane__title&quot;]&#39;)) item[&#39;salary&#39;] = null_if(dom.xpath(&#39;//*[@class=&quot;summary-plane__salary&quot;]&#39;)) item[&#39;company_name&#39;] = dom.xpath(&#39;//*[@class=&quot;company__title&quot;]&#39;)[0].text item[&#39;work_experience&#39;] = dom.xpath(&#39;//ul[@class=&quot;summary-plane__info&quot;]/li[2]&#39;)[0].text item[&#39;education_background&#39;] = dom.xpath(&#39;//ul[@class=&quot;summary-plane__info&quot;]/li[3]&#39;)[0].text item[&#39;job_requirements&#39;] = remove_html( etree.tostring(dom.xpath(&#39;//div[@class=&quot;describtion__detail-content&quot;]&#39;)[0], encoding=&quot;utf-8&quot;).decode( &#39;utf-8&#39;)) item[&#39;company_info&#39;] = null_if(dom.xpath(&#39;//div[@class=&quot;company__description&quot;]&#39;)) item[&#39;company_address&#39;] = remove_html( etree.tostring(dom.xpath(&#39;//span[@class=&quot;job-address__content-text&quot;]&#39;)[0], encoding=&quot;utf-8&quot;).decode( &#39;utf-8&#39;)) if len(dom.xpath(&#39;//div[@class=&quot;highlights__content&quot;]&#39;)): item[&#39;company_welfare&#39;] = remove_html(etree.tostring(dom.xpath(&#39;//div[@class=&quot;highlights__content&quot;]&#39;)[0], encoding=&quot;utf-8&quot;).decode(&#39;utf-8&#39;)) else: item[&#39;company_welfare&#39;] = &#39;无&#39; item[&#39;id&#39;] = get_md5(self.driver.current_url) item[&#39;keyword&#39;] = response.meta.get(&quot;kw&quot;, &quot;&quot;) item[&#39;url&#39;] = response.url item[&#39;crawl_date&#39;] = datetime.now().strftime(&quot;%Y-%m-%d&quot;) yield item 国家企业信用信息系统获取cookiecrack.py class Crack(object): &quot;&quot;&quot; 同一ip频繁使用： 出现正常200但是没有结果 第一次解密出来是错误的 &quot;&quot;&quot; def __init__(self, url, test_url): path = os.getcwd() with open(os.path.join(path, &quot;wc_js.js&quot;), encoding=&#39;utf-8&#39;) as f: wc_js = f.read() self.wc_js = execjs.compile(wc_js) self.url = url self.test_url = test_url # 固定user_agent,后台使用user-agent验证cookies, 之后的访问也需要使用这个 self.headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.96 Safari/537.36&#39; } def acquire_js(self): &quot;&quot;&quot; 不带cookies请求首页，获得返回的js :return:页面中的js,和set_cookies中的jsluid &quot;&quot;&quot; response = requests.get(self.url, headers=self.headers) if response.status_code == 521: return response.text, response.headers[&#39;Set-Cookie&#39;].split(&#39;=&#39;)[1].split(&#39;;&#39;)[0] else: print(response.text) print(self.headers) return None, None def first_decryption(self, first_js): &quot;&quot;&quot; 解密js,获得第二层加密的js :param first_js: :return: &quot;&quot;&quot; x = re.findall(&#39;var x=&quot;(.*?)&quot;&#39;, first_js)[0] y = re.findall(&#39;,y=&quot;(.*?)&quot;&#39;, first_js)[0] second_js = self.wc_js.call(&#39;once_js&#39;, x, y) # second_js = self.wc_js.call(&#39;get_js&#39;, x, y, z) return second_js def regex(self, js): regex = &quot;!*window\[.*?\]&quot; find = re.findall(regex, js) if find: for f in find: if &#39;!&#39; in f: if len(re.findall(&#39;!&#39;, f)) % 2 == 0: js = js.replace(f, &#39;false&#39;) else: js = js.replace(f, &#39;true&#39;) else: js = js.replace(f, &#39;undefined&#39;) js = js.replace(&#39;window.headless&#39;, &#39;undefined&#39;) return js def replace_url(self, js): # 替换1 # 取出两个变量名 _3d = re.findall(&quot;(var .{0,5}=)document\.createElement\(&#39;div&#39;\);&quot;, js) _2b = re.findall(&quot;(var .{0,5}=).{0,5}\.match\(/https\?:\\\/\\\//\)\[0\];&quot;, js) # 替换成要访问的url js = re.sub(&quot;var .{0,5}=document\.createElement\(&#39;div&#39;\);&quot;, _3d[0] + f&#39;&quot;{self.url.replace(&quot;http://&quot;, &quot;&quot;)}&quot;;&#39;, js) js = re.sub(&quot;_.{0,5}\.innerHTML=&#39;&lt;a href=.{0,25}&lt;/a&gt;&#39;;&quot;, &quot;&quot;, js) js = re.sub(&quot;_.{0,5}=.{0,5}\.firstChild\.href;&quot;, &quot;&quot;, js) js = re.sub(&quot;var .{0,5}=.{0,5}\.match\(/https\?:\\\/\\\//\)\[0\];&quot;, _2b[0] + &#39;&quot;http://&quot;;&#39;, js) js = re.sub(&quot;_.{0,5}=.{0,5}\.substr\(.{0,5}\.length\)\.toLowerCase\(\);&quot;, &quot;&quot;, js) return js def second_decryption(self, second_js): &quot;&quot;&quot; 把第二层js准换成本地可以运行的js !!!此处可能会出错!!! :param second_js: 第一次解密的js :return: __jsl_clearance的值 &quot;&quot;&quot; # 转义字符 js = second_js.replace(&#39;\\\\&#39;, &#39;\\&#39;) # 切割 js = &#39;cookie&#39; + js.split(&#39;document.cookie&#39;)[1] js = js.split(&#39;GMT;Path=/;&#39;)[0] + &quot;&#39;&quot; if re.findall(&quot;(var .{0,5}=)document\.createElement\(&#39;div&#39;\);&quot;, js): js = self.replace_url(js) # 替换可能出现的window js = self.regex(js) s = &quot;&quot;&quot; function cook() { %s return cookie } &quot;&quot;&quot; new_js = s % js ctx = execjs.compile(new_js) # 切割获得的__jsl_clearance jsl = ctx.call(&#39;cook&#39;) jsl = jsl.split(&#39;;&#39;)[0] jsl_clearance = jsl.split(&#39;=&#39;)[1] return jsl_clearance def test_cookies(self, jsluid, jsl_clearance): &quot;&quot;&quot; 带cookies访问,测试拿到的是否正确 :param jsluid:cookies中的参数 :param jsl_clearance: cookies中的参数 :return: &quot;&quot;&quot; headers = self.headers.copy() headers[&#39;Cookie&#39;] = f&#39;__jsluid_h={jsluid}; __jsl_clearance={jsl_clearance};&#39; response = requests.get(self.test_url, headers=headers) print(response.text) return response.status_code def run(self): while True: first_js, jsluid = self.acquire_js() second_js = self.first_decryption(first_js) try: jsl_clearance = self.second_decryption(second_js) except: # print(second_js) continue else: code = self.test_cookies(jsluid, jsl_clearance) if code == 200: return jsluid, jsl_clearance else: print(code) # print(second_js) continue if __name__ == &#39;__main__&#39;: # # 企业信息公示系统 url = &quot;http://www.gsxt.gov.cn/index.html&quot; test_url = &quot;http://www.gsxt.gov.cn/index.html&quot; # # 66代理 # url = &quot;http://www.66ip.cn/2.html&quot; # test_url = &quot;http://www.66ip.cn/2.html&quot; # # 公安部网站 # url = &#39;http://www.mps.gov.cn/&#39; # test_url = &#39;http://www.mps.gov.cn/&#39; ck = Crack(url, test_url) jsluid, jsl_clearance = ck.run() print(&#39;jsluid:&#39;, jsluid) print(&#39;jsl_clearance:&#39;, jsl_clearance) 利用超级鹰破解验证码class SearchResultParse(object): &#39;&#39;&#39;查询结果页解析 &#39;&#39;&#39; def __init__(self, pagesource, base_url, parse_rule): self.selector = etree.HTML(pagesource) self.url_list = [] self.base_url = base_url self.parse_rule = parse_rule[&#39;search_result_url&#39;] def search_result_parse(self): self.url_list = [self.base_url + i for i in self.selector.xpath(self.parse_rule)] return self.url_list class PageDetailParse(object): &#39;&#39;&#39;详情页解析 &#39;&#39;&#39; def __init__(self, pagesource, parse_rule): self.selector = etree.HTML(pagesource) self.parse_rule = parse_rule self.info_list = {} def search_result_parse(self, primary_info=None): if primary_info is None: primary_info = [] for i in self.parse_rule[&#39;primaryinfo&#39;]: primary_info.append( self.selector.xpath(i).replace(&quot;\n&quot;, &quot;&quot;).replace(&quot;\t&quot;, &quot;&quot;).replace(&quot;\r&quot;, &quot;&quot;).replace(&quot; &quot;, &quot;&quot;)) self.info_list[&#39;primary_info&#39;] = primary_info return self.info_list class CookieRequest(object): &#39;&#39;&#39;带cookie访问查询结果 &#39;&#39;&#39; def __init__(self, url_list=None): &#39;&#39;&#39;设置requests中的session的cookie &#39;&#39;&#39; self.url_list = url_list self.session = requests.Session() self.result = [] self.headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.96 Safari/537.36&#39; } def cookie_requests(self): &#39;&#39;&#39;带cookie依次访问各个查询结果 &#39;&#39;&#39; url = &quot;http://www.gsxt.gov.cn/index.html&quot; test_url = &quot;http://www.gsxt.gov.cn/corp-query-entprise-info-hot-search-list.html?province=100000&quot; ck = Crack(url, test_url) jsluid, jsl_clearance, JSESSIONID = ck.run() self.headers[&#39;Cookie&#39;] = f&#39;__jsluid_h={jsluid}; __jsl_clearance={jsl_clearance};JSESSIONID={JSESSIONID}&#39; for url in self.url_list: response = self.session.get(url=url, headers=self.headers) self.result.append(response.text) time.sleep(5) return self.result class MaxEnterError(Exception): &#39;&#39;&#39;输入关键字最大尝试次数 &#39;&#39;&#39; def __init__(self, ErrorInfo): super().__init__(self) # 初始化父类 self.errorinfo = ErrorInfo def __str__(self): return self.errorinfo class GtClickShot(object): def __init__(self, username, password,soft_id): &#39;&#39;&#39;初始化超级鹰 softid已固化到程序 args: username(str):超级鹰普通用户名 password(str):超级鹰密码 &#39;&#39;&#39; self.username = username self.password = md5(password.encode(&quot;utf-8&quot;)).hexdigest() self.soft_id = soft_id self.base_params = { &#39;user&#39;: self.username, &#39;pass2&#39;: self.password, &#39;softid&#39;: self.soft_id, } self.headers = { &#39;Connection&#39;: &#39;Keep-Alive&#39;, &#39;User-Agent&#39;: &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)&#39;, } def PostPic(self, im, codetype): &quot;&quot;&quot;发送图片至打码平台 args： im(Byte): 图片字节 codetype(str): 题目类型 参考 http://www.chaojiying.com/price.html return(json):返回打码信息，包含坐标信息，坐标信息用“|”隔开 &quot;&quot;&quot; params = { &#39;codetype&#39;: codetype, } params.update(self.base_params) files = {&#39;userfile&#39;: (&#39;ccc.jpg&#39;, im)} r = requests.post(&#39;http://upload.chaojiying.net/Upload/Processing.php&#39;, data=params, files=files, headers=self.headers) return r.json() def ReportError(self, im_id): &quot;&quot;&quot;识别错误返回题分 args： im_id(str):报错题目的图片ID return(str):报错反馈 &quot;&quot;&quot; params = { &#39;id&#39;: im_id, } params.update(self.base_params) r = requests.post(&#39;http://upload.chaojiying.net/Upload/ReportError.php&#39;, data=params, headers=self.headers) return r.json() class CorpSearch(object): def __init__(self, init_url, index_url, headers, max_click): &#39;&#39;&#39;初始化 args: init_url:初始化url,加速乐反爬JS要求访问目标网站前需先访问初始化url获取gt和challenge index_url:目标网站首页url headers：请求头信息 max_click：最大循环点击次数为了应对点击不灵敏，设置循环检查点击。 self.wait:默认条件等待最大时间 self.click_valitimes:点击验证次数，大于0时需返回题分，等于0时不需要 &#39;&#39;&#39; chrome_options = webdriver.ChromeOptions() prefs = { &#39;profile.default_content_setting_values&#39;: { &#39;images&#39;: 1, # 加载图片 &quot;User-Agent&quot;: UserAgent().random, # 更换UA } } chrome_options.add_experimental_option(&quot;prefs&quot;, prefs) self.init_url = init_url self.index_url = index_url if platform.system() == &quot;Windows&quot;: self.driver = webdriver.Chrome(&#39;chromedriver.exe&#39;, chrome_options=chrome_options) elif platform.system() == &quot;Linux&quot;: chrome_options.add_argument(&quot;--headless&quot;) chrome_options.add_argument(&#39;--disable-gpu&#39;) chrome_options.add_argument(&#39;--no-sandbox&#39;) self.driver = webdriver.Chrome( executable_path=&quot;/usr/bin/chromedriver&quot;, chrome_options=chrome_options) self.wait = WebDriverWait(self.driver, 50) self.max_entertimes = max_click self.click_valitimes = 0 self.action = ActionChains(self.driver) self.gt_shot = GtClickShot(&quot;zhaoys&quot;, &quot;501314&quot;,&quot;901554&quot;) self.options = webdriver.ChromeOptions() self.headers = headers for option in self.headers: self.options.add_argument(option) # 初始化页面，绕过过加速乐反爬，获取gt和challenge,并加载进入首页 def init(self): &#39;&#39;&#39; 请求初始化网站，并进入首页 &#39;&#39;&#39; self.driver.get(self.init_url) self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, &quot;body &gt; pre:nth-child(1)&quot;))) self.driver.get(self.index_url) # 加载首页，输入查询关键词，点击查询按钮 # 如果点击按钮失效,自动重新回车，并设定最大回车次数，一旦超过设定值，抛出异常，结束程序 def input_query(self, keyword): &#39;&#39;&#39;输入关键词进行查询 args: keyword:查询关键词 return: 仅用于方法返回 &#39;&#39;&#39; enter_word = self.wait.until(EC.presence_of_element_located((By.ID, &quot;keyword&quot;))) self.wait.until(EC.presence_of_element_located((By.ID, &quot;btn_query&quot;))) time.sleep(random.randint(8, 15) / 10) enter_word.send_keys(keyword) time.sleep(random.randint(5, 10) / 10) enter_word.send_keys(Keys.ENTER) while True: if self.max_entertimes == 0: raise MaxEnterError(&#39;---Out of max times on the search enter---&#39;) gt_panel = self.driver.find_element_by_css_selector(&quot;body &gt; div.geetest_panel.geetest_wind&quot;) style_value = gt_panel.value_of_css_property(&quot;display&quot;) if style_value.strip() == &quot;block&quot;: break else: enter_word.send_keys(Keys.ENTER) time.sleep(random.randint(1, 5) / 10) self.max_entertimes -= 1 return # 判断页面中是否包含某个元素，注意是class_name def is_element_exist(self, class_name): &#39;&#39;&#39;判断某个元素是否存在 args: class_name:元素class属性名称 return: 存在(True),不存在(False) &#39;&#39;&#39; try: self.driver.find_element_by_class_name(class_name) return True except: return False # 屏幕截图，并将截图内容读入内存，加速计算操作 def get_screenshot(self): &#39;&#39;&#39;屏幕截图 return: 返回截图 &#39;&#39;&#39; screenshot = self.driver.get_screenshot_as_png() screenshot = Image.open(BytesIO(screenshot)) return screenshot # 获取验证验证码图片的位置，用于裁图 def get_position(self, pos_img): &#39;&#39;&#39;验证图片的坐标尺寸信息 args: pos_img:验证码定位点元素 return: 验证码定位点的坐标信息，注意依次为：左底，左高，右高，右底 &#39;&#39;&#39; location = pos_img.location size = pos_img.size top, bottom, left, right = location[&#39;y&#39;], location[&#39;y&#39;] + size[&#39;height&#39;], location[&#39;x&#39;], location[&#39;x&#39;] + size[ &#39;width&#39;] return (left, top, right, bottom) # 对于滑块验证码，获取完整的和缺块的验证码图片截图 def get_slide_images(self): &#39;&#39;&#39;获取有缺口和没缺口的图片 &#39;&#39;&#39; canvas_img = self.wait.until( EC.presence_of_element_located((By.CSS_SELECTOR, &quot;.geetest_canvas_img.geetest_absolute &gt; div&quot;))) position = self.get_position(canvas_img) befor_screenshot = self.get_screenshot() befor_img = befor_screenshot.crop(position) befor_img.save(&quot;befor_click.png&quot;) btn_slide = self.wait.until(EC.presence_of_element_located((By.CLASS_NAME, &quot;geetest_slider_button&quot;))) self.action.click_and_hold(btn_slide).perform() after_screenshot = self.get_screenshot() after_img = after_screenshot.crop(position) after_img.save(&quot;after_click.png&quot;) # 获取缺口位置，计算滑动距离（灰度化，求差值，阈值去燥，计算缺口位置，计算滑动距离） def get_slide_distance(self): &#39;&#39;&#39;获取滑动距离 return: 返回滑动距离 &#39;&#39;&#39; befor_click_img = &quot;F:\\Anaconda3\\Lib\\captcha\\gt_validate\\befor_click.png&quot; after_click_path = &quot;F:\\Anaconda3\\Lib\\captcha\\gt_validate\\after_click.png&quot; befor_img = cv2.imread(befor_click_img) after_img = cv2.imread(after_click_path) befor_gray = cv2.cvtColor(befor_img, cv2.COLOR_BGR2GRAY) after_gray = cv2.cvtColor(after_img, cv2.COLOR_BGR2GRAY) img_diff = np.array(befor_gray) - np.array(after_gray) height, width = img_diff.shape for i in range(height): for j in range(width): if img_diff[i][j] &gt; 245 or img_diff[i][j] &lt; 60: img_diff[i][j] = 0 start_position = random.choice([4, 5, 6]) reshape_img = img_diff.T sum_color = list(map(lambda x: sum(x), reshape_img)) for i in range(1, len(sum_color)): if sum_color[i] &gt; 1000 and i &gt; 60: end_position = i break slide_distance = end_position - start_position return slide_distance # 模拟鼠标轨迹，按照开始慢加速（2），中间快加速（5），后面慢加速（2），最后慢减速的方式（1） # 返回值是x值与Y值坐标以及sleep时间截点，起始中间最后都要sleep def get_track(self, distance, track_list=None): &#39;&#39;&#39;获取滑动轨迹 args: distance:滑动距离 kargs: Track_list:滑动轨迹，初始化为空 return: 滑动轨迹，断点位置(2处) &#39;&#39;&#39; if track_list is None: track_list = [] base = distance / 10 x1 = round(base * 2) x2 = round(base * 5) x3 = x1 x4 = distance - x1 - x2 - x3 ynoise_num = random.randint(5, 10) y1 = [random.randint(-2, 2) for _ in range(ynoise_num)] yrdm = list(set(random.choice(range(distance)) for _ in range(ynoise_num))) x = [1] * distance y = [0] * distance for i, j in enumerate(yrdm): y[j] = y1[i] t1 = sorted([random.randint(8, 13) / 1000 for _ in range(x1)], reverse=True) t2 = sorted([random.randint(1, 8) / 1000 for _ in range(x2)], reverse=True) t3 = sorted([random.randint(8, 13) / 1000 for _ in range(x3)], reverse=True) t4 = sorted([random.randint(12, 20) / 1000 for _ in range(x4)]) t = t1 + t2 + t3 + t4 for i in (zip(x, y, t)): track_list.append(i) return (track_list, x1 + x2, x1 + x2 + x3) # 对于点击验证码，获取验证码的校验文字和待点击图片截图,以及验证码弹框元素 def get_click_images(self): &#39;&#39;&#39;获取需点击的图片 return: 需点击坐标的图片， 提示图片(用于调试打码时的计算点击次数)， 验证码图片定位元素(用于定位鼠标位置并计算相对坐标) &#39;&#39;&#39; click_img_element = self.wait.until(EC.presence_of_element_located((By.CLASS_NAME, &quot;geetest_widget&quot;))) self.wait.until(EC.presence_of_element_located((By.CLASS_NAME, &quot;geetest_item_img&quot;))) time.sleep(random.randint(1, 5) / 10) click_position = self.get_position(click_img_element) all_screenshot = self.get_screenshot() click_img = all_screenshot.crop(click_position) click_img.save(&quot;click_img.png&quot;) tip_img = self.wait.until(EC.presence_of_element_located((By.CLASS_NAME, &quot;geetest_tip_img&quot;))) tip_position = self.get_position(tip_img) tip_img = all_screenshot.crop(tip_position) tip_img.save(&quot;tip_img.png&quot;) return (click_img, tip_img, click_img_element) # 计算要点击的字符数量，灰度化，反向二值化,转置，沿X坐标对Y求和，判断分割点数量，判断字符数量 def cal_char_num(self, char_img_path): &#39;&#39;&#39;计算需点击的字符数量 args: char_img_path:提示图片的存储路径 return: 点击次数 &#39;&#39;&#39; flag = 0 origin_img = cv2.imread(char_img_path) gray_img = cv2.cvtColor(origin_img, cv2.COLOR_BGR2GRAY) ret, thresh1 = cv2.threshold(gray_img, 127, 255, cv2.THRESH_BINARY_INV) transpos_img = np.array(thresh1).T result = list(map(lambda x: sum(x), transpos_img)) for i in range(len(result) - 3): if result[i] == 0 and result[i + 1] == 0 and result[i + 2] &gt; 0: flag += 1 return flag # 返回验证码字符的坐标，每个点击点的坐标,并转化为整数坐标 def char_absolute_coord(self, img, num, coord=None): &#39;&#39;&#39;调试用，点击验证码图片返回整数值坐标 args: img:验证码图片 num：点击次数 kargs: coord:验证码字符坐标 return: 字符坐标 &#39;&#39;&#39; if coord is None: coord = [] img = Image.open(img) plt.imshow(img) points = plt.ginput(num) plt.close() for i in points: x_co, y_co = i coord.append((round(x_co), round(y_co))) return coord # 返回从起点开始依次到每个点击文字的相对位置，形式为[(xoffset,yoffset),(),(),...] def get_offset_coord(self, absolute_coord, click_track=None): &#39;&#39;&#39;获取相邻点击字符的相对坐标，用于鼠标移动点击 args: absolute_coord：验证码字符的绝对坐标 kargs: click_track:每个需点击字符间的相对坐标或位移 return: 相对坐标或位移 &#39;&#39;&#39; if click_track is None: click_track = [] for i, j in enumerate(absolute_coord): if i == 0: click_track.append(j) else: click_track.append((j[0] - absolute_coord[i - 1][0], j[1] - absolute_coord[i - 1][1])) return click_track # 验证点击验证码,获取验证码数量，人工点击，按照计算的坐标相对偏移位置，依次点击文字进行验证 # 通过打码平台，将验证码图片发送后返回坐标信息，通过超级鹰打码平台 def click_captcha_validate(self): &#39;&#39;&#39;根据打码平台返回的坐标进行验证 return: 仅仅用于方法返回 &#39;&#39;&#39; click_img, tip_img, click_img_element = self.get_click_images() bytes_array = BytesIO() click_img.save(bytes_array, format=&quot;PNG&quot;) coord_result = self.gt_shot.PostPic(bytes_array.getvalue(), &quot;9005&quot;) print(coord_result) groups = coord_result.get(&quot;pic_str&quot;).split(&#39;|&#39;) if groups == &quot;&quot;: raise RuntimeError(&quot;打码超时&quot;) pic_id = coord_result.get(&quot;pic_id&quot;) points = [[int(num) for num in group.split(&#39;,&#39;)] for group in groups] # tip_img_path=&quot;D:\\Anaconda3\\Lib\\captcha\\gt_validate\\tip_img.png&quot; # click_img_path=&quot;D:\\Anaconda3\\Lib\\captcha\\gt_validate\\click_img.png&quot; # num=self.cal_char_num(tip_img_path) # points=self.char_absolute_coord(click_img_path,num) mouse_track = self.get_offset_coord(points) print(mouse_track) self.action.move_to_element_with_offset(click_img_element, 0, 0) for position in mouse_track: self.action.move_by_offset(position[0], position[1]) self.action.click() self.action.pause(random.randint(3, 7) / 10) self.action.perform() time.sleep(random.randint(4, 6) / 10) click_submit_btn = self.wait.until(EC.presence_of_element_located((By.CLASS_NAME, &#39;geetest_commit_tip&#39;))) click_submit_btn.click() self.action.reset_actions() self.valide_process(pic_id=pic_id) return # 验证滑动验证码，获取滑动距离和滑动轨迹，分别在起始，中间，结束时随机停顿 def slide_captcha_validate(self): &#39;&#39;&#39;滑动验证码验证 return: 仅仅用于方法返回 &#39;&#39;&#39; self.get_slide_images() distance = self.get_slide_distance() track, p1, p2 = self.get_track(distance) time.sleep(random.randint(3, 7) / 10) for i, j in enumerate(track): if i == p1 or i == p2: time.sleep(random.randint(3, 7) / 10) self.action.move_by_offset(j[0], j[1]) time.sleep(j[2]) time.sleep(random.randint(3, 7) / 10) self.action.release() self.valide_process() return # 验证是否成功破解，设置重启机制 # 超过最大验证次数需点击“点击此处重试” def valide_process(self, pic_id=None): &#39;&#39;&#39;验证过程 1&gt;判断极验弹框消失且查询结果框出现，验证成功，结束验证； 2&gt;第一步验证失败，超时； 3&gt;超时原因：极验验证框没消失(跳转至第4步)或查询结果框没出现(跳转至第6步)； 4&gt;极验验证框没消失，检验是否超过最大验证次数，如果是，需点击重试，跳至第7步，如果不是，跳至第5步； 5&gt;如果不是，判断验证类型，调用响应验证方法，跳至第1步； 6&gt;如果查询结果框没出现，直接退出关闭浏览器； 7&gt;点击重试时，如果是空白响应则退出浏览器，或者判断验证类型，调用响应验证方法，跳至第1步。 args: cap_type:验证码类型 pic_id:点击类验证码图片id return: 要么验证成功，要么退出浏览器 &#39;&#39;&#39; try: WebDriverWait(self.driver, 3).until_not( EC.visibility_of_element_located((By.CSS_SELECTOR, &quot;body &gt; div.geetest_panel&quot;))) WebDriverWait(self.driver, 10).until(EC.visibility_of_element_located((By.ID, &quot;advs&quot;))) print(&quot;Validate Successful&quot;) return except TimeoutException: try: gt_panel_error = self.driver.find_element_by_css_selector( &quot;body &gt; div.geetest_panel.geetest_wind &gt; div.geetest_panel_box &gt; div.geetest_panel_error&quot;) error_display = gt_panel_error.value_of_css_property(&quot;display&quot;) if error_display.strip() == &quot;block&quot;: gt_panel_error_content = self.driver.find_element_by_css_selector( &quot;.geetest_panel_error &gt; div.geetest_panel_error_content&quot;) self.action.move_to_element(gt_panel_error_content).click().perform() self.action.reset_actions() try: WebDriverWait(self.driver, 3).until_not( EC.visibility_of_element_located((By.CSS_SELECTOR, &quot;body &gt; div.geetest_panel&quot;))) WebDriverWait(self.driver, 10).until(lambda x: x.find_element_by_id(&#39;advs&#39;).is_displayed()) print(&quot;Validate Successful&quot;) return except TimeoutException: self.slide_orclick_validate(pic_id) else: self.slide_orclick_validate(pic_id) except: print(&#39;error occured&#39;) return # 判断是执行点击还是滑块 def slide_orclick_validate(self, pic_id=None): &#39;&#39;&#39;判断下一步是选择滑动验证还是点击验证还是退出浏览器 args: pic_id:点击类验证码图片id return: 要么滑动验证，要么点击验证，要么None &#39;&#39;&#39; try: WebDriverWait(self.driver, 3).until(EC.presence_of_element_located((By.CLASS_NAME, &quot;geetest_close&quot;))) print(&#39;Validate Failed,retry again&#39;) if self.is_element_exist(&quot;geetest_canvas_img&quot;): print(&#39;captcha type is slide&#39;) return self.slide_captcha_validate() else: print(&#39;captcha type is click&#39;) if self.click_valitimes &gt; 0: self.gt_shot.ReportError(pic_id) self.click_valitimes += 1 return self.click_captcha_validate() except: print(&quot;Directly no click or slide validate&quot;) return # 带cookie切换至首页继续检索 def switch_hmpg(self): &#39;&#39;&#39;由结果页切换至首页 return: 用于方法返回 &#39;&#39;&#39; self.wait.until(EC.presence_of_element_located((By.ID, &quot;advs&quot;))) hmpg_btn = self.driver.find_element_by_css_selector( &quot;body &gt; div.container &gt; div.header_box &gt; div &gt; div &gt; a:nth-child(1)&quot;) self.action.move_to_element(hmpg_btn).click().perform() self.action.reset_actions() self.wait.until(lambda x: x.find_element_by_id(&#39;btn_query&#39;).is_displayed()) return # 通过index界面或者点击首页继续检索时的爬取步骤 def main(self, keyword, start_pg=None): &#39;&#39;&#39;操作主程序 args: keyword:查询关键词 kargs: start_pg:是否需要初始化访问加速乐，默认要 &#39;&#39;&#39; if start_pg == &quot;homepage&quot;: self.switch_hmpg() else: self.init() self.input_query(keyword) self.slide_orclick_validate() # 保存cookie和检索结果，用于requests及详情解析 def to_dict(self): &#39;&#39;&#39;保存cookie（用于requests请求及详情解析）和查询结果 args: cookie_name:cookie文件名称 &#39;&#39;&#39; htmlpage = self.driver.page_source return { &#39;page&#39;: htmlpage } if __name__ == &#39;__main__&#39;: init_url = &quot;http://www.gsxt.gov.cn/SearchItemCaptcha&quot; index_url = &quot;http://www.gsxt.gov.cn/index.html&quot; base_url = &#39;http://www.gsxt.gov.cn&#39; result_parse_rule = {&#39;search_result_url&#39;: &#39;//*[@id=&quot;advs&quot;]/div/div[2]/a/@href&#39;} detail_parse_rule = { &#39;primaryinfo&#39;: [&#39;string(//*[@id=&quot;primaryInfo&quot;]/div/div[@class=&quot;overview&quot;]/dl[{}])&#39;.format(i) for i in range(15)], } max_click = 10 chm_headers = [&#39;Host=&quot;www.gsxt.gov.cn&quot;&#39;, &#39;Connection=&quot;keep-alive&quot;&#39;, &#39;User-Agent=&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36&quot;&#39;, &#39;Upgrade-Insecure-Requests=1&#39;, &#39;Accept=&quot;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&quot;&#39;, &#39;Accept-Encoding=&quot;gzip, deflate&quot;&#39;, &#39;Accept-Language=&quot;zh-CN,zh;q=0.9&quot;&#39;] search = CorpSearch(init_url, index_url, chm_headers, max_click) search.main(&quot;腾讯&quot;) cookie_html = search.to_dict() search_result = SearchResultParse(cookie_html[&#39;page&#39;], base_url, result_parse_rule) url_list = search_result.search_result_parse() detail_request = CookieRequest(url_list=url_list) detail_result = detail_request.cookie_requests() for pg in detail_result: pg_detail = PageDetailParse(pg, detail_parse_rule) detail = pg_detail.search_result_parse() m = re.findall(r&#39;\[(.*?)\]&#39;, str(detail)) info_list = m[0].replace(&#39;\&#39;&#39;, &#39;&#39;).split(&#39;, &#39;) sql = &quot;insert into company(code,name,type,start,end,) values(%s,%s,%s,%s.%s)&quot; count, rt_list = MysqlConnection.execute_sql(sql, (info_list[0],info_list[1],info_list[2],info_list[3]))爬虫实现class EnterPriseSpider(scrapy.Spider): name = &#39;enterprise&#39; allowed_domains = [&#39;gsxt.gov.cn&#39;] start_urls = [&#39;http://www.gsxt.gov.cn/index.html&#39;] def __init__(self, word=None, *args, **kwargs): super(eval(self.__class__.__name__), self).__init__(*args, **kwargs) self.word = word def start_requests(self): init_url = &quot;http://www.gsxt.gov.cn/SearchItemCaptcha&quot; index_url = &quot;http://www.gsxt.gov.cn/index.html&quot; base_url = &#39;http://www.gsxt.gov.cn&#39; result_parse_rule = {&#39;search_result_url&#39;: &#39;//*[@id=&quot;advs&quot;]/div/div[2]/a/@href&#39;} max_click = 10 chm_headers = [&#39;Host=&quot;www.gsxt.gov.cn&quot;&#39;, &#39;Connection=&quot;keep-alive&quot;&#39;, &#39;User-Agent=&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36&quot;&#39;, &#39;Upgrade-Insecure-Requests=1&#39;, &#39;Accept=&quot;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&quot;&#39;, &#39;Accept-Encoding=&quot;gzip, deflate&quot;&#39;, &#39;Accept-Language=&quot;zh-CN,zh;q=0.9&quot;&#39;] search = CorpSearch(init_url, index_url, chm_headers, max_click) search.main(self.word) cookie_html = search.to_dict() search_result = SearchResultParse(cookie_html[&#39;page&#39;], base_url, result_parse_rule) url_list = search_result.search_result_parse() yield Request(url=&quot;https://www.baidu.com/&quot;,callback=self.parse, meta={&#39;url_list&#39;: url_list}) def parse(self, response): detail_parse_rule = { &#39;primaryinfo&#39;: [&#39;string(//*[@id=&quot;primaryInfo&quot;]/div/div[@class=&quot;overview&quot;]/dl[{}])&#39;.format(i) for i in range(15)], } url_list = response.meta.get(&quot;url_list&quot;, &quot;&quot;) detail_request = CookieRequest(url_list=url_list) detail_result = detail_request.cookie_requests() for pg in detail_result: pg_detail = PageDetailParse(pg, detail_parse_rule) detail = pg_detail.search_result_parse() m = re.findall(r&#39;\[(.*?)\]&#39;, str(detail)) info_list = m[0].replace(&#39;\&#39;&#39;, &#39;&#39;).split(&#39;, &#39;) item = CompanyItem() item[&#39;name&#39;] = company_info(info_list, &quot;企业名称：&quot;) item[&#39;code&#39;] = company_info(info_list, &quot;统一社会信用代码：&quot;) item[&#39;type&#39;] = company_info(info_list, &quot;类型：&quot;) start = company_info(info_list, &quot;营业期限自：&quot;) partner_start = company_info(info_list, &quot;合伙期限自：&quot;) item[&#39;start&#39;] = start if &quot;无&quot; == partner_start else partner_start end = company_info(info_list, &quot;合伙期限自：&quot;) partner_end = company_info(info_list, &quot;合伙期限至：&quot;) item[&#39;end&#39;] = end if &quot;无&quot; == partner_end else partner_end item[&#39;capital&#39;] = company_info(info_list, &quot;注册资本：&quot;) item[&#39;owner&#39;] = company_info(info_list, &quot;法定代表人：&quot;) item[&#39;establish&#39;] = company_info(info_list, &quot;成立日期：&quot;) item[&#39;registration&#39;] = company_info(info_list, &quot;登记机关：&quot;) item[&#39;check&#39;] = company_info(info_list, &quot;核准日期：&quot;) item[&#39;status&#39;] = company_info(info_list, &quot;登记状态：&quot;) residence = company_info(info_list, &quot;住所：&quot;) premises = company_info(info_list, &quot;主要经营场所：&quot;) item[&#39;address&#39;] = residence if &quot;无&quot; == premises else premises item[&#39;scope&#39;] = company_info(info_list, &quot;经营范围：&quot;) item[&#39;partner&#39;] = company_info(info_list, &quot;执行事务合伙人:&quot;) yield item main.pyfrom scrapy import cmdline from scrapy.cmdline import execute import sys import os sys.path.append(os.path.dirname(os.path.abspath(__file__))) # execute([&quot;scrapy&quot;, &quot;crawl&quot;, &quot;enterprise&quot;,&quot;-a&quot;,&quot;word=百度&quot;]) execute([&quot;scrapy&quot;, &quot;crawl&quot;, &quot;zhilian&quot;]) 安装chromewget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl https://intoli.com/install-google-chrome.sh | bash ldd /opt/google/chrome/chrome | grep &quot;not found&quot;]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
        <tag>selenium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贪心nlp基础]]></title>
    <url>%2F2019%2F09%2F04%2F%E8%B4%AA%E5%BF%83nlp%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[贪心nlp基础 机器翻译 分词-&gt;翻译-&gt;排列组合-&gt;语言模型获取概率-&gt;获得结果 通过维特比算法将Translation Model和Language Model基于动态规划解决时间复杂度O(2^n)-&gt;O(n^p) 联合概率计算过程，马尔科夫假设N-gram 自然语言处理四个维度 声音(Phonetics): 语音识别 单词(Morphology): 分词，pos(part of speech词性标注)，NER(named entity recognition命名实体识别) 句子结构(Syntax):句法分析(Parsing依赖语言，CYK基于DP)，依存分析(dependency parsing词与词之间依赖关系) 语义分析(Sematic):NLU 时间复杂度和空间复杂度int a = 0, b = 0; for (i = 0; i &lt; N; i++) { # O(N)+O(N)=2*O(N)=O(N) a = a + rand();# N*1个操作 = O(N) b = b + rand();# N*1个操作 = O(N) } for (j = 0; j &lt; N/2; j++) { b = b + rand(); # N/2 *1个操作 = 1/2*O(N)=O(N) } 时间复杂度：O(N)空间复杂度： 2个单位的内存空间 = O(1) # constant space complexity int a = 0; i,j for (i = 0; i &lt; N; i++) { for (j = N; j &gt; i; j--) { a = a + i + j; } } i=0: j=N...1 N i=1: j=N...2 N-1 i=2: j=N...3 N-2 i=N-1: j=N 1 total = 1+2+3,...+N = N*(N+1)/2 = N*N/2 + N/2 = 1/2*O(N^2) + 1/2*O(N) = O(N^2) + O(N) = O(N^2) 时间复杂度：O(N^2);空间复杂度:3个单位的内存空间，不随程序变化而改变内存，O(1) int a = 0, i = N; while (i &gt; 0) { a += i; # 1个操作 i /= 2; #1个操作 } N = 40; i=40 i=20 2 i=10 2 i=5 2 i=2 2 i=1 2 i=0 2 terminate C* O(N) = O(N) if only if C跟N没有相关性 2*6=2*log(N) = 2* O(log N) = O(log N) 时间复杂度： O(log N) int i, j, k = 0; for (i = n / 2; i &lt;= n; i++) { for (j = 2; j &lt;= n; j = j * 2) { k = k + n / 2; } } 时间复杂度：O(n*log n) 当说算法X的效率要高于Y时指的是？ 假设存在一个足够大的数M，当n&gt;M时，我们可以保证X的实际效率要优于Y的实际效率n，比较时间复杂度O(1) O(log n) o(n) o(nlog n): quicksort, heapsort, mergesort o(n^2) o(n^3).. o(2^n) o(3^n)o(log n): 寻找一个element (从tree,heap), binary search 归并排序复杂度分析利用主定理公式 T(n) = T(n-2) + T(n-1) def fib(n): # base case if n &lt; 3: return 1 return fib(n-2)+fib(n-1) print (fib(50)) Fibonanci number (斐波那契数)计算时间复杂度和空间复杂度 import numpy as np def fib(n): tmp = np.zeros(n) tmp[0] = 1 tmp[1] = 1 for i in range(2,n): tmp[i] = tmp[i-2]+tmp[i-1] return tmp[n-1] # 时间复杂度O(N)不再是O(2^n) def fib(n): a,b=1,1 c =0 for i in range(2,n): c = a + b a = b b = c return c # 空间复杂度4，时间复杂度一样 通过DP动态规划改进时间复杂度和空间复杂度 搭建一个智能客服系统相似度匹配:正则(无数据),字符串相似度(训练数据)复杂度太高，通过倒排表作为过滤器 nlp系统流程 分词贪心算法缺点： 无法细分(细分更有解) 局部最优 效率低下(依赖于max_len) 歧义(不能考虑语义)语言模型原理：首先输入语句，根据词典生成所有可能的分词情况，通过语言模型计算每一个分词后的结果概率，选择概率最高分词语句。复杂度太高维特比算法(DP)根据词典生成所有可能的分词情况，通过语言模型计算每一个分词后的结果概率 两步合为一步。拼写纠错 错别字 输入语法有误过滤词通常会过滤掉停用词,出现频率很低的词汇。词形还原/词干提取 词形还原（lemmatization），是把一个任何形式的语言词汇还原为一般形式（能表达完整语义），而词干提取（stemming）是抽取词的词干或词根形式（不一定能够表达完整语义）。 文本表示Boolean Representation Count Based Representation 并非出现次数越多越重要，并非出现次数越少越不重要 Tf-idf Representation以上Representation均属于one-hot representation,无法表达单词之间的语义相似度。 Distributed Representation词向量 长度不依赖于词典，每个位置都有具体数值，通过模型训练得到分布式词向量，可以用来形容词之间的相似度，word2vec，某种意义上可以理解成单词的意思。分布式表示方法解决了one-hot的稀疏问题(量级大且稀疏)，而分布式表示方法可以自由定义向量位数表达句子或向量，并计算出单词之间的相似度，并可视化在空间。 句子向量 Sentence Similarity计算文本相似度 欧式距离 余弦相似度]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenCV与TensorFlow手写数字刷脸识别]]></title>
    <url>%2F2019%2F09%2F03%2FOpenCV%E4%B8%8ETensorFlow%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E5%88%B7%E8%84%B8%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[OpenCV与TensorFlow手写数字刷脸识别 http://yann.lecun.com/exdb/mnist/ 手写数字识别KNN最近领域# 1 重要 # 2 KNN最近领域 CNN卷积神经网络 2种 # 3 样本 # 4 旧瓶装新酒 ：数字识别的不同 # 4.1 网络 4。2 每一级 4.3 先原理 后代码 # 本质：knn test 样本 K个 max4 3个1 -》1 # 1 load Data 1.1 随机数 1.2 4组 训练 测试 （图片 和 标签） # 2 knn test train distance 5*500 = 2500 784=28*28 # 3 knn k个最近的图片5 500 1-》500train （4） # 4 k个最近的图片-&gt; parse centent label # 5 label -》 数字 p9 测试图片-》数据 # 6 检测概率统计 import tensorflow as tf import numpy as np import random from tensorflow.examples.tutorials.mnist import input_data # load data 2 one_hot : 1 0000 1 fileName mnist = input_data.read_data_sets(&#39;MNIST_data&#39;,one_hot=True) # 属性设置 trainNum = 55000 testNum = 10000 trainSize = 500 testSize = 5 k = 4 # data 分解 1 trainSize 2范围0-trainNum 3 replace=False trainIndex = np.random.choice(trainNum,trainSize,replace=False) testIndex = np.random.choice(testNum,testSize,replace=False) trainData = mnist.train.images[trainIndex]# 训练图片 trainLabel = mnist.train.labels[trainIndex]# 训练标签 testData = mnist.test.images[testIndex] testLabel = mnist.test.labels[testIndex] # 28*28 = 784 print(&#39;trainData.shape=&#39;,trainData.shape)#500*784 1 图片个数 2 784? print(&#39;trainLabel.shape=&#39;,trainLabel.shape)#500*10 print(&#39;testData.shape=&#39;,testData.shape)#5*784 print(&#39;testLabel.shape=&#39;,testLabel.shape)#5*10 print(&#39;testLabel=&#39;,testLabel)# 4 :testData [0] 3:testData[1] 6 # tf input 784-&gt;image trainDataInput = tf.placeholder(shape=[None,784],dtype=tf.float32) trainLabelInput = tf.placeholder(shape=[None,10],dtype=tf.float32) testDataInput = tf.placeholder(shape=[None,784],dtype=tf.float32) testLabelInput = tf.placeholder(shape=[None,10],dtype=tf.float32) #knn distance 5*785. 5*1*784 # 5测试图片 500训练图片 784 (3D) 2500*784 f1 = tf.expand_dims(testDataInput,1) # 维度扩展 f2 = tf.subtract(trainDataInput,f1)# 784 sum(784) f3 = tf.reduce_sum(tf.abs(f2),reduction_indices=2)# 完成数据累加 784 abs # 5*500 f4 = tf.negative(f3)# 取反 f5,f6 = tf.nn.top_k(f4,k=4) # 选取f4 最大的四个值 # f3 最小的四个值 # f6 index-&gt;trainLabelInput f7 = tf.gather(trainLabelInput,f6) # f8 num reduce_sum reduction_indices=1 &#39;竖直&#39; f8 = tf.reduce_sum(f7,reduction_indices=1) # tf.argmax 选取在某一个最大的值 index f9 = tf.argmax(f8,dimension=1) # f9 -&gt; test5 image -&gt; 5 num with tf.Session() as sess: # f1 &lt;- testData 5张图片 p1 = sess.run(f1,feed_dict={testDataInput:testData[0:5]}) print(&#39;p1=&#39;,p1.shape)# p1= (5, 1, 784) p2 = sess.run(f2,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) print(&#39;p2=&#39;,p2.shape)#p2= (5, 500, 784) (1,100) p3 = sess.run(f3,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) print(&#39;p3=&#39;,p3.shape)#p3= (5, 500) print(&#39;p3[0,0]=&#39;,p3[0,0]) #130.451 knn distance p3[0,0]= 155.812 p4 = sess.run(f4,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) print(&#39;p4=&#39;,p4.shape) print(&#39;p4[0,0]&#39;,p4[0,0]) p5,p6 = sess.run((f5,f6),feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) #p5= (5, 4) 每一张测试图片（5张）分别对应4张最近训练图片 #p6= (5, 4) print(&#39;p5=&#39;,p5.shape) print(&#39;p6=&#39;,p6.shape) print(&#39;p5[0,0]&#39;,p5[0]) print(&#39;p6[0,0]&#39;,p6[0])# p6 index p7 = sess.run(f7,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel}) print(&#39;p7=&#39;,p7.shape)#p7= (5, 4, 10) print(&#39;p7[]&#39;,p7) p8 = sess.run(f8,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel}) print(&#39;p8=&#39;,p8.shape) print(&#39;p8[]=&#39;,p8) p9 = sess.run(f9,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel}) print(&#39;p9=&#39;,p9.shape) print(&#39;p9[]=&#39;,p9) p10 = np.argmax(testLabel[0:5],axis=1) print(&#39;p10[]=&#39;,p10) j = 0 for i in range(0,5): if p10[i] == p9[i]: j = j+1 print(&#39;ac=&#39;,j*100/5) CNN卷积神经网络#cnn : 1 卷积 # ABC # A: 激励函数+矩阵 乘法加法 # A CNN : pool（激励函数+矩阵 卷积 加法） # C：激励函数+矩阵 乘法加法（A-》B） # C：激励函数+矩阵 乘法加法（A-》B） + softmax（矩阵 乘法加法） # loss：tf.reduce_mean(tf.square(y-layer2)) # loss：code #1 import import tensorflow as tf import numpy as np from tensorflow.examples.tutorials.mnist import input_data # 2 load data mnist = input_data.read_data_sets(&#39;MNIST_data&#39;,one_hot = True) # 3 input imageInput = tf.placeholder(tf.float32,[None,784]) # 28*28 labeInput = tf.placeholder(tf.float32,[None,10]) # 10 列数 # 4 data reshape # [None,784]-&gt;M*28*28*1 2D-&gt;4D 28*28 wh 1 channel imageInputReshape = tf.reshape(imageInput,[-1,28,28,1]) # 5 卷积 w0 : 卷积内核 5*5 out:32 in:1 w0 = tf.Variable(tf.truncated_normal([5,5,1,32],stddev = 0.1)) b0 = tf.Variable(tf.constant(0.1,shape=[32])) # 6 # layer1：激励函数+卷积运算 # imageInputReshape : M*28*28*1 w0:5,5,1,32 layer1 = tf.nn.relu(tf.nn.conv2d(imageInputReshape,w0,strides=[1,1,1,1],padding=&#39;SAME&#39;)+b0) # M*28*28*32 # pool 采样 数据量减少很多M*28*28*32 =&gt; M*7*7*32 layer1_pool = tf.nn.max_pool(layer1,ksize=[1,4,4,1],strides=[1,4,4,1],padding=&#39;SAME&#39;) # [1 2 3 4]-&gt;[4] # 7 layer2 out : 激励函数+乘加运算： softmax（激励函数 + 乘加运算） # [7*7*32,1024] w1 = tf.Variable(tf.truncated_normal([7*7*32,1024],stddev=0.1)) b1 = tf.Variable(tf.constant(0.1,shape=[1024])) h_reshape = tf.reshape(layer1_pool,[-1,7*7*32])# M*7*7*32 -&gt; N*N1 # [N*7*7*32] [7*7*32,1024] = N*1024 h1 = tf.nn.relu(tf.matmul(h_reshape,w1)+b1) # 7.1 softMax w2 = tf.Variable(tf.truncated_normal([1024,10],stddev=0.1)) b2 = tf.Variable(tf.constant(0.1,shape=[10])) pred = tf.nn.softmax(tf.matmul(h1,w2)+b2)# N*1024 1024*10 = N*10 # N*10( 概率 )N1【0.1 0.2 0.4 0.1 0.2 。。。】 # label。 【0 0 0 0 1 0 0 0.。。】 loss0 = labeInput*tf.log(pred) loss1 = 0 # 7.2 for m in range(0,500):# test 100 for n in range(0,10): loss1 = loss1 - loss0[m,n] loss = loss1/500 # 8 train train = tf.train.GradientDescentOptimizer(0.01).minimize(loss) # 9 run with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for i in range(100): images,labels = mnist.train.next_batch(500) sess.run(train,feed_dict={imageInput:images,labeInput:labels}) pred_test = sess.run(pred,feed_dict={imageInput:mnist.test.images,labeInput:labels}) acc = tf.equal(tf.arg_max(pred_test,1),tf.arg_max(mnist.test.labels,1)) acc_float = tf.reduce_mean(tf.cast(acc,tf.float32)) acc_result = sess.run(acc_float,feed_dict={imageInput:mnist.test.images,labeInput:mnist.test.labels}) print(acc_result) 刷脸识别爬虫获取样本import urllib from bs4 import BeautifulSoup html = urllib.request.urlopen( &#39;https://www.duitang.com/album/?id=69001447&#39;).read() # parse url data 1 html 2 &#39;html.parser&#39; 3 &#39;utf-8&#39; soup = BeautifulSoup(html, &#39;html.parser&#39;, from_encoding=&#39;utf-8&#39;) # img images = soup.findAll(&#39;img&#39;) print(images) imageName = 0 for image in images: link = image.get(&#39;src&#39;) print(&#39;link=&#39;, link) fileFormat = link[-3:] if fileFormat == &#39;png&#39; or fileFormat == &#39;jpg&#39;: fileSavePath = &#39;C:/Users/codewj/AnacondaProjects/5刷脸识别/images/&#39; + str(imageName) + &#39;.jpg&#39; imageName = imageName + 1 urllib.request.urlretrieve(link, fileSavePath) ffmpegffmpeg -i input.mp4 -r 1 -q:v 2 -f image2 pic-%03d.jpeg 视频提取帧 ffmpeg -i input.mp4 -ss 00:00:20 -t 10 -r 1 -q:v 2 -f image2 pic-%03d.jpeg ffmpeg会从input.mp4的第20s时间开始，往下10s，即20~30s这10秒钟之间，每隔1s就抓一帧，总共会抓10帧。 ffmpeg -i input.avi output.mp4 视频转格式 ffmpeg -i a.mp4 -acodec copy -vn a.aac 视频提取音频 ffmpeg -i input.mp4 -vcodec copy -an output.mp4 视频提取音频 ffmpeg -ss 00:00:15 -t 00:00:05 -i input.mp4 -vcodec copy -acodec copy output.mp4 视频剪切 ffmpeg -i input.mp4 -b:v 2000k -bufsize 2000k -maxrate 2500k output.mp4 码率控制 ffmpeg -i input.mp4 -vcodec mpeg4 output.mp4 视频编码格式转换mpeg4 ffmpeg -i input.mp4 -vf scale=960:540 output.mp4 将输入视频缩小到960x540输出 ffmpeg -i input.mp4 -i iQIYI_logo.png -filter_complex overlay output.mp4 视频添加logo opencv预处理# 1 load xml 2 load jpg 3 haar gray 4 detect 5 draw import cv2 import numpy as np # load xml 1 file name face_xml = cv2.CascadeClassifier(&#39;haarcascade_frontalface_default.xml&#39;) eye_xml = cv2.CascadeClassifier(&#39;haarcascade_eye.xml&#39;) # load jpg img = cv2.imread(&#39;face.jpg&#39;) cv2.imshow(&#39;src&#39;,img) # haar gray gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) # detect faces 1 data 2 scale 3 5 faces = face_xml.detectMultiScale(gray,1.3,5) print(&#39;face=&#39;,len(faces)) # draw index = 0 for (x,y,w,h) in faces: cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) roi_face = gray[y:y+h,x:x+w] roi_color = img[y:y+h,x:x+w] fileName = str(index)+&#39;.jpg&#39; cv2.imwrite(fileName,roi_color) index = index + 1 # 1 gray eyes = eye_xml.detectMultiScale(roi_face) print(&#39;eye=&#39;,len(eyes)) #for (e_x,e_y,e_w,e_h) in eyes: #cv2.rectangle(roi_color,(e_x,e_y),(e_x+e_w,e_y+e_h),(0,255,0),2) cv2.imshow(&#39;dst&#39;,img) cv2.waitKey(0) 某个人脸识别# 1 数据yale 2 准备train label-》train # 3 cnn 4 检测 import tensorflow as tf import numpy as np import scipy.io as sio f = open(&#39;Yale_64x64.mat&#39;,&#39;rb&#39;) mdict = sio.loadmat(f) # fea gnd train_data = mdict[&#39;fea&#39;] train_label = mdict[&#39;gnd&#39;] # 数据无序排列 train_data = np.random.permutation(train_data) train_label = np.random.permutation(train_label) test_data = train_data[0:64] test_label = train_label[0:64] np.random.seed(100) test_data = np.random.permutation(test_data) np.random.seed(100) test_label = np.random.permutation(test_label) # train [0-9] [10*N] [15*N] [0 0 1 0 0 0 0 0 0 0] -&gt; 2 train_data = train_data.reshape(train_data.shape[0],64,64,1).astype(np.float32)/255 train_labels_new = np.zeros((165,15))# 165 image 15 for i in range(0,165): j = int(train_label[i,0])-1 # 1-15 0-14 train_labels_new[i,j] = 1 test_data_input = test_data.reshape(test_data.shape[0],64,64,1).astype(np.float32)/255 test_labels_input = np.zeros((64,15))# 165 image 15 for i in range(0,64): j = int(test_label[i,0])-1 # 1-15 0-14 test_labels_input[i,j] = 1 # cnn acc tf.nn tf.layer data_input = tf.placeholder(tf.float32,[None,64,64,1]) label_input = tf.placeholder(tf.float32,[None,15]) layer1 = tf.layers.conv2d(inputs=data_input,filters=32,kernel_size=2,strides=1,padding=&#39;SAME&#39;,activation=tf.nn.relu) layer1_pool = tf.layers.max_pooling2d(layer1,pool_size=2,strides=2) layer2 = tf.reshape(layer1_pool,[-1,32*32*32]) layer2_relu = tf.layers.dense(layer2,1024,tf.nn.relu) output = tf.layers.dense(layer2_relu,15) loss = tf.losses.softmax_cross_entropy(onehot_labels=label_input,logits=output) train = tf.train.GradientDescentOptimizer(0.01).minimize(loss) accuracy = tf.metrics.accuracy(labels=tf.argmax(label_input,axis=1),predictions=tf.argmax(output,axis=1))[1] # run acc init = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer()) with tf.Session() as sess: sess.run(init) for i in range(0,200): train_data_input = np.array(train_data) train_label_input = np.array(train_labels_new) sess.run([train,loss],feed_dict={data_input:train_data_input,label_input:train_label_input}) acc = sess.run(accuracy,feed_dict={data_input:test_data_input,label_input:test_labels_input}) print(&#39;acc:%.2f&#39;,acc)]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenCV与TensorFlow 机器学习]]></title>
    <url>%2F2019%2F09%2F02%2FOpenCV%E4%B8%8ETensorFlow%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[OpenCV与TensorFlow 机器学习 视频分解图片# 1 load 2 info 3 parse 4 imshow imwrite import cv2 cap = cv2.VideoCapture(&quot;1.mp4&quot;)# 获取一个视频打开cap 1 file name isOpened = cap.isOpened# 判断是否打开‘ print(isOpened) fps = cap.get(cv2.CAP_PROP_FPS)#帧率 width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))#w h height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) print(fps,width,height) i = 0 while(isOpened): if i == 10: break else: i = i+1 (flag,frame) = cap.read()# 读取每一张 flag frame fileName = &#39;image&#39;+str(i)+&#39;.jpg&#39; print(fileName) if flag == True: cv2.imwrite(fileName,frame,[cv2.IMWRITE_JPEG_QUALITY,100]) print(&#39;end!&#39;) 图片合成视频import cv2 img = cv2.imread(&#39;image1.jpg&#39;) imgInfo = img.shape size = (imgInfo[1],imgInfo[0]) print(size) videoWrite = cv2.VideoWriter(&#39;2.mp4&#39;,-1,5,size)# 写入对象 # 1 file name 2 编码器 3 帧率 4 size for i in range(1,11): fileName = &#39;image&#39;+str(i)+&#39;.jpg&#39; img = cv2.imread(fileName) videoWrite.write(img)# 写入方法 1 jpg data print(&#39;end!&#39;) 基于Haar+Adaboost人脸识别# 1 load xml 2 load jpg 3 haar gray 4 detect 5 draw import cv2 import numpy as np # load xml 1 file name face_xml = cv2.CascadeClassifier(&#39;haarcascade_frontalface_default.xml&#39;) eye_xml = cv2.CascadeClassifier(&#39;haarcascade_eye.xml&#39;) # load jpg img = cv2.imread(&#39;face.jpg&#39;) cv2.imshow(&#39;src&#39;,img) # haar gray gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) # detect faces 1 data 2 scale 3 5 faces = face_xml.detectMultiScale(gray,1.3,5) print(&#39;face=&#39;,len(faces)) # draw for (x,y,w,h) in faces: cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) roi_face = gray[y:y+h,x:x+w] roi_color = img[y:y+h,x:x+w] # 1 gray eyes = eye_xml.detectMultiScale(roi_face) print(&#39;eye=&#39;,len(eyes)) #for (e_x,e_y,e_w,e_h) in eyes: #cv2.rectangle(roi_color,(e_x,e_y),(e_x+e_w,e_y+e_h),(0,255,0),2) cv2.imshow(&#39;dst&#39;,img) cv2.waitKey(0) SVM身高体重预测# 1 思想 分类器 # 2 如何？ 寻求一个最优的超平面 分类 # 3 核：line # 4 数据：样本 # 5 训练 SVM_create train predict # svm本质 寻求一个最优的超平面 分类 # svm 核: line # 身高体重 训练 预测 import cv2 import numpy as np import matplotlib.pyplot as plt #1 准备data rand1 = np.array([[155,48],[159,50],[164,53],[168,56],[172,60]]) rand2 = np.array([[152,53],[156,55],[160,56],[172,64],[176,65]]) # 2 label label = np.array([[0],[0],[0],[0],[0],[1],[1],[1],[1],[1]]) # 3 data data = np.vstack((rand1,rand2)) data = np.array(data,dtype=&#39;float32&#39;) # svm 所有的数据都要有label # [155,48] -- 0 女生 [152,53] ---1 男生 # 监督学习 0 负样本 1 正样本 # 4 训练 svm = cv2.ml.SVM_create() # ml 机器学习模块 SVM_create() 创建 # 属性设置 svm.setType(cv2.ml.SVM_C_SVC) # svm type svm.setKernel(cv2.ml.SVM_LINEAR) # line svm.setC(0.01) # 训练 result = svm.train(data,cv2.ml.ROW_SAMPLE,label) # 预测 pt_data = np.vstack([[167,55],[162,57]]) #0 女生 1男生 pt_data = np.array(pt_data,dtype=&#39;float32&#39;) print(pt_data) (par1,par2) = svm.predict(pt_data) print(par2) Hog+SVM小狮子识别# 训练 # 1 参数 2hog 3 svm 4 computer hog 5 label 6 train 7 pred 8 draw import cv2 import numpy as np import matplotlib.pyplot as plt # 1 par PosNum = 820 # 正样本个数 NegNum = 1931 # 负样本个数 winSize = (64,128) blockSize = (16,16)# 105 blockStride = (8,8)#4 cell cellSize = (8,8) nBin = 9#9 bin 3780 # 2 hog create hog 1 win 2 block 3 blockStride 4 cell 5 bin hog = cv2.HOGDescriptor(winSize,blockSize,blockStride,cellSize,nBin) # 3 svm svm = cv2.ml.SVM_create() # 4 computer hog 特征提取存储 标签标识完成 featureNum = int(((128-16)/8+1)*((64-16)/8+1)*4*9) #3780 特征维度 featureArray = np.zeros(((PosNum+NegNum),featureNum),np.float32) # 用于装在特征，行正负样本个数，列就是特征维度 labelArray = np.zeros(((PosNum+NegNum),1),np.int32) # 标签 # svm 监督学习 样本 标签 svm -》image hog for i in range(0,PosNum): fileName = &#39;pos/&#39;+str(i+1)+&#39;.jpg&#39; img = cv2.imread(fileName) hist = hog.compute(img,(8,8))# 3780 for j in range(0,featureNum): featureArray[i,j] = hist[j] # featureArray hog [1,:] hog1 [2,:]hog2 labelArray[i,0] = 1 # 正样本 label 1 for i in range(0,NegNum): fileName = &#39;neg/&#39;+str(i+1)+&#39;.jpg&#39; img = cv2.imread(fileName) hist = hog.compute(img,(8,8))# 3780 for j in range(0,featureNum): featureArray[i+PosNum,j] = hist[j] labelArray[i+PosNum,0] = -1 # 负样本 label -1 svm.setType(cv2.ml.SVM_C_SVC) svm.setKernel(cv2.ml.SVM_LINEAR) svm.setC(0.01) # 6 train ret = svm.train(featureArray,cv2.ml.ROW_SAMPLE,labelArray) # 7 myHog ：《-myDetect # myDetect-《resultArray rho # myHog-》detectMultiScale # 7 检测 核心：create Hog -》 myDetect—》array-》 # resultArray-》resultArray = -1*alphaArray*supportVArray # rho-》svm-〉svm.train alpha = np.zeros((1),np.float32) rho = svm.getDecisionFunction(0,alpha) print(rho) print(alpha) alphaArray = np.zeros((1,1),np.float32) # 支持向量机数组 supportVArray = np.zeros((1,featureNum),np.float32) resultArray = np.zeros((1,featureNum),np.float32) alphaArray[0,0] = alpha resultArray = -1*alphaArray*supportVArray # detect检测创建 myDetect = np.zeros((3781),np.float32) for i in range(0,3780): myDetect[i] = resultArray[0,i] myDetect[3780] = rho[0] # rho svm （判决） myHog = cv2.HOGDescriptor() myHog.setSVMDetector(myDetect) # load 1表示彩色图片 imageSrc = cv2.imread(&#39;Test2.jpg&#39;,1) # (8,8) win 检测目标 objs = myHog.detectMultiScale(imageSrc,0,(8,8),(32,32),1.05,2) # xy wh 三维 最后一维 x = int(objs[0][0][0]) y = int(objs[0][0][1]) w = int(objs[0][0][2]) h = int(objs[0][0][3]) # 绘制展示 图片 起始位置 终止位置 颜色 线条宽度 cv2.rectangle(imageSrc,(x,y),(x+w,y+h),(255,0,0),2) cv2.imshow(&#39;dst&#39;,imageSrc) cv2.waitKey(0)]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenCV与TensorFlow 入门人工智能图像处理]]></title>
    <url>%2F2019%2F09%2F02%2FOpenCV%E4%B8%8ETensorFlow%20%E5%85%A5%E9%97%A8%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[OpenCV与TensorFlow 入门人工智能图像处理 AnacondaAnaconda Navigator新建Env环境,添加channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/tensorflow,opencv,numpy,matplotlib,beautifulsoup4,urllib3,scipy，并进入Home安装Jupyter notebook点击进入Jupyter Launch 基本命令yum install -y bzip2 sh Anaconda3-5.2.0-Linux-x86_64.sh 修改默认位置为/data1/anaconda3 vi /etc/profile PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin:$MAVEN_HOME/bin:/data1/anaconda3/bin source /etc/profile conda update conda conda info --envs conda create --name tensorflow36 activate tensorflow36 | source activate tensorflow36 | conda activate tensorflow36 source deactivate conda create -n spider python=3.6 创建虚拟环境 source activate spider pip install -i https://pypi.doubanio.com/simple/ --trusted-host pypi.doubanio.com fake_useragent scrapy browsercookie conda remove -n spider --all 删除虚拟环境 conda config --set show_channel_urls yes 设置搜索时显示通道地址 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --remove channels https://mirrors.tuna.tsinghua.edu.cn/tensorflow/linux/cpu/ conda list conda update anaconda-navigator conda update navigator-updater pip install opencv-pythonopencv入门opencv图片读取与展示 # 引入opencv2 import cv2 # 文件读取-封装格式解析-数据解码-数据加载 img = cv2.imread(&#39;a.jpg&#39;,1) cv2.imshow(&#39;image&#39;,img) # jpg png是文件封装格式，文件头（数据解码信息，附加信息，解码器根据附加信息将文件数据还原成最原始的数据）+文件数据（非文件原始数据，是压缩编码后的数据） # stop cv2.waitKey (0) opencv图片写入import cv2 img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imwrite(&#39;image1.jpg&#39;,img) # 1 name 2 data opencv图像质量jpg有损压缩import cv2 img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imwrite(&#39;imageTest.jpg&#39;,img,[cv2.IMWRITE_JPEG_QUALITY,50]) # 1M 100k 10k 0-100 # jpg RGB颜色分量组成 1.14M=720*547*3*8 bit/8 (b)=1.14M png无损压缩# 透明度属性 import cv2 img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imwrite(&#39;imageTest.png&#39;,img,[cv2.IMWRITE_PNG_COMPRESSION,0]) # jpg 0 压缩比高0-100 png 0 压缩比低0-9 # png RGB alpha opencv像素操作import cv2 img = cv2.imread(&#39;image0.jpg&#39;,1) (b,g,r) = img[100,100] print(b,g,r)# bgr #10 100 --- 110 100 for i in range(1,100): img[10+i,100] = (255,0,0) cv2.imshow(&#39;image&#39;,img) cv2.waitKey(0) #1000 ms tensorflow入门 tf_常量变量所有变量必须初始化完成 #opencv tensorflow #类比 语法 api 原理 #基础数据类型 运算符 流程 字典 数组 import tensorflow as tf data1 = tf.constant(2,dtype=tf.int32) data2 = tf.Variable(10,name=&#39;var&#39;) print(data1) print(data2) &#39;&#39;&#39; sess = tf.Session() print(sess.run(data1)) init = tf.global_variables_initializer() sess.run(init) print(sess.run(data2)) sess.close() # 本质 tf = tensor + 计算图 # tensor 数据 # op # graphs 数据操作 # session &#39;&#39;&#39; init = tf.global_variables_initializer() sess = tf.Session() with sess: sess.run(init) print(sess.run(data2)) tf_四则运算常量import tensorflow as tf data1 = tf.constant(6) data2 = tf.constant(2) dataAdd = tf.add(data1,data2) dataMul = tf.multiply(data1,data2) dataSub = tf.subtract(data1,data2) dataDiv = tf.divide(data1,data2) with tf.Session() as sess: print(sess.run(dataAdd)) print(sess.run(dataMul)) print(sess.run(dataSub)) print(sess.run(dataDiv)) print(&#39;end!&#39;) 变量import tensorflow as tf data1 = tf.constant(6) data2 = tf.Variable(2) dataAdd = tf.add(data1,data2) dataCopy = tf.assign(data2,dataAdd)# dataAdd -&gt;data2 dataMul = tf.multiply(data1,data2) dataSub = tf.subtract(data1,data2) dataDiv = tf.divide(data1,data2) init = tf.global_variables_initializer() with tf.Session() as sess: sess.run(init) # 所有变量必须完成初始化 print(sess.run(dataAdd)) print(sess.run(dataMul)) print(sess.run(dataSub)) print(sess.run(dataDiv)) print(&#39;sess.run(dataCopy)&#39;,sess.run(dataCopy))#8-&gt;data2 print(&#39;dataCopy.eval()&#39;,dataCopy.eval())#8+6-&gt;14-&gt;data2 = 14 print(&#39;tf.get_default_session()&#39;,tf.get_default_session().run(dataCopy)) print(&#39;end!&#39;) tf矩阵基础1占位#placehold import tensorflow as tf data1 = tf.placeholder(tf.float32) data2 = tf.placeholder(tf.float32) dataAdd = tf.add(data1,data2) with tf.Session() as sess: print(sess.run(dataAdd,feed_dict={data1:6,data2:2})) # 1 dataAdd 2 data (feed_dict = {1:6,2:2}) print(&#39;end!&#39;) 矩阵打印#类比 数组 M行N列 [] 内部[] [里面 列数据] [] 中括号整体 行数 #[[6,6]] [[6,6]] import tensorflow as tf data1 = tf.constant([[6,6]]) data2 = tf.constant([[2], [2]]) data3 = tf.constant([[3,3]]) data4 = tf.constant([[1,2], [3,4], [5,6]]) print(data4.shape)# 维度 with tf.Session() as sess: print(sess.run(data4)) #打印整体 print(sess.run(data4[0]))# 打印某一行 print(sess.run(data4[:,0]))#打印某列 print(sess.run(data4[0,1]))# 1 1 MN = 0 32 = M012 N01 矩阵计算import tensorflow as tf data1 = tf.constant([[6,6]]) data2 = tf.constant([[2], [2]]) data3 = tf.constant([[3,3]]) data4 = tf.constant([[1,2], [3,4], [5,6]]) matMul = tf.matmul(data1,data2) matMul2 = tf.multiply(data1,data2) matAdd = tf.add(data1,data3) with tf.Session() as sess: print(sess.run(matMul))#1 维 M=1 N2. 1X2(MK) 2X1(KN) = 1 print(sess.run(matAdd))#1行2列 print(sess.run(matMul2))# 1x2 2x1 = 2x2 print(sess.run([matMul,matAdd])) 矩阵定义import tensorflow as tf mat0 = tf.constant([[0,0,0],[0,0,0]]) mat1 = tf.zeros([2,3]) mat2 = tf.ones([3,2]) mat3 = tf.fill([2,3],15) with tf.Session() as sess: #print(sess.run(mat0)) #print(sess.run(mat1)) #print(sess.run(mat2)) print(sess.run(mat3)) mat4 = tf.constant([[2],[3],[4]]) mat5 = tf.zeros_like(mat1) mat6 = tf.linspace(0.0,2.0,11) mat7 = tf.random_uniform([2,3],-1,2) with tf.Session() as sess: print(sess.run(mat5)) print(sess.run(mat6)) print(sess.run(mat7)) tf模块Numpy的使用#CURD import numpy as np data1 = np.array([1,2,3,4,5]) print(data1) data2 = np.array([[1,2], [3,4]]) print(data2) #维度 print(data1.shape,data2.shape) # zero ones print(np.zeros([2,3]),np.ones([2,2])) # 改查 data2[1,0] = 5 print(data2) print(data2[1,1]) # 基本运算 data3 = np.ones([2,3]) print(data3*2)#对应相乘 print(data3/3) print(data3+2) # 矩阵+* data4 = np.array([[1,2,3],[4,5,6]]) print(data3+data4) print(data3*data4) tf模块matplotlib的使用import numpy as np import matplotlib.pyplot as plt # 折线 x = np.array([1,2,3,4,5,6,7,8]) y = np.array([3,5,7,6,2,6,10,15]) plt.plot(x,y,&#39;r&#39;)# 折线 1 x 2 y 3 color plt.plot(x,y,&#39;g&#39;,lw=10)# 4 line w # 柱状 x = np.array([1,2,3,4,5,6,7,8]) y = np.array([13,25,17,36,21,16,10,15]) plt.bar(x,y,0.2,alpha=1,color=&#39;b&#39;)# 5 color 4 透明度 3 0.9 plt.show() 绘制股票k线import tensorflow as tf import numpy as np import matplotlib.pyplot as plt date = np.linspace(1,15,15) endPrice = np.array([2511.90,2538.26,2510.68,2591.66,2732.98,2701.69,2701.29,2678.67,2726.50,2681.50,2739.17,2715.07,2823.58,2864.90,2919.08] ) beginPrice = np.array([2438.71,2500.88,2534.95,2512.52,2594.04,2743.26,2697.47,2695.24,2678.23,2722.13,2674.93,2744.13,2717.46,2832.73,2877.40]) print(date) # 定义绘图 plt.figure() # 数据装载 for i in range(0,15): # 1 柱状图 dateOne = np.zeros([2]) dateOne[0] = i; dateOne[1] = i; priceOne = np.zeros([2]) priceOne[0] = beginPrice[i] priceOne[1] = endPrice[i] if endPrice[i]&gt;beginPrice[i]: plt.plot(dateOne,priceOne,&#39;r&#39;,lw=8) else: plt.plot(dateOne,priceOne,&#39;g&#39;,lw=8) plt.show() 神经网络逼近股票收盘均价# layer1：激励函数+乘加运算 import tensorflow as tf import numpy as np import matplotlib.pyplot as plt date = np.linspace(1,15,15) endPrice = np.array([2511.90,2538.26,2510.68,2591.66,2732.98,2701.69,2701.29,2678.67,2726.50,2681.50,2739.17,2715.07,2823.58,2864.90,2919.08] ) beginPrice = np.array([2438.71,2500.88,2534.95,2512.52,2594.04,2743.26,2697.47,2695.24,2678.23,2722.13,2674.93,2744.13,2717.46,2832.73,2877.40]) print(date) plt.figure() for i in range(0,15): # 1 柱状图 dateOne = np.zeros([2]) dateOne[0] = i; dateOne[1] = i; priceOne = np.zeros([2]) priceOne[0] = beginPrice[i] priceOne[1] = endPrice[i] if endPrice[i]&gt;beginPrice[i]: plt.plot(dateOne,priceOne,&#39;r&#39;,lw=8) else: plt.plot(dateOne,priceOne,&#39;g&#39;,lw=8) #plt.show() # 输入矩阵A:15*1 隐藏层矩阵B:15*10 数据矩阵C:15*1 # A(15x1)*w1(1x10)+b1(1*10) = B(15x10) A-&gt;B # B(15x10)*w2(10x1)+b2(15x1) = C(15x1) B-&gt;C # 1次循环 A-|w1 w2 b1 b2|-&gt;C 与真实值相差，在2次循环时，梯度下降修改|w1 w2 b1 b2|减少2次误差..... # 1 A B C dateNormal = np.zeros([15,1]) priceNormal = np.zeros([15,1]) for i in range(0,15): dateNormal[i,0] = i/14.0; priceNormal[i,0] = endPrice[i]/3000.0; x = tf.placeholder(tf.float32,[None,1]) y = tf.placeholder(tf.float32,[None,1]) # B w1 = tf.Variable(tf.random_uniform([1,10],0,1)) b1 = tf.Variable(tf.zeros([1,10])) wb1 = tf.matmul(x,w1)+b1 layer1 = tf.nn.relu(wb1) # 激励函数 # C w2 = tf.Variable(tf.random_uniform([10,1],0,1)) b2 = tf.Variable(tf.zeros([15,1])) wb2 = tf.matmul(layer1,w2)+b2 layer2 = tf.nn.relu(wb2) loss = tf.reduce_mean(tf.square(y-layer2))#y 真实 layer2 计算 train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) # 梯度下降缩小loss with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for i in range(0,10000): sess.run(train_step,feed_dict={x:dateNormal,y:priceNormal}) # w1w2 b1b2 A + wb --&gt;layer2 pred = sess.run(layer2,feed_dict={x:dateNormal}) predPrice = np.zeros([15,1]) for i in range(0,15): predPrice[i,0]=(pred*3000)[i,0] plt.plot(date,predPrice,&#39;b&#39;,lw=1) plt.show() 计算机视觉加强之几何变换图片缩放API图片缩放# 1 load 2 info 3 resize 4 check import cv2 # 1表示彩色图片 img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape print(imgInfo) height = imgInfo[0] width = imgInfo[1] mode = imgInfo[2] # 1 放大 缩小 2 等比例 非 2:3 dstHeight = int(height*0.5) dstWidth = int(width*0.5) #最近临域插值 双线性插值 像素关系重采样 立方插值 dst = cv2.resize(img,(dstWidth,dstHeight)) cv2.imshow(&#39;image&#39;,dst) cv2.waitKey(0) 最近临域插值 原理 src 1020 dst 5*10 dst&lt;-src (1,2) &lt;- (2,4) dst x 1 -&gt; src x 2 newX newX = x(src 行/目标 行) newX = 1（10/5） = 2 newY = y(src 列/目标 列) newY = 2*（20/10）= 4 12.3 = 12 双线性插值 原理 A1 = 20% 上+80%下 A2 B1 = 30% 左+70%右 B2 1 最终点 = A1 30% + A2 70% 2 最终点 = B1 20% + B2 80% 实质：矩阵运算 源码图片缩放# 1 info 2 空白模版 3 xy import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] dstHeight = int(height/2) dstWidth = int(width/2) dstImage = np.zeros((dstHeight,dstWidth,3),np.uint8)#0-255 for i in range(0,dstHeight):#行 for j in range(0,dstWidth):#列 iNew = int(i*(height*1.0/dstHeight)) jNew = int(j*(width*1.0/dstWidth)) dstImage[i,j] = img[iNew,jNew] cv2.imshow(&#39;dst&#39;,dstImage) cv2.waitKey(0) # 1 opencv API resize 2 算法原理 3 源码 源码图片剪切#100 -》200 x #100-》300 y import cv2 img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape dst = img[100:200,100:300] cv2.imshow(&#39;image&#39;,dst) cv2.waitKey(0) 矩阵图片移位# 1 API 2 算法原理 3 源代码 import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] #### matShift = np.float32([[1,0,100],[0,1,200]])# 2*3 dst = cv2.warpAffine(img,matShift,(height,width))#1 data 2 mat 3 info # 移位 矩阵 cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) [1,0,100],[0,1,200] 22 21 组成[[1,0],[0,1]] 22 A[[100],[200]] 21 Bxy CAC+B = [[1x+0y],[0x+1*y]]+[[100],[200]] = [[x+100],[y+200]](10,20)-&gt;(110,120) 矩阵图片缩放#[[A1 A2 B1],[A3 A4 B2]] # [[A1 A2],[A3 A4]] [[B1],[B2]] # newX = A1*x + A2*y+B1 # newY = A3*x +A4*y+B2 # x-&gt;x*0.5 y-&gt;y*0.5 # newX = 0.5*x import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] matScale = np.float32([[0.5,0,0],[0,0.5,0]]) dst = cv2.warpAffine(img,matScale,(int(width/2),int(height/2))) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 源码图片移位import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape dst = np.zeros(img.shape,np.uint8) height = imgInfo[0] width = imgInfo[1] for i in range(0,height): for j in range(0,width-100): dst[i,j+100]=img[i,j] cv2.imshow(&#39;image&#39;,dst) cv2.waitKey(0) 源码图片镜像import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] deep = imgInfo[2] newImgInfo = (height*2,width,deep) dst = np.zeros(newImgInfo,np.uint8)#uint8 for i in range(0,height): for j in range(0,width): dst[i,j] = img[i,j] #x y = 2*h - y -1 dst[height*2-i-1,j] = img[i,j] for i in range(0,width): dst[height,i] = (0,0,255)#BGR cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 仿射变换import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] #src 3-&gt;dst 3 (左上角 左下角 右上角) matSrc = np.float32([[0,0],[0,height-1],[width-1,0]]) matDst = np.float32([[50,50],[300,height-200],[width-300,100]]) #组合 定义仿射变换矩阵 matAffine = cv2.getAffineTransform(matSrc,matDst)# mat 1 src 2 dst dst = cv2.warpAffine(img,matAffine,(width,height)) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) API图片旋转import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] # 2*3 matRotate = cv2.getRotationMatrix2D((height*0.5,width*0.5),45,1)# mat rotate 1 center 2 angle 3 scale #100*100 25 dst = cv2.warpAffine(img,matRotate,(height,width)) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 计算机视觉加强之图像特效API灰度处理#imread #方法1 imread import cv2 img0 = cv2.imread(&#39;image0.jpg&#39;,0) img1 = cv2.imread(&#39;image0.jpg&#39;,1) print(img0.shape) print(img1.shape) cv2.imshow(&#39;src&#39;,img0) cv2.waitKey(0) #方法2 cvtColor img = cv2.imread(&#39;image0.jpg&#39;,1) dst = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)# 颜色空间转换 1 data 2 BGR gray cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 源码灰度处理import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] # RGB R=G=B = gray (R+G+B)/3 dst = np.zeros((height,width,3),np.uint8) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] gray = (int(b)+int(g)+int(r))/3 dst[i,j] = np.uint8(gray) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) #方法4 gray = r*0.299+g*0.587+b*0.114 dst = np.zeros((height,width,3),np.uint8) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] b = int(b) g = int(g) r = int(r) gray = r*0.299+g*0.587+b*0.114 dst[i,j] = np.uint8(gray) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0)算法优化# 1 灰度 最重要 2 基础 3 实时性 # 定点-》浮点 +- */ &gt;&gt; # r*0.299+g*0.587+b*0.114 import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] # RGB R=G=B = gray (R+G+B)/3 dst = np.zeros((height,width,3),np.uint8) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] b = int(b) g = int(g) r = int(r) #gray = (r*1+g*2+b*1)/4 gray = (r+(g&lt;&lt;1)+b)&gt;&gt;2 dst[i,j] = np.uint8(gray) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 图片颜色反转#0-255 255-当前 灰度图片颜色反转 import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) # 1表示一个像素一种颜色 dst = np.zeros((height,width,1),np.uint8) for i in range(0,height): for j in range(0,width): grayPixel = gray[i,j] dst[i,j] = 255-grayPixel cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) #RGB 255-R=newR 彩色图片颜色反转 dst = np.zeros((height,width,3),np.uint8) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] dst[i,j] = (255-b,255-g,255-r) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 马赛克import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] for m in range(100,300): for n in range(100,200): # pixel -&gt;10*10 所有像素点用一个点替代 if m%10 == 0 and n%10==0: # for循环填充小矩形 for i in range(0,10): for j in range(0,10): (b,g,r) = img[m,n] img[i+m,j+n] = (b,g,r) cv2.imshow(&#39;dst&#39;,img) cv2.waitKey(0) 毛玻璃import cv2 import numpy as np import random img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] dst = np.zeros((height,width,3),np.uint8) mm = 8 ## -mm防止当前矩阵越界 for m in range(0,height-mm): for n in range(0,width-mm): index = int(random.random()*8)#0-8 (b,g,r) = img[m+index,n+index] dst[m,n] = (b,g,r) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 图片融合# dst = src1*a+src2*(1-a) 两张图片都需要大于第一张图片宽和高度的一半 import cv2 import numpy as np img0 = cv2.imread(&#39;image0.jpg&#39;,1) img1 = cv2.imread(&#39;image1.jpg&#39;,1) imgInfo = img0.shape height = imgInfo[0] width = imgInfo[1] # ROI roiH = int(height/2) roiW = int(width/2) img0ROI = img0[0:roiH,0:roiW] img1ROI = img1[0:roiH,0:roiW] # dst dst = np.zeros((roiH,roiW,3),np.uint8) dst = cv2.addWeighted(img0ROI,0.5,img1ROI,0.5,0)#add src1*a+src2*(1-a) # 1 src1 2 a 3 src2 4 1-a cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 图片边缘检测import cv2 import numpy as np import random img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] cv2.imshow(&#39;src&#39;,img) #canny 1 gray 2 高斯 3 canny gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) imgG = cv2.GaussianBlur(gray,(3,3),0) dst = cv2.Canny(img,50,50) #图片卷积——》th cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 源码边缘检测import cv2 import numpy as np import random import math img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] cv2.imshow(&#39;src&#39;,img) # sobel 1 算子模版 2 图片卷积 3 阈值判决 # [1 2 1 [ 1 0 -1 # 0 0 0 2 0 -2 # -1 -2 -1 ] 1 0 -1 ] # 四个点 [1 2 3 4] 计算模板 [a b c d] 卷积后 a*1+b*2+c*3+d*4 = dst # sqrt(a*a+b*b) = f&gt;th 则为边缘 gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) dst = np.zeros((height,width,1),np.uint8) for i in range(0,height-2): for j in range(0,width-2): # 竖直方向梯度 gy = gray[i,j]*1+gray[i,j+1]*2+gray[i,j+2]*1-gray[i+2,j]*1-gray[i+2,j+1]*2-gray[i+2,j+2]*1 # 水平方向梯度 gx = gray[i,j]+gray[i+1,j]*2+gray[i+2,j]-gray[i,j+2]-gray[i+1,j+2]*2-gray[i+2,j+2] # 计算梯度 grad = math.sqrt(gx*gx+gy*gy) if grad&gt;50: dst[i,j] = 255 else: dst[i,j] = 0 cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 浮雕效果import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) # newP = gray0-gray1+150 dst = np.zeros((height,width,1),np.uint8) for i in range(0,height): for j in range(0,width-1): grayP0 = int(gray[i,j]) grayP1 = int(gray[i,j+1]) newP = grayP0-grayP1+150 if newP &gt; 255: newP = 255 if newP &lt; 0: newP = 0 dst[i,j] = newP cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 颜色风格import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] #rgb -》RGB new “蓝色” # b=b*1.5 # g = g*1.3 dst = np.zeros((height,width,3),np.uint8) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] b = b*1.5 g = g*1.3 if b&gt;255: b = 255 if g&gt;255: g = 255 dst[i,j]=(b,g,r) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 油画特效# 1 gray # 2 7*7 10*10 3 0-255 256 4 64 0-63 64-127 # 3 10 0-63 99 64-127 # 4 count 5 dst = result import cv2 import numpy as np img = cv2.imread(&#39;image00.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) dst = np.zeros((height,width,3),np.uint8) for i in range(4,height-4): for j in range(4,width-4): array1 = np.zeros(8,np.uint8) # 定义8*8小方块 for m in range(-4,4): for n in range(-4,4): # 灰度等级划分8个段 每段32，/32知道p1投影在哪个灰度等级段 p1 = int(gray[i+m,j+n]/32) # 当前像素值完成累加 array1[p1] = array1[p1]+1 currentMax = array1[0] # l定义某一段 l = 0 for k in range(0,8): if currentMax&lt;array1[k]: currentMax = array1[k] l = k # 简化或者均值 for m in range(-4,4): for n in range(-4,4): if gray[i+m,j+n]&gt;=(l*32) and gray[i+m,j+n]&lt;=((l+1)*32): (b,g,r) = img[i+m,j+n] dst[i,j] = (b,g,r) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 线段绘制import cv2 import numpy as np newImageInfo = (500,500,3) dst = np.zeros(newImageInfo,np.uint8) # line # 绘制线段 1 dst 2 begin 3 end 4 color cv2.line(dst,(100,100),(400,400),(0,0,255)) # 5 line w cv2.line(dst,(100,200),(400,200),(0,255,255),20) # 6 line type cv2.line(dst,(100,300),(400,300),(0,255,0),20,cv2.LINE_AA) cv2.line(dst,(200,150),(50,250),(25,100,255)) cv2.line(dst,(50,250),(400,380),(25,100,255)) cv2.line(dst,(400,380),(200,150),(25,100,255)) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 矩形圆形绘制import cv2 import numpy as np newImageInfo = (500,500,3) dst = np.zeros(newImageInfo,np.uint8) # 1 2 左上角 3 右下角 4 5 fill -1 &gt;0 line w cv2.rectangle(dst,(50,100),(200,300),(255,0,0),5) # 2 center 3 r cv2.circle(dst,(250,250),(50),(0,255,0),2) # 2 center 3 轴 4 angle 5 begin 6 end 7 cv2.ellipse(dst,(256,256),(150,100),0,0,180,(255,255,0),-1) points = np.array([[150,50],[140,140],[200,170],[250,250],[150,50]],np.int32) print(points.shape) points = points.reshape((-1,1,2)) print(points.shape) cv2.polylines(dst,[points],True,(0,255,255)) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 文字图片绘制import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) font = cv2.FONT_HERSHEY_SIMPLEX cv2.rectangle(img,(200,100),(500,400),(0,255,0),3) # 1 dst 2 文字内容 3 坐标 4 5 字体大小 6 color 7 粗细 8 line type cv2.putText(img,&#39;this is flow&#39;,(100,300),font,1,(200,100,255),2,cv2.LINE_AA) cv2.imshow(&#39;src&#39;,img) cv2.waitKey(0) height = int(img.shape[0]*0.2) width = int(img.shape[1]*0.2) imgResize = cv2.resize(img,(width,height)) for i in range(0,height): for j in range(0,width): img[i+200,j+350] = imgResize[i,j] cv2.imshow(&#39;src&#39;,img) cv2.waitKey(0) 计算机视觉加强之图像美化灰度直方图源码# 1 0-255 2 概率 # 本质：统计每个像素灰度 出现的概率 0-255 p import cv2 import numpy as np import matplotlib.pyplot as plt img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) count = np.zeros(256,np.float) for i in range(0,height): for j in range(0,width): pixel = gray[i,j] index = int(pixel) count[index] = count[index]+1 for i in range(0,255): count[i] = count[i]/(height*width) x = np.linspace(0,255,256) y = count plt.bar(x,y,0.9,alpha=1,color=&#39;b&#39;) plt.show() cv2.waitKey(0)彩色直方图源码# 本质：统计每个像素灰度 出现的概率 0-255 p import cv2 import numpy as np import matplotlib.pyplot as plt img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] count_b = np.zeros(256,np.float) count_g = np.zeros(256,np.float) count_r = np.zeros(256,np.float) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] index_b = int(b) index_g = int(g) index_r = int(r) count_b[index_b] = count_b[index_b]+1 count_g[index_g] = count_g[index_g]+1 count_r[index_r] = count_r[index_r]+1 for i in range(0,256): count_b[i] = count_b[i]/(height*width) count_g[i] = count_g[i]/(height*width) count_r[i] = count_r[i]/(height*width) x = np.linspace(0,255,256) y1 = count_b plt.figure() plt.bar(x,y1,0.9,alpha=1,color=&#39;b&#39;) y2 = count_g plt.figure() plt.bar(x,y2,0.9,alpha=1,color=&#39;g&#39;) y3 = count_r plt.figure() plt.bar(x,y3,0.9,alpha=1,color=&#39;r&#39;) plt.show() cv2.waitKey(0) 灰度直方图均衡化# 本质：统计每个像素灰度 出现的概率 0-255 p # 累计概率 # 1 0.2 0.2 # 2 0.3 0.5 # 3 0.1 0.6 # 256 # 100 0.5 255*0.5 = new import cv2 import numpy as np import matplotlib.pyplot as plt img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) cv2.imshow(&#39;src&#39;,gray) count = np.zeros(256,np.float) for i in range(0,height): for j in range(0,width): pixel = gray[i,j] index = int(pixel) count[index] = count[index]+1 for i in range(0,255): count[i] = count[i]/(height*width) #计算累计概率 sum1 = float(0) for i in range(0,256): sum1 = sum1+count[i] count[i] = sum1 #print(count) # 计算映射表 map1 = np.zeros(256,np.uint16) for i in range(0,256): map1[i] = np.uint16(count[i]*255) # 映射 for i in range(0,height): for j in range(0,width): pixel = gray[i,j] gray[i,j] = map1[pixel] cv2.imshow(&#39;dst&#39;,gray) cv2.waitKey(0)彩色直方图均衡化# 本质：统计每个像素灰度 出现的概率 0-255 p # 累计概率 # 1 0.2 0.2 # 2 0.3 0.5 # 3 0.1 0.6 # 256 # 100 0.5 255*0.5 = new # 1 统计每个颜色出现的概率 2 累计概率 1 3 0-255 255*p # 4 pixel import cv2 import numpy as np import matplotlib.pyplot as plt img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] count_b = np.zeros(256,np.float) count_g = np.zeros(256,np.float) count_r = np.zeros(256,np.float) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] index_b = int(b) index_g = int(g) index_r = int(r) count_b[index_b] = count_b[index_b]+1 count_g[index_g] = count_g[index_g]+1 count_r[index_r] = count_r[index_r]+1 for i in range(0,255): count_b[i] = count_b[i]/(height*width) count_g[i] = count_g[i]/(height*width) count_r[i] = count_r[i]/(height*width) #计算累计概率 sum_b = float(0) sum_g = float(0) sum_r = float(0) for i in range(0,256): sum_b = sum_b+count_b[i] sum_g = sum_g+count_g[i] sum_r = sum_r+count_r[i] count_b[i] = sum_b count_g[i] = sum_g count_r[i] = sum_r #print(count) # 计算映射表 map_b = np.zeros(256,np.uint16) map_g = np.zeros(256,np.uint16) map_r = np.zeros(256,np.uint16) for i in range(0,256): map_b[i] = np.uint16(count_b[i]*255) map_g[i] = np.uint16(count_g[i]*255) map_r[i] = np.uint16(count_r[i]*255) # 映射 dst = np.zeros((height,width,3),np.uint8) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] b = map_b[b] g = map_g[g] r = map_r[r] dst[i,j] = (b,g,r) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 彩色直方图APIimport cv2 import numpy as np def ImageHist(image,type): color = (255,255,255) windowName = &#39;Gray&#39; if type == 31: color = (255,0,0) windowName = &#39;B Hist&#39; elif type == 32: color = (0,255,0) windowName = &#39;G Hist&#39; elif type == 33: color = (0,0,255) windowName = &#39;R Hist&#39; # 计算图片直方图1 image 2 [0]灰度直方图 3 mask None蒙版 4 256 5 0-255 hist = cv2.calcHist([image],[0],None,[256],[0.0,255.0]) # 获取像素值中最大最小值及各自下标 归一化处理 minV,maxV,minL,maxL = cv2.minMaxLoc(hist) histImg = np.zeros([256,256,3],np.uint8) for h in range(256): #处理完后绘制结果 intenNormal = int(hist[h]*256/maxV) cv2.line(histImg,(h,256),(h,256-intenNormal),color) cv2.imshow(windowName,histImg) return histImg img = cv2.imread(&#39;image0.jpg&#39;,1) channels = cv2.split(img)# RGB - R G B for i in range(0,3): ImageHist(channels[i],31+i) cv2.waitKey(0) 直方图均衡化#灰度 直方图均衡化 import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) cv2.imshow(&#39;src&#39;,gray) dst = cv2.equalizeHist(gray) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) #彩色 直方图均衡化 import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) (b,g,r) = cv2.split(img)#通道分解 bH = cv2.equalizeHist(b) gH = cv2.equalizeHist(g) rH = cv2.equalizeHist(r) result = cv2.merge((bH,gH,rH))# 通道合成 cv2.imshow(&#39;dst&#39;,result) cv2.waitKey(0) #YUV 直方图均衡化 import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgYUV = cv2.cvtColor(img,cv2.COLOR_BGR2YCrCb) cv2.imshow(&#39;src&#39;,img) channelYUV = cv2.split(imgYUV) channelYUV[0] = cv2.equalizeHist(channelYUV[0]) channels = cv2.merge(channelYUV) result = cv2.cvtColor(channels,cv2.COLOR_YCrCb2BGR) cv2.imshow(&#39;dst&#39;,result) cv2.waitKey(0) 图片修补生成坏图import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) for i in range(200,300): img[i,200] = (255,255,255) img[i,200+1] = (255,255,255) img[i,200-1] = (255,255,255) for i in range(150,250): img[250,i] = (255,255,255) img[250+1,i] = (255,255,255) img[250-1,i] = (255,255,255) cv2.imwrite(&#39;damaged.jpg&#39;,img) cv2.imshow(&#39;image&#39;,img) cv2.waitKey(0)修补#1 坏图 2 array 3 inpaint import cv2 import numpy as np img = cv2.imread(&#39;damaged.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] paint = np.zeros((height,width,1),np.uint8) # 描绘坏的部分的数组 for i in range(200,300): paint[i,200] = 255 paint[i,200+1] = 255 paint[i,200-1] = 255 for i in range(150,250): paint[250,i] = 255 paint[250+1,i] = 255 paint[250-1,i] = 255 cv2.imshow(&#39;paint&#39;,paint) #1 src 2 mask imgDst = cv2.inpaint(img,paint,3,cv2.INPAINT_TELEA) cv2.imshow(&#39;image&#39;,imgDst) cv2.waitKey(0) 亮度增强p = p+40import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] cv2.imshow(&#39;src&#39;,img) dst = np.zeros((height,width,3),np.uint8) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] bb = int(b)+40 gg = int(g)+40 rr = int(r)+40 if bb&gt;255: bb = 255 if gg&gt;255: gg = 255 if rr&gt;255: rr = 255 dst[i,j] = (bb,gg,rr) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) p = p*1.2+40import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] cv2.imshow(&#39;src&#39;,img) dst = np.zeros((height,width,3),np.uint8) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] bb = int(b*1.3)+10 gg = int(g*1.2)+15 if bb&gt;255: bb = 255 if gg&gt;255: gg = 255 dst[i,j] = (bb,gg,r) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 磨皮美白### 双边滤波 import cv2 img = cv2.imread(&#39;1.png&#39;,1) cv2.imshow(&#39;src&#39;,img) dst = cv2.bilateralFilter(img,15,35,35) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 高斯滤波import cv2 import numpy as np img = cv2.imread(&#39;image11.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) dst = cv2.GaussianBlur(img,(5,5),1.5) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 均值滤波#均值 6*6 1 。 * 【6*6】/36 = mean -》P import cv2 import numpy as np img = cv2.imread(&#39;image11.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] dst = np.zeros((height,width,3),np.uint8) for i in range(3,height-3): for j in range(3,width-3): sum_b = int(0) sum_g = int(0) sum_r = int(0) for m in range(-3,3):#-3 -2 -1 0 1 2 for n in range(-3,3): (b,g,r) = img[i+m,j+n] sum_b = sum_b+int(b) sum_g = sum_g+int(g) sum_r = sum_r+int(r) b = np.uint8(sum_b/36) g = np.uint8(sum_g/36) r = np.uint8(sum_r/36) dst[i,j] = (b,g,r) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 中值滤波# 中值滤波 3*3 import cv2 import numpy as np img = cv2.imread(&#39;image11.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] img = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY) cv2.imshow(&#39;src&#39;,img) dst = np.zeros((height,width,3),np.uint8) collect = np.zeros(9,np.uint8) for i in range(1,height-1): for j in range(1,width-1): k = 0 for m in range(-1,2): for n in range(-1,2): gray = img[i+m,j+n] collect[k] = gray k = k+1 # 0 1 2 3 4 5 6 7 8 # 1 for k in range(0,9): p1 = collect[k] for t in range(k+1,9): if p1&lt;collect[t]: mid = collect[t] collect[t] = p1 p1 = mid dst[i,j] = collect[4] cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) Q1:ImportError: libXext.so.6: cannot open shared object file: No such file or directory yum install libXext.x86_64 Q2: ImportError: libSM.so.6: cannot open shared object file: No such file or directory yum install libSM.x86_64 Q3:libXrender.so.1: cannot open shared object file: No such file or directory yum install libXrender.x86_64]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow与Flask结合打造手写体数字识别]]></title>
    <url>%2F2019%2F08%2F30%2FTensorFlow%E4%B8%8EFlask%E7%BB%93%E5%90%88%E6%89%93%E9%80%A0%E6%89%8B%E5%86%99%E4%BD%93%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[TensorFlow与Flask结合打造手写体数字识别 AnacondaAnaconda Navigator新建Env环境,添加channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/tensorflow和opencv，并进入Home安装Jupyter notebook 定义模型modelmnist_testdemo/mnist/model.py 线性模型import tensorflow as tf # Y=W*x+b 线性模型 def regression(x): W = tf.Variable(tf.zeros([784, 10]), name=&quot;W&quot;) b = tf.Variable(tf.zeros([10]), name=&quot;b&quot;) y = tf.nn.softmax(tf.matmul(x, W) + b) return y, [W, b] 卷积模型# 卷积模型 def convolutional(x, keep_prob): # 卷积层 def conv2d(x, W): return tf.nn.conv2d(x, W, [1, 1, 1, 1], padding=&#39;SAME&#39;) # 池化层 def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&#39;SAME&#39;) # 定义权重 def weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial) # 边 def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) x_image = tf.reshape(x, [-1, 28, 28, 1]) W_conv1 = weight_variable([5, 5, 1, 32]) b_conv1 = bias_variable([32]) h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) h_pool1 = max_pool_2x2(h_conv1) W_conv2 = weight_variable([5, 5, 32, 64]) b_conv2 = bias_variable([64]) h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) h_pool2 = max_pool_2x2(h_conv2) # full connection W_fc1 = weight_variable([7 * 7 * 64, 1024]) b_fc1 = bias_variable([1024]) h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64]) h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) W_fc2 = weight_variable([1024, 10]) b_fc2 = bias_variable([10]) y = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) return y, [W_conv1, b_conv1, W_conv2, b_conv2, W_fc1, b_fc1, W_fc2, b_fc2] 定义数据mnist_testdemo/mnist/input_data.py from __future__ import absolute_import from __future__ import division from __future__ import print_function import gzip import os import tempfile import numpy from six.moves import urllib from six.moves import xrange import tensorflow as tf from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets 训练线性模型import os import input_data import model import tensorflow as tf # 从input_data中下载数据到MNIST_data data = input_data.read_data_sets(&#39;MNIST_data&#39;, one_hot=True) # create model with tf.variable_scope(&quot;regression&quot;): # 用户输入占位符 x = tf.placeholder(tf.float32, [None, 784]) y, variables = model.regression(x) # train y_ = tf.placeholder(&quot;float&quot;, [None, 10]) cross_entropy = -tf.reduce_sum(y_ * tf.log(y)) # 训练步骤 train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy) # 预测 correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) # 准确度 accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # 保存训练变量参数 saver = tf.train.Saver(variables) # 开始训练 with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for _ in range(20000): batch_xs, batch_ys = data.train.next_batch(100) sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys}) # 打印测试集和训练集的精准度 print((sess.run(accuracy, feed_dict={x:data.test.images, y_:data.test.labels}))) # 保存训练好的模型 path = saver.save( sess,os.path.join(os.path.dirname(__file__),&#39;data&#39;,&#39;regression.ckpt&#39;), write_meta_graph=False,write_state=False) print(&quot;Saved:&quot;, path) 生成mnist_testdemo/mnist/data/regression.ckpt.data-00000-of-00001和mnist_testdemo/mnist/data/regression.ckpt.index 训练卷积模型import os import model import tensorflow as tf import input_data data = input_data.read_data_sets(&#39;MNIST_data&#39;, one_hot=True) #model with tf.variable_scope(&quot;convolutional&quot;): x = tf.placeholder(tf.float32, [None, 784], name=&#39;x&#39;) keep_prob = tf.placeholder(tf.float32) y, variables = model.convolutional(x, keep_prob) #train y_ = tf.placeholder(tf.float32, [None, 10], name=&#39;y&#39;) cross_entropy = -tf.reduce_sum(y_ * tf.log(y)) # 随机梯度下降 train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) saver = tf.train.Saver(variables) with tf.Session() as sess: merged_summary_op = tf.summary.merge_all() summay_writer = tf.summary.FileWriter(&#39;./mnist_log/1&#39;, sess.graph) summay_writer.add_graph(sess.graph) sess.run(tf.global_variables_initializer()) for i in range(20000): batch = data.train.next_batch(50) if i % 100 == 0: train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0}) print(&quot;step %d, training accuracy %g&quot; % (i, train_accuracy)) sess.run(train_step, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5}) print(sess.run(accuracy, feed_dict={x: data.test.images, y_: data.test.labels, keep_prob: 1.0})) path = saver.save( sess, os.path.join(os.path.dirname(__file__), &#39;data&#39;, &#39;convalutional.ckpt&#39;), write_meta_graph=False, write_state=False) print(&quot;Saved:&quot;, path) 生成 mnist_testdemo/mnist/data/convalutional.ckpt.data-00000-of-00001和mnist_testdemo/mnist/data/convalutional.ckpt.index 集成flaskmnist_testdemo/main.py # -*- coding:utf-8 -*- import numpy as np import tensorflow as tf from flask import Flask, jsonify, render_template, request import pprint from mnist import model x = tf.placeholder(&quot;float&quot;, [None, 784]) sess = tf.Session() # 取出训练好的线性模型 with tf.variable_scope(&quot;regression&quot;): y1, variables = model.regression(x) saver = tf.train.Saver(variables) saver.restore(sess, &quot;mnist/data/regression.ckpt&quot;) # 取出训练好的卷积模型 with tf.variable_scope(&quot;convolutional&quot;): keep_prob = tf.placeholder(&quot;float&quot;) y2, variables = model.convolutional(x, keep_prob) saver = tf.train.Saver(variables) saver.restore(sess, &quot;mnist/data/convalutional.ckpt&quot;) # 根据输入调用线性模型并返回识别结果 def regression(input): return sess.run(y1, feed_dict={x: input}).flatten().tolist() # 根据输入调用卷积模型并返回识别结果 def convolutional(input): return sess.run(y2, feed_dict={x: input, keep_prob: 1.0}).flatten().tolist() app = Flask(__name__) @app.route(&#39;/api/mnist&#39;, methods=[&#39;POST&#39;]) def mnist(): # pprint.pprint(request.json) input = ((255 - np.array(request.json, dtype=np.uint8)) / 255.0).reshape(1, 784) output1 = regression(input) output2 = convolutional(input) pprint.pprint(output1) pprint.pprint(output2) return jsonify(results=[output1, output2]) @app.route(&#39;/&#39;) def main(): return render_template(&#39;index.html&#39;) if __name__ == &#39;__main__&#39;: app.debug = True app.run(host=&#39;0.0.0.0&#39;, port=8889) js核心代码drawInput() { var ctx = this.input.getContext(&#39;2d&#39;); var img = new Image(); img.onload = () =&gt; { var inputs = []; var small = document.createElement(&#39;canvas&#39;).getContext(&#39;2d&#39;); small.drawImage(img, 0, 0, img.width, img.height, 0, 0, 28, 28); var data = small.getImageData(0, 0, 28, 28).data; for (var i = 0; i &lt; 28; i++) { for (var j = 0; j &lt; 28; j++) { var n = 4 * (i * 28 + j); inputs[i * 28 + j] = (data[n + 0] + data[n + 1] + data[n + 2]) / 3; ctx.fillStyle = &#39;rgb(&#39; + [data[n + 0], data[n + 1], data[n + 2]].join(&#39;,&#39;) + &#39;)&#39;; ctx.fillRect(j * 5, i * 5, 5, 5); } } if (Math.min(...inputs) === 255) { return; } $.ajax({ url: &#39;/api/mnist&#39;, type: &#39;POST&#39;, contentType: &#39;application/json&#39;, data: JSON.stringify(inputs), success: (data) =&gt; { data = JSON.parse(data); for (let i = 0; i &lt; 2; i++) { var max = 0; var max_index = 0; for (let j = 0; j &lt; 10; j++) { var value = Math.round(data.results[i][j] * 1000); if (value &gt; max) { max = value; max_index = j; } var digits = String(value).length; for (var k = 0; k &lt; 3 - digits; k++) { value = &#39;0&#39; + value; } var text = &#39;0.&#39; + value; if (value &gt; 999) { text = &#39;1.000&#39;; } $(&#39;#output tr&#39;).eq(j + 1).find(&#39;td&#39;).eq(i).text(text); } for (let j = 0; j &lt; 10; j++) { if (j === max_index) { $(&#39;#output tr&#39;).eq(j + 1).find(&#39;td&#39;).eq(i).addClass(&#39;success&#39;); } else { $(&#39;#output tr&#39;).eq(j + 1).find(&#39;td&#39;).eq(i).removeClass(&#39;success&#39;); } } } } }); }; img.src = this.canvas.toDataURL(); } 前端将数据inputs以json传入/api/mnist regression(input)和convolutional(input)调用模型feed_dict喂参数返回结果]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nlp依存句法和语义依存分析]]></title>
    <url>%2F2019%2F08%2F27%2Fnlp%E4%BE%9D%E5%AD%98%E5%8F%A5%E6%B3%95%E5%92%8C%E8%AF%AD%E4%B9%89%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[nlp依存句法和语义依存分析 依存句法分析 依存语法 (Dependency Parsing, DP) 通过分析语言单位内成分之间的依存关系揭示其句法结构。 直观来讲,依存句法分析识别句子中的“主谓宾”、“定状补”这些语法成分,并分析各成分之间的关系。 依存句法分析标注关系 (共14种) 及含义如下:主谓关系 SBV subject-verb 我送她一束花 (我 &lt;– 送) 动宾关系 VOB 直接宾语,verb-object 我送她一束花 (送 –&gt; 花) 间宾关系 IOB 间接宾语,indirect-object 我送她一束花 (送 –&gt; 她) 前置宾语 FOB 前置宾语,fronting-object 他什么乢都读 (乢 &lt;– 读) 兼语 DBL double 他请我吃饭 (请 –&gt; 我) 定中关系 ATT attribute 红苹果 (红 &lt;– 苹果) 状中结构 ADV adverbial 非常美丽 (非常 &lt;– 美丽) 动补结构 CMP complement 做完了作业 (做 –&gt; 完) 并列关系 COO coordinate 大山和大海 (大山 –&gt; 大海) 介宾关系 POB preposition-object 在贸易区内 (在 –&gt; 内) 左附加关系 LAD left adjunct 大山和大海 (和 &lt;– 大海) 右附加关系 RAD right adjunct 孩子们 (孩子 –&gt; 们) 独立结构 IS independent structure 两个单句在结构上彼此独立 核心关系 HED head 指整个句子的核心 依存句法树解析recursionSearch.py # encoding=utf8 import re, os, json from stanfordParse import pos from stanfordParse import parse_sentence from recursionSearch import search def split_long_sentence_by_pos(text): del_flag = [&#39;DEC&#39;, &#39;AD&#39;, &#39;DEG&#39;, &#39;DER&#39;, &#39;DEV&#39;, &#39;SP&#39;, &#39;AS&#39;, &#39;ETC&#39;, &#39;SP&#39;, &#39;MSP&#39;, &#39;IJ&#39;, &#39;ON&#39;, &#39;JJ&#39;, &#39;FW&#39;, &#39;LB&#39;, &#39;SB&#39;, &#39;BA&#39;, &#39;AD&#39;, &#39;PN&#39;, &#39;RB&#39;] pos_tag = pos(text) new_str = &#39;&#39; for apos in pos_tag: if apos[1] not in del_flag: new_str += apos[0] return new_str def extract_parallel(text): parallel_text = [] pattern = re.compile(&#39;[，,][\u4e00-\u9fa5]{2,4}[，,]&#39;) search_obj = pattern.search(text) if search_obj: start_start, end = search_obj.span() rep = text[start_start:end - 2] rep1 = text[start_start:end - 1] if &#39;，&#39; in rep1: rep1.replace(&#39;，&#39;, &#39;、&#39;) if &#39;,&#39; in rep1: rep1.replace(&#39;,&#39;, &#39;、&#39;) text.replace(rep1, text) parallel_text.append(rep[1:]) text_leave = text.replace(rep, &#39;&#39;) while pattern.search(text_leave): start, end = pattern.search(text_leave).span() rep = text_leave[start:end - 2] rep1 = text[start_start:end - 1] if &#39;，&#39; in rep1: rep1.replace(&#39;，&#39;, &#39;、&#39;) if &#39;,&#39; in rep1: rep1.replace(&#39;,&#39;, &#39;、&#39;) text.replace(rep1, text) text_leave = text_leave.replace(rep, &#39;&#39;) parallel_text.append(rep[1:]) return parallel_text, text else: return None, text def split_long_sentence_by_sep(text): segment = [] if &#39;。&#39; or &#39;.&#39; or &#39;!&#39; or &#39;！&#39; or &#39;?&#39; or &#39;？&#39; or &#39;;&#39; or &#39;；&#39; in text: text = re.split(r&#39;[。.!！?？;；]&#39;, text) for seg in text: if seg == &#39;&#39; or seg == &#39; &#39;: continue para, seg = extract_parallel(seg) if len(seg) &gt; 19: seg = split_long_sentence_by_pos(seg) if len(seg) &gt; 19: seg = re.split(&#39;[，,]&#39;, seg) if isinstance(seg, list) and &#39;&#39; in seg: seg = seg.remove(&#39;&#39;) if isinstance(seg, list) and &#39; &#39; in seg: seg = seg.remove(&#39; &#39;) segment.append(seg) return segment def read_data(path): return open(path, &quot;r&quot;, encoding=&quot;utf8&quot;) def get_np_words(t): noun_phrase_list = [] for tree in t.subtrees(lambda t: t.height() == 3): if tree.label() == &#39;NP&#39; and len(tree.leaves()) &gt; 1: noun_phrase = &#39;&#39;.join(tree.leaves()) noun_phrase_list.append(noun_phrase) return noun_phrase_list def get_n_v_pair(t): for tree in t.subtrees(lambda t: t.height() == 3): if tree.label() == &#39;NP&#39; and len(tree.leaves()) &gt; 1: noun_phrase = &#39;&#39;.join(tree.leaves()) if __name__ == &quot;__main__&quot;: out = open(&quot;dependency.txt&quot;, &#39;w&#39;, encoding=&#39;utf8&#39;) itera = read_data(&#39;text.txt&#39;) for it in itera: s = parse_sentence(it) # 通过Stanfordnlp依存句法分析得到一个句法树 用nltk包装成树的结构 res = search(s) # 使用nltk遍历树，然后把短语合并 print(res) stanfordParse.py # encoding=utf8 from stanfordcorenlp import StanfordCoreNLP from nltk import Tree, ProbabilisticTree nlp = StanfordCoreNLP(&#39;E:/stanford-corenlp-full-2018-10-05&#39;, lang=&#39;zh&#39;) import nltk, re grammer = &quot;NP: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN&gt;}&quot; cp = nltk.RegexpParser(grammer) # 生成规则 pattern = re.compile(u&#39;[^a-zA-Z\u4E00-\u9FA5]&#39;) pattern_del = re.compile(&#39;(\a-zA-Z0-9+)&#39;) def _replace_c(text): &quot;&quot;&quot; 将英文标点符号替换成中文标点符号，并去除html语言的一些标志等噪音 :param text: :return: &quot;&quot;&quot; intab = &quot;,?!()&quot; outtab = &quot;，？！（）&quot; deltab = &quot; \n&lt;li&gt;&lt; li&gt;+_-.&gt;&lt;li \U0010fc01 _&quot; trantab = text.maketrans(intab, outtab, deltab) return text.translate(trantab) def parse_sentence(text): text = _replace_c(text) # 文本去噪 try: if len(text.strip()) &gt; 6: # 判断，文本是否大于6个字，小于6个字的我们认为不是句子 return Tree.fromstring( nlp.parse(text.strip())) # nlp.parse(text.strip())：是将句子变成依存句法树 Tree.fromstring是将str类型的树转换成nltk的结构的树 except: pass def pos(text): text = _replace_c(text) if len(text.strip()) &gt; 6: return nlp.pos_tag(text) else: return False def denpency_parse(text): return nlp.dependency_parse(text) from nltk.chunk.regexp import * sentenceSplit_host.py # encoding=utf8 import re, os, json from stanfordParse import pos from stanfordParse import parse_sentence from recursionSearch import search def split_long_sentence_by_pos(text): del_flag = [&#39;DEC&#39;, &#39;AD&#39;, &#39;DEG&#39;, &#39;DER&#39;, &#39;DEV&#39;, &#39;SP&#39;, &#39;AS&#39;, &#39;ETC&#39;, &#39;SP&#39;, &#39;MSP&#39;, &#39;IJ&#39;, &#39;ON&#39;, &#39;JJ&#39;, &#39;FW&#39;, &#39;LB&#39;, &#39;SB&#39;, &#39;BA&#39;, &#39;AD&#39;, &#39;PN&#39;, &#39;RB&#39;] pos_tag = pos(text) new_str = &#39;&#39; for apos in pos_tag: if apos[1] not in del_flag: new_str += apos[0] return new_str def extract_parallel(text): parallel_text = [] pattern = re.compile(&#39;[，,][\u4e00-\u9fa5]{2,4}[，,]&#39;) search_obj = pattern.search(text) if search_obj: start_start, end = search_obj.span() rep = text[start_start:end - 2] rep1 = text[start_start:end - 1] if &#39;，&#39; in rep1: rep1.replace(&#39;，&#39;, &#39;、&#39;) if &#39;,&#39; in rep1: rep1.replace(&#39;,&#39;, &#39;、&#39;) text.replace(rep1, text) parallel_text.append(rep[1:]) text_leave = text.replace(rep, &#39;&#39;) while pattern.search(text_leave): start, end = pattern.search(text_leave).span() rep = text_leave[start:end - 2] rep1 = text[start_start:end - 1] if &#39;，&#39; in rep1: rep1.replace(&#39;，&#39;, &#39;、&#39;) if &#39;,&#39; in rep1: rep1.replace(&#39;,&#39;, &#39;、&#39;) text.replace(rep1, text) text_leave = text_leave.replace(rep, &#39;&#39;) parallel_text.append(rep[1:]) return parallel_text, text else: return None, text def split_long_sentence_by_sep(text): segment = [] if &#39;。&#39; or &#39;.&#39; or &#39;!&#39; or &#39;！&#39; or &#39;?&#39; or &#39;？&#39; or &#39;;&#39; or &#39;；&#39; in text: text = re.split(r&#39;[。.!！?？;；]&#39;, text) for seg in text: if seg == &#39;&#39; or seg == &#39; &#39;: continue para, seg = extract_parallel(seg) if len(seg) &gt; 19: seg = split_long_sentence_by_pos(seg) if len(seg) &gt; 19: seg = re.split(&#39;[，,]&#39;, seg) if isinstance(seg, list) and &#39;&#39; in seg: seg = seg.remove(&#39;&#39;) if isinstance(seg, list) and &#39; &#39; in seg: seg = seg.remove(&#39; &#39;) segment.append(seg) return segment def read_data(path): return open(path, &quot;r&quot;, encoding=&quot;utf8&quot;) def get_np_words(t): noun_phrase_list = [] for tree in t.subtrees(lambda t: t.height() == 3): if tree.label() == &#39;NP&#39; and len(tree.leaves()) &gt; 1: noun_phrase = &#39;&#39;.join(tree.leaves()) noun_phrase_list.append(noun_phrase) return noun_phrase_list def get_n_v_pair(t): for tree in t.subtrees(lambda t: t.height() == 3): if tree.label() == &#39;NP&#39; and len(tree.leaves()) &gt; 1: noun_phrase = &#39;&#39;.join(tree.leaves()) if __name__ == &quot;__main__&quot;: out = open(&quot;dependency.txt&quot;, &#39;w&#39;, encoding=&#39;utf8&#39;) itera = read_data(&#39;text.txt&#39;) for it in itera: s = parse_sentence(it) # 通过Stanfordnlp依存句法分析得到一个句法树 用nltk包装成树的结构 res = search(s) # 使用nltk遍历树，然后把短语合并 print(res) 语义依存分析 语义依存分析：分析句子各个语言单位之间的语义关联,并将语义关联以依存结构呈现。使用语义依存刻画句子语义,好处在于丌需要去抽象词汇本身,而是通过词汇所承受的语义框架来描述该词汇,而论元的数目相对词汇来说数量总是少了很多的。语义依存分析目标是跨越句子表层句法结构的束缚,直接获取深层的语义信息。 例如以下三个句子,用不同的表达方式表达了同一个语义信息,即张三实施了一个吃的动作,吃的动作是对苹果实施的。 • 语义依存分析不受句法结构的影响,将具有直接语义关联的语言单元直接连接依存弧并标记上相应的语义关系。这也是语义依存分析不句法依存分析的重要区别。 • 语义依存关系分为三类,分别是主要语义角色,每一种语义角色对应存在一个嵌套关系和反关系;事件关系,描述两个事件间的关系;语义依附标记,标记说话者语气等依附性信息。 语义依存分析标注关系及含义如下:关系类型 Tag Description Example 施事关系 Agt Agent 我送她一束花 (我 &lt;-- 送) 当事关系 Exp Experiencer 我跑得快 (跑 --&gt; 我) 感事关系 Aft Affection 我思念家乡 (思念 --&gt; 我) 领事关系 Poss Possessor 他有一本好读 (他 &lt;-- 有) 受事关系 Pat Patient 他打了小明 (打 --&gt; 小明) 客事关系 Cont Content 他听到鞭炮声 (听 --&gt; 鞭炮声) 成事关系 Prod Product 他写了本小说 (写 --&gt; 小说) 源事关系 Orig Origin 我军缴获敌人四辆坦克 (缴获 --&gt; 坦克) 涉事关系 Datv Dative 他告诉我个秘密 ( 告诉 --&gt; 我 ) 比较角色 Comp Comitative 他成绩比我好 (他 --&gt; 我) 属事角色 Belg Belongings 老赵有俩女儿 (老赵 &lt;-- 有) 类事角色 Clas Classification 他是中学生 (是 --&gt; 中学生) 依据角色 Accd According 本庭依法宣判 (依法 &lt;-- 宣判) 缘故角色 Reas Reason 他在愁女儿婚事 (愁 --&gt; 婚事) 。。。。。。 名词短语块挖掘# encoding=utf8 import os, json, nltk, re from jpype import * from tokenizer import cut_hanlp huanhang = set([&#39;。&#39;, &#39;？&#39;, &#39;！&#39;, &#39;?&#39;]) keep_pos = &quot;q,qg,qt,qv,s,t,tg,g,gb,gbc,gc,gg,gm,gp,mg,Mg,n,an,ude1,nr,ns,nt,nz,nb,nba,nbc,nbp,nf,ng,nh,nhd,o,nz,nx,ntu,nts,nto,nth,ntch,ntcf,ntcb,ntc,nt,nsf,ns,nrj,nrf,nr2,nr1,nr,nnt,nnd,nn,nmc,nm,nl,nit,nis,nic,ni,nhm,nhd&quot; keep_pos_nouns = set(keep_pos.split(&quot;,&quot;)) keep_pos_v = &quot;v,vd,vg,vf,vl,vshi,vyou,vx,vi,vn&quot; keep_pos_v = set(keep_pos_v.split(&quot;,&quot;)) keep_pos_p = set([&#39;p&#39;, &#39;pbei&#39;, &#39;pba&#39;]) merge_pos = keep_pos_p | keep_pos_v keep_flag = set( [&#39;：&#39;, &#39;，&#39;, &#39;？&#39;, &#39;。&#39;, &#39;！&#39;, &#39;；&#39;, &#39;、&#39;, &#39;-&#39;, &#39;.&#39;, &#39;!&#39;, &#39;,&#39;, &#39;:&#39;, &#39;;&#39;, &#39;?&#39;, &#39;(&#39;, &#39;)&#39;, &#39;（&#39;, &#39;）&#39;, &#39;&lt;&#39;, &#39;&gt;&#39;, &#39;《&#39;, &#39;》&#39;]) drop_pos_set = set( [&#39;xu&#39;, &#39;xx&#39;, &#39;y&#39;, &#39;yg&#39;, &#39;wh&#39;, &#39;wky&#39;, &#39;wkz&#39;, &#39;wp&#39;, &#39;ws&#39;, &#39;wyy&#39;, &#39;wyz&#39;, &#39;wb&#39;, &#39;u&#39;, &#39;ud&#39;, &#39;ude1&#39;, &#39;ude2&#39;, &#39;ude3&#39;, &#39;udeng&#39;, &#39;udh&#39;]) def getNodes(parent, model_tagged_file): # 使用for循环遍历树 text = &#39;&#39; for node in parent: if type(node) is nltk.Tree: # 如果是NP或者VP的合并分词 if node.label() == &#39;NP&#39;: text += &#39;&#39;.join(node_child[0].strip() for node_child in node.leaves()) + &quot;/NP&quot; + 3 * &quot; &quot; if node.label() == &#39;VP&#39;: text += &#39;&#39;.join(node_child[0].strip() for node_child in node.leaves()) + &quot;/VP&quot; + 3 * &quot; &quot; else: # 不是树的，就是叶子节点，我们直接表解词PP或者其他O if node[1] in keep_pos_p: text += node[0].strip() + &quot;/PP&quot; + 3 * &quot; &quot; if node[0] in huanhang: text += node[0].strip() + &quot;/O&quot; + 3 * &quot; &quot; if node[1] not in merge_pos: text += node[0].strip() + &quot;/O&quot; + 3 * &quot; &quot; # print(&quot;hh&quot;) model_tagged_file.write(text + &quot;\n&quot;) def grammer(sentence, model_tagged_file): # {内/f 训/v 师/ng 单/b 柜/ng} &quot;&quot;&quot; input sentences shape like :[(&#39;工作&#39;, &#39;vn&#39;), (&#39;描述&#39;, &#39;v&#39;), (&#39;：&#39;, &#39;w&#39;), (&#39;我&#39;, &#39;rr&#39;), (&#39;曾&#39;, &#39;d&#39;), (&#39;在&#39;, &#39;p&#39;)] &quot;&quot;&quot; # 定义名词块 “&lt; &gt;”:一个单元 “*”：匹配零次或多次 “+”：匹配一次或多次 “&lt;ude1&gt;?”： “的”出现零次或一次 grammar1 = r&quot;&quot;&quot;NP: {&lt;m|mg|Mg|mq|q|qg|qt|qv|s|&gt;*&lt;a|an|ag&gt;*&lt;s|g|gb|gbc|gc|gg|gm|gp|n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|o|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd&gt;+&lt;f&gt;?&lt;ude1&gt;?&lt;g|gb|gbc|gc|gg|gm|gp|n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|o|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd&gt;+} {&lt;n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd&gt;+&lt;cc&gt;+&lt;n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd&gt;+} {&lt;m|mg|Mg|mq|q|qg|qt|qv|s|&gt;*&lt;q|qg|qt|qv&gt;*&lt;f|b&gt;*&lt;vi|v|vn|vg|vd&gt;+&lt;ude1&gt;+&lt;n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd&gt;+} {&lt;g|gb|gbc|gc|gg|gm|gp|n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd&gt;+&lt;vi&gt;?} VP:{&lt;v|vd|vg|vf|vl|vshi|vyou|vx|vi|vn&gt;+} &quot;&quot;&quot; # 动词短语块 cp = nltk.RegexpParser(grammar1) try: result = cp.parse(sentence) # nltk的依存语法分析，输出是以grammer设置的名词块为单位的树 except: pass else: getNodes(result, model_tagged_file) # 使用 getNodes 遍历树【这个是使用for循环，上一个是使用栈动态添加】 def data_read(): fout = open(&#39;nvp.txt&#39;, &#39;w&#39;, encoding=&#39;utf8&#39;) for line in open(&#39;text.txt&#39;, &#39;r&#39;, encoding=&#39;utf8&#39;): line = line.strip() grammer(cut_hanlp(line), fout) # 先进行hanlp进行分词，在使用grammer进行合并短语 fout.close() if __name__ == &#39;__main__&#39;: data_read() 自定义语法与CFG什么是语法解析? • 在自然语言学习过程中,每个人一定都学过语法,例如句子可以用主语、谓语、宾语来表示。在自然语言的处理过程中,有许多应用场景都需要考虑句子的语法,因此研究语法解析变得非常重要。 • 语法解析有两个主要的问题,其一是句子语法在计算机中的表达与存储方法,以及语料数据集;其二是语法解析的算法。 句子语法在计算机中的表达与存储方法• 对于第一个问题,我们可以用树状结构图来表示,如下图所示,S表示句子;NP、VP、PP是名词、动词、介词短语(短语级别);N、V、P分别是名词、动词、介词。 语法解析的算法上下文无关语法(Context-Free Grammer)• 为了生成句子的语法树,我们可以定义如下的一套上下文无关语法。 • 1)N表示一组非叶子节点的标注,例如{S、NP、VP、N...} • 2)Σ表示一组叶子结点的标注,例如{boeing、is...} • 3)R表示一组觃则,每条规则可以表示为 • 4)S表示语法树开始的标注 • 举例来说,语法的一个语法子集可以表示为下图所示。 当给定一个句子时,我们便可以按照从左到右的顺序来解析语法。 例如,句子the man sleeps就可以表示为(S (NP (DT the) (NN man)) (VP sleeps))。 概率分布的上下文无关语法(Probabilistic Context-Free Grammar)• 上下文无关的语法可以很容易的推导出一个句子的语法结构,但是缺点是推导出的结构可能存在二义性。 • 由于语法的解析存在二义性,我们就需要找到一种方法从多种可能的语法树中找出最可能的一棵树。 一种常见的方法既是PCFG (Probabilistic Context-Free Grammar)。 如下图所示,除了常见的语法规则以外,我们还对每一条规则赋予了一个概率。 对于每一棵生成的语法树,我们将其中所有规则的概率的乘积作为语法树的出现概率。 当我们获得多颗语法树时,我们可以分别计算每颗语法树的概率p(t),出现概率最大的那颗语法树就是我们希望得到的结果,即arg max p(t)。 训练算法• 我们已经定义了语法解析的算法,而这个算法依赖于CFG中对于N、Σ、 R、S的定义以及PCFG中的p(x)。上文中我们提到了Penn Treebank通 过手工的方法已经提供了一个非常大的语料数据集,我们的任务就是从 语料库中训练出PCFG所需要的参数。 • 1)统计出语料库中所有的N与Σ; • 2)利用语料库中的所有规则作为R; • 3)针对每个规则A -&gt; B,从语料库中估算p(x) = p(A -&gt; B) / p(A); • 在CFG的定义的基础上,我们重新定义一种叫Chomsky的语法格式。 这种格式要求每条规则只能是X -&gt; Y1 Y2或者X -&gt; Y的格式。实际上 Chomsky语法格式保证生产的语法树总是二叉树的格式,同时任意一 棵语法树总是能够转化成Chomsky语法格式。语法树预测算法• 假设我们已经有一个PCFG的模型,包含N、Σ、R、S、p(x)等参数,并 且语法树总是Chomsky语法格式。当输入一个句子x1, x2, ... , xn时, 我们要如何计算句子对应的语法树呢? • 第一种方法是暴力遍历的方法,每个单词x可能有m = len(N)种取值, 句子长度是n,每种情况至少存在n个规则,所以在时间复杂度O(m n n) 的情况下,我们可以判断出所有可能的语法树并计算出最佳的那个。 • 第二种方法当然是动态规划,我们定义w[i, j, X]是第i个单词至第j个单 词由标注X来表示的最大概率。直观来讲,例如xi, xi+1, ... , xj,当 X=PP时,子树可能是多种解释方式,如(P NP)或者(PP PP),但是w[i, j, PP]代表的是继续往上一层递归时,我们只选择当前概率最大的组合 方式。 语法解析按照上述的算法过程便完成了。虽说PCFG也有一些缺点,例如:1)缺乏词法信息;2)连续短语(如名词、介词)的处理等。但总体来讲它给语法解析提供了一种非常有效的实现方法。 # encoding=utf8 def exec_cmd(cmd): p = subprocess.Popen( cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, env=ENVIRON) out, err = p.communicate() return out, err import nltk, os, jieba from nltk.tree import Tree from nltk.draw import TreeWidget from nltk.draw.tree import TreeView from nltk.draw.util import CanvasFrame from nltk.parse import RecursiveDescentParser class Cfg(): &#39;&#39;&#39; &#39;&#39;&#39; def setUp(self): pass def tearDown(self): pass def test_sample(self): print(&quot;test_sample&quot;) # This is a CFG grammar, where: # Start Symbol : S # Nonterminal : NP,VP,DT,NN,VB # Terminal : &quot;I&quot;, &quot;a&quot; ,&quot;saw&quot; ,&quot;dog&quot; grammar = nltk.grammar.CFG.fromstring(&quot;&quot;&quot; S -&gt; NP VP NP -&gt; DT NN | NN VP -&gt; VB NP DT -&gt; &quot;a&quot; NN -&gt; &quot;I&quot; | &quot;dog&quot; VB -&gt; &quot;saw&quot; &quot;&quot;&quot;) sentence = &quot;I saw a dog&quot;.split() parser = RecursiveDescentParser(grammar) final_tree = parser.parse(sentence) for i in final_tree: print(i) def test_nltk_cfg_qtype(self): print(&quot;test_nltk_cfg_qtype&quot;) gfile = os.path.join( curdir, os.path.pardir, &quot;config&quot;, &quot;grammar.question-type.cfg&quot;) question_grammar = nltk.data.load(&#39;file:%s&#39; % gfile) def get_missing_words(grammar, tokens): &quot;&quot;&quot; Find list of missing tokens not covered by grammar &quot;&quot;&quot; missing = [tok for tok in tokens if not grammar._lexical_index.get(tok)] return missing sentence = &quot;what is your name&quot; sent = sentence.split() missing = get_missing_words(question_grammar, sent) target = [] for x in sent: if x in missing: continue target.append(x) rd_parser = RecursiveDescentParser(question_grammar) result = [] print(&quot;target: &quot;, target) for tree in rd_parser.parse(target): result.append(x) print(&quot;Question Type\n&quot;, tree) if len(result) == 0: print(&quot;Not Question Type&quot;) def cfg_en(self): print(&quot;test_nltk_cfg_en&quot;) # 定义英文语法规则 grammar = nltk.CFG.fromstring(&quot;&quot;&quot; S -&gt; NP VP VP -&gt; V NP | V NP PP V -&gt; &quot;saw&quot; | &quot;ate&quot; NP -&gt; &quot;John&quot; | &quot;Mary&quot; | &quot;Bob&quot; | Det N | Det N PP Det -&gt; &quot;a&quot; | &quot;an&quot; | &quot;the&quot; | &quot;my&quot; N -&gt; &quot;dog&quot; | &quot;cat&quot; | &quot;cookie&quot; | &quot;park&quot; PP -&gt; P NP P -&gt; &quot;in&quot; | &quot;on&quot; | &quot;by&quot; | &quot;with&quot; &quot;&quot;&quot;) sent = &quot;Mary saw Bob&quot;.split() rd_parser = RecursiveDescentParser(grammar) result = [] for i, tree in enumerate(rd_parser.parse(sent)): result.append(tree) assert len(result) &gt; 0, &quot; CFG tree parse fail.&quot; print(result) def cfg_zh(self): grammar = nltk.CFG.fromstring(&quot;&quot;&quot; S -&gt; N VP VP -&gt; V NP | V NP | V N V -&gt; &quot;尊敬&quot; N -&gt; &quot;我们&quot; | &quot;老师&quot; &quot;&quot;&quot;) sent = &quot;我们 尊敬 老师&quot;.split() rd_parser = RecursiveDescentParser(grammar) result = [] for i, tree in enumerate(rd_parser.parse(sent)): result.append(tree) print(&quot;Tree [%s]: %s&quot; % (i + 1, tree)) assert len(result) &gt; 0, &quot;Can not recognize CFG tree.&quot; if len(result) == 1: print(&quot;Draw tree with Display ...&quot;) result[0].draw() else: print(&quot;WARN: Get more then one trees.&quot;) print(result) if __name__ == &#39;__main__&#39;: cfg = Cfg() cfg.cfg_en() cfg.cfg_zh()]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nlp分词词性标注及命名实体]]></title>
    <url>%2F2019%2F08%2F27%2Fnlp%E5%88%86%E8%AF%8D%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8%E5%8F%8A%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%2F</url>
    <content type="text"><![CDATA[nlp分词词性标注及命名实体 分词==中文分词==(Chinese Word Segmentation) 指的是将一个汉字序列切分成一个一个单独的词。分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。 词性标注==词性标注==(Part-of-Speech tagging 或POS tagging) 又称词类标注或者简称标注,是指为分词结果中的每个单词标注一个正确的词性的程 序,也即确定每个词是名词、动词、形容词或其他词性的过程。在汉语中,词性标注比较简单,因为汉语词汇词性多变的情况比较少见,大多词语只有一个词性,或者出现频次最高的词性远远高于第二位的词性。据说,只需选取最高频词性,即可实现80%准确率的中文词性标注程序。 命名实体识别==命名实体识别==(Named Entity Recognition,简称NER) 又称作“专名识别”,是指识别文本中具有特定意义的实体,主要包括人名、地名、机构名、专有名词等。一般来说,命名实体识别的任务就是识别出待处理文本中三大类(实体类、时间类和数字类)、七小类(人名、机构名、地名、时间、日期、货币和百分比)命名实体。 在不同的顷目中,命名实体类别具有不同的定义。 准确分词之加载自定义字典分词当分词工具分词不准确时,该怎么办? 加载自定义字典?该如何加载?cut_data.py # -*- coding=utf8 -*- import jieba import re from tokenizer import cut_hanlp # 加载字典 jieba.load_userdict(&quot;dict.txt&quot;) # 交叉拼接list，把FLAG*替换成*期 def merge_two_list(a, b): c = [] len_a, len_b = len(a), len(b) minlen = min(len_a, len_b) for i in range(minlen): c.append(a[i]) c.append(b[i]) if len_a &gt; len_b: for i in range(minlen, len_a): c.append(a[i]) else: for i in range(minlen, len_b): c.append(b[i]) return c if __name__ == &quot;__main__&quot;: fp = open(&quot;text.txt&quot;, &quot;r&quot;, encoding=&quot;utf8&quot;) fout = open(&quot;result_cut.txt&quot;, &quot;w&quot;, encoding=&quot;utf8&quot;) # 特殊符号字典无法分离，正则区分 regex1 = u&#39;(?:[^\u4e00-\u9fa5（）*&amp;……%￥$，,。.@! ！]){1,5}期&#39; # 非汉字xxx期 regex2 = r&#39;(?:[0-9]{1,3}[.]?[0-9]{1,3})%&#39; # xx.xx% p1 = re.compile(regex1) p2 = re.compile(regex2) for line in fp.readlines(): # 逐行读取 result1 = p1.findall(line) # 返回匹配到的list if result1: regex_re1 = result1 line = p1.sub(&quot;FLAG1&quot;, line) # 将匹配到的替换成FLAG1 result2 = p2.findall(line) if result2: line = p2.sub(&quot;FLAG2&quot;, line) words = jieba.cut(line) # 结巴分词，type(word)返回一个generator object result = &quot; &quot;.join(words) # 结巴分词结果 本身是一个generator object，所以使用 “ ”.join() 拼接起来 # E:\hanlp\data\dictionary\custom\resume_nouns.txt在E:\hanlp\hanlp.properties配置 # CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; 现代汉语补充词库.txt; 全国地名大全.txt ns; 人名词典.txt; 机构名词典.txt; resume_nouns.txt; 上海地名.txt ns;data/dictionary/person/nrf.txt nrf; words1 = cut_hanlp(line) # hanlp分词结果，返回的是str if &quot;FLAG1&quot; in result: result = result.split(&quot;FLAG1&quot;) result = merge_two_list(result, result1) ss = result result = &quot;&quot;.join(result) # 本身是个list，我们需要的是str，所以使用 &quot;&quot;.join() 拼接起来 if &quot;FLAG2&quot; in result: result = result.split(&quot;FLAG2&quot;) result = merge_two_list(result, result2) result = &quot;&quot;.join(result) # print(result) fout.write(&quot;jieba:&quot; + result) fout.write(&quot;hanlp:&quot; + words1) fout.close() tokenizer.py # encoding=utf8 import os, gc, re, sys from jpype import * root_path = &quot;E:/hanlp&quot; djclass_path = &quot;-Djava.class.path=&quot; + root_path + os.sep + &quot;hanlp-1.7.4.jar;&quot; + root_path startJVM(getDefaultJVMPath(), djclass_path, &quot;-Xms1g&quot;, &quot;-Xmx1g&quot;) Tokenizer = JClass(&#39;com.hankcs.hanlp.tokenizer.StandardTokenizer&#39;) def to_string(sentence, return_generator=False): if return_generator: return (word_pos_item.toString().split(&#39;/&#39;) for word_pos_item in Tokenizer.segment(sentence)) else: return &quot; &quot;.join([word_pos_item.toString().split(&#39;/&#39;)[0] for word_pos_item in Tokenizer.segment(sentence)]) # 这里的“”.split(&#39;/&#39;)可以将string拆分成list 如：&#39;ssfa/fsss&#39;.split(&#39;/&#39;) =&gt; [&#39;ssfa&#39;, &#39;fsss&#39;] def seg_sentences(sentence, with_filter=True, return_generator=False): segs = to_string(sentence, return_generator=return_generator) if with_filter: g = [word_pos_pair[0] for word_pos_pair in segs if len(word_pos_pair) == 2 and word_pos_pair[0] != &#39; &#39; and word_pos_pair[1] not in drop_pos_set] else: g = [word_pos_pair[0] for word_pos_pair in segs if len(word_pos_pair) == 2 and word_pos_pair[0] != &#39; &#39;] return iter(g) if return_generator else g def cut_hanlp(raw_sentence, return_list=True): if len(raw_sentence.strip()) &gt; 0: return to_string(raw_sentence) if return_list else iter(to_string(raw_sentence)) 准确分词之动态调整词频和字典顺序当分词字典的词冲突,相互影响该怎么办? 调整词频和字典顺序。cut_data.py # -*- coding=utf8 -*- import jieba import re from tokenizer import cut_hanlp jieba.load_userdict(&quot;dict.txt&quot;) # # 设置高词频：一个 # jieba.suggest_freq(&#39;台中&#39;,tune=True) # 设置高词频：dict.txt中的每一行都设置一下 # fp=open(&quot;dict.txt&quot;, &#39;r&#39;, encoding=&#39;utf8&#39;) # for line in fp: # line = line.strip() # jieba.suggest_freq(line, tune=True) # # 设置高词频：dict.txt中的每一行都设置一下快速方法 [jieba.suggest_freq(line.strip(), tune=True) for line in open(&quot;dict.txt&quot;, &#39;r&#39;, encoding=&#39;utf8&#39;)] if __name__ == &quot;__main__&quot;: string = &quot;台中正确应该不会被切开。&quot; # 通过调整词频 suggest_freq(line, tune=True) words_jieba = &quot; &quot;.join(jieba.cut(string, HMM=False)) # 通过排序sort_dict_by_lenth，优先按照长的字典项匹配 words_hanlp = cut_hanlp(string) print(&quot;words_jieba:&quot; + words_jieba, &#39;\n&#39;, &quot;words_hanlp:&quot; + words_hanlp) sort_dict_by_lenth.py # encoding=utf8 import os dict_file = open( &quot;E:&quot; + os.sep + &quot;hanlp&quot; + os.sep + &quot;data&quot; + os.sep + &quot;dictionary&quot; + os.sep + &quot;custom&quot; + os.sep + &quot;resume_nouns.txt&quot;, &#39;r&#39;, encoding=&#39;utf8&#39;) d = {} [d.update({line: len(line.split(&quot; &quot;)[0])}) for line in dict_file] # 读取源字典文件并从长到短排序 优先匹配长字典项 f = sorted(d.items(), key=lambda x: x[1], reverse=True) dict_file = open( &quot;E:&quot; + os.sep + &quot;hanlp&quot; + os.sep + &quot;data&quot; + os.sep + &quot;dictionary&quot; + os.sep + &quot;custom&quot; + os.sep + &quot;resume_nouns1.txt&quot;, &#39;w&#39;, encoding=&#39;utf8&#39;) [dict_file.write(item[0]) for item in f] dict_file.close() 词性标注代码实现及信息提取extract_data.py # -*- coding=utf8 -*- import jieba import re from tokenizer import seg_sentences fp = open(&quot;text.txt&quot;, &#39;r&#39;, encoding=&#39;utf8&#39;) fout = open(&quot;out.txt&quot;, &#39;w&#39;, encoding=&#39;utf8&#39;) for line in fp: line = line.strip() if len(line) &gt; 0: fout.write(&#39; &#39;.join(seg_sentences(line)) + &quot;\n&quot;) fout.close() if __name__ == &quot;__main__&quot;: pass tokenizer.py # encoding=utf8 import os, gc, re, sys from jpype import * root_path = &quot;E:/hanlp&quot; djclass_path = &quot;-Djava.class.path=&quot; + root_path + os.sep + &quot;hanlp-1.7.4.jar;&quot; + root_path startJVM(getDefaultJVMPath(), djclass_path, &quot;-Xms1g&quot;, &quot;-Xmx1g&quot;) Tokenizer = JClass(&#39;com.hankcs.hanlp.tokenizer.StandardTokenizer&#39;) keep_pos = &quot;q,qg,qt,qv,s,t,tg,g,gb,gbc,gc,gg,gm,gp,m,mg,Mg,mq,n,an,vn,ude1,nr,ns,nt,nz,nb,nba,nbc,nbp,nf,ng,nh,nhd,o,nz,nx,ntu,nts,nto,nth,ntch,ntcf,ntcb,ntc,nt,nsf,ns,nrj,nrf,nr2,nr1,nr,nnt,nnd,nn,nmc,nm,nl,nit,nis,nic,ni,nhm,nhd&quot; keep_pos_nouns = set(keep_pos.split(&quot;,&quot;)) keep_pos_v = &quot;v,vd,vg,vf,vl,vshi,vyou,vx,vi&quot; keep_pos_v = set(keep_pos_v.split(&quot;,&quot;)) keep_pos_p = set([&#39;p&#39;, &#39;pbei&#39;, &#39;pba&#39;]) drop_pos_set = set( [&#39;xu&#39;, &#39;xx&#39;, &#39;y&#39;, &#39;yg&#39;, &#39;wh&#39;, &#39;wky&#39;, &#39;wkz&#39;, &#39;wp&#39;, &#39;ws&#39;, &#39;wyy&#39;, &#39;wyz&#39;, &#39;wb&#39;, &#39;u&#39;, &#39;ud&#39;, &#39;ude1&#39;, &#39;ude2&#39;, &#39;ude3&#39;, &#39;udeng&#39;, &#39;udh&#39;, &#39;p&#39;, &#39;rr&#39;, &#39;w&#39;]) han_pattern = re.compile(r&#39;[^\dA-Za-z\u3007\u4E00-\u9FCB\uE815-\uE864]+&#39;) HanLP = JClass(&#39;com.hankcs.hanlp.HanLP&#39;) def to_string(sentence, return_generator=False): if return_generator: return (word_pos_item.toString().split(&#39;/&#39;) for word_pos_item in Tokenizer.segment(sentence)) else: return [(word_pos_item.toString().split(&#39;/&#39;)[0], word_pos_item.toString().split(&#39;/&#39;)[1]) for word_pos_item in Tokenizer.segment(sentence)] def seg_sentences(sentence, with_filter=True, return_generator=False): segs = to_string(sentence, return_generator=return_generator) if with_filter: g = [word_pos_pair[0] for word_pos_pair in segs if len(word_pos_pair) == 2 and word_pos_pair[0] != &#39; &#39; and word_pos_pair[1] not in drop_pos_set] else: g = [word_pos_pair[0] for word_pos_pair in segs if len(word_pos_pair) == 2 and word_pos_pair[0] != &#39; &#39;] return iter(g) if return_generator else g TextRank算法原理介绍tex_rank.py # -*- coding=utf8 -*- from jieba import analyse # 引入TextRank关键词抽取接口 textrank = analyse.textrank # 原始文本 text = &quot;非常线程是程序执行时的最小单位，它是进程的一个执行流，\ 是CPU调度和分派的基本单位，一个进程可以由很多个线程组成，\ 线程间共享进程的所有资源，每个线程有自己的堆栈和局部变量。\ 线程由CPU独立调度执行，在多CPU环境下就允许多个线程同时运行。\ 同样多线程也可以实现并发操作，每个请求分配一个线程来处理。&quot; print(&quot;\nkeywords by textrank:&quot;) # 基于TextRank算法进行关键词抽取 keywords = textrank(text, topK=10, withWeight=True, allowPOS=(&#39;ns&#39;, &#39;n&#39;)) # 输出抽取出的关键词 f words = [keyword for keyword, w in keywords if w &gt; 0.2] print(&#39; &#39;.join(words) + &quot;\n&quot;) jieba 词性标注 标注 含义 来源 Ag 形语素 形容词性语素形容词代码为 a,语素代码g前面置以A a 形容词 取英语形容词 adjective的第1个字母 ad 副形词 直接作状语的形容词形容词代码 a和副词代码d并在一起 an 名形词 具有名词功能的形容词形容词代码 a和名词代码n并在一起 b 区别词 取汉字“别”的声母 c 连词 取英语连词 conjunction的第1个字母 dg 副语素 副词性语素副词代码为 d,语素代码g前面置以D d 副词 取 adverb的第2个字母,因其第1个字母已用于形容词 e 叹词 取英语叹词 exclamation的第1个字母 f 方位词 取汉字“方” g 语素 绝大多数语素都能作为合成词的“词根”,取汉字“根”的声母 h 前接成分 取英语 head的第1个字母 i 成语 取英语成语 idiom的第1个字母 j 简称略语 取汉字“简”的声母 k 后接成分 l 习用语 习用语尚未成为成语,有点“临时性”,取“临”的声母 m 数词 取英语 numeral的第3个字母,n,u已有他用 Ng 名语素 名词性语素名词代码为 n,语素代码g前面置以N n 名词 取英语名词 noun的第1个字母 nr 人名 名词代码 n和“人(ren)”的声母并在一起 ns 地名 名词代码 n和处所词代码s并在一起 nt 机构团体 “团”的声母为 t,名词代码n和t并在一起 nz 其他丏名 “丏”的声母的第 1个字母为z,名词代码n和z并在一起 o 拟声词 取英语拟声词 onomatopoeia的第1个字母 p 介词 取英语介词 prepositional的第1个字母 q 量词 取英语 quantity的第1个字母 r 代词 取英语代词 pronoun的第2个字母,因p已用于介词 s 处所词 取英语 space的第1个字母 tg 时语素 时间词性语素时间词代码为 t,在语素的代码g前面置以T t 时间词 取英语 time的第1个字母 u 助词 取英语助词 auxiliary vg 动语素 动词性语素动词代码为 v在语素的代码g前面置以V v 动词 取英语动词 verb的第一个字母 vd 副动词 直接作状语的动词动词和副词的代码并在一起 vn 名动词 指具有名词功能的动词动词和名词的代码并在一起 w 标点符号 x 非语素字 非语素字只是一个符号,字母 x通常用于代表未知数、符号 y 语气词 取汉字“语”的声母 z 状态词 取汉字“状”的声母的前一个字母 un 未知词 不可识别词及用户自定义词组取英文Unkonwn首两个字母(非北大标准,CSW分词中定义) hanlp词性标注 标注 含义 a 形容词 ad 副形词 ag 形容词性语素 al 形容词性惯用语 an 名形词 b 区别词 begin 仅用于始##始 bg 区别语素 bl 区别词性惯用语 c 连词 cc 并列连词 d 副词 dg 辄,俱,复之类的副词 dl 连语 e 叹词 end 仅用于终##终 f 方位词 g 学术词汇 gb 生物相关词汇 gbc 生物类别 gc 化学相关词汇 gg 地理地质相关词汇 gi 计算机相关词汇 gm 数学相关词汇 gp 物理相关词汇 h 前缀 i 成语 j 简称略语 k 后缀 l 习用语 m 数词 mg 数语素 Mg 甲乙丙丁之类的数词 mq 数量词 n 名词 nb 生物名 nba 动物名 nbc 动物纲目 nbp 植物名 nf 食品，比如“薯片” ng 名词性语素 nh 医药疾病等健康相关名词 nhd 疾病 nhm 药品 ni 机构相关（不是独立机构名） nic 下属机构 nis 机构后缀 nit 教育相关机构 nl 名词性惯用语 nm 物品名 nmc 化学品名 nn 工作相关名词 nnd 职业 nnt 职务职称 nr 人名 nr1 复姓 nr2 蒙古姓名 nrf 音译人名 nrj 日语人名 ns 地名 nsf 音译地名 nt 机构团体名 ntc 公司名 ntcb 银行 ntcf 工厂 ntch 酒店宾馆 nth 医院 nto 政府机构 nts 中小学 ntu 大学 nx 字母专名 nz 其他专名 o 拟声词 p 介词 pba 介词“把” pbei 介词“被” q 量词 qg 量词语素 qt 时量词 qv 动量词 r 代词 rg 代词性语素 Rg 古汉语代词性语素 rr 人称代词 ry 疑问代词 rys 处所疑问代词 ryt 时间疑问代词 ryv 谓词性疑问代词 rz 指示代词 rzs 处所指示代词 rzt 时间指示代词 rzv 谓词性指示代词 s 处所词 t 时间词 tg 时间词性语素 u 助词 ud 助词 ude1 的 底 ude2 地 ude3 得 udeng 等 等等 云云 udh 的话 ug 过 uguo 过 uj 助词 ul 连词 ule 了 喽 ulian 连 （“连小学生都会”） uls 来讲 来说 而言 说来 usuo 所 uv 连词 uyy 一样 一般 似的 般 uz 着 uzhe 着 uzhi 之 v 动词 vd 副动词 vf 趋向动词 vg 动词性语素 vi 不及物动词（内动词） vl 动词性惯用语 vn 名动词 vshi 动词“是” vx 形式动词 vyou 动词“有” w 标点符号 wb 百分号千分号，全角：％ ‰ 半角：% wd 逗号，全角：， 半角：, wf 分号，全角：； 半角： ; wh 单位符号，全角：￥ ＄ ￡ ° ℃ 半角：$ wj 句号，全角：。 wky 右括号，全角：） 〕 ］ ｝ 》 】 〗 〉 半角： ) ] { &gt; wkz 左括号，全角：（ 〔 ［ ｛ 《 【 〖 〈 半角：( [ { &lt; wm 冒号，全角：： 半角： : wn 顿号，全角：、 wp 破折号，全角：—— －－ ——－ 半角：— —- ws 省略号，全角：…… … wt 叹号，全角：！ ww 问号，全角：？ wyy 右引号，全角：” ’ 』 wyz 左引号，全角：“ ‘ 『 x 字符串 xu 网址URL xx 非语素字 y 语气词(delete yg) yg 语气语素 z 状态词 zg 状态词]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nlp基础]]></title>
    <url>%2F2019%2F08%2F26%2Fnlp%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[nlp基础 简介 NLP( Natural Language Processing ) 是 自然 语言 处理 的 简称,是研究人与计算机交互的语言问题的一门学科。机器理解并解释人类写作与说话方式的能力。近年来,深度学习技术在自然语言处理方面的研究和应用也取得了显著的成果。 提问和回答、知识工程、语言生成、语音识别,语音合成,自动分词,句法分析,语法纠错,关键词提取,文本分类/聚类,文本自动摘要,信息检索(ES,Solr),信息抽取,知识图谱,机器翻译,人机对话,机器写作,情感分析,文字识别,阅读理解,推荐系统,高考机器人等。 环境搭建Anaconda3-5.1.0-Windows-x86_64.exe将Anaconda加入系统环境变量 常用开发包numpy numpy系统是Python的一种开源的数值计算包。 包括：1、一个强大的N维数组对象Array；2、比较成熟的（广播）函数库；3、用于整合C/C++和Fortran代码的工具包；4、实用的线性代数、傅里叶变换和随机数生成函数。numpy和稀疏矩阵运算包scipy配合使用更加方便。 conda install numpy NLTK Natural Language Toolkit，自然语言处理工具包，在NLP领域中， 最常使用的一个Python库。 conda install nltk Gensim Gensim是一个占内存低，接口简单，免费的Python库，它可以用来从文档中自动提取语义主题。它包含了很多非监督学习算法如：TF/IDF，潜在语义分析（Latent Semantic Analysis，LSA）、隐含狄利克雷分配（Latent Dirichlet Allocation，LDA），层次狄利克雷过程 （Hierarchical Dirichlet Processes，HDP）等。 Gensim支持Word2Vec,Doc2Vec等模型。 conda install gensimpip install gensim如不可安装python库gensim‑3.8.0‑cp36‑cp36m‑win_amd64.whl下载后pip install Tensorflow TensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统。TensorFlow可被用于语音识别或图像识别等多项机器学习和深度学习领域。TensorFlow是一个采用数据流图（data flow graphs），用于数值计算的开源软件库。节点（Nodes）在图中表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor）。它灵活的架构让你可以在多种平台上展开计算，例如台式计算机中的一个或多个CPU（戒GPU），服务器，移动设备等等。TensorFlow 最初由Google大脑小组（隶属于Google机器智能研究机构）的研究员和工程师们开发出来，用于机器学习和深度神经网络方面的研究，但这个系统的通用 性使其也可广泛用于其他计算领域。 conda install tensorflowpip install tensorflow python库pip install tensorflow-1.9.0-cp36-cp36m-win_amd64.whl下载后pip install jieba “结巴”中文分词：是广泛使用的中文分词工具，具有以下特点： 1）三种分词模式：精确模式，全模式和搜索引擎模式 2）词性标注和返回词语在原文的起止位置（ Tokenize） 3）可加入自定义字典 4）代码对 Python 2/3 均兼容 5）支持多种语言，支持简体繁体 项目地址 pip install jieba demo# encoding=utf-8 import jieba import jieba.posseg as pseg print(&quot;\njieba分词全模式：&quot;) seg_list = jieba.cut(&quot;我来到北京清华大学&quot;, cut_all=True) print(&quot;Full Mode: &quot; + &quot;/ &quot;.join(seg_list)) # 全模式 print(&quot;\njieba分词精确模式：&quot;) seg_list = jieba.cut(&quot;我来到北京清华大学&quot;, cut_all=False) print(&quot;Default Mode: &quot; + &quot;/ &quot;.join(seg_list)) # 精确模式 print(&quot;\njieba默认分词是精确模式：&quot;) seg_list = jieba.cut(&quot;他来到了网易杭研大厦&quot;) # 默认是精确模式 print(&quot;, &quot;.join(seg_list)) print(&quot;\njiba搜索引擎模式：&quot;) seg_list = jieba.cut_for_search(&quot;小明硕士毕业于中国科学院计算所，后在日本京都大学深造&quot;) # 搜索引擎模式 print(&quot;, &quot;.join(seg_list)) strings=&quot;是广泛使用的中文分词工具，具有以下特点：&quot; words = pseg.cut(strings) print(&quot;\njieba词性标注：&quot;) for word, flag in words: print(&#39;%s %s&#39; % (word, flag)) Stanford NLP Stanford NLP提供了一系列自然语言分析工具。它能够给出基本的 词形，词性，不管是公司名还是人名等，格式化的日期，时间，量词， 并且能够标记句子的结构，语法形式和字词依赖，指明那些名字指向同 样的实体，指明情绪，提取发言中的开放关系等。 1.一个集成的语言分析工具集； 2.进行快速，可靠的任意文本分析； 3.整体的高质量的文本分析; 4.支持多种主流语言; 5.多种编程语言的易用接口; 6.方便的简单的部署web服务。 Python 版本stanford nlp 安装 • 1)安装stanford nlp自然语言处理包: pip install stanfordcorenlp • 2)下载Stanford CoreNLP文件 https://stanfordnlp.github.io/CoreNLP/download.html • 3)下载中文模型jar包,https://nlp.stanford.edu/software/stanford-chinese-corenlp-2018-10-05-models.jar • 4)把下载的stanford-chinese-corenlp-2018-10-05-models.jar放在解压后的Stanford CoreNLP文件夹中，改Stanford CoreNLP文件夹名为stanfordnlp（可选） • 5)在Python中引用模型: • from stanfordcorenlp import StanfordCoreNLP • nlp = StanfordCoreNLP(r‘path&#39;, lang=&#39;zh&#39;) 例如： nlp = StanfordCoreNLP(r&#39;/home/kuo/NLP/module/stanfordnlp/&#39;, lang=&#39;zh&#39;) demo# -*-encoding=utf8-*- from stanfordcorenlp import StanfordCoreNLP nlp = StanfordCoreNLP(r&#39;E:\stanford-corenlp-full-2018-10-05&#39;, lang=&#39;zh&#39;) fin = open(&#39;news.txt&#39;, &#39;r&#39;, encoding=&#39;utf8&#39;) fner = open(&#39;ner.txt&#39;, &#39;w&#39;, encoding=&#39;utf8&#39;) ftag = open(&#39;pos_tag.txt&#39;, &#39;w&#39;, encoding=&#39;utf8&#39;) for line in fin: line = line.strip() # 去掉空行 if len(line) &lt; 1: continue # 命名实体识别 fner.write(&quot; &quot;.join([each[0] + &quot;/&quot; + each[1] for each in nlp.ner(line) if len(each) == 2]) + &quot;\n&quot;) # 词性识别 ftag.write(&quot; &quot;.join([each[0] + &quot;/&quot; + each[1] for each in nlp.pos_tag(line) if len(each) == 2]) + &quot;\n&quot;) fner.close() ftag.close() print(&quot;okkkkk&quot;) sentence = &#39;清华大学位于北京。&#39; print(nlp.word_tokenize(sentence)) print(nlp.pos_tag(sentence)) print(nlp.ner(sentence)) print(nlp.parse(sentence)) print(nlp.dependency_parse(sentence)) Hanlp HanLP是由一系列模型与算法组成的Java工具包，目标是普及自然 语言处理在生产环境中的应用。HanLP具备功能完善、性能高效、架构 清晰、语料时新、可自定义的特点。 功能：中文分词 词性标注 命名实体识别 依存句法分析 关键词提取 新词发现 短语提取 自动摘要 文本分类 拼音简繁 • 1、安装Java:我装的是Java 1.8 • 2、安裝Jpype, conda install -c conda-forge jpype1=0.7 [或者]pip install jpype1 • 3、测试是否按照成功: from jpype import * startJVM(getDefaultJVMPath(), &quot;-ea&quot;) java.lang.System.out.println(&quot;Hello World&quot;) shutdownJVM() • 比如data目录是root=E:/hanlp/data,那么root=root=E:/hanlp • 1、https://github.com/hankcs/HanLP/releases 下载hanlp-1.7.4-release.zip包，data-for-1.7.4.zip包,解压后重命名为hanlp• 2、配置文件• 示例配置文件:hanlp.properties• 配置文件的作用是告诉HanLP数据包的位置,只需修改第一行:root=E:/hanlp demo#-*- coding:utf-8 -*- from jpype import * startJVM(getDefaultJVMPath(), &quot;-Djava.class.path=E:\hanlp\hanlp-1.7.4.jar;E:\hanlp&quot;, &quot;-Xms1g&quot;, &quot;-Xmx1g&quot;) # 启动JVM，Linux需替换分号;为冒号: print(&quot;=&quot; * 30 + &quot;HanLP分词&quot; + &quot;=&quot; * 30) HanLP = JClass(&#39;com.hankcs.hanlp.HanLP&#39;) # 中文分词 print(HanLP.segment(&#39;你好，欢迎在Python中调用HanLP的API&#39;)) print(&quot;-&quot; * 70) print(&quot;=&quot; * 30 + &quot;标准分词&quot; + &quot;=&quot; * 30) StandardTokenizer = JClass(&#39;com.hankcs.hanlp.tokenizer.StandardTokenizer&#39;) print(StandardTokenizer.segment(&#39;你好，欢迎在Python中调用HanLP的API&#39;)) print(&quot;-&quot; * 70) # NLP分词NLPTokenizer会执行全部命名实体识别和词性标注 print(&quot;=&quot; * 30 + &quot;NLP分词&quot; + &quot;=&quot; * 30) NLPTokenizer = JClass(&#39;com.hankcs.hanlp.tokenizer.NLPTokenizer&#39;) print(NLPTokenizer.segment(&#39;中国科学院计算技术研究所的宗成庆教授正在教授自然语言处理课程&#39;)) print(&quot;-&quot; * 70) print(&quot;=&quot; * 30 + &quot;索引分词&quot; + &quot;=&quot; * 30) IndexTokenizer = JClass(&#39;com.hankcs.hanlp.tokenizer.IndexTokenizer&#39;) termList = IndexTokenizer.segment(&quot;主副食品&quot;); for term in termList: print(str(term) + &quot; [&quot; + str(term.offset) + &quot;:&quot; + str(term.offset + len(term.word)) + &quot;]&quot;) print(&quot;-&quot; * 70) print(&quot;=&quot; * 30 + &quot; CRF分词&quot; + &quot;=&quot; * 30) print(&quot;-&quot; * 70) print(&quot;=&quot; * 30 + &quot; 极速词典分词&quot; + &quot;=&quot; * 30) SpeedTokenizer = JClass(&#39;com.hankcs.hanlp.tokenizer.SpeedTokenizer&#39;) print(NLPTokenizer.segment(&#39;江西鄱阳湖干枯，中国最大淡水湖变成大草原&#39;)) print(&quot;-&quot; * 70) print(&quot;=&quot; * 30 + &quot; 自定义分词&quot; + &quot;=&quot; * 30) CustomDictionary = JClass(&#39;com.hankcs.hanlp.dictionary.CustomDictionary&#39;) CustomDictionary.add(&#39;攻城狮&#39;) CustomDictionary.add(&#39;单身狗&#39;) HanLP = JClass(&#39;com.hankcs.hanlp.HanLP&#39;) print(HanLP.segment(&#39;攻城狮逆袭单身狗，迎娶白富美，走上人生巅峰&#39;)) print(&quot;-&quot; * 70) print(&quot;=&quot; * 20 + &quot;命名实体识别与词性标注&quot; + &quot;=&quot; * 30) NLPTokenizer = JClass(&#39;com.hankcs.hanlp.tokenizer.NLPTokenizer&#39;) print(NLPTokenizer.segment(&#39;中国科学院计算技术研究所的宗成庆教授正在教授自然语言处理课程&#39;)) print(&quot;-&quot; * 70) document = &quot;水利部水资源司司长陈明忠9月29日在国务院新闻办举行的新闻发布会上透露，&quot; \ &quot;根据刚刚完成了水资源管理制度的考核，有部分省接近了红线的指标，&quot; \ &quot;有部分省超过红线的指标。对一些超过红线的地方，陈明忠表示，对一些取用水项目进行区域的限批，&quot; \ &quot;严格地进行水资源论证和取水许可的批准。&quot; print(&quot;=&quot; * 30 + &quot;关键词提取&quot; + &quot;=&quot; * 30) print(HanLP.extractKeyword(document, 8)) print(&quot;-&quot; * 70) print(&quot;=&quot; * 30 + &quot;自动摘要&quot; + &quot;=&quot; * 30) print(HanLP.extractSummary(document, 3)) print(&quot;-&quot; * 70) text = r&quot;算法工程师\n 算法（Algorithm）是一系列解决问题的清晰指令，也就是说，能够对一定规范的输入，在有限时间内获得所要求的输出。如果一个算法有缺陷，或不适合于某个问题，执行这个算法将不会解决这个问题。不同的算法可能用不同的时间、空间或效率来完成同样的任务。一个算法的优劣可以用空间复杂度与时间复杂度来衡量。算法工程师就是利用算法处理事物的人。\n \n 1职位简介\n 算法工程师是一个非常高端的职位；\n 专业要求：计算机、电子、通信、数学等相关专业；\n 学历要求：本科及其以上的学历，大多数是硕士学历及其以上；\n 语言要求：英语要求是熟练，基本上能阅读国外专业书刊；\n 必须掌握计算机相关知识，熟练使用仿真工具MATLAB等，必须会一门编程语言。\n\n2研究方向\n 视频算法工程师、图像处理算法工程师、音频算法工程师 通信基带算法工程师\n \n 3目前国内外状况\n 目前国内从事算法研究的工程师不少，但是高级算法工程师却很少，是一个非常紧缺的专业工程师。算法工程师根据研究领域来分主要有音频/视频算法处理、图像技术方面的二维信息算法处理和通信物理层、雷达信号处理、生物医学信号处理等领域的一维信息算法处理。\n 在计算机音视频和图形图像技术等二维信息算法处理方面目前比较先进的视频处理算法：机器视觉成为此类算法研究的核心；另外还有2D转3D算法(2D-to-3D conversion)，去隔行算法(de-interlacing)，运动估计运动补偿算法(Motion estimation/Motion Compensation)，去噪算法(Noise Reduction)，缩放算法(scaling)，锐化处理算法(Sharpness)，超分辨率算法(Super Resolution),手势识别(gesture recognition),人脸识别(face recognition)。\n 在通信物理层等一维信息领域目前常用的算法：无线领域的RRM、RTT，传送领域的调制解调、信道均衡、信号检测、网络优化、信号分解等。\n 另外数据挖掘、互联网搜索算法也成为当今的热门方向。\n&quot; print(&quot;=&quot; * 30 + &quot;短语提取&quot; + &quot;=&quot; * 30) print(HanLP.extractPhrase(text, 10)) print(&quot;-&quot; * 70) shutdownJVM()]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
        <tag>nlp</tag>
        <tag>numpy</tag>
        <tag>NLTK</tag>
        <tag>Gensim</tag>
        <tag>Stanford NLP</tag>
        <tag>Hanlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jmeter基础]]></title>
    <url>%2F2019%2F08%2F26%2Fjmeter%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[jmeter基本环境搭建及基础操作 压力测试工具对比 loadrunner 性能稳定，压测结果及细粒度大，可以自定义脚本进行压测，但是太过于重大，功能比较繁多 apache ab(单接口压测最方便) 模拟多线程并发请求,ab命令对发出负载的计算机要求很低，既不会占用很多CPU，也不会占用太多的内存，但却会给目标服务器造成巨大的负载, 简单DDOS攻击等 webbench webbench首先fork出多个子进程，每个子进程都循环做web访问测试。子进程把访问的结果通过pipe告诉父进程，父进程做最终的统计结果。 jmeter 压测不同的协议和应用 1) Web - HTTP, HTTPS (Java, NodeJS, PHP, ASP.NET, …) 2) SOAP / REST Webservices 3) FTP 4) Database via JDBC 5) LDAP 轻量目录访问协议 6) Message-oriented middleware (MOM) via JMS 7) Mail - SMTP(S), POP3(S) and IMAP(S) 8) TCP等等 使用场景及优点 1）功能测试 2）压力测试 3）分布式压力测试 4）纯java开发 5）上手容易，高性能 4）提供测试数据分析 5）各种报表数据图形展示环境搭建 需要安装JDK8。或者JDK9,JDK10 快速下载 windows： https://archive.apache.org/dist/jmeter/binaries/apache-jmeter-4.0.zip mac或者linux：https://archive.apache.org/dist/jmeter/binaries/apache-jmeter-4.0.tgz 目录 bin:核心可执行文件，包含配置 jmeter.bat: windows启动文件： jmeter: mac或者linux启动文件： jmeter-server：mac或者Liunx分布式压测使用的启动文件 jmeter-server.bat：mac或者Liunx分布式压测使用的启动文件 jmeter.properties: 核心配置文件 extras：插件拓展的包 lib:核心的依赖包 ext:核心包 junit:单元测试包 修改界面语言 1、控制台修改 menu -&gt; options -&gt; choose language 2、配置文件修改 bin目录 -&gt; jmeter.properties 默认 #language=en 改为 language=zh_CN 基本测试 java -jar gs-spring-boot-0.1.0.jar 添加-&gt;threads-&gt;线程组（控制总体并发） 线程数：虚拟用户数。一个虚拟用户占用一个进程或线程准备时长（Ramp-Up Period(in seconds)）：全部线程启动的时长，比如100个线程，20秒，则表示20秒内100个线程都要启动完成，每秒启动5个线程循环次数：每个线程发送的次数，假如值为5，100个线程，则会发送500次请求，可以勾选永远循环 线程组-&gt;添加-&gt; Sampler(采样器) -&gt; Http （一个线程组下面可以增加几个Sampler） 名称：采样器名称 注释：对这个采样器的描述web服务器： 默认协议是http 默认端口是80 服务器名称或IP ：请求的目标服务器名称或IP地址路径：服务器URLUse multipart/from-data for HTTP POST ：当发送POST请求时，使用Use multipart/from-data方法发送，默认不选中。 线程组-&gt;添加-&gt;监听器-&gt;察看结果树 该结果树属于全局，可已针对每一个请求设置结果树 线程组 -&gt; 添加 -&gt; 断言 -&gt; 响应断言 apply to(应用范围): Main sample only: 仅当前父取样器 进行断言，一般一个请求，如果发一个请求会触发多个，则就有sub sample（比较少用）要测试的响应字段： 响应文本：即响应的数据，比如json等文本 响应代码：http的响应状态码，比如200，302，404这些 响应信息：http响应代码对应的响应信息，例如：OK, Found Response Header: 响应头模式匹配规则： 包括：包含在里面就成功 匹配：响应内容完全匹配，不区分大小写 equals：完全匹配，区分大小写 线程组-&gt; 添加 -&gt; 监听器 -&gt; 断言结果 里面的内容是sampler采样器的名称断言失败，查看结果树任务结果颜色标红(通过结果数里面双击不通过的记录，可以看到错误信息)每个sample下面可以加单独的结果树，然后同时加多个断言，最外层可以加个结果树进行汇总 线程组-&gt;添加-&gt;监听器-&gt;聚合报告（Aggregate Report） lable: sampler的名称 Samples: 一共发出去多少请求,例如10个用户，循环10次，则是 100 Average: 平均响应时间 Median: 中位数，也就是 50％ 用户的响应时间 90% Line : 90％ 用户的响应不会超过该时间 （90% of the samples took no more than this time. The remaining samples at least as long as this） 95% Line : 95％ 用户的响应不会超过该时间 99% Line : 99％ 用户的响应不会超过该时间 min : 最小响应时间 max : 最大响应时间 Error%：错误的请求的数量/请求的总数 Throughput： 吞吐量——默认情况下表示每秒完成的请求数（Request per Second) 可类比为qps,并发数提高，qps不涨则瓶颈 KB/Sec: 每秒接收数据量 启动测试http请求 变量实操很多变量在全局中都有使用，或者测试数据更改，可以在一处定义，四处使用，比如服务器地址 线程组-&gt;add -&gt; Config Element(配置原件)-&gt; User Definde Variable（用户定义的变量） 通过${xx}调用 线程组-&gt;add -&gt; Config Element(配置原件)-&gt; CSV data set config (CSV数据文件设置) csv变量使用csv_name txt多变量使用csv_name csv_pwd 数据库test1 Add directory or jar to classpath添加mysql-connector-java-5.1.30.jarThread Group -&gt; add -&gt; sampler -&gt; jdbc request 无参查询 有参查询 预编译 更新 预编译 JDBC request-&gt;add -&gt; config element -&gt; JDBC connection configuration Variable Name for created pool同JDBC request 的Variable Name for created pool declared in JDBC connection configuration Thread Group -&gt; add -&gt; sampler -&gt; debug sampler variable name of pool declared in JDBC connection configuration（和配置文件同名）Query Type 查询类型parameter values 参数值parameter types 参数类型variable names sql执行结果变量名result variable names 所有结果当做一个对象存储query timeouts 查询超时时间handle results 处理结果集 分布式压测]]></content>
      <categories>
        <category>测试</category>
      </categories>
      <tags>
        <tag>jmeter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy爬取ssr链接]]></title>
    <url>%2F2019%2F08%2F23%2Fscrapy%E7%88%AC%E5%8F%96ssr%E9%93%BE%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[基于scrapy爬取ssr链接 环境搭建python3.5 虚拟环境virtualenvpip install virtualenv 提示pip版本太低 python -m pip install --upgrade pip pip install -i https://pypi.doubanio.com/simple/ --trusted-host pypi.doubanio.com django 使用豆瓣源加速 pip uninstall django 卸载django virtualenv scrapytest 默认环境创建虚拟环境 cd scrapytest/Scripts &amp;&amp; activate.bat &amp;&amp; python 进入3.5虚拟环境 virtualenv -p D:\Python27\python.exe scrapytest cd scrapytest/Scripts &amp;&amp; activate.bat &amp;&amp; python 进入2.7虚拟环境 deactivate.bat 退出虚拟环境 apt-get install python-virtualenv 安装虚拟环境 virtualenv py2 &amp;&amp; cd py2 &amp;&amp; cd bin &amp;&amp; source activate &amp;&amp; python 进入2.7虚拟环境 virtualenv -p /usr/bin/python3 py3 &amp;&amp; &amp;&amp; cd py3 &amp;&amp; cd bin &amp;&amp; source activate &amp;&amp; python 进入3.5虚拟环境虚拟环境virtualenvwrapperpip install virtualenvwrapper pip install virtualenvwrapper-win 解决workon不是内部指令 workon 列出所有虚拟环境 新建环境变量 WORKON_HOME=E:\envs mkvirtualenv py3scrapy 新建并进入虚拟环境 deactivate 退出虚拟环境 workon py3scrapy 进入指定虚拟环境 pip install -i https://pypi.douban.com/simple scrapy 安装scrapy源 若缺少lxml出错https://www.lfd.uci.edu/~gohlke/pythonlibs/寻找对应版本的lxml的whl源 python -m pip install --upgrade pip 更新pip pip install lxml-4.1.1-cp35-cp35m-win_amd64.whl 若缺少Twisted出错http://www.lfd.uci.edu/~gohlke/pythonlibs/#lxml搜对应版本Twisted pip install Twisted‑17.9.0‑cp35‑cp35m‑win_amd64.whl mkvirtualenv --python=D:\Python27\python.exe py2scrapy 一般不会出问题 pip install -i https://pypi.douban.com/simple scrapy pip install virtualenvwrapper find / -name virualenvwrapper.sh vim ~/.bashrc export WORKON_HOME=$HOME/.virtualenvs source /home/wj/.local/bin/virtualenvwrapper.sh source ~/.bashrc mkvirtualenv py2scrapy 指向生成~/.virtualenv deactivate 退出虚拟环境 mkdirtualenv --python=/usr/bin/python3 py3scrapy项目实战项目搭建pip install virtualenvwrapper-win mkvirtualenv --python=F:\Python\Python35\python.exe ssr pip install Twisted-17.9.0-cp35-cp35m-win_amd64.whl pip install -i https://pypi.douban.com/simple/ scrapy scrapy startproject ssr cd ssr scrapy genspider ssr https://freevpn-ss.tk/category/technology/ scrapy genspider --list scrapy genspider -t crawl lagou www.lagou.com 使用crawl模板 pycharm--新建项目---Pure Python---Interpreter为E:\envs\ssr\Scripts\python.exe pycharm--打开---ssr,修改settings--project Interpreter为D:\Envs\ss pip list pip install -i https://pypi.douban.com/simple pypiwin32 pillow requests redis fake-useragent pip install mysqlclient-1.4.4-cp35-cp35m-win_amd64.whl 或者pip install -i https://pypi.douban.com/simple mysqlclient 出错apt-get install libmysqlclient-dev 或者yum install python-devel mysql-devel scrapy crawl jobbole 修改settings.py ROBOTSTXT_OBEY = False scrapy shell http://blog.jobbole.com/ 可以在脚本中调试xpath或者chrome浏览器右键copy xpath,chrome浏览器右键copy selector scrapy shell -s USER_AGENT=&quot;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:51.0) Gecko/20100101 Firefox/51.0&quot; https://www.zhihu.com/question/56320032 pip freeze &gt; requirements.txt 生成依赖到文件 pip install -r requirements.txt 一键安装依赖 job_list = json.loads(response.text)[&quot;data&quot;][&quot;results返回response爬虫开发1scrapy shell https://freevpn-ss.tk/category/technology/ shell中查看节点 response.css(&quot;.posts-list .panel a::attr(href)&quot;).extract_first() response.css(&quot;.posts-list .panel a img::attr(src)&quot;).extract_first() response.xpath(&quot;//*[@id=&#39;container&#39;]/div/ul/li/article/a/img/@src&quot;).extract_first() view(response) 启动类main.py from scrapy.cmdline import execute import sys import os sys.path.append(os.path.dirname(os.path.abspath(__file__))) execute([&quot;scrapy&quot;, &quot;crawl&quot;, &quot;freevpn-ss.tk&quot;]) 基础配置ssr/settings.py import os BOT_NAME = &#39;ssr&#39; SPIDER_MODULES = [&#39;ssr.spiders&#39;] NEWSPIDER_MODULE = &#39;ssr.spiders&#39; # Crawl responsibly by identifying yourself (and your website) on the user-agent #USER_AGENT = &#39;ssr (+http://www.yourdomain.com)&#39; # Obey robots.txt rules ROBOTSTXT_OBEY = False import sys BASE_DIR = os.path.dirname(os.path.abspath(os.path.dirname(__file__))) sys.path.insert(0, os.path.join(BASE_DIR, &#39;ssr&#39;)) MYSQL_HOST = &quot;127.0.0.1&quot; MYSQL_DBNAME = &quot;scrapy&quot; MYSQL_USER = &quot;root&quot; MYSQL_PASSWORD = &quot;&quot; ITEM_PIPELINES = { &#39;ssr.pipelines.MysqlTwistedPipline&#39;: 2,#连接池异步插入 &#39;ssr.pipelines.JsonExporterPipleline&#39;: 1,#连接池异步插入 } ssr/pipelines.py from scrapy.exporters import JsonItemExporter from scrapy.pipelines.images import ImagesPipeline import codecs import json import MySQLdb import MySQLdb.cursors from twisted.enterprise import adbapi from ssr.utils.common import DateEncoder class SsrPipeline(object): def process_item(self, item, spider): return item class SsrImagePipeline(ImagesPipeline): def item_completed(self, results, item, info): if &quot;front_image_url&quot; in item: for ok, value in results: image_file_path = value[&quot;path&quot;] # 填充自定义路径 item[&quot;front_image_path&quot;] = image_file_path return item class JsonWithEncodingPipeline(object): # 自定义json文件的导出 def __init__(self): self.file = codecs.open(&#39;article.json&#39;, &#39;w&#39;, encoding=&quot;utf-8&quot;) def process_item(self, item, spider): # 序列化，ensure_ascii利于中文,json没法序列化date格式，需要新写函数 lines = json.dumps(dict(item), ensure_ascii=False, cls=DateEncoder) + &quot;\n&quot; self.file.write(lines) return item def spider_closed(self, spider): self.file.close() class JsonExporterPipleline(object): # 调用scrapy提供的json export导出json文件 def __init__(self): self.file = open(&#39;ssr.json&#39;, &#39;wb&#39;) self.exporter = JsonItemExporter(self.file, encoding=&quot;utf-8&quot;, ensure_ascii=False) self.exporter.start_exporting() def close_spider(self, spider): self.exporter.finish_exporting() self.file.close() def process_item(self, item, spider): self.exporter.export_item(item) return item class MysqlPipeline(object): # 采用同步的机制写入mysql def __init__(self): self.conn = MySQLdb.connect(&#39;127.0.0.1&#39;, &#39;root&#39;, &#39;123456&#39;, &#39;scrapy&#39;, charset=&quot;utf8&quot;, use_unicode=True) self.cursor = self.conn.cursor() def process_item(self, item, spider): insert_sql = &quot;&quot;&quot; insert into ssr(url, ip,ssr, port,password,secret) VALUES (%s, %s, %s, %s, %s) &quot;&quot;&quot; self.cursor.execute(insert_sql, (item[&quot;url&quot;],item[&quot;ssr&quot;], item[&quot;ip&quot;], item[&quot;port&quot;], item[&quot;password&quot;], item[&quot;secret&quot;])) self.conn.commit() class MysqlTwistedPipline(object): # 异步连接池插入数据库，不会阻塞 def __init__(self, dbpool): self.dbpool = dbpool @classmethod def from_settings(cls, settings):# 初始化时即被调用静态方法 dbparms = dict( host = settings[&quot;MYSQL_HOST&quot;],#setttings中定义 db = settings[&quot;MYSQL_DBNAME&quot;], user = settings[&quot;MYSQL_USER&quot;], passwd = settings[&quot;MYSQL_PASSWORD&quot;], charset=&#39;utf8&#39;, cursorclass=MySQLdb.cursors.DictCursor, use_unicode=True, ) dbpool = adbapi.ConnectionPool(&quot;MySQLdb&quot;, **dbparms) return cls(dbpool) def process_item(self, item, spider): #使用twisted将mysql插入变成异步执行 query = self.dbpool.runInteraction(self.do_insert, item) query.addErrback(self.handle_error, item, spider) #处理异常 def handle_error(self, failure, item, spider): #处理异步插入的异常 print (failure) def do_insert(self, cursor, item): #执行具体的插入，不具体的如MysqlPipeline.process_item() #根据不同的item 构建不同的sql语句并插入到mysql中 insert_sql, params = item.get_insert_sql() cursor.execute(insert_sql, params) 实体类ssr/items.py import scrapy from scrapy.loader import ItemLoader from scrapy.loader.processors import MapCompose, TakeFirst, Join import re import datetime from w3lib.html import remove_tags def date_convert(value): try: create_date = datetime.datetime.strptime(value, &quot;%Y/%m/%d&quot;).date() except Exception as e: create_date = datetime.datetime.now().date() return create_date def get_nums(value): match_re = re.match(&quot;.*?(\d+).*&quot;, value) if match_re: nums = int(match_re.group(1)) else: nums = 0 return nums def return_value(value): return value class SsrItemLoader(ItemLoader): # 自定义itemloader default_output_processor = TakeFirst() class SsrItem(scrapy.Item): url = scrapy.Field() ip = scrapy.Field( input_processor=MapCompose(return_value),#传递进来可以预处理 ) port = scrapy.Field() ssr = scrapy.Field() front_image_url = scrapy.Field() password = scrapy.Field() secret = scrapy.Field() def get_insert_sql(self): insert_sql = &quot;&quot;&quot; insert into ssr(url,ssr, ip, port, password,secret) VALUES (%s, %s,%s, %s, %s,%s) ON DUPLICATE KEY UPDATE ssr=VALUES(ssr) &quot;&quot;&quot; params = (self[&quot;url&quot;],self[&quot;ssr&quot;], self[&quot;ip&quot;],self[&quot;port&quot;], self[&quot;password&quot;],self[&quot;secret&quot;]) return insert_sql, params 核心代码ssr/spiders/freevpn_ss_tk.py # -*- coding: utf-8 -*- import time from datetime import datetime from urllib import parse import scrapy from scrapy.http import Request from ssr.items import SsrItemLoader, SsrItem class FreevpnSsTkSpider(scrapy.Spider): name = &#39;freevpn-ss.tk&#39; # 必须一级域名 allowed_domains = [&#39;freevpn-ss.tk&#39;] start_urls = [&#39;https://freevpn-ss.tk/category/technology/&#39;] custom_settings = { # 优先并覆盖项目，避免被重定向 &quot;COOKIES_ENABLED&quot;: False, # 关闭cookies &quot;DOWNLOAD_DELAY&quot;: 1, &#39;DEFAULT_REQUEST_HEADERS&#39;: { &#39;Accept&#39;: &#39;application/json, text/javascript, */*; q=0.01&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate, br&#39;, &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.8&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;, &#39;Cookie&#39;: &#39;&#39;, &#39;Host&#39;: &#39;freevpn-ss.tk&#39;, &#39;Origin&#39;: &#39;https://freevpn-ss.tk/&#39;, &#39;Referer&#39;: &#39;https://freevpn-ss.tk/&#39;, &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36&#39;, } } def parse(self, response): # post_nodes = response.css(&quot;.posts-list .panel&gt;a&quot;) # for post_node in post_nodes: # image_url = post_node.css(&quot;img::attr(src)&quot;).extract_first(&quot;&quot;) # post_url = post_node.css(&quot;::attr(href)&quot;).extract_first(&quot;&quot;) # yield Request(url=parse.urljoin(response.url, post_url), meta={&quot;front_image_url&quot;: image_url},callback=self.parse_detail) # response获取meta # # next_url = response.css(&quot;.next-page a::attr(href)&quot;).extract_first(&quot;&quot;) # if next_url: # print(next_url) # yield Request(url=parse.urljoin(response.url, next_url), callback=self.parse) post_node = response.css(&quot;.posts-list .panel&gt;a&quot;)[0] image_url = post_node.css(&quot;img::attr(src)&quot;).extract_first(&quot;&quot;) post_url = post_node.css(&quot;::attr(href)&quot;).extract_first(&quot;&quot;) yield Request(url=parse.urljoin(response.url, post_url), meta={&quot;front_image_url&quot;: image_url}, callback=self.parse_detail) # response获取meta def parse_detail(self, response): # 通过item loader加载item front_image_url = response.meta.get(&quot;front_image_url&quot;, &quot;&quot;) # 文章封面图 ssr_nodes = response.css(&quot;table tbody tr&quot;) with open(datetime.now().strftime(&#39;%Y-%m-%d&#39;), &#39;a&#39;) as file_object: for ssr in ssr_nodes: item_loader = SsrItemLoader(item=SsrItem(), response=response) # 默认ItemLoader是一个list，自定义TakeFirst() print(ssr.xpath(&quot;td[4]/text()&quot;).extract_first(&quot;&quot;)) item_loader.add_value(&quot;url&quot;, response.url) item_loader.add_value(&quot;ssr&quot;, ssr.css(&quot;td:nth-child(1)&gt;a::attr(href)&quot;).extract_first(&quot;&quot;)) item_loader.add_value(&quot;ip&quot;, ssr.css(&quot;td:nth-child(2)::text&quot;).extract_first(&quot;&quot;)) item_loader.add_value(&quot;front_image_url&quot;, front_image_url) item_loader.add_value(&quot;port&quot;, ssr.xpath(&quot;td[3]/text()&quot;).extract_first(&quot;&quot;)) item_loader.add_value(&quot;password&quot;, ssr.xpath(&quot;td[4]/text()&quot;).extract_first(&quot;&quot;)) item_loader.add_value(&quot;secret&quot;, ssr.xpath(&quot;td[5]/text()&quot;).extract_first(&quot;&quot;)) ssr_item = item_loader.load_item() file_object.write(ssr.css(&quot;td:nth-child(1)&gt;a::attr(href)&quot;).extract_first(&quot;&quot;)+&quot;\n&quot;) yield ssr_item # 将传到piplines中 爬虫开发2# -*- coding: utf-8 -*- import time from datetime import datetime from urllib import parse import scrapy from scrapy.http import Request from ssr.items import SsrItemLoader, SsrItem class FanQiangSpider(scrapy.Spider): name = &#39;fanqiang.network&#39; # 必须一级域名 allowed_domains = [&#39;fanqiang.network&#39;] start_urls = [&#39;https://fanqiang.network/免费ssr&#39;] def parse(self, response): post_nodes = response.css(&quot;.post-content table tbody tr&quot;) item_loader = SsrItemLoader(item=SsrItem(), response=response) with open(datetime.now().strftime(&#39;%Y-%m-%d&#39;), &#39;a&#39;) as file_object: for post_node in post_nodes: item_loader.add_value(&quot;url&quot;, response.url) item_loader.add_value(&quot;ssr&quot;, post_node.css(&quot;td:nth-child(1)&gt;a::attr(href)&quot;).extract_first(&quot;&quot;).replace(&quot;http://freevpn-ss.tk/&quot;, &quot;&quot;)) item_loader.add_value(&quot;ip&quot;, post_node.css(&quot;td:nth-child(2)::text&quot;).extract_first(&quot;&quot;)) item_loader.add_value(&quot;port&quot;, post_node.xpath(&quot;td[3]/text()&quot;).extract_first(&quot;&quot;)) item_loader.add_value(&quot;password&quot;, post_node.xpath(&quot;td[4]/text()&quot;).extract_first(&quot;&quot;)) item_loader.add_value(&quot;secret&quot;, post_node.xpath(&quot;td[5]/text()&quot;).extract_first(&quot;&quot;)) ssr_item = item_loader.load_item() file_object.write(post_node.css(&quot;td:nth-child(1)&gt;a::attr(href)&quot;).extract_first(&quot;&quot;).replace(&quot;http://freevpn-ss.tk/&quot;, &quot;&quot;) + &quot;\n&quot;) yield ssr_item # 将传到piplines中 多爬虫同时运行settings.py COMMANDS_MODULE = &#39;ssr&#39; ssr/crawlall.py from scrapy.commands import ScrapyCommand class Command(ScrapyCommand): requires_project = True def syntax(self): return &#39;[options]&#39; def short_desc(self): return &#39;Runs all of the spiders&#39; def run(self, args, opts): spider_list = self.crawler_process.spiders.list() for name in spider_list: self.crawler_process.crawl(name, **opts.__dict__) self.crawler_process.start()main.py from scrapy import cmdline from scrapy.cmdline import execute import sys import os sys.path.append(os.path.dirname(os.path.abspath(__file__))) # execute([&quot;scrapy&quot;, &quot;crawl&quot;, &quot;freevpn-ss.tk&quot;]) # execute([&quot;scrapy&quot;, &quot;crawl&quot;, &quot;fanqiang.network&quot;]) cmdline.execute(&quot;scrapy crawlall&quot;.split()) 防反爬随机uapip install -i https://pypi.doubanio.com/simple/ –trusted-host pypi.doubanio.com scrapy-fake-useragent DOWNLOADER_MIDDLEWARES = { &#39;scrapy_fake_useragent.middleware.RandomUserAgentMiddleware&#39;: 1, } 报错socket.timeout: timed out，查看F:/Anaconda3/Lib/site-packages/fake_useragent/settings.py __version__ = &#39;0.1.11&#39; DB = os.path.join( tempfile.gettempdir(), &#39;fake_useragent_{version}.json&#39;.format( version=__version__, ), ) CACHE_SERVER = &#39;https://fake-useragent.herokuapp.com/browsers/{version}&#39;.format( version=__version__, ) BROWSERS_STATS_PAGE = &#39;https://www.w3schools.com/browsers/default.asp&#39; BROWSER_BASE_PAGE = &#39;http://useragentstring.com/pages/useragentstring.php?name={browser}&#39; # noqa BROWSERS_COUNT_LIMIT = 50 REPLACEMENTS = { &#39; &#39;: &#39;&#39;, &#39;_&#39;: &#39;&#39;, } SHORTCUTS = { &#39;internet explorer&#39;: &#39;internetexplorer&#39;, &#39;ie&#39;: &#39;internetexplorer&#39;, &#39;msie&#39;: &#39;internetexplorer&#39;, &#39;edge&#39;: &#39;internetexplorer&#39;, &#39;google&#39;: &#39;chrome&#39;, &#39;googlechrome&#39;: &#39;chrome&#39;, &#39;ff&#39;: &#39;firefox&#39;, } OVERRIDES = { &#39;Edge/IE&#39;: &#39;Internet Explorer&#39;, &#39;IE/Edge&#39;: &#39;Internet Explorer&#39;, } HTTP_TIMEOUT = 5 HTTP_RETRIES = 2 HTTP_DELAY = 0.1 http://useragentstring.com/pages/useragentstring.php?name=Chrome 打开超时报错，其中CACHE_SERVER是存储了所有UserAgent的json数据，再次观察其中DB这个变量，结合fake_useragent\fake.py中的逻辑，判断这个变量应该是存储json数据的，所以大体逻辑应该是，首次初始化时，会自动爬取CACHE_SERVER中的json数据，然后将其存储到本地，所以我们直接将json存到指定路径下，再次初始化时，应该就不会报错 &gt;&gt;&gt; import tempfile &gt;&gt;&gt; print(tempfile.gettempdir()) C:\Users\codewj\AppData\Local\Temp 将CACHE_SERVER的json数据保存为fake_useragent_0.1.11.json,并放到目录C:\Users\codewj\AppData\Local\Temp中 &gt;&gt;&gt; import fake_useragent &gt;&gt;&gt; ua = fake_useragent.UserAgent() &gt;&gt;&gt; ua.data_browsers[&#39;chrome&#39;][0] &#39;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36&#39; 注：如果CACHE_SERVER不是https://fake-useragent.herokuapp.com/browsers/0.1.11，请更新一下库pip install –upgrade fake_useragent]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
        <tag>selenium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[好客租房mybatisplus]]></title>
    <url>%2F2019%2F08%2F22%2F%E5%A5%BD%E5%AE%A2%E7%A7%9F%E6%88%BFmybatisplus%2F</url>
    <content type="text"><![CDATA[基于dubbo react mybatisplus elk实战整合开发 mysqldocker run -di --name=mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 docker.io/mysql mysql -u root -p ALTER USER &#39;root&#39;@&#39;%&#39; IDENTIFIED WITH mysql_native_password BY &#39;123456&#39;; docker logs -fn 500 mysql MybatisPlus入门执行建表 haoke.sql mybatis-plus/pom.xml &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--mybatis-plus的springboot支持--&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.0.5&lt;/version&gt; &lt;/dependency&gt; &lt;!--mysql驱动--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; application.properties spring.application.name = itcast-mybatis-plus spring.datasource.driver-class-name=com.mysql.jdbc.Driver spring.datasource.url=jdbc:mysql://192.168.3.237:3306/haoke?useUnicode=true&amp;characterEncoding=utf8&amp;autoReconnect=true&amp;allowMultiQueries=true&amp;useSSL=false spring.datasource.username=root spring.datasource.password=123456 com/onejane/mybatisplus/pojo/User.java @Data public class User { @TableId(value = &quot;ID&quot;, type = IdType.AUTO) private Long id; private String name; private Integer age; private String email; } com/onejane/mybatisplus/mapper/UserMapper.java public interface UserMapper extends BaseMapper&lt;User&gt; { } com/onejane/mybatisplus/MyApplication.java @MapperScan(&quot;com.onejane.mybatisplus.mapper&quot;) //设置mapper接口的扫描包 @SpringBootApplication public class MyApplication { /** * 分页插件 */ @Bean public PaginationInterceptor paginationInterceptor() { return new PaginationInterceptor(); } public static void main(String[] args) { SpringApplication.run(MyApplication.class, args); } } com/onejane/mybatisplus/mapper/UserMaperTest.java @RunWith(SpringRunner.class) @SpringBootTest public class UserMaperTest { @Autowired private UserMapper userMapper; @Test public void testSelect(){ List&lt;User&gt; users = this.userMapper.selectList(null); for (User user : users) { System.out.println(user); } } @Test public void testSelectById(){ User user = this.userMapper.selectById(3L); System.out.println(user); } @Test public void testSelectByLike(){ QueryWrapper&lt;User&gt; wrapper = new QueryWrapper&lt;User&gt;(); wrapper.like(&quot;name&quot;, &quot;o&quot;); List&lt;User&gt; list = this.userMapper.selectList(wrapper); for (User user : list) { System.out.println(user); } } @Test public void testSelectByLe(){ QueryWrapper&lt;User&gt; wrapper = new QueryWrapper&lt;User&gt;(); wrapper.le(&quot;age&quot;, 20); List&lt;User&gt; list = this.userMapper.selectList(wrapper); for (User user : list) { System.out.println(user); } } @Test public void testSave(){ User user = new User(); user.setAge(25); user.setEmail(&quot;zhangsan@qq.com&quot;); user.setName(&quot;zhangsan&quot;); int count = this.userMapper.insert(user); System.out.println(&quot;新增数据成功! count =&gt; &quot; + count); } @Test public void testDelete(){ this.userMapper.deleteById(7L); System.out.println(&quot;删除成功!&quot;); } @Test public void testUpdate(){ User user = new User(); user.setId(6L); user.setName(&quot;lisi&quot;); this.userMapper.updateById(user); System.out.println(&quot;修改成功!&quot;); } @Test public void testSelectPage() { Page&lt;User&gt; page = new Page&lt;&gt;(2, 2); IPage&lt;User&gt; userIPage = this.userMapper.selectPage(page, null); System.out.println(&quot;总条数 ------&gt; &quot; + userIPage.getTotal()); System.out.println(&quot;当前页数 ------&gt; &quot; + userIPage.getCurrent()); System.out.println(&quot;当前每页显示数 ------&gt; &quot; + userIPage.getSize()); List&lt;User&gt; records = userIPage.getRecords(); for (User user : records) { System.out.println(user); } } } 兼容配置application.properties ## 指定全局配置文件 mybatis-plus.config-location = classpath:mybatis-config.xml # 指定mapper.xml文件 mybatis-plus.mapper-locations = classpath*:mybatis/*.xml Lombok&lt;!--简化代码的工具包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;version&gt;1.18.4&lt;/version&gt; &lt;/dependency&gt; 安装idea Lombok插件 @Data：注解在类上；提供类所有属性的 getting 和 setting 方法，此外还提供了equals、canEqual、hashCode、toString 方法 @Setter setting 方法 @Getter：注解在属性上；为属性提供 getting 方法 @Slf4j：注解在类上；为类提供一个 属性名为log 的 slf4j日志对象 @NoArgsConstructor：注解在类上；为类提供一个无参的构造方法 @AllArgsConstructor：注解在类上；为类提供一个全参的构造方法 @Builder：使用Builder模式构建对象 @Slf4j @Data @AllArgsConstructor @Builder public class Item { private Long id; private String title; private Long price; public Item() { log.info(&quot;写日志。。。。。&quot;); } public static void main(String[] args) { Item item1 = new Item(1L,&quot;哈哈哈&quot;,10L); Item item2 = Item.builder().price(100L)title(&quot;hello&quot;).id(1L).build(); System.out.println(item1.getId()); } } 搭建后台服务系统haoke-manage├─haoke-manage-api-server├─haoke-manage-dubbo-server│ ├─haoke-manage-dubbo-server-ad│ ├─haoke-manage-dubbo-server-common│ ├─haoke-manage-dubbo-server-generator MybatisPlus的AutoGenerator插件生成代码文件│ ├─haoke-manage-dubbo-server-house-resources│ │ ├─haoke-manage-dubbo-server-house-resources-dubbo-interface 对外提供的sdk包 只提供pojo实体以及接口，不提供实现类│ │ ├─haoke-manage-dubbo-server-house-resources-dubbo-service 具体实现 haoke-manage&lt;!--spring boot的支持放在groupId上面--&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.0.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;!--springboot 测试支持--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--dubbo的springboot支持--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;0.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--dubbo框架--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;dubbo&lt;/artifactId&gt; &lt;version&gt;2.6.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;3.7&lt;/version&gt; &lt;/dependency&gt; &lt;!--zk依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.13&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.sgroschupf&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; haoke-manage-api-serverpom.xml &lt;dependencies&gt; &lt;!--springboot的web支持--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.onejane&lt;/groupId&gt; &lt;artifactId&gt;haoke-manage-dubbo-server-house-resources-dubbo-interface&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; application.properties # Spring boot application spring.application.name = haoke-manage-api-server server.port = 18080 #logging.level.root=DEBUG # dubbo的应用名称 dubbo.application.name = dubbo-consumer-haoke-manage # zk注册中心 dubbo.registry.address = zookeeper://192.168.3.237:2181 dubbo.registry.client = zkclient Pagination @Data @AllArgsConstructor public class Pagination { private Integer current; private Integer pageSize; private Integer total; }TableResult @Data @AllArgsConstructor public class TableResult&lt;T&gt; { private List&lt;T&gt; list; private Pagination pagination; } HouseResourcesService @Service public class HouseResourcesService { @Reference(version = &quot;1.0.0&quot;) private ApiHouseResourcesService apiHouseResourcesService; public boolean save(HouseResources houseResources) { int result = this.apiHouseResourcesService.saveHouseResources(houseResources); return result == 1; } public TableResult&lt;HouseResources&gt; queryList(HouseResources houseResources, Integer currentPage, Integer pageSize) { PageInfo&lt;HouseResources&gt; pageInfo = this.apiHouseResourcesService. queryHouseResourcesList(currentPage, pageSize, houseResources); return new TableResult&lt;&gt;(pageInfo.getRecords(), new Pagination(currentPage, pageSize, pageInfo.getTotal())); } /** * 根据id查询房源数据 * * @param id * @return */ public HouseResources queryHouseResourcesById(Long id){ // 调用dubbo中的服务进行查询数据 return this.apiHouseResourcesService.queryHouseResourcesById(id); } public boolean update(HouseResources houseResources) { return this.apiHouseResourcesService.updateHouseResources(houseResources); } }HouseResourcesController @Controller @RequestMapping(&quot;house/resources&quot;) public class HouseResourcesController { @Autowired private HouseResourcesService houseResourcesService; /** * 新增房源 * * @param houseResources json数据 * @return */ @PostMapping @ResponseBody public ResponseEntity&lt;Void&gt; save(@RequestBody HouseResources houseResources) { try { boolean bool = this.houseResourcesService.save(houseResources); if (bool) { return ResponseEntity.status(HttpStatus.CREATED).build(); } } catch (Exception e) { e.printStackTrace(); } return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).build(); } /** * 查询房源列表 * * @param houseResources * @param currentPage * @param pageSize * @return */ @GetMapping @ResponseBody public ResponseEntity&lt;TableResult&gt; list(HouseResources houseResources, @RequestParam(name = &quot;currentPage&quot;, defaultValue = &quot;1&quot;) Integer currentPage, @RequestParam(name = &quot;pageSize&quot;, defaultValue = &quot;10&quot;) Integer pageSize) { return ResponseEntity.ok(this.houseResourcesService.queryList(houseResources, currentPage, pageSize)); } /** * 修改房源 * * @param houseResources json数据 * @return */ @PutMapping @ResponseBody public ResponseEntity&lt;Void&gt; update(@RequestBody HouseResources houseResources) { try { boolean bool = this.houseResourcesService.update(houseResources); if (bool) { return ResponseEntity.status(HttpStatus.NO_CONTENT).build(); } } catch (Exception e) { e.printStackTrace(); } return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).build(); } } DubboApiApplication @SpringBootApplication public class DubboApiApplication { public static void main(String[] args) { SpringApplication.run(DubboApiApplication.class, args); } }haoke-manage-dubbo-serverpom.xml &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; haoke-manage-dubbo-server-generatorpom.xml &lt;dependencies&gt; &lt;!-- freemarker 模板引擎 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.freemarker&lt;/groupId&gt; &lt;artifactId&gt;freemarker&lt;/artifactId&gt; &lt;version&gt;2.3.28&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.0.5&lt;/version&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt; CodeGenerator 配置数据源及账户密码，运行即可生成对应文件，并将pojo类拷贝到工程中备用 public class CodeGenerator { /** * &lt;p&gt; * 读取控制台内容 * &lt;/p&gt; */ public static String scanner(String tip) { Scanner scanner = new Scanner(System.in); StringBuilder help = new StringBuilder(); help.append(&quot;请输入&quot; + tip + &quot;：&quot;); System.out.println(help.toString()); if (scanner.hasNext()) { String ipt = scanner.next(); if (StringUtils.isNotEmpty(ipt)) { return ipt; } } throw new MybatisPlusException(&quot;请输入正确的&quot; + tip + &quot;！&quot;); } public static void main(String[] args) { // 代码生成器 AutoGenerator mpg = new AutoGenerator(); // 全局配置 GlobalConfig gc = new GlobalConfig(); String projectPath = System.getProperty(&quot;user.dir&quot;); gc.setOutputDir(projectPath + &quot;/src/main/java&quot;); gc.setAuthor(&quot;onejane&quot;); gc.setOpen(false); mpg.setGlobalConfig(gc); // 数据源配置 DataSourceConfig dsc = new DataSourceConfig(); dsc.setUrl(&quot;jdbc:mysql://192.168.3.237:3306/haoke?useUnicode=true&amp;characterEncoding=utf8&amp;autoReconnect=true&amp;allowMultiQueries=true&quot;); // dsc.setSchemaName(&quot;public&quot;); dsc.setDriverName(&quot;com.mysql.jdbc.Driver&quot;); dsc.setUsername(&quot;root&quot;); dsc.setPassword(&quot;123456&quot;); mpg.setDataSource(dsc); // 包配置 PackageConfig pc = new PackageConfig(); pc.setModuleName(scanner(&quot;模块名&quot;)); pc.setParent(&quot;com.onejane.haoke.dubbo.server&quot;); mpg.setPackageInfo(pc); // 自定义配置 InjectionConfig cfg = new InjectionConfig() { @Override public void initMap() { // to do nothing } }; List&lt;FileOutConfig&gt; focList = new ArrayList&lt;&gt;(); focList.add(new FileOutConfig(&quot;/templates/mapper.xml.ftl&quot;) { @Override public String outputFile(TableInfo tableInfo) { // 自定义输入文件名称 return projectPath + &quot;/src/main/resources/mapper/&quot; + pc.getModuleName() + &quot;/&quot; + tableInfo.getEntityName() + &quot;Mapper&quot; + StringPool.DOT_XML; } }); cfg.setFileOutConfigList(focList); mpg.setCfg(cfg); mpg.setTemplate(new TemplateConfig().setXml(null)); // 策略配置 StrategyConfig strategy = new StrategyConfig(); strategy.setNaming(NamingStrategy.underline_to_camel); strategy.setColumnNaming(NamingStrategy.underline_to_camel); strategy.setSuperEntityClass(&quot;com.onejane.haoke.dubbo.server.pojo.BasePojo&quot;); strategy.setEntityLombokModel(true); strategy.setRestControllerStyle(true); strategy.setSuperControllerClass(&quot;com.baomidou.ant.common.BaseController&quot;); strategy.setInclude(scanner(&quot;表名&quot;)); strategy.setSuperEntityColumns(&quot;id&quot;); strategy.setControllerMappingHyphenStyle(true); strategy.setTablePrefix(pc.getModuleName() + &quot;_&quot;); mpg.setStrategy(strategy); mpg.setTemplateEngine(new FreemarkerTemplateEngine()); mpg.execute(); } } 测试 haoke-manage-dubbo-server-house-resourcespom.xml &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.onejane&lt;/groupId&gt; &lt;artifactId&gt;haoke-manage-dubbo-server-common&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;!--需要注意：传递依赖中，如果需要使用，请显示引入--&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.0.5&lt;/version&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt; haoke-manage-dubbo-server-house-resources-dubbo-interfaceApiHouseResourcesService // dubbo service public interface ApiHouseResourcesService { /** * 新增房源 * * @param houseResources * @return -1:输入的参数不符合要求，0：数据插入数据库失败，1：成功 */ int saveHouseResources(HouseResources houseResources); /** * 分页查询房源列表 * * @param page 当前页 * @param pageSize 页面大小 * @param queryCondition 查询条件 * @return */ PageInfo&lt;HouseResources&gt; queryHouseResourcesList(int page, int pageSize, HouseResources queryCondition); /** * 根据id查询房源数据 * * @param id * @return */ HouseResources queryHouseResourcesById(Long id); boolean updateHouseResources(HouseResources houseResources); } HouseResources @Data @Accessors(chain = true) @TableName(&quot;tb_house_resources&quot;) public class HouseResources extends BasePojo { private static final long serialVersionUID = 779152022777511825L; @TableId(value = &quot;id&quot;, type = IdType.AUTO) private Long id; /** * 房源标题 */ private String title; /** * 楼盘id */ private Long estateId; /** * 楼号（栋） */ private String buildingNum; /** * 单元号 */ private String buildingUnit; /** * 门牌号 */ private String buildingFloorNum; /** * 租金 */ private Integer rent; /** * 租赁方式，1-整租，2-合租 */ private Integer rentMethod; /** * 支付方式，1-付一押一，2-付三押一，3-付六押一，4-年付押一，5-其它 */ private Integer paymentMethod; /** * 户型，如：2室1厅1卫 */ private String houseType; /** * 建筑面积 */ private String coveredArea; /** * 使用面积 */ private String useArea; /** * 楼层，如：8/26 */ private String floor; /** * 朝向：东、南、西、北 */ private String orientation; /** * 装修，1-精装，2-简装，3-毛坯 */ private Integer decoration; /** * 配套设施， 如：1,2,3 */ private String facilities; /** * 图片，最多5张 */ private String pic; /** * 描述 */ private String houseDesc; /** * 联系人 */ private String contact; /** * 手机号 */ private String mobile; /** * 看房时间，1-上午，2-中午，3-下午，4-晚上，5-全天 */ private Integer time; /** * 物业费 */ private String propertyCost; } haoke-manage-dubbo-server-house-resources-dubbo-servicepom.xml &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.onejane&lt;/groupId&gt; &lt;artifactId&gt;haoke-manage-dubbo-server-house-resources-dubbo-interface&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; application.properties # Spring boot application spring.application.name = haoke-manage-dubbo-server-house-resources # 数据库 spring.datasource.driver-class-name=com.mysql.jdbc.Driver spring.datasource.url=jdbc:mysql://192.168.3.237:3306/haoke?useUnicode=true&amp;characterEncoding=utf8&amp;autoReconnect=true&amp;allowMultiQueries=true&amp;useSSL=false spring.datasource.username=root spring.datasource.password=123456 # 服务的扫描包 dubbo.scan.basePackages = com.onejane.haoke.dubbo.server.api # 应用名称 dubbo.application.name = dubbo-provider-house-resources # 协议以及端口 dubbo.protocol.name = dubbo dubbo.protocol.port = 20880 # zk注册中心 dubbo.registry.address = zookeeper://192.168.3.237:2181 dubbo.registry.client = zkclient MybatisConfig @MapperScan(&quot;com.onejane.haoke.dubbo.server.mapper&quot;) @Configuration public class MybatisConfig { } HouseResourcesMapper public interface HouseResourcesMapper extends BaseMapper&lt;HouseResources&gt; { } HouseResourcesService // spring service public interface HouseResourcesService { /** * @param houseResources * * @return -1:输入的参数不符合要求，0：数据插入数据库失败，1：成功 */ int saveHouseResources(HouseResources houseResources); PageInfo&lt;HouseResources&gt; queryHouseResourcesList(int page, int pageSize, HouseResources queryCondition); /** * 根据房源id查询房源数据 * * @param id * @return */ HouseResources queryHouseResourcesById(Long id); /** * 更新房源数据 * * @param houseResources * @return */ boolean updateHouseResources(HouseResources houseResources); }HouseResourcesServiceImpl @Transactional @Service public class HouseResourcesServiceImpl extends BaseServiceImpl&lt;HouseResources&gt; implements HouseResourcesService { /** * @param houseResources * @return -1:输入的参数不符合要求，0：数据插入数据库失败，1：成功 */ @Override public int saveHouseResources(HouseResources houseResources) { // 添加校验或者是其他的一些逻辑 if (StringUtils.isBlank(houseResources.getTitle())) { // 不符合要求 return -1; } return super.save(houseResources); } @Override public PageInfo&lt;HouseResources&gt; queryHouseResourcesList(int page, int pageSize, HouseResources queryCondition) { QueryWrapper queryWrapper = new QueryWrapper(); // 根据数据的更新时间做倒序排序 queryWrapper.orderByDesc(&quot;updated&quot;); IPage iPage = super.queryPageList(queryWrapper, page, pageSize); return new PageInfo&lt;HouseResources&gt;(Long.valueOf(iPage.getTotal()).intValue(), page, pageSize, iPage.getRecords()); } public HouseResources queryHouseResourcesById(Long id) { return super.queryById(id); } @Override public boolean updateHouseResources(HouseResources houseResources) { return super.update(houseResources) == 1; } } DubboProvider @SpringBootApplication public class DubboProvider { /** * 分页插件 */ @Bean public PaginationInterceptor paginationInterceptor() { return new PaginationInterceptor(); } public static void main(String[] args) { new SpringApplicationBuilder(DubboProvider.class) .web(WebApplicationType.NONE) // 非 Web 应用 .run(args); } } ApiHouseResourcesServiceImpl @Service(version = &quot;1.0.0&quot;) // 分离dubbo服务和spring服务，易于扩展 public class ApiHouseResourcesServiceImpl implements ApiHouseResourcesService { @Autowired private HouseResourcesService houseResourcesService; @Override public int saveHouseResources(HouseResources houseResources) { return this.houseResourcesService.saveHouseResources(houseResources); } @Override public PageInfo&lt;HouseResources&gt; queryHouseResourcesList(int page, int pageSize, HouseResources queryCondition) { return this.houseResourcesService.queryHouseResourcesList(page, pageSize, queryCondition); } public HouseResources queryHouseResourcesById(Long id) { return this.houseResourcesService.queryHouseResourcesById(id); } @Override public boolean updateHouseResources(HouseResources houseResources) { return this.houseResourcesService.updateHouseResources(houseResources); } } 启动DubboAdmin中显示 测试 整合前端src/pages/haoke/House/AddResource.js添加标题及修改表单提交地址 &lt;FormItem {...formItemLayout} label=&quot;房源标题&quot;&gt; {getFieldDecorator(&#39;title&#39;,{rules:[{ required: true, message:&quot;此项为必填项&quot; }]})(&lt;Input style={{ width: '100%' }} /&gt;)} &lt;/FormItem&gt; dispatch({ type: &#39;house/submitHouseForm&#39;, payload: values, }); src/pages/haoke/House/models/form.js增加model import { routerRedux } from &#39;dva/router&#39;; import { message } from &#39;antd&#39;; import { addHouseResource } from &#39;@/services/haoke&#39;; export default { namespace: &#39;house&#39;, state: { }, effects: { *submitHouseForm({ payload }, { call }) { yield call(addHouseResource, payload); message.success(&#39;提交成功&#39;); } }, reducers: { saveStepFormData(state, { payload }) { return { ...state }; }, }, }; src/services/haoke.js增加服务，请求服务并且处理业务逻辑 import request from &#39;@/utils/request&#39;; export async function addHouseResource(params) { return request(&#39;/haoke/house/resources&#39;, { method: &#39;POST&#39;, body: params }); } 由于我们前端系统8000和后台服务系统18080的端口不同，会导致跨域问题，我们通过umi提供的反向代理功能解决这个问题。haoke-manage-web/config/config.js proxy: { &#39;/haoke/&#39;: { target: &#39;http://127.0.0.1:18080/&#39;, changeOrigin: true, pathRewrite: { &#39;^/haoke/&#39;: &#39;&#39; } } }, 代理效果是这样的：以haoke开头的请求都会被代理请求：http://localhost:8000/haoke/house/resources实际：http://127.0.0.1:18080/house/resources 测试启动前端：itcast-haoke-manage-web&gt;tyarn start启动提供者：itcast-haoke-manage-dubbo-server-house-resources-dubbo-service/DubboProvider启动消费者：tcast-haoke-manage/itcast-haoke-manage-api-server/DubboApiApplication]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>mybatisplus</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[好客租房dubbo]]></title>
    <url>%2F2019%2F08%2F21%2F%E5%A5%BD%E5%AE%A2%E7%A7%9F%E6%88%BFdubbo%2F</url>
    <content type="text"><![CDATA[基于dubbo react elk实战整合开发 整体架构 后端架构：SpringBoot+StringMVC+Dubbo+Mybatis+ELK+区块链 前端架构：React.js+html5+百度地图+微信小程序前端初始化haoke-manage-web tyarn install tyarn start 修改logo及版权信息src/layouts/BasicLayout.js 全局的布局文件 &lt;SiderMenu![itcast-haoke-manage-web](./attachments/itcast-haoke-manage-web.zip) logo={logo} Authorized={Authorized} theme={navTheme} onCollapse={this.handleMenuCollapse} menuData={menuData} isMobile={isMobile} {...this.props} /&gt; src/components/SiderMenu/index.js &lt;SiderMenu {...props} flatMenuKeys={getFlatMenuKeys(menuData)} collapsed={isMobile ? false : collapsed} /&gt; src/components/SiderMenu/SiderMenu.js &lt;div className={styles.logo} id=&quot;logo&quot;&gt; &lt;Link to=&quot;/&quot;&gt; {/* &lt;img src={logo} alt=&quot;logo&quot; /&gt; */} &lt;h1&gt;好客租房 · 管理系统&lt;/h1&gt; &lt;/Link&gt; &lt;/div&gt; src/layouts/Footer.js &lt;Footer style={{ padding: 0 }}&gt; &lt;GlobalFooter copyright={ &lt;Fragment&gt; Copyright &lt;Icon type=&quot;copyright&quot; /&gt; 2018 黑马程序员 博学谷 出品 &lt;/Fragment&gt; } /&gt; &lt;/Footer&gt; 新增房源config/router.config.js { path: &#39;/&#39;, redirect: &#39;/house/resource&#39; }, // 进入系统默认打开房源管理 { //房源管理 path: &#39;/house&#39;, name: &#39;house&#39;, icon: &#39;home&#39;, routes: [ { path: &#39;/house/resource&#39;, name: &#39;resource&#39;, component: &#39;./haoke/House/Resource&#39; }, { path: &#39;/house/addResource&#39;, name: &#39;addResource&#39;, component: &#39;./haoke/House/AddResource&#39; }, { path: &#39;/house/kanfang&#39;, name: &#39;kanfang&#39;, component: &#39;./haoke/House/KanFang&#39; }, { path: &#39;/house/zufang&#39;, name: &#39;zufang&#39;, component: &#39;./haoke/House/ZuFang&#39; } ] }, src/pages/haoke/House/AddResource.js @Form.create() 对页面进行了包装，包装之后，会在this.props中增加form对象,将拥有getFieldDecorator 双向绑定等功能,经过 getFieldDecorator 包装的控件，表单控件会自动添加 value （或 valuePropName 指定的其他属性） onChange （或 trigger 指定的其他属性），数据同步将被 Form 接管 你不再需要也不应该用 onChange 来做同步，但还是可以继续监听 onChange 等事件。 你不能用控件的 value defaultValue 等属性来设置表单域的值，默认值可以用getFieldDecorator 里的 initialValue,利用rule进行参数规则校验 你不应该用 setState ，可以使用 this.props.form.setFieldsValue 来动态改变表单值。 表单提交&lt;Button type=&quot;primary&quot; htmlType=&quot;submit&quot; loading={submitting}&gt; &lt;Form onSubmit={this.handleSubmit} hideRequiredMark style={{ marginTop: 8 }}&gt; 进行提交拦截 handleSubmit = e =&gt; { 通过form.validateFieldsAndScroll()对表单进行校验，通过values获取表单中输入的值。通过dispatch()调用model中定义的方法。 const { dispatch, form } = this.props; e.preventDefault(); console.log(this.state.fileList); form.validateFieldsAndScroll((err, values) =&gt; { if (!err) { if(values.facilities){ values.facilities = values.facilities.join(&quot;,&quot;); } if(values.floor_1 &amp;&amp; values.floor_2){ values.floor = values.floor_1 + &quot;/&quot; + values.floor_2; } values.houseType = values.houseType_1 + &quot;室&quot; + values.houseType_2 + &quot;厅&quot; + values.houseType_3 + &quot;卫&quot; + values.houseType_4 + &quot;厨&quot; + values.houseType_2 + &quot;阳台&quot;; delete values.floor_1; delete values.floor_2; delete values.houseType_1; delete values.houseType_2; delete values.houseType_3; delete values.houseType_4; delete values.houseType_5; dispatch({ type: &#39;form/submitRegularForm&#39;, payload: values, }); } }); }; 自动完成const estateMap = new Map([ [&#39;中远两湾城&#39;,&#39;1001|上海市,上海市,普陀区,远景路97弄&#39;], [&#39;上海康城&#39;,&#39;1002|上海市,上海市,闵行区,莘松路958弄&#39;], [&#39;保利西子湾&#39;,&#39;1003|上海市,上海市,松江区,广富林路1188弄&#39;], [&#39;万科城市花园&#39;,&#39;1004|上海市,上海市,闵行区,七莘路3333弄2区-15区&#39;], [&#39;上海阳城&#39;,&#39;1005|上海市,上海市,闵行区,罗锦路888弄&#39;] ]); &lt;AutoComplete style={{ width: '100%' }} dataSource={this.state.estateDataSource} placeholder=&quot;搜索楼盘&quot; onSelect={(value, option)=&gt;{ let v = estateMap.get(value); this.setState({ estateAddress: v.substring(v.indexOf(&#39;|&#39;)+1), estateId : v.substring(0,v.indexOf(&#39;|&#39;)) }); }} onSearch={this.handleSearch} filterOption={(inputValue, option) =&gt; option.props.children.toUpperCase().indexOf(inputValue.toUpperCase()) !== -1} /&gt; 通过onSearch进行动态设置数据源，这里使用的数据是静态数据 handleSearch = (value)=&gt;{ let arr = new Array(); if(value.length &gt; 0 ){ estateMap.forEach((v, k) =&gt; { if(k.startsWith(value)){ arr.push(k); } }); } this.setState({ estateDataSource: arr }); } ; 通过onSelect设置 选中楼盘后，在楼盘地址中填写地址数据 onSelect={(value, option)=&gt;{ let v = estateMap.get(value); this.setState({ estateAddress: v.substring(v.indexOf(&#39;|&#39;)+1), estateId : v.substring(0,v.indexOf(&#39;|&#39;)) }); }} 图片上传父组件通过属性的方式进行引用子组件，自组件在bind方法中改变this的引用为父组件 &lt;FormItem {...formItemLayout} label=&quot;上传室内图&quot;&gt; &lt;PicturesWall handleFileList={this.handleFileList.bind(this)}/&gt; &lt;/FormItem&gt; 父组件中获取数据 handleFileList = (obj)=&gt;{ console.log(obj, &quot;图片列表&quot;); } src/pages/haoke/Utils/PicturesWall.js 在子组件中通过this.props获取父组件方法传入的函数，进行调用，即可把数据传递到父组件中 handleChange = ({ fileList }) =&gt; { this.setState({ fileList }); this.props.handleFileList(this.state.fileList); } &lt;Upload action=&quot;1111111&quot; listType=&quot;picture-card&quot; fileList={fileList} onPreview={this.handlePreview} onChange={this.handleChange} &gt; {fileList.length &gt;= 5 ? null : uploadButton} &lt;/Upload&gt; 后端后台系统服务采用RPC+微服务的架构思想，RPC采用dubbo架构作为服务治理框架，对外接口采用RESTFul+GraphQL接口方式。 单一应用架构当网站流量很小时，只需一个应用，将所有功能都部署在一起，以减少部署节点和成本。此时，用于简化增删改查工作量的数据访问框架(ORM)是关键。 垂直应用架构当访问量逐渐增大，单一应用增加机器带来的加速度越来越小，将应用拆成互不相干的几个应用，以提升效率。此时，用于加速前端页面开发的Web框架(MVC)是关键。 分布式服务架构当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。此时，用于提高业务复用及整合的分布式服务框架(RPC)是关键。 流动计算架构当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。此时，用于提高机器利用率的资源调度和治理中心(SOA)是关键。调用关系说明 服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。流程说明： 服务提供者启动时: 向 /dubbo/com.foo.BarService/providers 目录下写入自己的 URL 地址 服务消费者启动时: 订阅 /dubbo/com.foo.BarService/providers 目录下的提供者 URL 地址。并向/dubbo/com.foo.BarService/consumers 目录下写入自己的 URL 地址 监控中心启动时: 订阅 /dubbo/com.foo.BarService 目录下的所有提供者和消费者 URL 地址。支持以下功能： 当提供者出现断电等异常停机时，注册中心能自动删除提供者信息 当注册中心重启时，能自动恢复注册数据，以及订阅请求 当会话过期时，能自动恢复注册数据，以及订阅请求 当设置 &lt;dubbo:registry check=”false” /&gt; 时，记录失败注册和订阅请求，后台定时重试 可通过 &lt;dubbo:registry username=”admin” password=”1234” /&gt; 设置 zookeeper 登录信息 可通过 &lt;dubbo:registry group=”dubbo” /&gt; 设置 zookeeper 的根节点，不设置将使用无根树 支持 * 号通配符 &lt;dubbo:reference group=”*” version=”*” /&gt; ，可订阅服务的所有分组和所有版本的提供者 zk安装apt-get install --reinstall systemd -y apt-get install -y docker.io systemctl start docker docker create --name zk -p 2181:2181 zookeeper:3.5 docker start zk 可以通过ZooInspector连接查看 yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum makecache fast yum list docker-ce --showduplicates | sort -r yum install -y docker-ce-18.09.5 systemctl restart docker docker create --name zk -p 2181:2181 zookeeper:3.5 docker start zk ZooInspector执行ZooInspector\build\start.bat查看zk信息 服务提供方dubbo/pom.xml &lt;!--添加SpringBoot parent支持--&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.0.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;!--添加SpringBoot测试--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--添加dubbo的springboot依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;0.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--添加dubbo依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;dubbo&lt;/artifactId&gt; &lt;version&gt;2.6.4&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;!--添加springboot的maven插件--&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; dubbo/dubbo-service/pom.xml &lt;dependencies&gt; &lt;!--添加springboot依赖，非web项目--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.13&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.sgroschupf&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; com/onejane/dubbo/pojo/User.java // 使用dubbo要求传输的对象必须实现序列化接口 public class User implements java.io.Serializable { private static final long serialVersionUID = -7341603933521593227L; private Long id; private String username; private String password; private Integer age; } com/onejane/dubbo/service/UserService.java public interface UserService { List&lt;User&gt; queryAll(); } com/onejane/dubbo/service/impl/UserServiceImpl.java @Service(version = &quot;${dubbo.service.version}&quot;) //声明这是一个dubbo服务 public class UserServiceImpl implements UserService { public List&lt;User&gt; queryAll() { List&lt;User&gt; list = new ArrayList&lt;User&gt;(); for (int i = 0; i &lt; 10; i++) { User user = new User(); user.setAge(10 + i); user.setId(Long.valueOf(i + 1)); user.setPassword(&quot;123456&quot;); user.setUsername(&quot;username_&quot; + i); list.add(user); } System.out.println(&quot;---------Service 3------------&quot;); return list; } } application.properties # Spring boot application spring.application.name = dubbo-service server.port = 9090 # Service version dubbo.service.version = 1.0.0 # 服务的扫描包 dubbo.scan.basePackages =com.onejane.dubbo.service # 应用名称 dubbo.application.name = dubbo-provider-demo # 协议以及端口 dubbo.protocol.name = dubbo dubbo.protocol.port = 20882 # zk注册中心 dubbo.registry.address = zookeeper://192.168.3.237:2181 dubbo.registry.client = zkclient com/onejane/dubbo/DubboProvider.java @SpringBootApplication public class DubboProvider { public static void main(String[] args) { new SpringApplicationBuilder(DubboProvider.class) .web(WebApplicationType.NONE) // 非 Web 应用 .run(args); } } 服务消费方dubbo/dubbo-comsumer/pom.xml &lt;dependencies&gt; &lt;!--添加springboot依赖，非web项目--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.13&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.sgroschupf&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.1&lt;/version&gt; &lt;/dependency&gt; &lt;!--引入service的依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;com.onejane&lt;/groupId&gt; &lt;artifactId&gt;dubbo-service&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; application.properties # Spring boot application spring.application.name = dubbo-consumer server.port = 9091 # 应用名称 dubbo.application.name = dubbo-consumer-demo # zk注册中心 dubbo.registry.address = zookeeper://192.168.3.237:2181 dubbo.registry.client = zkclient com/onejane/dubbo/UserServiceTest.java @RunWith(SpringRunner.class) @SpringBootTest public class UserServiceTest { // 负载均衡策略 默认随机 测试时启动dubbo.protocol.port多个不同端口的userService服务，并修改打印值进行区分 // loadbalance = &quot;roundrobin&quot;设置负载均衡策略 @Reference(version = &quot;1.0.0&quot;, loadbalance = &quot;roundrobin&quot;) private UserService userService; @Test public void testQueryAll() { for (int i = 0; i &lt; 100; i++) { System.out.println(&quot;开始调用远程服务 &gt;&gt;&gt;&gt;&gt;&quot; + i); List&lt;User&gt; users = this.userService.queryAll(); for (User user : users) { System.out.println(user); } try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } } } }启动服务后，即可测试 Dubbo Admintar zxf incubator-dubbo-ops.tar.gz -C /usr/local/ tar zxf apache-maven-3.6.0-bin.tar.gz -C /usr/local/ vim incubator-dubbo-ops/dubbo-admin-backend/src/main/resources/application.properties dubbo.registry.address=zookeeper://192.168.3.237:2181 vim /etc/profile 如误操作导致基础命令丢失，export PATH=/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin export MAVEN_HOME=/usr/local/apache-maven-3.6.0 export JAVA_HOME=/usr/local/jdk export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$MAVEN_HOME/bin:$PATH source /etc/profile cd /usr/local/incubator-dubbo-ops &amp;&amp; mvn clean install vim dubbo-admin-backend/src/main/resources/application.properties server.port=8888 mvn --projects dubbo-admin-backend spring-boot:run http://192.168.3.237:8888Dubbo 缺省协议不适合传送大数据量的服务，比如传文件，传视频等，除非请求量很低。Dubbo 缺省协议dubbo:// 协议采用单一长连接和 NIO 异步通讯，适合于小数据量大并发的服务调用，以及服务消费者机器数远大于服务提供者机器数的情况。 Transporter （传输）: mina, netty, grizzySerialization（序列化）: dubbo, hessian2, java, jsonDispatcher（分发调度）: all, direct, message, execution, connectionThreadPool（线程池）: fixed, cached 连接个数：单连接 连接方式：长连接 传输协议：TCP 传输方式：NIO 异步传输 序列化：Hessian 二进制序列化 适用范围：传入传出参数数据包较小（建议小于100K），消费者比提供者个数多，单一消费者无法压满提供 者，尽量不要用 dubbo 协议传输大文件或超大字符串。 适用场景：常规远程服务方法调用]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>ant design pro</tag>
        <tag>dubbo</tag>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[好客租房react]]></title>
    <url>%2F2019%2F08%2F21%2F%E5%A5%BD%E5%AE%A2%E7%A7%9F%E6%88%BFreact%2F</url>
    <content type="text"><![CDATA[基于spring cloud alibaba实战整合开发 React入门mock dvareact/config/config.js export default { plugins:[ [&#39;umi-plugin-react&#39;,{ dva: true, //开启dva进行数据分层管理 }] ] }; react/mock/MockListData.js export default { &#39;get /ds/list&#39;: function (req, res) { //模拟请求返回数据 res.json({ data: [1, 2, 3, 4, 5], maxNum: 5 }); } } react/src/util/request.js // import fetch from &#39;dva/fetch&#39;; function checkStatus(response) { if (response.status &gt;= 200 &amp;&amp; response.status &lt; 300) { return response; } const error = new Error(response.statusText); error.response = response; throw error; } /** * Requests a URL, returning a promise. * * @param {string} url The URL we want to request * @param {object} [options] The options we want to pass to &quot;fetch&quot; * @return {object} An object containing either &quot;data&quot; or &quot;err&quot; */ export default async function request(url, options) { const response = await fetch(url, options); checkStatus(response); return await response.json(); } react/src/models/ListData.js import request from &#39;../util/request&#39;; export default { namespace: &#39;list&#39;, state: { data: [], maxNum: 1 }, reducers : { // 定义的一些函数 addNewData : function (state, result) { // state：指的是更新之前的状态数据, result: 请求到的数据 if(result.data){ //如果state中存在data数据，直接返回，在做初始化的操作 return result.data; } let maxNum = state.maxNum + 1; let newArr = [...state.data, maxNum]; return { data : newArr, maxNum : maxNum } //通过return 返回更新后的数据 } }, effects: { //新增effects配置，用于异步加载数据 *initData(params, sagaEffects) { //定义异步方法 const {call, put} = sagaEffects; //获取到call、put方法 const url = &quot;/ds/list&quot;; // 定义请求的url let data = yield call(request, url); //执行请求 yield put({ // 调用reducers中的方法 type : &quot;addNewData&quot;, //指定方法名 data : data //传递ajax回来的数据 }); } } } react/src/pages/List.js import React from &#39;react&#39;; import { connect } from &#39;dva&#39;; const namespace = &quot;list&quot;; // 说明：第一个回调函数，作用：将page层和model层进行链接，返回modle中的数据,并且将返回的数据，绑定到this.props // 接收第二个函数，这个函数的作用：将定义的函数绑定到this.props中，调用model层中定义的函数 @connect((state) =&gt; { return { dataList : state[namespace].data, maxNum : state[namespace].maxNum } }, (dispatch) =&gt; { // dispatch的作用：可以调用model层定义的函数 return { // 将返回的函数，绑定到this.props中 add : function () { dispatch({ //通过dispatch调用modle中定义的函数,通过type属性，指定函数命名，格式：namespace/函数名 type : namespace + &quot;/addNewData&quot; }); }, init : () =&gt; { dispatch({ //通过dispatch调用modle中定义的函数,通过type属性，指定函数命名，格式：namespace/函数名 type : namespace + &quot;/initData&quot; }); } } }) class List extends React.Component{ componentDidMount(){ //初始化的操作 this.props.init(); } render(){ return ( &lt;div&gt; &lt;ul&gt; { this.props.dataList.map((value,index)=&gt;{ return &lt;li key={index}&gt;{value}&lt;/li&gt; }) } &lt;/ul&gt; &lt;button onClick={() =&gt; { this.props.add(); }}&gt;点我&lt;/button&gt; &lt;/div&gt; ); } } export default List; umi dev Ant Design 入门react/config/config.js export default { plugins:[ [&#39;umi-plugin-react&#39;,{ dva: true, //开启dva进行数据分层管理 antd: true // 开启Ant Design功能 }] ], routes: [{ path: &#39;/&#39;, component: &#39;../layouts&#39;, //配置布局路由 routes: [ { path: &#39;/&#39;, component: &#39;./index&#39; }, { path: &#39;/myTabs&#39;, component: &#39;./myTabs&#39; }, { path: &#39;/user&#39;, routes: [ { path: &#39;/user/list&#39;, component: &#39;./user/UserList&#39; }, { path: &#39;/user/add&#39;, component: &#39;./user/UserAdd&#39; } ] } ] }] }; react/mock/MockListData.js &#39;get /ds/user/list&#39;: function (req, res) { res.json([{ key: &#39;1&#39;, name: &#39;张三1&#39;, age: 32, address: &#39;上海市&#39;, tags: [&#39;程序员&#39;, &#39;帅气&#39;], }, { key: &#39;2&#39;, name: &#39;李四2&#39;, age: 42, address: &#39;北京市&#39;, tags: [&#39;屌丝&#39;], }, { key: &#39;3&#39;, name: &#39;王五3&#39;, age: 32, address: &#39;杭州市&#39;, tags: [&#39;高富帅&#39;, &#39;富二代&#39;], }]); react/src/models/UserListData.js import request from &quot;../util/request&quot;; export default { namespace: &#39;userList&#39;, state: { list: [] }, effects: { *initData(params, sagaEffects) { const {call, put} = sagaEffects; const url = &quot;/ds/user/list&quot;; let data = yield call(request, url); yield put({ type : &quot;queryList&quot;, data : data }); } }, reducers: { queryList(state, result) { let data = [...result.data]; return { //更新状态值 list: data } } } } react/src/layouts/index.js import React from &#39;react&#39;; import { Layout, Menu, Icon } from &#39;antd&#39;; import Link from &#39;umi/link&#39;; const { Header, Footer, Sider, Content } = Layout; const SubMenu = Menu.SubMenu; // layouts/index.js文件将被作为全 局的布局文件。 class BasicLayout extends React.Component{ constructor(props){ super(props); this.state = { collapsed: true, } } render(){ return ( &lt;Layout&gt; &lt;Sider width={256} style={{minHeight: '100vh', color: 'white'}}&gt; &lt;div style={{ height: '32px', background: 'rgba(255,255,255,.2)', margin: '16px'}}/&gt; &lt;Menu defaultSelectedKeys={[&#39;1&#39;]} defaultOpenKeys={[&#39;sub1&#39;]} mode=&quot;inline&quot; theme=&quot;dark&quot; inlineCollapsed={this.state.collapsed} &gt; &lt;SubMenu key=&quot;sub1&quot; title={&lt;span&gt;&lt;Icon type=&quot;user&quot;/&gt;&lt;span&gt;用户管理&lt;/span&gt;&lt;/span&gt;}&gt; &lt;Menu.Item key=&quot;1&quot;&gt;&lt;Link to=&quot;/user/add&quot;&gt;新增用户&lt;/Link&gt;&lt;/Menu.Item&gt; &lt;Menu.Item key=&quot;2&quot;&gt;&lt;Link to=&quot;/user/list&quot;&gt;新增列表&lt;/Link&gt;&lt;/Menu.Item&gt; &lt;/SubMenu&gt; &lt;/Menu&gt; &lt;/Sider&gt; &lt;Layout&gt; &lt;Header style={{ background: '#fff', textAlign: 'center', padding: 0 }}&gt;Header&lt;/Header&gt; &lt;Content style={{ margin: '24px 16px 0' }}&gt; &lt;div style={{ padding: 24, background: '#fff', minHeight: 360 }}&gt; { this.props.children } &lt;/div&gt; &lt;/Content&gt; &lt;Footer style={{ textAlign: 'center' }}&gt;后台系统&lt;/Footer&gt; &lt;/Layout&gt; &lt;/Layout&gt; ) } } export default BasicLayout; react/src/pages/MyTabs.js import React from &#39;react&#39;; import { Tabs } from &#39;antd&#39;; // 第一步，导入需要使用的组件 const TabPane = Tabs.TabPane; function callback(key) { console.log(key); } class MyTabs extends React.Component{ render(){ return ( &lt;Tabs defaultActiveKey=&quot;1&quot; onChange={callback}&gt; &lt;TabPane tab=&quot;Tab 1&quot; key=&quot;1&quot;&gt;hello antd wo de 第一个 tabs&lt;/TabPane&gt; &lt;TabPane tab=&quot;Tab 2&quot; key=&quot;2&quot;&gt;Content of Tab Pane 2&lt;/TabPane&gt; &lt;TabPane tab=&quot;Tab 3&quot; key=&quot;3&quot;&gt;Content of Tab Pane 3&lt;/TabPane&gt; &lt;/Tabs&gt; ) } } export default MyTabs;react/src/pages/user/UserList.js import React from &#39;react&#39;; import { connect } from &#39;dva&#39;; import {Table, Divider, Tag, Pagination } from &#39;antd&#39;; const {Column} = Table; const namespace = &#39;userList&#39;; @connect((state)=&gt;{ return { data : state[namespace].list } }, (dispatch) =&gt; { return { initData : () =&gt; { dispatch({ type: namespace + &quot;/initData&quot; }); } } }) class UserList extends React.Component { componentDidMount(){ this.props.initData(); } render() { return ( &lt;div&gt; &lt;Table dataSource={this.props.data} pagination={{position:"bottom",total:500,pageSize:10, defaultCurrent:3}}&gt; &lt;Column title=&quot;姓名&quot; dataIndex=&quot;name&quot; key=&quot;name&quot; /&gt; &lt;Column title=&quot;年龄&quot; dataIndex=&quot;age&quot; key=&quot;age&quot; /&gt; &lt;Column title=&quot;地址&quot; dataIndex=&quot;address&quot; key=&quot;address&quot; /&gt; &lt;Column title=&quot;标签&quot; dataIndex=&quot;tags&quot; key=&quot;tags&quot; render={tags =&gt; ( &lt;span&gt; {tags.map(tag =&gt; &lt;Tag color=&quot;blue&quot; key={tag}&gt;{tag}&lt;/Tag&gt;)} &lt;/span&gt; )} /&gt; &lt;Column title=&quot;操作&quot; key=&quot;action&quot; render={(text, record) =&gt; ( &lt;span&gt; &lt;a href=&quot;javascript:;&quot;&gt;编辑&lt;/a&gt; &lt;Divider type=&quot;vertical&quot;/&gt; &lt;a href=&quot;javascript:;&quot;&gt;删除&lt;/a&gt; &lt;/span&gt; )} /&gt; &lt;/Table&gt; &lt;/div&gt; ); } } export default UserList; react/src/pages/user/UserAdd.js import React from &#39;react&#39; class UserAdd extends React.Component{ render(){ return ( &lt;div&gt;新增用户&lt;/div&gt; ); } } export default UserAdd; react/src/pages/index.js import React from &#39;react&#39; class Index extends React.Component { render(){ return &lt;div&gt;首页&lt;/div&gt; } } export default Index; // http://localhost:8000/Ant Design Pro 入门https://github.com/ant-design/ant-design-pro├── config # umi 配置，包含路由，构建等配置├── mock # 本地模拟数据├── public│ └── favicon.png # Favicon├── src│ ├── assets # 本地静态资源│ ├── components # 业务通用组件│ ├── e2e # 集成测试用例│ ├── layouts # 通用布局│ ├── models # 全局 dva model│ ├── pages # 业务页面入口和常用模板│ ├── services # 后台接口服务│ ├── utils # 工具库│ ├── locales # 国际化资源│ ├── global.less # 全局样式│ └── global.js # 全局 JS├── tests # 测试工具├── README.md└── package.json tyarn install #安装相关依赖 tyarn start #启动服务 http://localhost:8000/dashboard/analysis 测试新增路由config/router.config.js 默认配置两套路由 { path: &#39;/new&#39;, name: &#39;new&#39;, icon: &#39;user&#39;, routes: [ { path: &#39;/new/analysis&#39;, name: &#39;analysis&#39;, component: &#39;./New/NewAnalysis&#39;, }, { path: &#39;/new/monitor&#39;, name: &#39;monitor&#39;, component: &#39;./Dashboard/Monitor&#39;, }, { path: &#39;/new/workplace&#39;, name: &#39;workplace&#39;, component: &#39;./Dashboard/Workplace&#39;, }, ], }, src/pages/New/NewAnalysis.js import React from &#39;react&#39; class NewAnalysis extends React.Component { render() { return (&lt;div&gt;NewAnalysis&lt;/div&gt;); } } export default NewAnalysis; src/locales/zh-CN.js &#39;menu.new&#39;: &#39;New Dashboard&#39;, &#39;menu.new.analysis&#39;: &#39;New 分析页&#39;, &#39;menu.new.monitor&#39;: &#39;New 监控页&#39;, &#39;menu.new.workplace&#39;: &#39;New 工作台&#39;, model执行流程http://localhost:8000/list/table-listsrc/pages/List/TableList.js Table组件生成表格，数据源是data &lt;StandardTable selectedRows={selectedRows} loading={loading} data={data} columns={this.columns} onSelectRow={this.handleSelectRows} onChange={this.handleStandardTableChange} /&gt; data数据从构造方法的props中获取 const { rule: { data }, loading, } = this.props; rule数据由@connect装饰器获取，{ rule, loading }是解构表达式，props从connect中获取数据 @connect(({ rule, loading }) =&gt; ({ rule, loading: loading.models.rule, })) src/pages/List/models/rule.js 生成数据rule reducers: { save(state, action) { return { ...state, data: action.payload, }; }, src/pages/List/TableList.js 组件加载完成后加载数据 componentDidMount() { const { dispatch } = this.props; dispatch({ type: &#39;rule/fetch&#39;, }); } src/pages/List/models/rule.js 从rule.js中reducers加载save方法数据 *fetch({ payload }, { call, put }) { const response = yield call(queryRule, payload); yield put({ type: &#39;save&#39;, payload: response, }); }, queryRule是在/services/api中进行了定义 export async function queryRule(params) { return request(`/api/rule?${stringify(params)}`); } 数据的mock在mock/rule.js中完成 export default { &#39;GET /api/rule&#39;: getRule, &#39;POST /api/rule&#39;: postRule, }; git commit -m “ant-design-pro 下载” –no-verify]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>react</tag>
        <tag>ant design</tag>
        <tag>ant design pro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[好客租房es6语法]]></title>
    <url>%2F2019%2F08%2F21%2F%E5%A5%BD%E5%AE%A2%E7%A7%9F%E6%88%BFes6%2F</url>
    <content type="text"><![CDATA[基于dubbo react elk实战整合开发 基础语法&lt;!DOCTYPE html&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;script&gt; // var 定义全局变量 for (var i = 0; i &lt; 5; i++) { console.log(i); } console.log(&quot;循环外：&quot; + i); // let 控制变量作用域 for (let j = 0; j &lt; 5; j++) { console.log(j); } console.log(&quot;循环外：&quot; + j); // const声明常量 不可修改 const a = 1; console.log(&quot;a=&quot;, a); a = 2; console.log(&quot;a=&quot;, a) // 字符串拓展函数 console.log(&quot;hello world&quot;.includes(&quot;hello&quot;)); console.log(&quot;hello world&quot;.startsWith(&quot;hello&quot;)); console.log(&quot;hello world&quot;.endsWith(&quot;world&quot;)); // 字符串模板保留换行源格式 let str = ` hello world `; console.log(str); // 新定义数组解构获取顺序获取值 let arr = [1, 2, 3] const [x, y, z] = arr; console.log(x, y, z) const [m] = arr; console.log(m) // 新定义对象结构顺序获取属性值 const person = { name: &#39;wj&#39;, age: 21, language: [&#39;java&#39;, &#39;js&#39;, &#39;php&#39;] } const {name, age, language} = person console.log(name, age, language) const {name: n, age: a, language: l} = person; console.log(n) // 函数默认值 function add(a, b = 1) { // b = b || 1; return a + b; } console.log(add(10)) // 箭头函数 var print = obj =&gt; (console.log(obj)) //一个参数 var sum = (a, b) =&gt; a + b // 多个参数 var sayHello1 = () =&gt; console.log(&quot;hello&quot;) // 没有参数 var sayHello2 = (a, b) =&gt; { // 多个函数 console.log(&quot;hello&quot;) console.log(&quot;world&quot;) return a + b; } // 对象函数属性简写 let persons = { name: &quot;jack&quot;, eat: food =&gt; console.log(persons.name + &quot;再吃&quot; + food), // 这里拿不到this eat1(food) { console.log(this.name + &quot;再吃&quot; + food) }, eat2(food) { console.log(this.name + &quot;再吃&quot; + food) } }; persons.eat(&quot;西瓜&quot;) // 箭头函数结合解构表达式 var hi = ({name}) =&gt; console.log(&quot;hello,&quot; + name) hi(persons) // map reduce let array = [&#39;1&#39;, &#39;2&#39;].map(s =&gt; parseInt(s)); //将原数组所有元素用函数处理哇年后放入新数组返回 let array1 = arr.map(function (s) { return parseInt(s); }) console.log(array) let num = [1, 20, 6, 5]; console.log(num.reduce((a, b) =&gt; a + b)) // 从左到友依次用reduce处理，并把结果作为下次reduce的第一个参数 let result = arr.reduce((a, b) =&gt; { return a + b; }, 1); //接受函数必须，初始值可选 // 扩展运算符 console.log(...[2, 3], ...[1, 20, 6, 5], 0) // 数组合并 let add1 = (x, y) =&gt; x + y; console.log(add1(...[1, 2])) const [f, ...l] = [1, 2, 3, 4, 5] //结合结构表达式 console.log(f, l) console.log(...&#39;heool&#39;) //字符串转数组 // promise 异步执行 const p = new Promise((resolve, reject) =&gt; { setTimeout(() =&gt; { const num = Math.random(); // 随机返回成功或失败 if (num &lt; 0.5) { resolve(&quot;成功 num=&quot; + num) } else { reject(&quot;失败 num=&quot; + num) } }, 300) }) p.then(function (msg) { console.log(msg) }).catch(function (msg) { console.log(msg) }) // set map let set = new Set() set.add(1) // clear delete has forEach(function(){}) size set.forEach(value =&gt; console.log(value)) let set2 = new Set([1, 2, 2, 1, 1]) // map为&lt;object,object&gt; const map = new Map([ [&#39;key1&#39;, &#39;value1&#39;], [&#39;value2&#39;, &#39;value2&#39;] ]) const set3 = new Set([ [&#39;t&#39;, &#39;t&#39;], [&#39;h&#39;, &#39;h&#39;] ]) const map2 = new Map(set3) const map3 = new Map(map) map.set(&#39;z&#39;, &#39;z&#39;) // clear delete(key) has(key) forEach(function(value,key){}) size values keys entries for (let key of map.keys()) { console.log(key) } console.log(...map.values()) // 类的基本用法 class User { constructor(name, age = 20) { this.name = name; this.age = age; } sayHi() { return &quot;hi&quot; } static isAdult(age) { if (age &gt;= 18) { return &quot;成年人&quot; } return &quot;未成年人&quot; } } class zhangsan extends User { // 类的继承 constructor() { super(&quot;张三&quot;, 10) this.address = &quot;上海&quot; } test(){ return &quot;name=&quot;+this.name; } } let user = new User(&quot;张三&quot;) let zs = new zhangsan() console.log(user) console.log(user.sayHi()) console.log(User.isAdult(20)) console.log(zs.name, zs.address) console.log(zs.sayHi()) // Generator函数 function* hello() { yield &quot;h&quot; yield &quot;e&quot; return &quot;a&quot; } let h = hello(); for (let obj of h) { //循环遍历或next遍历 console.log(&quot;===&quot; + obj) } console.log(h.next()) console.log(h.next()) console.log(h.next()) // 修饰器 修改类的行为 @T class Animal { constructor(name,age=20){ this.name=name; this.age=age; } } function T(target){ console.log(target); target.contry = &quot;china&quot; //通过修饰器添加的属性是静态属性 } console.log(Animal.contry) // 无法运行，需要转码：将ES6活ES2017转为ES5使用（将箭头函数转为普通函数） &lt;/script&gt; &lt;/html&gt;umi转码 node -v v8.12.0 npm i yarn tyarn -g tyarn使用淘宝源 tyarn -v 1.16.0 若报错通过yarn global bin获取路径加入Path tyarn global add umi umi tyarn init -y 多一个package.json umi g page index 生成page文件夹 编辑index.js index.js // 修饰器 function T(target) { console.log(target); target.country=&quot;中国&quot; } @T class People{ constructor(name,age=20){ this.name=name; this.age=age; } } console.log(People.country); import Util from &#39;./Util&#39;; console.log(Util.sum(10, 5)); Util.js class Util { static sum = (a,b) =&gt; { return a + b; } } export default Util; umi dev,通过http://localhost:8000/ 查看控制台 reactjs tyarn init -y tyarn add umi –dev tyarn add umi-plugin-react –dev umi jsxreact/config/config.js export default {}; react/src/pages/HelloWorld.js export default ()=&gt;{ const t=()=&gt;&quot;pig&quot; return ( &lt;div&gt;hello world {t()}&lt;/div&gt; ); } umi build 转码生成文件 dist\umi.jsumi dev 访问http://localhost:8000 todolistreact/src/pages/HelloWorld.js import React from &#39;react&#39; class HelloWorld extends React.Component{ render() { // this.props.name接受属性，this.props.children接受标签内容 return &lt;div&gt;hello name={this.props.name},say={this.props.children}&lt;/div&gt; } } export default HelloWorld; react/src/pages/Show.js import React from &#39;react&#39; import HelloWorld from &#39;./HelloWorld&#39; class Show extends React.Component{ render() { return &lt;HelloWorld name=&quot;zhansan&quot;&gt;haha&lt;/HelloWorld&gt; } } export default Show; react/src/pages/List.js import React from &#39;react&#39;; class List extends React.Component{ constructor(props){ super(props); this.state = { dataList : [1,2,3], maxNum : 3 }; } /*this.state值在构造参数中完成，要修改this.state的值，需要调用this.setState()完成*/ render(){ return ( &lt;div&gt; &lt;ul&gt; { this.state.dataList.map((value,index)=&gt;{ return &lt;li key={index}&gt;{value}&lt;/li&gt; }) } &lt;/ul&gt; &lt;button onClick={() =&gt; { let maxNum = this.state.maxNum + 1; let list = [...this.state.dataList,maxNum]; this.setState({ dataList: list, maxNum: maxNum }); }}&gt;点我&lt;/button&gt; &lt;/div&gt; ); } } export default List; LifeCycleimport React from &#39;react&#39;; //第一步，导入React class LifeCycle extends React.Component { constructor(props) { super(props); //构造方法 console.log(&quot;constructor()&quot;); } componentDidMount() { //组件挂载后调用 console.log(&quot;componentDidMount()&quot;); } componentWillUnmount() { //在组件从 DOM 中移除之前立刻被调用。 console.log(&quot;componentWillUnmount()&quot;); } componentDidUpdate() { //在组件完成更新后立即调用。在初始化时不会被调用。 console.log(&quot;componentDidUpdate()&quot;); } shouldComponentUpdate(nextProps, nextState){ // 每当this.props或this.state有变化，在render方法执行之前，就会调用这个方法。 // 该方法返回一个布尔值，表示是否应该继续执行render方法，即如果返回false，UI 就不会更新，默认返回true。 // 组件挂载时，render方法的第一次执行，不会调用这个方法。 console.log(&quot;shouldComponentUpdate()&quot;); } render() { return ( &lt;div&gt; &lt;h1&gt;React Life Cycle!&lt;/h1&gt; &lt;/div&gt; ); } } export default LifeCycle;]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>react</tag>
        <tag>umi</tag>
        <tag>es6</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Alibaba网关服务]]></title>
    <url>%2F2019%2F08%2F21%2FAlibaba%E7%BD%91%E5%85%B3%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[基于spring cloud alibaba实战整合开发 ht-micro-record-service-gateway&lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-service-gateway&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-service-gateway&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt; &lt;artifactId&gt;jsr305&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt; &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;!-- Commons Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Commons Begin --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;!-- docker的maven插件，官网 https://github.com/spotify/docker-maven-plugin --&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt; &lt;version&gt;0.4.13&lt;/version&gt; &lt;configuration&gt; &lt;imageName&gt;192.168.2.7:5000/${project.artifactId}:${project.version}&lt;/imageName&gt; &lt;baseImage&gt;onejane-jdk1.8 &lt;/baseImage&gt; &lt;entryPoint&gt;[&quot;java&quot;, &quot;-jar&quot;,&quot;/${project.build.finalName}.jar&quot;]&lt;/entryPoint&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/&lt;/targetPath&gt; &lt;directory&gt;${project.build.directory}&lt;/directory&gt; &lt;include&gt;${project.build.finalName}.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;dockerHost&gt;http://192.168.2.7:2375&lt;/dockerHost&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;com.ht.micro.record.service.gateway.GatewayServiceApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; bootstrap.properties spring.application.name=ht-micro-record-service-gateway-config spring.main.allow-bean-definition-overriding=true spring.cloud.nacos.config.file-extension=yaml spring.cloud.nacos.config.server-addr=192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 nacos配置 spring: application: name: ht-micro-record-service-gateway jackson: date-format: yyyy-MM-dd HH:mm:ss time-zone: GMT+8 default-property-inclusion: non_null cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8721 dashboard: 192.168.2.7:190 gateway: discovery: locator: enabled: true routes: http://localhost:9000/api/user/user/20 访问user服务 - id: HT-MICRO-RECORD-SERVICE-USER uri: lb://ht-micro-record-service-user predicates: - Path=/api/user/** filters: - StripPrefix=2 server: port: 9000 feign: sentinel: enabled: true management: endpoints: web: exposure: include: &quot;*&quot; logging: level: org.springframework.cloud.gateway: debug GatewayServiceApplication.java @SpringBootApplication @EnableDiscoveryClient @EnableFeignClients public class GatewayServiceApplication { // ----------------------------- 解决跨域 Begin ----------------------------- private static final String ALL = &quot;*&quot;; private static final String MAX_AGE = &quot;18000L&quot;; @Bean public RouteDefinitionLocator discoveryClientRouteDefinitionLocator(DiscoveryClient discoveryClient, DiscoveryLocatorProperties properties) { return new DiscoveryClientRouteDefinitionLocator(discoveryClient, properties); } @Bean public ServerCodecConfigurer serverCodecConfigurer() { return new DefaultServerCodecConfigurer(); } @Bean public WebFilter corsFilter() { return (ServerWebExchange ctx, WebFilterChain chain) -&gt; { ServerHttpRequest request = ctx.getRequest(); if (!CorsUtils.isCorsRequest(request)) { return chain.filter(ctx); } HttpHeaders requestHeaders = request.getHeaders(); ServerHttpResponse response = ctx.getResponse(); HttpMethod requestMethod = requestHeaders.getAccessControlRequestMethod(); HttpHeaders headers = response.getHeaders(); headers.add(HttpHeaders.ACCESS_CONTROL_ALLOW_ORIGIN, requestHeaders.getOrigin()); headers.addAll(HttpHeaders.ACCESS_CONTROL_ALLOW_HEADERS, requestHeaders.getAccessControlRequestHeaders()); if (requestMethod != null) { headers.add(HttpHeaders.ACCESS_CONTROL_ALLOW_METHODS, requestMethod.name()); } headers.add(HttpHeaders.ACCESS_CONTROL_ALLOW_CREDENTIALS, &quot;true&quot;); headers.add(HttpHeaders.ACCESS_CONTROL_EXPOSE_HEADERS, ALL); headers.add(HttpHeaders.ACCESS_CONTROL_MAX_AGE, MAX_AGE); if (request.getMethod() == HttpMethod.OPTIONS) { response.setStatusCode(HttpStatus.OK); return Mono.empty(); } return chain.filter(ctx); }; } // ----------------------------- 解决跨域 End ----------------------------- public static void main(String[] args) { SpringApplication.run(GatewayServiceApplication.class, args); } }]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>spring cloud alibaba</tag>
        <tag>gateway</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Alibaba分布式session]]></title>
    <url>%2F2019%2F08%2F21%2FAlibaba%E5%88%86%E5%B8%83%E5%BC%8Fsession%2F</url>
    <content type="text"><![CDATA[基于spring cloud alibaba实战整合开发 com.ht.micro.record.commons.domain.User @Data public class User implements Serializable { private long userId; private String username; private String password; } ht-micro-record-service-smspom.xml &lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-freemarker&lt;/artifactId&gt; &lt;/dependency&gt;SmsServiceApplication.java @EnableRedisHttpSession(maxInactiveIntervalInSeconds= 1800) //开启redis session支持,并配置session过期时间UserController.java @Controller @RequestMapping(value = &quot;user&quot;) public class UserController extends AbstractBaseController&lt;TbUser&gt; { @RequestMapping public String index() { return &quot;index&quot;; } @RequestMapping(&quot;home&quot;) public String home() { return &quot;home&quot;; } @PostMapping(&quot;login&quot;) public String login(User user, HttpSession session) { // 随机生成用户id user.setUserId(Math.round(Math.floor(Math.random() * 10 * 1000))); // 将用户信息保存到id中 session.setAttribute(&quot;USER&quot;, user); return &quot;home&quot;; } @PostMapping(&quot;logout&quot;) public String logout(HttpSession session) { session.removeAttribute(&quot;USER&quot;); session.invalidate(); return &quot;home&quot;; } }templates/home.ftl &lt;!doctype html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;title&gt;主页面&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h5&gt;登录用户: ${Session[&quot;USER&quot;].username} &lt;/h5&gt; &lt;h5&gt;用户编号: ${Session[&quot;USER&quot;].userId} &lt;/h5&gt; &lt;/body&gt; &lt;/html&gt;templates/index.ftl &lt;!doctype html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;title&gt;登录页面&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;form action=&quot;/user/login&quot; method=&quot;post&quot;&gt; 用户：&lt;input type=&quot;text&quot; name=&quot;username&quot;&gt;&lt;br/&gt; 密码：&lt;input type=&quot;password&quot; name=&quot;password&quot;&gt;&lt;br/&gt; &lt;button type=&quot;submit&quot;&gt;登录&lt;/button&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt;ht-micro-record-service-user pom.xml &lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-freemarker&lt;/artifactId&gt; &lt;/dependency&gt; application.yml所有模块加入redis配置 redis: #host: 127.0.0.1 #port: 6379 jedis: pool: # 连接池最大连接数,使用负值表示无限制。 max-active: 8 # 连接池最大阻塞等待时间,使用负值表示无限制。 max-wait: -1s # 连接池最大空闲数,使用负值表示无限制。 max-idle: 8 # 连接池最小空闲连接，只有设置为正值时候才有效 min-idle: 1 timeout: 300ms session: # session 存储方式 支持redis、mongo、jdbc、hazelcast store-type: redis cluster: nodes: 192.168.2.5:8001,192.168.2.5:8002,192.168.2.5:8003,192.168.2.7:8004,192.168.2.7:8005,192.168.2.7:8006 # 如果是集群节点 采用如下配置指定节点 #spring.redis.cluster.nodes templates/index.ftl &lt;!doctype html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;title&gt;登录页面&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;form action=&quot;/user/login&quot; method=&quot;post&quot;&gt; 用户：&lt;input type=&quot;text&quot; name=&quot;username&quot;&gt;&lt;br/&gt; 密码：&lt;input type=&quot;password&quot; name=&quot;password&quot;&gt;&lt;br/&gt; &lt;button type=&quot;submit&quot;&gt;登录&lt;/button&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; templates/home.ftl &lt;!doctype html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;title&gt;主页面&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h5&gt;登录用户: ${Session[&quot;USER&quot;].username} &lt;/h5&gt; &lt;h5&gt;用户编号: ${Session[&quot;USER&quot;].userId} &lt;/h5&gt; &lt;/body&gt; &lt;/html&gt;UserServiceApplication.java @EnableRedisHttpSession(maxInactiveIntervalInSeconds= 1800) //开启redis session支持,并配置session过期时间 UserController.java @Controller @RequestMapping(value = &quot;user&quot;) public class UserController extends AbstractBaseController&lt;TbUser&gt; { @Autowired private TbUserService tbUserService; @Autowired private UserService userService; @RequestMapping public String index() { return &quot;index&quot;; } @RequestMapping(&quot;home&quot;) public String home() { return &quot;home&quot;; } @PostMapping(&quot;login&quot;) public String login(User user, HttpSession session) { // 随机生成用户id user.setUserId(Math.round(Math.floor(Math.random() * 10 * 1000))); // 将用户信息保存到id中 session.setAttribute(&quot;USER&quot;, user); return &quot;home&quot;; } @PostMapping(&quot;logout&quot;) public String logout(HttpSession session) { session.removeAttribute(&quot;USER&quot;); session.invalidate(); return &quot;home&quot;; } } http://localhost:9507/user 登陆后，进入http://localhost:9507/user/login 页面查看用户信息手动进入http://localhost:9506/user/home 查看相同用户信息，User实体位置在两个服务中保持一致。如果项目中有es配置，需要es优先配置Netty @PostConstruct public void init() { /*由于netty的冲突，需要在ElasticConfig中显示指定早于RedisConfig装配，并且指定初始化时再一次添加忽略es中netty的一些配置*/ System.setProperty(&quot;es.set.netty.runtime.available.processors&quot;, &quot;false&quot;); bulkRequest = elasticClientSingleton.getBulkRequest(esConfig); }]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>session</tag>
        <tag>redis</tag>
        <tag>freemarker</tag>
        <tag>spring cloud alibaba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Alibaba整合xxl-job]]></title>
    <url>%2F2019%2F08%2F21%2FAlibaba%E5%AE%9A%E6%97%B6%E5%99%A8%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[基于spring cloud alibaba实战整合开发 ht-micro-record-service-job &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-service-job&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-service-job&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.xuxueli&lt;/groupId&gt; &lt;artifactId&gt;xxl-job-core&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt; &lt;artifactId&gt;jsr305&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt; &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rocketmq&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;!-- Projects Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-commons-service&lt;/artifactId&gt; &lt;version&gt;${project.parent.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Projects End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;ht.micro.record.service.job.JobServiceApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; bootstrap.properties spring.application.name=ht-micro-record-service-xxl-job-config spring.cloud.nacos.config.file-extension=yaml spring.cloud.nacos.config.server-addr=192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 application.yaml spring: datasource: druid: url: jdbc:mysql://192.168.2.7:185/ht_micro_record?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false username: root password: 123456 initial-size: 1 min-idle: 1 max-active: 20 test-on-borrow: true driver-class-name: com.mysql.jdbc.Driver cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 config: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 server: port: 9701 xxl: job: executor: logpath: logs/xxl-job/jobhandler appname: xxl-job-executor port: 1234 logretentiondays: -1 ip: 192.168.3.233 admin: addresses: http://192.168.2.7:183/xxl-job-admin accessToken:nacos配置 spring: application: name: ht-micro-record-service-xxl-job cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8719 dashboard: 192.168.2.7:190 server: port: 9701 management: endpoints: web: exposure: include: &quot;*&quot; com/ht/micro/record/service/job/JobServiceApplication.java @SpringBootApplication(scanBasePackages = &quot;com.ht.micro.record&quot;) @EnableDiscoveryClient @MapperScan(basePackages = &quot;com.ht.micro.record.commons.mapper&quot;) public class JobServiceApplication { public static void main(String[] args) { SpringApplication.run(JobServiceApplication.class, args); } } com/ht/micro/record/service/job/config/XxlJobConfig.java @Configuration @ComponentScan(basePackages = &quot;com.ht.micro.record.service.job.handler&quot;) public class XxlJobConfig { private Logger logger = LoggerFactory.getLogger(XxlJobConfig.class); @Value(&quot;${xxl.job.admin.addresses}&quot;) private String adminAddresses; @Value(&quot;${xxl.job.executor.appname}&quot;) private String appName; @Value(&quot;${xxl.job.executor.ip}&quot;) private String ip; @Value(&quot;${xxl.job.executor.port}&quot;) private int port; @Value(&quot;${xxl.job.accessToken}&quot;) private String accessToken; @Value(&quot;${xxl.job.executor.logpath}&quot;) private String logPath; @Value(&quot;${xxl.job.executor.logretentiondays}&quot;) private int logRetentionDays; @Bean(initMethod = &quot;start&quot;, destroyMethod = &quot;destroy&quot;) public XxlJobSpringExecutor xxlJobExecutor() { logger.info(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; xxl-job config init.&quot;); XxlJobSpringExecutor xxlJobSpringExecutor = new XxlJobSpringExecutor(); xxlJobSpringExecutor.setAdminAddresses(adminAddresses); xxlJobSpringExecutor.setAppName(appName); xxlJobSpringExecutor.setIp(ip); xxlJobSpringExecutor.setPort(port); xxlJobSpringExecutor.setAccessToken(accessToken); xxlJobSpringExecutor.setLogPath(logPath); xxlJobSpringExecutor.setLogRetentionDays(logRetentionDays); return xxlJobSpringExecutor; } } com/ht/micro/record/service/job/handler/TestJobHandler.java @JobHandler(value=&quot;testJobHandler&quot;) @Component public class TestJobHandler extends IJobHandler { @Override public ReturnT&lt;String&gt; execute(String param) throws Exception { XxlJobLogger.log(&quot;XXL-JOB, Hello World.&quot;); for (int i = 0; i &lt; 5; i++) { XxlJobLogger.log(&quot;beat at:&quot; + i); TimeUnit.SECONDS.sleep(2); } return SUCCESS; } } http://192.168.2.7:183/xxl-job-admin]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>spring cloud alibaba</tag>
        <tag>xxl-job</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Alibaba服务间调用的多种方式]]></title>
    <url>%2F2019%2F08%2F21%2FAlibaba%E6%9C%8D%E5%8A%A1%E9%97%B4%E8%B0%83%E7%94%A8%E7%9A%84%E5%A4%9A%E7%A7%8D%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[基于spring cloud alibaba实战整合开发 @[TOC] ht-micro-record-service-dubbopom.xml &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;modules&gt; &lt;module&gt;ht-micro-record-service-dubbo-provider&lt;/module&gt; &lt;module&gt;ht-micro-record-service-dubbo-consumer&lt;/module&gt; &lt;/modules&gt; &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo&lt;/artifactId&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;name&gt;ht-micro-record-service-dubbo&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; ht-micro-record-service-dubbo-apipom.xml &lt;parent&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo&lt;/artifactId&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo-api&lt;/artifactId&gt; PortApi.java public interface PortApi { String showPort(); } ht-micro-record-service-dubbo-providerpom.xml &lt;parent&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo&lt;/artifactId&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo-provider&lt;/artifactId&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo-api&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-dubbo&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--elasticsearch--&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.java.dev.jna&lt;/groupId&gt; &lt;artifactId&gt;jna&lt;/artifactId&gt; &lt;!-- &lt;version&gt;3.0.9&lt;/version&gt; --&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt; &lt;artifactId&gt;gson&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;com.ht.micro.record.service.dubbo.provider.DubboProviderApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; logback.xml &lt;configuration&gt; &lt;!-- 控制台输出 --&gt; &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;layout class=&quot;ch.qos.logback.classic.PatternLayout&quot;&gt; &lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符 --&gt; &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n&lt;/pattern&gt; &lt;/layout&gt; &lt;/appender&gt; &lt;!--定义日志文件的存储地址 勿在 LogBack 的配置中使用相对路径 --&gt; &lt;property name=&quot;LOG_HOME&quot; value=&quot;E:/log&quot; /&gt; &lt;!-- 按照每天生成日志文件 --&gt; &lt;appender name=&quot;FILE&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;!--日志文件输出的文件名 --&gt; &lt;FileNamePattern&gt;${LOG_HOME}/log.%d{yyyy-MM-dd}.log&lt;/FileNamePattern&gt; &lt;MaxHistory&gt;30&lt;/MaxHistory&gt; &lt;/rollingPolicy&gt; &lt;layout class=&quot;ch.qos.logback.classic.PatternLayout&quot;&gt; &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n&lt;/pattern&gt; &lt;/layout&gt; &lt;!--日志文件最大的大小 --&gt; &lt;triggeringPolicy class=&quot;ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy&quot;&gt; &lt;MaxFileSize&gt;10MB&lt;/MaxFileSize&gt; &lt;/triggeringPolicy&gt; &lt;/appender&gt; &lt;!-- 日志输出级别 --&gt; &lt;root level=&quot;INFO&quot;&gt; &lt;appender-ref ref=&quot;STDOUT&quot; /&gt; &lt;appender-ref ref=&quot;FILE&quot; /&gt; &lt;/root&gt; &lt;!--mybatis 日志配置--&gt; &lt;logger name=&quot;com.mapper&quot; level=&quot;DEBUG&quot; /&gt; &lt;/configuration&gt; bootstrap.yml spring: application: name: ht-micro-record-service-dubbo-provider cloud: nacos: config: file-extension: yaml server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 main: allow-bean-definition-overriding: true dubbo: scan: base-packages: com.ht.micro.record.service.dubbo.provider.dubbo protocol: name: dubbo port: -1 registry: address: spring-cloud://192.168.2.7:8848?backup=192.168.2.7:8849,192.168.2.7:8850 es: nodes: 192.168.2.5:1800,192.168.2.7:1800 host: 192.168.2.5,192.168.2.7 port: 1801,1801 types: doc clusterName: elasticsearch-cluster #笔录分析库 blDbName: t_record_analyze #接警信息库 jjDbName: p_answer_alarm #处警信息库 cjDbName: p_handle_alarm #警情分析结果信息库 jqfxDbName: t_alarm_analysis_result #标准化分析 cjjxqDbName: t_cjjxq #接处警整合库 jcjDbName: p_answer_handle_alarm #警情分析 daDbName: t_data_analysis #警情结果表(neo4j使用) neo4jData: t_neo4j_data #市局接处警表 cityDbName: city_answer_handle_alarm nacos配置ht-micro-record-service-dubbo-provider.yaml spring: application: name: ht-micro-record-service-dubbo-provider cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8719 dashboard: 192.168.2.7:190 server: port: 9502com.ht.micro.record.service.dubbo.provider.DubboProviderApplication @SpringBootApplication @EnableDiscoveryClient public class DubboProviderApplication { public static void main(String[] args) { SpringApplication.run(DubboProviderApplication.class, args); } } com.ht.micro.record.service.dubbo.provider.utils.ElasticClient @Service public class ElasticClient { @Autowired private EsConfig esConfig; private static final Logger logger = LoggerFactory.getLogger(ElasticClient.class); private static BulkRequestBuilder bulkRequest; @Autowired private ElasticClientSingleton elasticClientSingleton; @PostConstruct public void init() { bulkRequest = elasticClientSingleton.getBulkRequest(esConfig); } /** * @param url * @param query * @return * @Description 发送请求 * @Author 裴健(peij@htdatacloud.com) * @Date 2016年6月13日 * @History * @his1 */ public static String postRequest(String url, String query) { RestTemplate restTemplate = new RestTemplate(); MediaType type = MediaType.parseMediaType(&quot;application/json; charset=UTF-8&quot;); HttpHeaders headers = new HttpHeaders(); headers.setContentType(type); headers.add(&quot;Accept&quot;, MediaType.APPLICATION_JSON.toString()); HttpEntity&lt;String&gt; formEntity = new HttpEntity&lt;String&gt;(query, headers); String result = restTemplate.postForObject(url, formEntity, String.class); return result; } /** * action 提交操作 */ // public void action() { // int reqSize = bulkRequest.numberOfActions(); // //读不到数据了，默认已经全部读取 // if (reqSize == 0) { // bulkRequest.request().requests().clear(); // } // bulkRequest.setTimeout(new TimeValue(1000 * 60 * 5)); //超时30秒 // BulkResponse bulkResponse = bulkRequest.execute().actionGet(); // //持久化异常 // if (bulkResponse.hasFailures()) { // logger.error(bulkResponse.buildFailureMessage()); // bulkRequest.request().requests().clear(); // } // logger.info(&quot;import over....&quot; + bulkResponse.getItems().length); // } }com.ht.micro.record.service.dubbo.provider.utils.ElasticClientSingleton @Service public class ElasticClientSingleton { protected final Logger logger = LoggerFactory.getLogger(ElasticClientSingleton.class); private AtomicInteger atomicPass = new AtomicInteger(); // 0 未初始化, 1 已初始化 private TransportClient transportClient; private BulkRequestBuilder bulkRequest; public synchronized void init(EsConfig esConfig) { try { String ipArray = esConfig.getHost(); String portArray = esConfig.getPort(); String cluster = esConfig.getClusterName(); Settings settings = Settings.builder() .put(&quot;cluster.name&quot;, cluster) //连接的集群名 .put(&quot;client.transport.ignore_cluster_name&quot;, true) .put(&quot;client.transport.sniff&quot;, false)//如果集群名不对，也能连接 .build(); transportClient = new PreBuiltTransportClient(settings); String[] ips = ipArray.split(&quot;,&quot;); String[] ports = portArray.split(&quot;,&quot;); for (int i = 0; i &lt; ips.length; i++) { transportClient.addTransportAddress(new TransportAddress(InetAddress.getByName(ips[i]), Integer .parseInt(ports[i]))); } atomicPass.set(1); } catch (Exception e) { e.printStackTrace(); logger.error(e.getMessage()); atomicPass.set(0); destroy(); } } public void destroy() { if (transportClient != null) { transportClient.close(); transportClient = null; } } public BulkRequestBuilder getBulkRequest(EsConfig esConfig) { if (atomicPass.get() == 0) { // 初始化 init(esConfig); } bulkRequest = transportClient.prepareBulk(); return bulkRequest; } public TransportClient getTransportClient(EsConfig esConfig) { if (atomicPass.get() == 0) { // 初始化 init(esConfig); } return transportClient; } }com.ht.micro.record.service.dubbo.provider.utils.EsConfig @Service public class EsConfig { @Value(&quot;${es.nodes}&quot;) private String nodes; @Value(&quot;${es.host}&quot;) private String host; @Value(&quot;${es.port}&quot;) private String port; @Value(&quot;${es.blDbName}&quot;) private String blDbName; @Value(&quot;${es.jjDbName}&quot;) private String jjDbName; @Value(&quot;${es.cjDbName}&quot;) private String cjDbName; @Value(&quot;${es.jqfxDbName}&quot;) private String jqfxDbName; @Value(&quot;${es.clusterName}&quot;) private String clusterName; @Value(&quot;${es.jjDbName}&quot;) private String answerDbName; @Value(&quot;${es.cjDbName}&quot;) private String handleDbName; @Value(&quot;${es.cjjxqDbName}&quot;) private String cjjxqDbName; @Value(&quot;${es.jcjDbName}&quot;) private String jcjDbName; @Value(&quot;${es.daDbName}&quot;) private String daDbName; @Value(&quot;${es.daDbName}&quot;) private String fxDbName; @Value(&quot;${es.types}&quot;) private String types; @Value(&quot;${es.neo4jData}&quot;) private String neo4jData; @Value(&quot;${es.cityDbName}&quot;) private String cityDbName; public String getCityDbName() { return cityDbName; } public String getTypes() { return types; } public String getFxDbName() { return fxDbName; } public String getDaDbName() { return daDbName; } public String getJcjDbName() { return jcjDbName; } public String getCjjxqDbName() { return cjjxqDbName; } public String getNodes() { return nodes; } public String getHost() { return host; } public String getPort() { return port; } public String getClusterName() { return clusterName; } public String getBlDbName() { return blDbName; } public String getJjDbName() { return jjDbName; } public String getCjDbName() { return cjDbName; } public String getJqfxDbName() { return jqfxDbName; } public String getNeo4jData() { return neo4jData; } } com.ht.micro.record.service.dubbo.provider.dubbo.PortApiImpl @Service public class PortApiImpl implements PortApi { @Value(&quot;${server.port}&quot;) private Integer port; @Override public String showPort() { return &quot;port= &quot;+ port; } }com.ht.micro.record.service.dubbo.provider.controller.ProviderController @RestController @RequestMapping(&quot;/provider&quot;) public class ProviderController { @Autowired private ConfigurableApplicationContext applicationContext; @Value(&quot;${server.port}&quot;) private Integer port; @GetMapping(&quot;/port&quot;) public Object port() { return &quot;port= &quot;+ port + &quot;, name=&quot; + applicationContext.getEnvironment().getProperty(&quot;user.name&quot;); } }ht-micro-record-service-dubbo-consumerpom.xml &lt;parent&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo&lt;/artifactId&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo-consumer&lt;/artifactId&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo-api&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-dubbo&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt; &lt;artifactId&gt;jsr305&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt; &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;com.ht.micro.record.service.dubbo.consumer.DubboConsumerApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;bootstrap.yml spring: application: name: ht-micro-record-service-dubbo-consumer main: allow-bean-definition-overriding: true cloud: nacos: config: file-extension: yaml server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 # 与nacos兼容性不好，配置到项目中 dubbo: scan: base-packages: com.ht.micro.record.service.dubbo.consumer protocol: name: dubbo port: -1 registry: address: spring-cloud://192.168.2.7:8848?backup=192.168.2.7:8849,192.168.2.7:8850com.ht.micro.record.service.dubbo.consumer.DubboConsumerApplication @SpringBootApplication(scanBasePackages = &quot;com.ht.micro.record&quot;, exclude = {DataSourceAutoConfiguration.class}) @EnableFeignClients @EnableDiscoveryClient public class DubboConsumerApplication { public static void main(String[] args) { SpringApplication.run(DubboConsumerApplication.class, args); } } com.ht.micro.record.service.dubbo.consumer.service.PortService @FeignClient(value = &quot;ht-micro-record-service-dubbo-provider&quot;) public interface PortService { @GetMapping(value = &quot;/provider/port&quot;) String showPort(); }com.ht.micro.record.service.dubbo.consumer.controller.ConsumerController @RestController @RequestMapping(&quot;/consumer&quot;) public class ConsumerController { @Autowired private LoadBalancerClient loadBalancerClient; @Autowired private PortService portService; private RestTemplate restTemplate = new RestTemplate(); @Reference(check = false) private PortApi portApi; @GetMapping(&quot;/rest&quot;) public Object rest() { ServiceInstance serviceInstance = loadBalancerClient.choose(&quot;ht-micro-record-service-dubbo-provider&quot;); String url = String.format(&quot;http://%s:%s/provider/port&quot;, serviceInstance.getHost(), serviceInstance.getPort()); System.out.println(&quot;request url:&quot; + url); return restTemplate.getForObject(url, String.class); } @GetMapping(&quot;/rpc&quot;) public Object rpc() { return portApi.showPort(); } @GetMapping(&quot;/feign&quot;) public Object feign(){ return portService.showPort(); } }]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>spring cloud alibaba</tag>
        <tag>dubbo</tag>
        <tag>nacos</tag>
        <tag>feign</tag>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Alibaba服务搭建]]></title>
    <url>%2F2019%2F08%2F21%2FAlibaba%E6%9C%8D%E5%8A%A1%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[基于spring cloud alibaba实战整合开发 基础服务通用类com.ht.micro.record.commonsdto.AbstractBaseDomain @Data public abstract class AbstractBaseDomain implements Serializable { /** * 该注解需要保留，用于 tk.mybatis 回显 ID */ @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; /** * 格式化日期，由于是北京时间（我们是在东八区），所以时区 +8 */ @JsonFormat(pattern = &quot;yyyy-MM-dd HH:mm:ss&quot;, timezone = &quot;GMT+8&quot;) private Date created; @JsonFormat(pattern = &quot;yyyy-MM-dd HH:mm:ss&quot;, timezone = &quot;GMT+8&quot;) private Date updated; } dto.AbstractBaseResult @Data public abstract class AbstractBaseResult implements Serializable { /** * 此为内部类,JsonInclude.Include.NON_NULL去掉返回值中为null的属性 */ @Data @JsonInclude(JsonInclude.Include.NON_NULL) protected static class Links { private String self; private String next; private String last; } @Data @JsonInclude(JsonInclude.Include.NON_NULL) protected static class DataBean&lt;T extends AbstractBaseDomain&gt; { private String type; private Long id; private T attributes; private T relationships; private Links links; } } dto.BaseResultFactory public class BaseResultFactory&lt;T extends AbstractBaseDomain&gt; { /** * 设置日志级别，用于限制发生错误时，是否显示调试信息(detail) * * @see ErrorResult#detail */ public static final String LOGGER_LEVEL_DEBUG = &quot;DEBUG&quot;; private static BaseResultFactory baseResultFactory; private BaseResultFactory() { } // 设置通用的响应 private static HttpServletResponse response; public static BaseResultFactory getInstance(HttpServletResponse response) { if (baseResultFactory == null) { synchronized (BaseResultFactory.class) { if (baseResultFactory == null) { baseResultFactory = new BaseResultFactory(); } } } BaseResultFactory.response = response; // 设置通用响应 baseResultFactory.initResponse(); return baseResultFactory; } public static BaseResultFactory getInstance() { if (baseResultFactory == null) { synchronized (BaseResultFactory.class) { if (baseResultFactory == null) { baseResultFactory = new BaseResultFactory(); } } } return baseResultFactory; } /** * 构建单笔数据结果集 * * @param self 当前请求路径 * @return */ public AbstractBaseResult build(String self, T attributes) { return new SuccessResult(self, attributes); } /** * 构建多笔数据结果集 * * @param self 当前请求路径 * @param next 下一页的页码 * @param last 最后一页的页码 * @return */ public AbstractBaseResult build(String self, int next, int last, List&lt;T&gt; attributes) { return new SuccessResult(self, next, last, attributes); } /** * 构建请求错误的响应结构，调试显示detail，上线不显示，通过配置日志级别 * * @param code HTTP 状态码 * @param title 错误信息 * @param detail 调试信息 * @param level 日志级别，只有 DEBUG 时才显示详情 * @return */ public AbstractBaseResult build(int code, String title, String detail, String level) { // 设置请求失败的响应码 response.setStatus(code); if (LOGGER_LEVEL_DEBUG.equals(level)) { return new ErrorResult(code, title, detail); } else { return new ErrorResult(code, title, null); } } /** * 初始化 HttpServletResponse */ private void initResponse() { // 需要符合 JSON API 规范 response.setHeader(&quot;Content-Type&quot;, &quot;application/vnd.api+json&quot;); } } dto.ErrorResult @Data @AllArgsConstructor @EqualsAndHashCode(callSuper = false) // JSON 不显示为 null 的属性 @JsonInclude(JsonInclude.Include.NON_NULL) public class ErrorResult extends AbstractBaseResult { private int code; private String title; /** * 调试信息 */ private String detail; } dto.SuccessResult @Data @EqualsAndHashCode(callSuper = false) public class SuccessResult&lt;T extends AbstractBaseDomain&gt; extends AbstractBaseResult { private Links links; private List&lt;DataBean&gt; data; /** * 请求的结果（单笔） * @param self 当前请求路径 * @param attributes 领域模型 */ public SuccessResult(String self, T attributes) { links = new Links(); links.setSelf(self); createDataBean(null, attributes); } /** * 请求的结果（分页） * @param self 当前请求路径 * @param next 下一页的页码 * @param last 最后一页的页码 * @param attributes 领域模型集合 */ public SuccessResult(String self, int next, int last, List&lt;T&gt; attributes) { links = new Links(); links.setSelf(self); links.setNext(self + &quot;?page=&quot; + next); links.setLast(self + &quot;?page=&quot; + last); attributes.forEach(attribute -&gt; createDataBean(self, attribute)); } /** * 创建 DataBean * @param self 当前请求路径 * @param attributes 领域模型 */ private void createDataBean(String self, T attributes) { if (data == null) { data = new ArrayList&lt;&gt;(); } DataBean dataBean = new DataBean(); dataBean.setId(attributes.getId()); dataBean.setType(attributes.getClass().getSimpleName()); dataBean.setAttributes(attributes); if (StringUtils.isNotBlank(self)) { Links links = new Links(); links.setSelf(self + &quot;/&quot; + attributes.getId()); dataBean.setLinks(links); } data.add(dataBean); } } utils.MapperUtils public class MapperUtils { private final static ObjectMapper objectMapper = new ObjectMapper(); public static ObjectMapper getInstance() { return objectMapper; } /** * 转换为 JSON 字符串 * * @param obj * @return * @throws Exception */ public static String obj2json(Object obj) throws Exception { return objectMapper.writeValueAsString(obj); } /** * 转换为 JSON 字符串，忽略空值 * * @param obj * @return * @throws Exception */ public static String obj2jsonIgnoreNull(Object obj) throws Exception { ObjectMapper mapper = new ObjectMapper(); mapper.setSerializationInclusion(JsonInclude.Include.NON_NULL); return mapper.writeValueAsString(obj); } /** * 转换为 JavaBean * * @param jsonString * @param clazz * @return * @throws Exception */ public static &lt;T&gt; T json2pojo(String jsonString, Class&lt;T&gt; clazz) throws Exception { objectMapper.configure(DeserializationFeature.ACCEPT_SINGLE_VALUE_AS_ARRAY, true); return objectMapper.readValue(jsonString, clazz); } /** * 字符串转换为 Map&lt;String, Object&gt; * * @param jsonString * @return * @throws Exception */ public static &lt;T&gt; Map&lt;String, Object&gt; json2map(String jsonString) throws Exception { ObjectMapper mapper = new ObjectMapper(); mapper.setSerializationInclusion(JsonInclude.Include.NON_NULL); return mapper.readValue(jsonString, Map.class); } /** * 字符串转换为 Map&lt;String, T&gt; */ public static &lt;T&gt; Map&lt;String, T&gt; json2map(String jsonString, Class&lt;T&gt; clazz) throws Exception { Map&lt;String, Map&lt;String, Object&gt;&gt; map = objectMapper.readValue(jsonString, new TypeReference&lt;Map&lt;String, T&gt;&gt;() { }); Map&lt;String, T&gt; result = new HashMap&lt;String, T&gt;(); for (Map.Entry&lt;String, Map&lt;String, Object&gt;&gt; entry : map.entrySet()) { result.put(entry.getKey(), map2pojo(entry.getValue(), clazz)); } return result; } /** * 深度转换 JSON 成 Map * * @param json * @return */ public static Map&lt;String, Object&gt; json2mapDeeply(String json) throws Exception { return json2MapRecursion(json, objectMapper); } /** * 把 JSON 解析成 List，如果 List 内部的元素存在 jsonString，继续解析 * * @param json * @param mapper 解析工具 * @return * @throws Exception */ private static List&lt;Object&gt; json2ListRecursion(String json, ObjectMapper mapper) throws Exception { if (json == null) { return null; } List&lt;Object&gt; list = mapper.readValue(json, List.class); for (Object obj : list) { if (obj != null &amp;&amp; obj instanceof String) { String str = (String) obj; if (str.startsWith(&quot;[&quot;)) { obj = json2ListRecursion(str, mapper); } else if (obj.toString().startsWith(&quot;{&quot;)) { obj = json2MapRecursion(str, mapper); } } } return list; } /** * 把 JSON 解析成 Map，如果 Map 内部的 Value 存在 jsonString，继续解析 * * @param json * @param mapper * @return * @throws Exception */ private static Map&lt;String, Object&gt; json2MapRecursion(String json, ObjectMapper mapper) throws Exception { if (json == null) { return null; } Map&lt;String, Object&gt; map = mapper.readValue(json, Map.class); for (Map.Entry&lt;String, Object&gt; entry : map.entrySet()) { Object obj = entry.getValue(); if (obj != null &amp;&amp; obj instanceof String) { String str = ((String) obj); if (str.startsWith(&quot;[&quot;)) { List&lt;?&gt; list = json2ListRecursion(str, mapper); map.put(entry.getKey(), list); } else if (str.startsWith(&quot;{&quot;)) { Map&lt;String, Object&gt; mapRecursion = json2MapRecursion(str, mapper); map.put(entry.getKey(), mapRecursion); } } } return map; } /** * 将 JSON 数组转换为集合 * * @param jsonArrayStr * @param clazz * @return * @throws Exception */ public static &lt;T&gt; List&lt;T&gt; json2list(String jsonArrayStr, Class&lt;T&gt; clazz) throws Exception { JavaType javaType = getCollectionType(ArrayList.class, clazz); List&lt;T&gt; list = (List&lt;T&gt;) objectMapper.readValue(jsonArrayStr, javaType); return list; } /** * 获取泛型的 Collection Type * * @param collectionClass 泛型的Collection * @param elementClasses 元素类 * @return JavaType Java类型 * @since 1.0 */ public static JavaType getCollectionType(Class&lt;?&gt; collectionClass, Class&lt;?&gt;... elementClasses) { return objectMapper.getTypeFactory().constructParametricType(collectionClass, elementClasses); } /** * 将 Map 转换为 JavaBean * * @param map * @param clazz * @return */ public static &lt;T&gt; T map2pojo(Map map, Class&lt;T&gt; clazz) { return objectMapper.convertValue(map, clazz); } /** * 将 Map 转换为 JSON * * @param map * @return */ public static String mapToJson(Map map) { try { return objectMapper.writeValueAsString(map); } catch (Exception e) { e.printStackTrace(); } return &quot;&quot;; } /** * 将 JSON 对象转换为 JavaBean * * @param obj * @param clazz * @return */ public static &lt;T&gt; T obj2pojo(Object obj, Class&lt;T&gt; clazz) { return objectMapper.convertValue(obj, clazz); } } utils.RegexpUtils public class RegexpUtils { /** * 验证手机号 */ public static final String PHONE = &quot;^((13[0-9])|(15[^4,\\D])|(18[0,5-9]))\\d{8}$&quot;; /** * 验证邮箱地址 */ public static final String EMAIL = &quot;\\w+(\\.\\w)*@\\w+(\\.\\w{2,3}){1,3}&quot;; /** * 验证手机号 * @param phone * @return */ public static boolean checkPhone(String phone) { return phone.matches(PHONE); } /** * 验证邮箱 * @param email * @return */ public static boolean checkEmail(String email) { return email.matches(EMAIL); } } web.AbstractBaseController public abstract class AbstractBaseController&lt;T extends AbstractBaseDomain&gt; { // 用于动态获取配置文件的属性值 private static final String ENVIRONMENT_LOGGING_LEVEL_MY_SHOP = &quot;logging.level.com.ht.micro.record&quot;; @Resource protected HttpServletRequest request; @Resource protected HttpServletResponse response; @Autowired private ConfigurableApplicationContext applicationContext; @ModelAttribute public void initReqAndRes(HttpServletRequest request, HttpServletResponse response) { this.request = request; this.response = response; } /** * 请求成功 * @param self * @param attribute * @return */ protected AbstractBaseResult success(String self, T attribute) { return BaseResultFactory.getInstance(response).build(self, attribute); } /** * 请求成功 * @param self * @param next * @param last * @param attributes * @return */ protected AbstractBaseResult success(String self, int next, int last, List&lt;T&gt; attributes) { return BaseResultFactory.getInstance(response).build(self, next, last, attributes); } /** * 请求失败 * @param title * @param detail * @return */ protected AbstractBaseResult error(String title, String detail) { return error(HttpStatus.UNAUTHORIZED.value(), title, detail); } /** * 请求失败 * @param code * @param title * @param detail * @return */ protected AbstractBaseResult error(int code, String title, String detail) { return BaseResultFactory.getInstance(response).build(code, title, detail, applicationContext.getEnvironment().getProperty(ENVIRONMENT_LOGGING_LEVEL_MY_SHOP)); } } ht-micro-record-commons-domaindomain.TbUser @Table(name = &quot;tb_user&quot;) @JsonInclude(JsonInclude.Include.NON_NULL) public class TbUser extends AbstractBaseDomain { /** * 用户名 */ @NotNull(message = &quot;用户名不可为空&quot;) @Length(min = 5, max = 20, message = &quot;用户名长度必须介于 5 和 20 之间&quot;) private String username; /** * 密码，加密存储 */ @JsonIgnore private String password; /** * 注册手机号 */ private String phone; /** * 注册邮箱 */ @NotNull(message = &quot;邮箱不可为空&quot;) @Pattern(regexp = RegexpUtils.EMAIL, message = &quot;邮箱格式不正确&quot;) private String email; /** * 获取用户名 * * @return username - 用户名 */ public String getUsername() { return username; } /** * 设置用户名 * * @param username 用户名 */ public void setUsername(String username) { this.username = username; } /** * 获取密码，加密存储 * * @return password - 密码，加密存储 */ public String getPassword() { return password; } /** * 设置密码，加密存储 * * @param password 密码，加密存储 */ public void setPassword(String password) { this.password = password; } /** * 获取注册手机号 * * @return phone - 注册手机号 */ public String getPhone() { return phone; } /** * 设置注册手机号 * * @param phone 注册手机号 */ public void setPhone(String phone) { this.phone = phone; } /** * 获取注册邮箱 * * @return email - 注册邮箱 */ public String getEmail() { return email; } /** * 设置注册邮箱 * * @param email 注册邮箱 */ public void setEmail(String email) { this.email = email; } } validator.BeanValidator @Component public class BeanValidator { @Autowired private Validator validatorInstance; private static Validator validator; // 系统启动时将静态对象注入容器，不可用Autowire直接注入 @PostConstruct public void init() { BeanValidator.validator = validatorInstance; } /** * 调用 JSR303 的 validate 方法, 验证失败时抛出 ConstraintViolationException. */ private static void validateWithException(Validator validator, Object object, Class&lt;?&gt;... groups) throws ConstraintViolationException { Set constraintViolations = validator.validate(object, groups); if (!constraintViolations.isEmpty()) { throw new ConstraintViolationException(constraintViolations); } } /** * 辅助方法, 转换 ConstraintViolationException 中的 Set&lt;ConstraintViolations&gt; 中为 List&lt;message&gt;. */ private static List&lt;String&gt; extractMessage(ConstraintViolationException e) { return extractMessage(e.getConstraintViolations()); } /** * 辅助方法, 转换 Set&lt;ConstraintViolation&gt; 为 List&lt;message&gt; */ private static List&lt;String&gt; extractMessage(Set&lt;? extends ConstraintViolation&gt; constraintViolations) { List&lt;String&gt; errorMessages = new ArrayList&lt;&gt;(); for (ConstraintViolation violation : constraintViolations) { errorMessages.add(violation.getMessage()); } return errorMessages; } /** * 辅助方法, 转换 ConstraintViolationException 中的 Set&lt;ConstraintViolations&gt; 为 Map&lt;property, message&gt;. */ private static Map&lt;String, String&gt; extractPropertyAndMessage(ConstraintViolationException e) { return extractPropertyAndMessage(e.getConstraintViolations()); } /** * 辅助方法, 转换 Set&lt;ConstraintViolation&gt; 为 Map&lt;property, message&gt;. */ private static Map&lt;String, String&gt; extractPropertyAndMessage(Set&lt;? extends ConstraintViolation&gt; constraintViolations) { Map&lt;String, String&gt; errorMessages = new HashMap&lt;&gt;(); for (ConstraintViolation violation : constraintViolations) { errorMessages.put(violation.getPropertyPath().toString(), violation.getMessage()); } return errorMessages; } /** * 辅助方法, 转换 ConstraintViolationException 中的 Set&lt;ConstraintViolations&gt; 为 List&lt;propertyPath message&gt;. */ private static List&lt;String&gt; extractPropertyAndMessageAsList(ConstraintViolationException e) { return extractPropertyAndMessageAsList(e.getConstraintViolations(), &quot; &quot;); } /** * 辅助方法, 转换 Set&lt;ConstraintViolations&gt; 为 List&lt;propertyPath message&gt;. */ private static List&lt;String&gt; extractPropertyAndMessageAsList(Set&lt;? extends ConstraintViolation&gt; constraintViolations) { return extractPropertyAndMessageAsList(constraintViolations, &quot; &quot;); } /** * 辅助方法, 转换 ConstraintViolationException 中的 Set&lt;ConstraintViolations&gt; 为 List&lt;propertyPath + separator + message&gt;. */ private static List&lt;String&gt; extractPropertyAndMessageAsList(ConstraintViolationException e, String separator) { return extractPropertyAndMessageAsList(e.getConstraintViolations(), separator); } /** * 辅助方法, 转换 Set&lt;ConstraintViolation&gt; 为 List&lt;propertyPath + separator + message&gt;. */ private static List&lt;String&gt; extractPropertyAndMessageAsList(Set&lt;? extends ConstraintViolation&gt; constraintViolations, String separator) { List&lt;String&gt; errorMessages = new ArrayList&lt;&gt;(); for (ConstraintViolation violation : constraintViolations) { errorMessages.add(violation.getPropertyPath() + separator + violation.getMessage()); } return errorMessages; } /** * 服务端参数有效性验证 * * @param object 验证的实体对象 * @param groups 验证组 * @return 验证成功：返回 null；验证失败：返回错误信息 */ public static String validator(Object object, Class&lt;?&gt;... groups) { try { validateWithException(validator, object, groups); } catch (ConstraintViolationException ex) { List&lt;String&gt; list = extractMessage(ex); list.add(0, &quot;数据验证失败：&quot;); // 封装错误消息为字符串 StringBuilder sb = new StringBuilder(); for (int i = 0; i &lt; list.size(); i++) { String exMsg = list.get(i); if (i != 0) { sb.append(String.format(&quot;%s. %s&quot;, i, exMsg)).append(list.size() &gt; 1 ? &quot;&lt;br/&gt;&quot; : &quot;&quot;); } else { sb.append(exMsg).append(list.size() &gt; 1 ? &quot;&lt;br/&gt;&quot; : &quot;&quot;); } } return sb.toString(); } return null; } } ht-micro-record-commons-mapper├─src│ └─main│ ├─java│ │ ├─com│ │ │ └─ht│ │ │ └─micro│ │ │ └─record│ │ │ └─commons│ │ │ └─mapper│ │ │ ├─baseMapper│ │ │ └─dicMapper│ │ └─tk│ │ └─mybatis│ │ └─mapper│ └─resources│ ├─baseMapper│ └─dicMappermapper分开存储用来识别多数据源tk.mybatis.mapper.MyMapper public interface MyMapper&lt;T&gt; extends Mapper&lt;T&gt;, MySqlMapper&lt;T&gt; { } ht-micro-record-commons-serviceBaseCrudService public interface BaseCrudService&lt;T extends AbstractBaseDomain&gt; { /** * 查询属性值是否唯一 * * @param property * @param value * @return true/唯一，false/不唯一 */ default boolean unique(String property, String value) { return false; } /** * 保存 * * @param domain * @return */ default T save(T domain) { return null; } /** * 分页查询 * @param domain * @param pageNum * @param pageSize * @return */ default PageInfo&lt;T&gt; page(T domain, int pageNum, int pageSize) { return null; } } impl.BaseCrudServiceImpl public class BaseCrudServiceImpl&lt;T extends AbstractBaseDomain, M extends MyMapper&lt;T&gt;&gt; implements BaseCrudService&lt;T&gt; { @Autowired protected M mapper; private Class&lt;T&gt; entityClass = (Class&lt;T&gt;) ((ParameterizedType) getClass().getGenericSuperclass()).getActualTypeArguments()[0]; @Override public boolean unique(String property, String value) { Example example = new Example(entityClass); example.createCriteria().andEqualTo(property, value); int result = mapper.selectCountByExample(example); if (result &gt; 0) { return false; } return true; } @Override public T save(T domain) { int result = 0; Date currentDate = new Date(); // 创建 if (domain.getId() == null) { /** * 用于自动回显 ID，领域模型中需要 @ID 注解的支持 * {@link AbstractBaseDomain} */ result = mapper.insertUseGeneratedKeys(domain); } // 更新 else { result = mapper.updateByPrimaryKey(domain); } // 保存数据成功 if (result &gt; 0) { return domain; } // 保存数据失败 return null; } @Override public PageInfo&lt;T&gt; page(T domain, int pageNum, int pageSize) { Example example = new Example(entityClass); example.createCriteria().andEqualTo(domain); PageHelper.startPage(pageNum, pageSize); PageInfo&lt;T&gt; pageInfo = new PageInfo&lt;&gt;(mapper.selectByExample(example)); return pageInfo; } } TbUserService public interface TbUserService extends BaseCrudService&lt;TbUser&gt; { TbUser getById(long id); } impl.TbUserServiceImpl @Service public class TbUserServiceImpl extends BaseCrudServiceImpl&lt;TbUser, TbUserMapper&gt; implements TbUserService { @Autowired private TbUserMapper tbUserMapper; public TbUser getById(long id){ return tbUserMapper.selectByPrimaryKey(id); } } ht-micro-record-service-user &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath/&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-service-user&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-service-user&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt; &lt;artifactId&gt;jsr305&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt; &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt; &lt;artifactId&gt;jsr305&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt; &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rocketmq&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;!-- Projects Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-commons-service&lt;/artifactId&gt; &lt;version&gt;${project.parent.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Projects End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;!-- docker的maven插件，官网 https://github.com/spotify/docker-maven-plugin --&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt; &lt;version&gt;0.4.13&lt;/version&gt; &lt;configuration&gt; &lt;imageName&gt;192.168.2.7:5000/${project.artifactId}:${project.version}&lt;/imageName&gt; &lt;baseImage&gt;onejane-jdk1.8 &lt;/baseImage&gt; &lt;entryPoint&gt;[&quot;java&quot;, &quot;-jar&quot;,&quot;/${project.build.finalName}.jar&quot;]&lt;/entryPoint&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/&lt;/targetPath&gt; &lt;directory&gt;${project.build.directory}&lt;/directory&gt; &lt;include&gt;${project.build.finalName}.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;dockerHost&gt;http://192.168.2.7:2375&lt;/dockerHost&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;com.ht.micro.record.UserServiceApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; application.yml配置多数据源及RocketMQ spring: cloud: stream: rocketmq: binder: name-server: 192.168.2.7:9876 bindings: output: content-type: application/json destination: topic-email producer: group: group-email datasource: base: #监控统计拦截的filters filters: stat type: com.alibaba.druid.pool.DruidDataSource jdbc-url: jdbc:mysql://192.168.2.7:185/ht_micro_record?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false username: root password: 123456 driver-class-name: com.mysql.jdbc.Driver max-idle: 10 max-wait: 10000 min-idle: 5 initial-size: 5 validation-query: SELECT 1 test-on-borrow: false test-while-idle: true time-between-eviction-runs-millis: 18800 dic: #监控统计拦截的filters filters: stat type: com.alibaba.druid.pool.DruidDataSource jdbc-url: jdbc:mysql://192.168.1.48:3306/ht_nlp?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false username: root password: Sonar@1234 driver-class-name: com.mysql.jdbc.Driver max-idle: 10 max-wait: 10000 min-idle: 5 initial-size: 5 validation-query: SELECT 1 test-on-borrow: false test-while-idle: true time-between-eviction-runs-millis: 18800 bootstrap.properties spring.application.name=ht-micro-record-service-user-config spring.cloud.nacos.config.file-extension=yaml spring.cloud.nacos.config.server-addr=192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 logging.level.com.ht.micro.record=DEBUG nacos配置 spring: application: name: ht-micro-record-service-user cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8719 dashboard: 192.168.2.7:190 server: port: 9506 management: endpoints: web: exposure: include: &quot;*&quot; com.ht.micro.record.UserServiceApplication @SpringBootApplication(scanBasePackages = &quot;com.ht.micro.record&quot;,exclude = {DataSourceAutoConfiguration.class, DataSourceTransactionManagerAutoConfiguration.class, MybatisAutoConfiguration.class}) @EnableDiscoveryClient @EnableBinding({Source.class}) @MapperScan(basePackages = &quot;com.ht.micro.record.commons.mapper&quot;) @EnableAsync @EnableSwagger2 public class UserServiceApplication { public static void main(String[] args) { SpringApplication.run(UserServiceApplication.class, args); } } com.ht.micro.record.user.config.BaseMybatisConfig @Configuration @MapperScan(basePackages = {&quot;com.ht.micro.record.commons.mapper.baseMapper&quot;}, sqlSessionTemplateRef = &quot;baseSqlSessionTemplate&quot;) public class BaseMybatisConfig { @Value(&quot;${spring.datasource.base.filters}&quot;) String filters; @Value(&quot;${spring.datasource.base.driver-class-name}&quot;) String driverClassName; @Value(&quot;${spring.datasource.base.username}&quot;) String username; @Value(&quot;${spring.datasource.base.password}&quot;) String password; @Value(&quot;${spring.datasource.base.jdbc-url}&quot;) String url; @Bean(name=&quot;baseDataSource&quot;) @Primary//必须加此注解，不然报错，下一个类则不需要添加 spring.datasource @ConfigurationProperties(prefix=&quot;spring.datasource.base&quot;)//prefix值必须是application.properteis中对应属性的前缀 public DataSource baseDataSource() throws SQLException { DruidDataSource druid = new DruidDataSource(); // 监控统计拦截的filters druid.setFilters(filters); // 配置基本属性 druid.setDriverClassName(driverClassName); druid.setUsername(username); druid.setPassword(password); druid.setUrl(url); return druid; } @Bean(name=&quot;baseSqlSessionFactory&quot;) @Primary public SqlSessionFactory baseSqlSessionFactory(@Qualifier(&quot;baseDataSource&quot;)DataSource dataSource)throws Exception{ // 创建Mybatis的连接会话工厂实例 SqlSessionFactoryBean bean=new SqlSessionFactoryBean(); bean.setDataSource(dataSource);//// 设置数据源bean //添加XML目录 ResourcePatternResolver resolver=new PathMatchingResourcePatternResolver(); try{ bean.setMapperLocations(resolver.getResources(&quot;classpath:baseMapper/*Mapper.xml&quot;));//// 设置mapper文件路径 return bean.getObject(); }catch(Exception e){ e.printStackTrace(); throw new RuntimeException(e); } } @Bean(name=&quot;baseSqlSessionTemplate&quot;) @Primary public SqlSessionTemplate baseSqlSessionTemplate(@Qualifier(&quot;baseSqlSessionFactory&quot;)SqlSessionFactory sqlSessionFactory)throws Exception{ SqlSessionTemplate template=new SqlSessionTemplate(sqlSessionFactory);//使用上面配置的Factory return template; } } com.ht.micro.record.user.config.DicMybatisConfig @Configuration @MapperScan(basePackages = {&quot;com.ht.micro.record.commons.mapper.dicMapper&quot;}, sqlSessionTemplateRef = &quot;dicSqlSessionTemplate&quot;) public class DicMybatisConfig { @Value(&quot;${spring.datasource.dic.filters}&quot;) String filters; @Value(&quot;${spring.datasource.dic.driver-class-name}&quot;) String driverClassName; @Value(&quot;${spring.datasource.dic.username}&quot;) String username; @Value(&quot;${spring.datasource.dic.password}&quot;) String password; @Value(&quot;${spring.datasource.dic.jdbc-url}&quot;) String url; @Bean(name = &quot;dicDataSource&quot;) @ConfigurationProperties(prefix=&quot;spring.datasource.dic&quot;) public DataSource dicDataSource() throws SQLException { DruidDataSource druid = new DruidDataSource(); // 监控统计拦截的filters // druid.setFilters(filters); // 配置基本属性 druid.setDriverClassName(driverClassName); druid.setUsername(username); druid.setPassword(password); druid.setUrl(url); /* //初始化时建立物理连接的个数 druid.setInitialSize(initialSize); //最大连接池数量 druid.setMaxActive(maxActive); //最小连接池数量 druid.setMinIdle(minIdle); //获取连接时最大等待时间，单位毫秒。 druid.setMaxWait(maxWait); //间隔多久进行一次检测，检测需要关闭的空闲连接 druid.setTimeBetweenEvictionRunsMillis(timeBetweenEvictionRunsMillis); //一个连接在池中最小生存的时间 druid.setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis); //用来检测连接是否有效的sql druid.setValidationQuery(validationQuery); //建议配置为true，不影响性能，并且保证安全性。 druid.setTestWhileIdle(testWhileIdle); //申请连接时执行validationQuery检测连接是否有效 druid.setTestOnBorrow(testOnBorrow); druid.setTestOnReturn(testOnReturn); //是否缓存preparedStatement，也就是PSCache，oracle设为true，mysql设为false。分库分表较多推荐设置为false druid.setPoolPreparedStatements(poolPreparedStatements); // 打开PSCache时，指定每个连接上PSCache的大小 druid.setMaxPoolPreparedStatementPerConnectionSize(maxPoolPreparedStatementPerConnectionSize); */ return druid; } @Bean(name = &quot;dicSqlSessionFactory&quot;) public SqlSessionFactory dicSqlSessionFactory(@Qualifier(&quot;dicDataSource&quot;)DataSource dataSource)throws Exception{ SqlSessionFactoryBean bean=new SqlSessionFactoryBean(); bean.setDataSource(dataSource); //添加XML目录 ResourcePatternResolver resolver=new PathMatchingResourcePatternResolver(); try{ bean.setMapperLocations(resolver.getResources(&quot;classpath:dicMapper/*Mapper.xml&quot;)); return bean.getObject(); }catch(Exception e){ e.printStackTrace(); throw new RuntimeException(e); } } @Bean(name=&quot;dicSqlSessionTemplate&quot;) public SqlSessionTemplate dicSqlSessionTemplate(@Qualifier(&quot;dicSqlSessionFactory&quot;)SqlSessionFactory sqlSessionFactory)throws Exception{ SqlSessionTemplate template=new SqlSessionTemplate(sqlSessionFactory);//使用上面配置的Factory return template; } } com.ht.micro.record.user.service.UserService @Service public class UserService { @Autowired private MessageChannel output; // @EnableAsync在Application中开启异步 @Async public void sendEmail(TbUser tbUser) throws Exception { output.send(MessageBuilder.withPayload(MapperUtils.obj2json(tbUser)).build()); } } com.ht.micro.record.user.controller.UserController @RestController @RequestMapping(value = &quot;user&quot;) public class UserController extends AbstractBaseController&lt;TbUser&gt; { @Autowired private TbUserService tbUserService; @Autowired private UserService userService; // http://localhost:9506/user/2 // @ApiOperation(value = &quot;查询用户&quot;, notes = &quot;根据id获取用户名&quot;) @GetMapping(value = {&quot;{id}&quot;}) public String getName(@PathVariable long id){ return tbUserService.getById(id).getUsername(); } @ApiOperation(value = &quot;用户注册&quot;, notes = &quot;参数为实体类，注意用户名和邮箱不要重复&quot;) @PostMapping(value = &quot;reg&quot;) public AbstractBaseResult reg(@ApiParam(name = &quot;tbUser&quot;, value = &quot;用户模型&quot;) TbUser tbUser) { // 数据校验 String message = BeanValidator.validator(tbUser); if (StringUtils.isNotBlank(message)) { return error(message, null); } // 验证密码是否为空 if (StringUtils.isBlank(tbUser.getPassword())) { return error(&quot;密码不可为空&quot;, null); } // 验证用户名是否重复 if (!tbUserService.unique(&quot;username&quot;, tbUser.getUsername())) { return error(&quot;用户名已存在&quot;, null); } // 验证邮箱是否重复 if (!tbUserService.unique(&quot;email&quot;, tbUser.getEmail())) { return error(&quot;邮箱重复，请重试&quot;, null); } // 注册用户 try { tbUser.setPassword(DigestUtils.md5DigestAsHex(tbUser.getPassword().getBytes())); TbUser user = tbUserService.save(tbUser); if (user != null) { userService.sendEmail(user); response.setStatus(HttpStatus.CREATED.value()); return success(request.getRequestURI(), user); } } catch (Exception e) { // 这里补一句，将 RegService 中的异常抛到 Controller 中，这样可以打印出调试信息 return error(HttpStatus.INTERNAL_SERVER_ERROR.value(), &quot;注册邮件发送失败&quot;, e.getMessage()); } // 注册失败 return error(&quot;注册失败，请重试&quot;, null); } } ht-micro-record-service-sms &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-service-sms&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-service-sms&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-mail&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.sourceforge.nekohtml&lt;/groupId&gt; &lt;artifactId&gt;nekohtml&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt; &lt;artifactId&gt;jsr305&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt; &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rocketmq&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;!-- Projects Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-commons-domain&lt;/artifactId&gt; &lt;version&gt;${project.parent.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Projects End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;ht-micro-record-service-sms&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;com.ht.micro.record.service.email.SmsServiceApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- docker的maven插件，官网 https://github.com/spotify/docker-maven-plugin --&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt; &lt;version&gt;0.4.13&lt;/version&gt; &lt;configuration&gt; &lt;imageName&gt;192.168.2.7:5000/${project.artifactId}:${project.version}&lt;/imageName&gt; &lt;baseImage&gt;jdk1.8&lt;/baseImage&gt; &lt;entryPoint&gt;[&quot;java&quot;, &quot;-jar&quot;,&quot;/${project.build.finalName}.jar&quot;]&lt;/entryPoint&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/&lt;/targetPath&gt; &lt;directory&gt;${project.build.directory}&lt;/directory&gt; &lt;include&gt;${project.build.finalName}.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;dockerHost&gt;http://192.168.2.7:2375&lt;/dockerHost&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; application.yml spring: cloud: stream: rocketmq: binder: name-server: 192.168.2.7:9876 bindings: input: consumer: orderly: true bindings: input: destination: topic-email content-type: application/json group: group-email consumer: maxAttempts: 1 thymeleaf: cache: false mode: HTML encoding: UTF-8 servlet: content-type: text/html bootstrap.properties spring.application.name=ht-micro-record-service-sms-config spring.cloud.nacos.config.file-extension=yaml spring.cloud.nacos.config.server-addr=192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 nacos配置 spring: application: name: ht-micro-record-service-sms mail: host: smtp.163.com port: 25 # 你的邮箱授权码 password: codewj123456 properties: mail: smtp: auth: true starttls: enable: true required: true # 发送邮件的邮箱地址 username: m15806204096@163.com cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8719 dashboard: 192.168.2.7:190 server: port: 9507 management: endpoints: web: exposure: include: &quot;*&quot; com.ht.micro.record.service.email.SmsServiceApplication @SpringBootApplication(scanBasePackages = &quot;com.ht.micro.record&quot;) @EnableDiscoveryClient @EnableBinding({Sink.class}) @EnableAsync public class SmsServiceApplication { public static void main(String[] args) { SpringApplication.run(SmsServiceApplication.class, args); } } com.ht.micro.record.service.email.service.EmailService @Service public class EmailService { @Autowired private ConfigurableApplicationContext applicationContext; @Autowired private JavaMailSender javaMailSender; @Autowired private TemplateEngine templateEngine; @StreamListener(&quot;input&quot;) public void receive(String json) { try { // 发送普通邮件 TbUser tbUser = MapperUtils.json2pojo(json, TbUser.class); sendEmail(&quot;欢迎注册&quot;, &quot;欢迎 &quot; + tbUser.getUsername() + &quot; 加入华通晟云！&quot;, tbUser.getEmail()); // 发送 HTML 模板邮件 Context context = new Context(); context.setVariable(&quot;username&quot;, tbUser.getUsername()); String emailTemplate = templateEngine.process(&quot;reg&quot;, context); sendTemplateEmail(&quot;欢迎注册&quot;, emailTemplate, tbUser.getEmail()); } catch (Exception e) { e.printStackTrace(); } } /** * 发送普通邮件 * @param subject * @param body * @param to */ @Async public void sendEmail(String subject, String body, String to) { SimpleMailMessage message = new SimpleMailMessage(); message.setFrom(applicationContext.getEnvironment().getProperty(&quot;spring.mail.username&quot;)); message.setTo(to); message.setSubject(subject); message.setText(body); javaMailSender.send(message); } /** * 发送 HTML 模板邮件 * @param subject * @param body * @param to */ @Async public void sendTemplateEmail(String subject, String body, String to) { MimeMessage message = javaMailSender.createMimeMessage(); try { MimeMessageHelper helper = new MimeMessageHelper(message, true); helper.setFrom(applicationContext.getEnvironment().getProperty(&quot;spring.mail.username&quot;)); helper.setTo(to); helper.setSubject(subject); helper.setText(body, true); javaMailSender.send(message); } catch (Exception e) { } } } templates/reg.html &lt;!DOCTYPE html SYSTEM &quot;http://www.thymeleaf.org/dtd/xhtml1-strict-thymeleaf-spring4-4.dtd&quot;&gt; &lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot; xmlns:th=&quot;http://www.thymeleaf.org&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot;&gt; &lt;title&gt;注册通知&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div&gt; 欢迎 &lt;span th:text=&quot;${username}&quot;&gt;&lt;/span&gt; 加入 华通晟云！！ &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; post http://localhost:9506/user/reg?password=123456&amp;username=codewj&amp;email=1051103813@qq.com 实现注册 集成Swagger2ht-micro-record-commons/pom.xml &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;/dependency&gt;com.ht.micro.record.commons.config.Swagger2Configuration @Configuration public class Swagger2Configuration { @Bean public Docket createRestApi() { return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) .select() .apis(RequestHandlerSelectors.basePackage(&quot;com.ht.micro.record&quot;)) // 配置controller地址 .paths(PathSelectors.any()) .build(); } private ApiInfo apiInfo() { return new ApiInfoBuilder() .title(&quot;微服务 API 文档&quot;) .description(&quot;微服务 API 网关接口，http://www.htdatacloud.com/&quot;) .termsOfServiceUrl(&quot;http://www.htdatacloud.com&quot;) .version(&quot;1.0.0&quot;) .build(); } }com.ht.micro.record.UserServiceApplication @EnableSwagger2 com.ht.micro.record.user.controller.UserController @ApiOperation(value = &quot;用户注册&quot;, notes = &quot;参数为实体类，注意用户名和邮箱不要重复&quot;) @PostMapping 服务提供消费配置项目skywalking链路追踪并启动 -javaagent:E:\Project\ht-micro-record\ht-micro-record-external-skywalking\agent\skywalking-agent.jar -Dskywalking.agent.service_name=ht-micro-record-service-user -Dskywalking.collector.backend_service=192.168.2.7:11800 -javaagent:E:\Project\hello-spring-cloud-alibaba\hello-spring-cloud-external-skywalking\agent\skywalking-agent.jar -Dskywalking.agent.service_name=nacos-consumer-feign -Dskywalking.collector.backend_service=192.168.3.229:11800http://192.168.3.233:9501/user/10 调用服务http://192.168.2.7:193/#/monitor/dashboard 查看skywalking的链路追踪]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>spring cloud alibaba</tag>
        <tag>skywalking</tag>
        <tag>rocketmq</tag>
        <tag>validate</tag>
        <tag>multi druid</tag>
        <tag>swagger</tag>
        <tag>email</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Alibaba环境搭建]]></title>
    <url>%2F2019%2F08%2F21%2FAlibaba%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[基于spring cloud alibaba实战整合开发 创建统一的依赖管理ht-micro-record-dependencies &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.2.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;name&gt;ht-micro-record-dependencies&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;properties&gt; &lt;!-- Environment Settings --&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;!-- Spring Cloud Settings --&gt; &lt;spring-cloud.version&gt;Greenwich.RELEASE&lt;/spring-cloud.version&gt; &lt;spring-cloud-alibaba.version&gt;0.9.0.RELEASE&lt;/spring-cloud-alibaba.version&gt; &lt;!-- Spring Boot Settings --&gt; &lt;spring-boot-alibaba-druid.version&gt;1.1.18&lt;/spring-boot-alibaba-druid.version&gt; &lt;spring-boot-tk-mybatis.version&gt;2.1.4&lt;/spring-boot-tk-mybatis.version&gt; &lt;spring-boot-pagehelper.version&gt;1.2.12&lt;/spring-boot-pagehelper.version&gt; &lt;!-- Commons Settings --&gt; &lt;mysql.version&gt;5.1.38&lt;/mysql.version&gt; &lt;swagger2.version&gt;2.9.2&lt;/swagger2.version&gt; &lt;/properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;${spring-cloud.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;${spring-cloud-alibaba.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;${spring-boot-alibaba-druid.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;${spring-boot-tk-mybatis.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;${spring-boot-pagehelper.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Commons Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;${mysql.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;${swagger2.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;${swagger2.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!--防止版本冲突，统一版本在父pom中--&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;24.0-jre&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Commons End --&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;build&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;!-- Compiler 插件, 设定 JDK 版本，所有子服务统一打包jdk版本 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.8.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- 打包 jar 文件时，配置 manifest 文件，加入 lib 包的 jar 依赖 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;addMavenDescriptor&gt;false&lt;/addMavenDescriptor&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;!-- Add directory entries --&gt; &lt;addDefaultImplementationEntries&gt;true&lt;/addDefaultImplementationEntries&gt; &lt;addDefaultSpecificationEntries&gt;true&lt;/addDefaultSpecificationEntries&gt; &lt;addClasspath&gt;true&lt;/addClasspath&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- resource --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!-- install --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-install-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!-- clean --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-clean-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!-- ant --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!-- dependency --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;!-- 资源文件配置 --&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;excludes&gt; &lt;exclude&gt;**/*.java&lt;/exclude&gt; &lt;/excludes&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/build&gt; &lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;name&gt;Nexus Release Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;name&gt;Nexus Snapshot Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;Nexus Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-public/&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;aliyun-repos&lt;/id&gt; &lt;name&gt;Aliyun Repository&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;sonatype-repos&lt;/id&gt; &lt;name&gt;Sonatype Repository&lt;/name&gt; &lt;url&gt;https://oss.sonatype.org/content/groups/public&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;sonatype-repos-s&lt;/id&gt; &lt;name&gt;Sonatype Repository&lt;/name&gt; &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;spring-snapshots&lt;/id&gt; &lt;name&gt;Spring Snapshots&lt;/name&gt; &lt;url&gt;https://repo.spring.io/snapshot&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;spring-milestones&lt;/id&gt; &lt;name&gt;Spring Milestones&lt;/name&gt; &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;Nexus Plugin Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-public/&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;/pluginRepository&gt; &lt;pluginRepository&gt; &lt;id&gt;aliyun-repos&lt;/id&gt; &lt;name&gt;Aliyun Repository&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt;创建通用的工具类库ht-micro-record-commons &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath/&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-commons&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-commons&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;!-- Spring Begin ，每个RequestMapping之上都执行@ModelAttribute注解的方法，请求时都会带入request和response--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; &lt;artifactId&gt;tomcat-embed-core&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring End HttpServletRequest等实体，provided 其他服务依赖commons时不会自动引入tomcat依赖--&gt; &lt;!-- Apache Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Apache End --&gt; &lt;!-- Commons Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.ben-manes.caffeine&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.persistence&lt;/groupId&gt; &lt;artifactId&gt;javax.persistence-api&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Commons End --&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;ht-micro-record-commons&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;classifier&gt;exec&lt;/classifier&gt; &lt;mainClass&gt;com.ht.micro.record.commons.CommonsApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;创建通用的领域模型ht-micro-record-commons-domain &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath/&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-commons-domain&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-commons-domain&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;!-- Spring Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring End --&gt; &lt;!-- Commons Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate.validator&lt;/groupId&gt; &lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Commons End --&gt; &lt;!-- Projects Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-commons&lt;/artifactId&gt; &lt;version&gt;${project.parent.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Projects End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;ht-micro-record-commons-domain&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;classifier&gt;exec&lt;/classifier&gt; &lt;mainClass&gt;com.ht.micro.record.commons.CommonsDomainApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;创建通用的数据访问ht-micro-record-commons-mapper &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-commons-mapper&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-commons-mapper&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Commons Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Commons End --&gt; &lt;!-- Projects Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-commons-domain&lt;/artifactId&gt; &lt;version&gt;${project.parent.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Projects End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;ht-micro-record-commons-domain&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;classifier&gt;exec&lt;/classifier&gt; &lt;mainClass&gt;com.ht.micro.record.commons.CommonsMapperApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;创建通用的业务逻辑ht-micro-record-commons-service &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-commons-service&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-commons-service&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-commons-mapper&lt;/artifactId&gt; &lt;version&gt;${project.parent.version}&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;ht-micro-record-commons-domain&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;classifier&gt;exec&lt;/classifier&gt; &lt;mainClass&gt;com.ht.micro.record.commons.CommonsMapperApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;创建通用的代码生成ht-micro-record-database &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-database&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-database&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2018-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper&lt;/artifactId&gt; &lt;version&gt;4.1.4&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.3.5&lt;/version&gt; &lt;configuration&gt; &lt;configurationFile&gt;${basedir}/src/main/resources/generator/generatorConfig.xml&lt;/configurationFile&gt; &lt;overwrite&gt;true&lt;/overwrite&gt; &lt;verbose&gt;true&lt;/verbose&gt; &lt;/configuration&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.38&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper&lt;/artifactId&gt; &lt;version&gt;4.1.4&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;tk.mybatis.mapper.MyMapper public interface MyMapper&lt;T&gt; extends Mapper&lt;T&gt;, MySqlMapper&lt;T&gt; { }generator/generatorConfig.xml &lt;generatorConfiguration&gt; &lt;!-- 引入数据库连接配置 --&gt; &lt;properties resource=&quot;jdbc.properties&quot;/&gt; &lt;context id=&quot;Mysql&quot; targetRuntime=&quot;MyBatis3Simple&quot; defaultModelType=&quot;flat&quot;&gt; &lt;property name=&quot;beginningDelimiter&quot; value=&quot;`&quot;/&gt; &lt;property name=&quot;endingDelimiter&quot; value=&quot;`&quot;/&gt; &lt;!-- 配置 tk.mybatis 插件 --&gt; &lt;plugin type=&quot;tk.mybatis.mapper.generator.MapperPlugin&quot;&gt; &lt;property name=&quot;mappers&quot; value=&quot;tk.mybatis.mapper.MyMapper&quot;/&gt; &lt;/plugin&gt; &lt;!-- 配置数据库连接 --&gt; &lt;jdbcConnection driverClass=&quot;${jdbc.driverClass}&quot; connectionURL=&quot;${jdbc.connectionURL}&quot; userId=&quot;${jdbc.username}&quot; password=&quot;${jdbc.password}&quot;&gt; &lt;/jdbcConnection&gt; &lt;!-- 配置实体类存放路径 --&gt; &lt;javaModelGenerator targetPackage=&quot;com.ht.micro.record.commons.domain&quot; targetProject=&quot;src/main/java&quot;/&gt; &lt;!-- 配置 XML 存放路径 --&gt; &lt;sqlMapGenerator targetPackage=&quot;mapper&quot; targetProject=&quot;src/main/resources/baseMapper&quot;/&gt; &lt;!-- 配置 DAO 存放路径 --&gt; &lt;javaClientGenerator targetPackage=&quot;com.ht.micro.record.commons.mapper.baseMapper&quot; targetProject=&quot;src/main/java&quot; type=&quot;XMLMAPPER&quot;/&gt; &lt;!-- 配置需要指定生成的数据库和表，% 代表所有表 生成@Table中删除ht-micro-record.. --&gt; &lt;table catalog=&quot;ht_micro_record&quot; tableName=&quot;%&quot;&gt; &lt;!-- mysql 配置 --&gt; &lt;generatedKey column=&quot;id&quot; sqlStatement=&quot;Mysql&quot; identity=&quot;true&quot;/&gt; &lt;/table&gt; &lt;/context&gt; &lt;/generatorConfiguration&gt;jdbc.properties jdbc.driverClass=com.mysql.jdbc.Driver jdbc.connectionURL=jdbc:mysql://192.168.2.5:185/ht_micro_record?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false jdbc.username=root jdbc.password=123456mvn mybatis-generator:generate 自动生成表实体和mapper接口 创建外部链路追踪ht-micro-record-external-skywalking &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-external-skywalking&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;name&gt;ht-micro-record-external-skywalking&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;!-- 配置执行器 --&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;!-- 绑定到 package 生命周期阶段上 --&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;!-- 只运行一次 --&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;finalName&gt;skywalking&lt;/finalName&gt; &lt;descriptors&gt; &lt;!-- 配置描述文件路径 --&gt; &lt;descriptor&gt;src/main/assembly/assembly.xml&lt;/descriptor&gt; &lt;/descriptors&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;ht-micro-record-external-skywalking/src/main/assembly/assembly.xml &lt;assembly&gt; &lt;id&gt;6.0.0-Beta&lt;/id&gt; &lt;formats&gt; &lt;!-- 打包的文件格式，支持 zip、tar.gz、tar.bz2、jar、dir、war --&gt; &lt;format&gt;tar.gz&lt;/format&gt; &lt;/formats&gt; &lt;!-- tar.gz 压缩包下是否生成和项目名相同的根目录，有需要请设置成 true --&gt; &lt;includeBaseDirectory&gt;false&lt;/includeBaseDirectory&gt; &lt;dependencySets&gt; &lt;dependencySet&gt; &lt;!-- 是否把本项目添加到依赖文件夹下，有需要请设置成 true --&gt; &lt;useProjectArtifact&gt;false&lt;/useProjectArtifact&gt; &lt;outputDirectory&gt;lib&lt;/outputDirectory&gt; &lt;!-- 将 scope 为 runtime 的依赖包打包 --&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependencySet&gt; &lt;/dependencySets&gt; &lt;fileSets&gt; &lt;fileSet&gt; &lt;!-- 设置需要打包的文件路径 --&gt; &lt;directory&gt;agent&lt;/directory&gt; &lt;!-- 打包后的输出路径 --&gt; &lt;outputDirectory&gt;&lt;/outputDirectory&gt; &lt;/fileSet&gt; &lt;/fileSets&gt; &lt;/assembly&gt;apache-skywalking-apm-incubating-6.0.0-beta.tar.gz解压获取apache-skywalking-apm-bin/agent到ht-micro-record-external-skywalking下mvn clean package 会在 target 目录下创建名为 skywalking-6.0.0-Beta.tar.gz 的压缩包mvn clean install 会在本地仓库目录下创建名为 hello-spring-cloud-external-skywalking-1.0.0-SNAPSHOT-6.0.0-Beta.tar.gz 的压缩包 mkdir /data2/skywalking vim docker-compose.yml version: &#39;3.3&#39; services: elasticsearch: image: wutang/elasticsearch-shanghai-zone:6.3.2 container_name: elasticsearch restart: always ports: - 191:9200 - 192:9300 environment: cluster.name: elasticsearch docker-compose up -d https://mirrors.huaweicloud.com/apache/incubator/skywalking/6.0.0-beta/apache-skywalking-apm-incubating-6.0.0-beta.tar.gz tar zxf apache-skywalking-apm-incubating-6.0.0-beta.tar.gz cd apache-skywalking-apm-incubating vim apache-skywalking-apm-bin/config/application.yml 注释h2,打开es并修改clusterNodes地址 # h2: # driver: ${SW_STORAGE_H2_DRIVER:org.h2.jdbcx.JdbcDataSource} # url: ${SW_STORAGE_H2_URL:jdbc:h2:mem:skywalking-oap-db} # user: ${SW_STORAGE_H2_USER:sa} elasticsearch: nameSpace: ${SW_NAMESPACE:&quot;&quot;} clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:192.168.2.7:191} indexShardsNumber: ${SW_STORAGE_ES_INDEX_SHARDS_NUMBER:2} indexReplicasNumber: ${SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:0} # Batch process setting, refer to https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.5/java-docs-bulk-processor.html bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:2000} # Execute the bulk every 2000 requests bulkSize: ${SW_STORAGE_ES_BULK_SIZE:20} # flush the bulk every 20mb flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10} # flush the bulk every 10 seconds whatever the number of requests concurrentRequests: ${SW_STORAGE_ES_CONCURRENT_REQUESTS:2} # the number of concurrent requests apache-skywalking-apm-incubating/webapp/webapp.yml port 193 ./apache-skywalking-apm-incubating/bin/startup.sh]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>spring cloud alibaba</tag>
        <tag>tk mybatis</tag>
        <tag>skywalking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Alibaba限流熔断降级]]></title>
    <url>%2F2019%2F08%2F21%2FAlibaba%E9%99%90%E6%B5%81%E7%86%94%E6%96%AD%E9%99%8D%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[基于spring cloud alibaba实战整合开发 限流sentinel存储 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; application.yml spring: application: name: ht-micro-record-service-user cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8719 dashboard: 192.168.2.7:19 访问ht-micro-record-service-user服务，http://localhost:9506/apply/page/1/2快速的调用两次http://localhost:9506/apply/page/1/2 接口之后，第三次调用被限流了 nacos存储 &lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-datasource-nacos&lt;/artifactId&gt; &lt;/dependency&gt; application.yml spring: cloud: sentinel: datasource: ds: nacos: dataId: ${spring.application.name}-sentinel groupId: DEFAULT_GROUP rule-type: flow server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850nacos中配置ht-micro-record-service-user-sentinel [ { &quot;resource&quot;: &quot;/apply/page/1/2&quot;, &quot;limitApp&quot;: &quot;default&quot;, &quot;grade&quot;: 1, &quot;count&quot;: 5, &quot;strategy&quot;: 0, &quot;controlBehavior&quot;: 0, &quot;clusterMode&quot;: false } ] 进入http://192.168.2.7:190/#/dashboard/identity/ht-micro-record-service-user 访问 Sentinel控制台中修改规则：仅存在于服务的内存中，不会修改Nacos中的配置值，重启后恢复原来的值。 Nacos控制台中修改规则：服务的内存中规则会更新，Nacos中持久化规则也会更新，重启后依然保持。限流捕获处理异常ht-micro-record-service-dubbo-provider &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-datasource-nacos&lt;/artifactId&gt; &lt;/dependency&gt; com.ht.micro.record.service.dubbo.provider.DubboProviderApplication // 注解支持的配置Bean @Bean public SentinelResourceAspect sentinelResourceAspect() { return new SentinelResourceAspect(); } bootstrap.yml spring: application: name: ht-micro-record-service-dubbo-provider cloud: nacos: config: file-extension: yaml server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8720 dashboard: 192.168.2.7:190 datasource: ds: nacos: dataId: ${spring.application.name}-sentinel groupId: DEFAULT_GROUP rule-type: flow data-type: json server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850nacos配置ht-micro-record-service-dubbo-provider-sentinel [ { &quot;resource&quot;: &quot;protected-resource&quot;, &quot;controlBehavior&quot;: 2, &quot;count&quot;: 1, &quot;grade&quot;: 1, &quot;limitApp&quot;: &quot;default&quot;, &quot;strategy&quot;: 0 } ] nacos配置ht-micro-record-service-dubbo-provider.yaml spring: application: name: ht-micro-record-service-dubbo-provider cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 server: port: 9502 com.ht.micro.record.service.dubbo.provider.controller.ProviderController @GetMapping(&quot;/port&quot;) @SentinelResource(value = &quot;protected-resource&quot;, blockHandler = &quot;handleBlock&quot;) public Object port() { return &quot;port= &quot;+ port + &quot;, name=&quot; + applicationContext.getEnvironment().getProperty(&quot;user.name&quot;); } public String handleBlock(BlockException ex) { return &quot;限流了&quot;; }启动服务后http://192.168.2.7:190/#/dashboard/flow/ht-micro-record-service-dubbo-provider快速访问http://localhost:9502/provider/port 每秒超过1个请求将会显示限流了 熔断 @GetMapping(&quot;name&quot;) @SentinelResource(value = &quot;getName&quot;, fallback = &quot;getNameFallback&quot;) public String userName(String name){ for (int i = 0; i &lt; 100000000L; i++) { } return &quot;getName &quot; + name; } // 该方法降级处理函数，参数要与原函数getName相同，并且返回值类型也要与原函数相同，此外，该方法必须与原函数在同一个类中 public String getNameFallback(String name){ return &quot;getNameFallback&quot;; } 访问http://localhost:9502/provider/name查看http://192.168.2.7:190/#/dashboard/identity/ht-micro-record-service-dubbo-provider根据响应时间，大于10毫秒，则熔断降级（要连续超过5个请求超过10毫秒才会熔断）快速访问http://localhost:9502/provider/name @GetMapping(&quot;name&quot;) @SentinelResource(value = &quot;getName&quot;, fallback = &quot;getNameFallback&quot;) public String userName(String name){ for (int i = 0; i &lt; 100000000L; i++) { throw new RuntimeException(); } return &quot;getName &quot; + name; } // 该方法降级处理函数，参数要与原函数getName相同，并且返回值类型也要与原函数相同，此外，该方法必须与原函数在同一个类中 public String getNameFallback(String name){ return &quot;getNameFallback&quot;; } 显示抛出是10个异常，然后返回结果是熔断处理方法返回结果 流控规则 降级规则timeWindow为熔断恢复时间熔断模式，当熔断触发后，需要等待timewindow时间，再关闭熔断器。 0 根据rt时间，当超过指定规则的时间连续超过5笔，则触发熔断。 1 根据异常比例熔断 DEGRADE_GRADE_EXCEPTION_RATIO 2 根据单位时间内异常总数做熔断热点规则系统规则授权规则 Sentinel整合Nacos动态发布git clone https://github.com/alibaba/Sentinel.git cd Sentinel/sentinel-dashboard vim pom.xml &lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-datasource-nacos&lt;/artifactId&gt; &lt;!--&lt;scope&gt;test&lt;/scope&gt;--&gt; &lt;/dependency&gt; 修改resources/app/scripts/directives/sidebar/sidebar.html &lt;li ui-sref-active=&quot;active&quot;&gt; &lt;a ui-sref=&quot;dashboard.flowV1({app: entry.app})&quot;&gt; &lt;i class=&quot;glyphicon glyphicon-filter&quot;&gt;&lt;/i&gt;&amp;nbsp;&amp;nbsp;流控规则 &lt;/a&gt; &lt;/li&gt; 为 &lt;li ui-sref-active=&quot;active&quot;&gt; &lt;a ui-sref=&quot;dashboard.flow({app: entry.app})&quot;&gt; &lt;i class=&quot;glyphicon glyphicon-filter&quot;&gt;&lt;/i&gt;&amp;nbsp;&amp;nbsp;流控规则 &lt;/a&gt; &lt;/li&gt; com.alibaba.csp.sentinel.dashboard.rule.nacos.NacosConfigConstant public final class NacosConfigConstant { public static final String GROUP_ID = &quot;DEFAULT_GROUP&quot;; public static final String FLOW_DATA_ID_POSTFIX = &quot;-flow-rules&quot;; public static final String PARAM_FLOW_DATA_ID_POSTFIX = &quot;-param-flow-rules&quot;; public static final String DEGRADE_DATA_ID_POSTFIX = &quot;-degrade-rules&quot;; public static final String SYSTEM_DATA_ID_POSTFIX = &quot;-system-rules&quot;; public static final String AUTHORITY_DATA_ID_POSTFIX = &quot;-authority-rules&quot;; } com.alibaba.csp.sentinel.dashboard.rule.nacos.NacosConfigProperties @Component public class NacosConfigProperties { @Value(&quot;${houyi.nacos.server.ip}&quot;) private String ip; @Value(&quot;${houyi.nacos.server.port}&quot;) private String port; @Value(&quot;${houyi.nacos.server.namespace}&quot;) private String namespace; @Value(&quot;${houyi.nacos.server.group-id}&quot;) private String groupId; public String getIp() { return ip; } public void setIp(String ip) { this.ip = ip; } public String getPort() { return port; } public void setPort(String port) { this.port = port; } public String getNamespace() { return namespace; } public void setNamespace(String namespace) { this.namespace = namespace; } public String getGroupId() { return groupId; } public void setGroupId(String groupId) { this.groupId = groupId; } public String getServerAddr() { return this.getIp()+&quot;:&quot;+this.getPort(); } @Override public String toString() { return &quot;NacosConfigProperties [ip=&quot; + ip + &quot;, port=&quot; + port + &quot;, namespace=&quot; + namespace + &quot;, groupId=&quot; + groupId + &quot;]&quot;; } } com.alibaba.csp.sentinel.dashboard.rule.nacos.NacosConfig @Configuration public class NacosConfig { @Autowired private NacosConfigProperties nacosConfigProperties; @Bean public ConfigService nacosConfigService() throws Exception { Properties properties = new Properties(); properties.put(PropertyKeyConst.SERVER_ADDR, nacosConfigProperties.getServerAddr()); if(nacosConfigProperties.getNamespace() != null &amp;&amp; !&quot;&quot;.equals(nacosConfigProperties.getNamespace())) properties.put(PropertyKeyConst.NAMESPACE, nacosConfigProperties.getNamespace()); return ConfigFactory.createConfigService(properties); } } application.properties houyi.nacos.server.ip=192.168.2.7 houyi.nacos.server.port=8848 houyi.nacos.server.namespace= houyi.nacos.server.group-id=DEFAULT_GROUP FlowRulecom.alibaba.csp.sentinel.dashboard.rule.nacos.FlowRuleNacosPublisher @Component(&quot;flowRuleNacosPublisher&quot;) public class FlowRuleNacosPublisher implements DynamicRulePublisher&lt;List&lt;FlowRuleEntity&gt;&gt; { @Autowired private ConfigService configService; @Autowired private NacosConfigProperties nacosConfigProperties; @Override public void publish(String app, List&lt;FlowRuleEntity&gt; rules) throws Exception { AssertUtil.notEmpty(app, &quot;app name cannot be empty&quot;); if (rules == null) { return; } configService.publishConfig(app + NacosConfigConstant.FLOW_DATA_ID_POSTFIX, nacosConfigProperties.getGroupId(), JSON.toJSONString(rules.stream().map(FlowRuleEntity::toRule).collect(Collectors.toList()))); } } com.alibaba.csp.sentinel.dashboard.rule.nacos.FlowRuleNacosProvider @Component(&quot;flowRuleNacosProvider&quot;) public class FlowRuleNacosProvider implements DynamicRuleProvider&lt;List&lt;FlowRuleEntity&gt;&gt; { private static Logger logger = LoggerFactory.getLogger(FlowRuleNacosProvider.class); @Autowired private ConfigService configService; @Autowired private NacosConfigProperties nacosConfigProperties; @Override public List&lt;FlowRuleEntity&gt; getRules(String appName) throws Exception { String rulesStr = configService.getConfig(appName + NacosConfigConstant.FLOW_DATA_ID_POSTFIX, nacosConfigProperties.getGroupId(), 3000); logger.info(&quot;nacosConfigProperties{}:&quot;, nacosConfigProperties); logger.info(&quot;从Nacos中获取到限流规则信息{}&quot;, rulesStr); if (StringUtil.isEmpty(rulesStr)) { return new ArrayList&lt;&gt;(); } List&lt;FlowRule&gt; rules = RuleUtils.parseFlowRule(rulesStr); if (rules != null) { return rules.stream().map(rule -&gt; FlowRuleEntity.fromFlowRule(appName, nacosConfigProperties.getIp(), Integer.valueOf(nacosConfigProperties.getPort()), rule)) .collect(Collectors.toList()); } else { return new ArrayList&lt;&gt;(); } } } com.alibaba.csp.sentinel.dashboard.controller.FlowControllerV1将原始HTTP调用改为对应的Provider及Publisher去除 @Autowired private SentinelApiClient sentinelApiClient; 新增Qualifier和Component值保持一致 @Autowired @Qualifier(&quot;flowRuleNacosProvider&quot;) private DynamicRuleProvider&lt;List&lt;FlowRuleEntity&gt;&gt; provider; @Autowired @Qualifier(&quot;flowRuleNacosPublisher&quot;) private DynamicRulePublisher&lt;List&lt;FlowRuleEntity&gt;&gt; publisher; @GetMapping(&quot;/rules&quot;) public Result&lt;List&lt;FlowRuleEntity&gt;&gt; apiQueryMachineRules(HttpServletRequest request, @RequestParam String app, @RequestParam String ip, @RequestParam Integer port) { AuthUser authUser = authService.getAuthUser(request); authUser.authTarget(app, PrivilegeType.READ_RULE); if (StringUtil.isEmpty(app)) { return Result.ofFail(-1, &quot;app can&#39;t be null or empty&quot;); } if (StringUtil.isEmpty(ip)) { return Result.ofFail(-1, &quot;ip can&#39;t be null or empty&quot;); } if (port == null) { return Result.ofFail(-1, &quot;port can&#39;t be null&quot;); } try { List&lt;FlowRuleEntity&gt; rules = provider.getRules(app); rules = repository.saveAll(rules); return Result.ofSuccess(rules); } catch (Throwable throwable) { logger.error(&quot;Error when querying flow rules&quot;, throwable); return Result.ofThrowable(-1, throwable); } } private boolean publishRules(String app, String ip, Integer port) { List&lt;FlowRuleEntity&gt; rules = repository.findAllByMachine(MachineInfo.of(app, ip, port)); try { publisher.publish(app, rules); logger.info(&quot;添加限流规则成功{}&quot;, JSON.toJSONString(rules.stream().map(FlowRuleEntity::toRule).collect(Collectors.toList()))); return true; } catch (Exception e) { logger.info(&quot;添加限流规则失败{}&quot;,JSON.toJSONString(rules.stream().map(FlowRuleEntity::toRule).collect(Collectors.toList()))); e.printStackTrace(); return false; } } DegradeRulecom.alibaba.csp.sentinel.dashboard.rule.nacos.DegradeRuleNacosPublisher @Component(&quot;degradeRuleNacosPublisher&quot;) public class DegradeRuleNacosPublisher implements DynamicRulePublisher&lt;List&lt;DegradeRuleEntity&gt;&gt; { @Autowired private ConfigService configService; @Autowired private NacosConfigProperties nacosConfigProperties; @Override public void publish(String app, List&lt;DegradeRuleEntity&gt; rules) throws Exception { AssertUtil.notEmpty(app, &quot;app name cannot be empty&quot;); if (rules == null) { return; } configService.publishConfig(app + NacosConfigConstant.DEGRADE_DATA_ID_POSTFIX, nacosConfigProperties.getGroupId(), JSON.toJSONString(rules.stream().map(DegradeRuleEntity::toRule).collect(Collectors.toList()))); } } com.alibaba.csp.sentinel.dashboard.rule.nacos.DegradeRuleNacosProvider @Component(&quot;degradeRuleNacosProvider&quot;) public class DegradeRuleNacosProvider implements DynamicRuleProvider&lt;List&lt;DegradeRuleEntity&gt;&gt; { private static Logger logger = LoggerFactory.getLogger(DegradeRuleNacosProvider.class); @Autowired private ConfigService configService; @Autowired private NacosConfigProperties nacosConfigProperties; @Override public List&lt;DegradeRuleEntity&gt; getRules(String appName) throws Exception { String rulesStr = configService.getConfig(appName + NacosConfigConstant.DEGRADE_DATA_ID_POSTFIX, nacosConfigProperties.getGroupId(), 3000); logger.info(&quot;nacosConfigProperties{}:&quot;, nacosConfigProperties); logger.info(&quot;从Nacos中获取到熔断降级规则信息{}&quot;, rulesStr); if (StringUtil.isEmpty(rulesStr)) { return new ArrayList&lt;&gt;(); } List&lt;DegradeRule&gt; rules = RuleUtils.parseDegradeRule(rulesStr); if (rules != null) { return rules.stream().map(rule -&gt; DegradeRuleEntity.fromDegradeRule(appName, nacosConfigProperties.getIp(), Integer.valueOf(nacosConfigProperties.getPort()), rule)) .collect(Collectors.toList()); } else { return new ArrayList&lt;&gt;(); } } } com.alibaba.csp.sentinel.dashboard.controller.DegradeController去除 @Autowired private SentinelApiClient sentinelApiClient;新增 @Autowired @Qualifier(&quot;degradeRuleNacosProvider&quot;) private DynamicRuleProvider&lt;List&lt;DegradeRuleEntity&gt;&gt; provider; @Autowired @Qualifier(&quot;degradeRuleNacosPublisher&quot;) private DynamicRulePublisher&lt;List&lt;DegradeRuleEntity&gt;&gt; publisher; @ResponseBody @RequestMapping(&quot;/rules.json&quot;) public Result&lt;List&lt;DegradeRuleEntity&gt;&gt; queryMachineRules(HttpServletRequest request, String app, String ip, Integer port) { AuthUser authUser = authService.getAuthUser(request); authUser.authTarget(app, PrivilegeType.READ_RULE); if (StringUtil.isEmpty(app)) { return Result.ofFail(-1, &quot;app can&#39;t be null or empty&quot;); } if (StringUtil.isEmpty(ip)) { return Result.ofFail(-1, &quot;ip can&#39;t be null or empty&quot;); } if (port == null) { return Result.ofFail(-1, &quot;port can&#39;t be null&quot;); } try { List&lt;DegradeRuleEntity&gt; rules = provider.getRules(app); rules = repository.saveAll(rules); return Result.ofSuccess(rules); } catch (Throwable throwable) { logger.error(&quot;queryApps error:&quot;, throwable); return Result.ofThrowable(-1, throwable); } } private boolean publishRules(String app, String ip, Integer port) { List&lt;DegradeRuleEntity&gt; rules = repository.findAllByMachine(MachineInfo.of(app, ip, port)); try { publisher.publish(app, rules); logger.info(&quot;添加熔断降级规则成功{}&quot;, JSON.toJSONString(rules.stream().map(DegradeRuleEntity::toRule).collect(Collectors.toList()))); return true; } catch (Exception e) { logger.info(&quot;添加熔断降级规则失败{}&quot;,JSON.toJSONString(rules.stream().map(DegradeRuleEntity::toRule).collect(Collectors.toList()))); e.printStackTrace(); return false; } }重新打包启动 mvn clean package -DskipTests cd sentinel-dashboard/target/ nohup java -Dserver.port=190 -Dcsp.sentinel.dashboard.server=localhost:190 -Dproject.name=sentinel-dashboard -jar sentinel-dashboard.jar &amp;项目中bootstrap.yml spring: application: name: ht-micro-record-service-dubbo-provider cloud: nacos: config: file-extension: yaml server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8720 dashboard: localhost:8080 datasource: ds: nacos: dataId: ${spring.application.name}-flow-rules groupId: DEFAULT_GROUP rule-type: flow # 流控 # rule-type: degrade # 熔断 data-type: json server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 ds1: nacos: dataId: ${spring.application.name}-degrade-rules groupId: DEFAULT_GROUP rule-type: degrade # 熔断 data-type: json server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 nacos配置ht-micro-record-service-dubbo-provider.yaml spring: application: name: ht-micro-record-service-dubbo-provider cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 server: port: 9502http://localhost:8080/#/dashboard 新增规则在http://192.168.2.7:8848/nacos 中自动同步暂时实现flow，degrade 基于Feign的熔断controller @GetMapping(&quot;name&quot;) @SentinelResource(value = &quot;getName&quot;, fallback = &quot;getNameFallback&quot;) public String userName(String name){ microServiceUserInf.getUserInfoById(1); return &quot;getName &quot; + name; } service@FeignClient(value = &quot;ht-micro-record-service-user&quot;,fallback = MicroServiceUserInfFallBack.class) public interface MicroServiceUserInf { /** * 通过用户id 获取用户信息 * @param id * @return */ @GetMapping(value = &quot;/user/getUserInfoById/{id}&quot;) TUser getUserInfoById(@PathVariable(value=&quot;id&quot; ) Integer id); } serviceFallBack@Component public class MicroServiceUserInfFallBack implements MicroServiceUserInf { @Autowired private TUserMapper userMapper; @Override public TUser getUserInfoById(Integer id) { log.info(&quot;开启熔断&quot;); return userMapper.selectByPrimaryKey(id); } } nacosspring: application: name: ht-micro-record-service-caserecord cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8723 dashboard: 192.168.2.7:190 datasource: ds: nacos: dataId: ${spring.application.name}-degrade-rules groupId: DEFAULT_GROUP rule-type: degrade data-type: json server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 feign: sentinel: enabled: true pom&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-datasource-nacos&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt; &lt;artifactId&gt;jsr305&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt; &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt;]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>spring cloud alibaba</tag>
        <tag>sentinel</tag>
        <tag>nacos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux]]></title>
    <url>%2F2019%2F08%2F20%2Flinux%2F</url>
    <content type="text"><![CDATA[linux使用技巧及常规软件安装 网卡防火墙vi /etc/sysconfig/network-scripts/ifcfg-eth0 ONBOOT=yes IPADDR=192.168.56.101 BOOTPROTO=static IPADDR与当前网卡在同一个网段 service network restart service iptables stop firewall-cmd --permanent --add-port=8080-8085/tcp firewall-cmd --permanent --remove-port=8080-8085/tcp firewall-cmd --permanent --list-ports firewall-cmd --permanent --list-services 查看使用互联网的程序 firewall-cmd --reload chkconfig --del iptables setenforce 0 systemctl stop firewalld.service systemctl disable firewalld.service vim /etc/sysconfig/selinux SELINUX=disabled 基本命令du -h --max-depth=1 查看各文件夹大小 nohup sh inotify3.sh &gt;&gt;333.out &amp; 后台执行脚本并把输出都指定文件 jobs -l 查看运行的后台进程 fg 1 通过jobid将后台进程提取到前台运行 ctrl + z 将暂停当前正在运行到进程，fg放入后台运行 yum -c /etc/yum.conf --installroot=/usr/local --releasever=/ install lszrz 安装文件到其他目录ekillvim /usr/local/bin/ekill ps aux | grep -e $* | grep -v grep | awk &#39;{print $2}&#39; | xargs -i kill {}chmod a+x /usr/local/bin/ekill 通过ekill删除进程 免密登陆192.168.2.7： ssh-keygen -t rsa ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.2.5 ssh root@192.168.2.5 jdk+maventar zxf jdk-8u60-linux-x64.tar.gz -C /data2/ &amp;&amp; mv jdk1.8.0_60 jdk tar zxf apache-maven-3.6.1-bin.tar.gz -C /data2 &amp;&amp; mv apache-maven-3.6.1 maven vim /etc/profile JAVA_HOME=/data2/jdk JRE_HOME=$JAVA_HOME/jre MAVEN_HOME=/data2/maven PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin:$MAVEN_HOME/bin CLASSPATH=:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib/dt.jar export JAVA_HOME JRE_HOME PATH CLASSPATH MAVEN_HOME source /etc/profile vim setting.xml &lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt; &lt;!-- localRepository | The path to the local repository maven will use to store artifacts. | | Default: ${user.home}/.m2/repository &lt;localRepository&gt;/path/to/local/repo&lt;/localRepository&gt; --&gt; &lt;localRepository&gt;E:\apache-maven-3.6.1\respository&lt;/localRepository&gt; &lt;servers&gt; &lt;server&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;123456&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;123456&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;3rdParty&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;123456&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt; &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;ui&lt;/id&gt; &lt;name&gt;Mirror from UK&lt;/name&gt; &lt;url&gt;http://uk.maven.org/maven2/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;mirrorOf&gt;maven-releases&lt;/mirrorOf&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-releases/&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;mirrorOf&gt;maven-snapshots&lt;/mirrorOf&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-snapshots/&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-releases/&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-snapshots/&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;3rdParty&lt;/id&gt; &lt;url&gt;http://192.168.2.7:182/repository/3rdParty/&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/repository&gt; &lt;!-- 私有库地址--&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;url&gt;http://192.168.2.4:8081/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;ui&lt;/id&gt; &lt;name&gt;ui&lt;/name&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;url&gt;http://uk.maven.org/maven2/&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;!--插件库地址--&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;url&gt;http://192.168.2.4:8081/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-releases/&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-snapshots/&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;pluginRepository&gt; &lt;id&gt;aliyun&lt;/id&gt; &lt;name&gt;aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;activeProfiles&gt; &lt;activeProfile&gt;nexus&lt;/activeProfile&gt; &lt;/activeProfiles&gt; &lt;/settings&gt; Gitgitignore无效git rm -r --cached . git add . git commit -m &#39;.gitignore&#39; git push origin master git config –system core.longpaths true 提交长文件名或者idea取消run git hooks Mongowindows 管理员身份启动cmd https://fastdl.mongodb.org/win32/mongodb-win32-x86_64-2008plus-ssl-3.6.2-signed.msi mkdir data\db 进入bin执行mongod --dbpath D:\MongoDB\Server\3.6\data\db 进入bin执行mongo进入客户端，db.test.insert({&#39;a&#39;:&#39;b&#39;}) , db.test.find() 管理员cmd：touch data\logs\mongo.log ,进入bin: mongod --bind_ip 0.0.0.0 --logpath D:\MongoDB\Server\3.6\data\logs\mongo.log --logappend --dbpath D:\MongoDB\Server\3.6\data\db --port 27017 --serviceName &quot;MongoDB&quot; --serviceDisplayName &quot;MongoDB&quot; --install linux https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-ubuntu1404-3.6.2.tgz uname -a 查看系统描述 lsb_release -a 查看系统版本 tar -zxvf mongodb-linux-i686-2.0.1.tgz &amp;&amp; mv mongodb-linux-i686-2.0.1 mongodb cd mongodb/ &amp;&amp; mkdir log &amp;&amp; mkdir data touch log/mongodb ./mongod -port 10001 --dbpath ~/mongodb/data/ --logpath ~/mongodb/log/mongodb.log 新开窗口./bin/mongo 127.0.0.1:10001 mongo --port 27017 配置密码 show dbs 查看所有库 use yapi 使用yapi库 db 查看当前所在库 db.qa.insert({&#39;username&#39;:&#39;aaa&#39;,&#39;age&#39;:&#39;18&#39;}) db.qa.find() 查看所有集合 db.createUser( { user: &quot;yapi&quot;, pwd: &quot;df123456&quot;, roles: [ { role: &quot;dbOwner&quot;, db: &quot;yapi&quot; } ] } ) db.system.users.find() db.system.users.remove({}) systemctl restart mongod /etc/mongodb.conf 远程访问 net: bind_ip = 0.0.0.0 port = 27017 security: authorization: enabled javascriptEnabled: false db.auth(&#39;yapi&#39;,&#39;df123456&#39;)GitLab方案1docker run \ --publish 1443:443 --publish 180:80 --publish 122:22 \ --name gitlab \ --volume /usr/local/docker/gitlab/config:/etc/gitlab \ --volume /usr/local/docker/gitlab/logs:/var/log/gitlab \ --volume /usr/local/docker/gitlab/data:/var/opt/gitlab \ gitlab/gitlab-ce方案2vim /usr/local/docker/gitlab/docker-compose.yml version: &#39;3&#39; services: gitlab: image: &#39;twang2218/gitlab-ce-zh:10.5&#39; restart: always hostname: &#39;192.168.2.5&#39; container_name: gitlab environment: TZ: &#39;Asia/Shanghai&#39; GITLAB_OMNIBUS_CONFIG: | external_url &#39;http://192.168.2.5:180&#39; gitlab_rails[&#39;gitlab_shell_ssh_port&#39;] = 2222 unicorn[&#39;port&#39;] = 8888 nginx[&#39;listen_port&#39;] = 8080 ports: - &#39;180:8080&#39; - &#39;8443:443&#39; - &#39;2222:22&#39; volumes: - /usr/local/docker/gitlab/config:/etc/gitlab - /usr/local/docker/gitlab/data:/var/opt/gitlab - /usr/local/docker/gitlab/logs:/var/log/gitlab ERROR: error while removing network: network gitlab_default id e3f084651bcc6b6ca5d5b7fb122d0ef3aba108292989441abc82f14343fea827 has active endpoints docker network inspect gitlab_default docker network disconnect -f gitlab_default gitlab docker-compose down --remove-orphans docker-compose logs -ft gitlab 配置邮箱 vim /usr/local/docker/gitlab/config/gitlab.rb gitlab_rails[&#39;smtp_enable&#39;] = true gitlab_rails[&#39;smtp_address&#39;] = &quot;smtp.163.com&quot; gitlab_rails[&#39;smtp_port&#39;] = 25 gitlab_rails[&#39;smtp_user_name&#39;] = &quot;m15806204096@163.com&quot; gitlab_rails[&#39;smtp_password&#39;] = &quot;codewj123456&quot; gitlab_rails[&#39;smtp_domain&#39;] = &quot;163.com&quot; gitlab_rails[&#39;smtp_authentication&#39;] = &quot;login&quot; gitlab_rails[&#39;smtp_enable_starttls_auto&#39;] = true gitlab_rails[&#39;smtp_tls&#39;] = false gitlab_rails[&#39;gitlab_email_from&#39;] = &quot;m15806204096@163.com&quot; user[&quot;git_user_email&quot;] = &quot;m15806204096@163.com&quot;root/123456 管理区域-&gt;设置-&gt;开启注册 注册时发送确认邮件docker-compose restart新增ssh密钥 ssh-keygen -t rsa -C “15806204096@163.com“,将.ssh的公钥加入Gitlab的SSH密钥访问 http://192.168.2.5:180/ root 12345678 方案3安装gityum –y install git cd /usr/local mkdir git cd git git init --bare learngit.git useradd git passwd git chown -R git:git learngit.git vi /etc/passwd git:x:1000:1000::/home/git:/usr/bin/git-shell 复制客户端的ssh-keygen -t rsa -C &quot;你的邮箱&quot; 获得的id_rsa.pub公钥到/root/.ssh/authorized_keys和/root/.ssh/authorized_keys gitignore无效的解决方案 git rm -r --cached . git add . git commit -m &#39;.gitignore&#39; git push origin master *.cache *.cache.lock *.iml *.log **/target **/logs .idea **/.project **/.settingsgitlab安装git config --global http.sslVerify false yum -y install curl policycoreutils openssh-server openssh-clients postfix systemctl start sshd systemctl start postfix systemctl enable sshd systemctl enable postfix curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash yum -y install gitlab-ce 太慢的话使用清华的源，yum makecache再install vim /etc/yum.repos.d/gitlab_gitlab-ce.repo [gitlab-ce] name=gitlab-ce baseurl=http://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el6 repo_gpgcheck=0 gpgcheck=0 enabled=1 gpgkey=https://packages.gitlab.com/gpg.key mkdir -p /etc/gitlab/ssl openssl genrsa -out &quot;/etc/gitlab/ssl/gitlab.example.com.key&quot; 2048 openssl req -new -key &quot;/etc/gitlab/ssl/gitlab.example.com.key&quot; -out &quot;/etc/gitlab/ssl/gitlab.example.com.csr&quot; Country Name (2 letter code) [XX]:cn State or Province Name (full name) []:bj Locality Name (eg, city) (Default City]:bj Organization Name (eg, company) [Default Company Ltd]: Organizational Unit Name (eg, section) []: Common Name (eg, your name or your server&#39;s hostname) []:gitlab.example.com Email Address (]:admin@example.com Please enter the following &#39;extra&#39; attributes to be sent with your certificate request A challenge password [):123456 An optional company name []:GitLab基本配置openssl x509 -req -days 3650 -in &quot;/etc/gitlab/ssl/gitlab.example.com.csr&quot; -signkey &quot;/etc/gitlab/ssl/gitlab.example.com.key&quot; -out &quot;/etc/gitlab/ssl/gitlab.example.com.crt&quot; openssl dhparam -out /etc/gitlab/ssl/dhparams.pem 2048 chmod 600 /etc/gitlab/ssl/* vim /etc/gitlab/gitlab.rb 将external_url &#39;http://gitlab.example.com&#39;的http修改为https 将# nginx[&#39;redirect_http_to_https&#39;] = false的注释去掉，修改为nginx[&#39;redirect_http_to_https&#39;] = true 将# nginx[&#39;ssl_certificate&#39;] = &quot;/etc/gitlab/ssl/#{node[&#39;fqdn&#39;]}.crt&quot;修改为# nginx[&#39;ssl_certificate&#39;] = &quot;/etc/gitlab/ssl/gitlab.example.com.crt&quot; 将# nginx[&#39;ssl_certificate_key&#39;] = &quot;/etc/gitlab/ssl/#{node[&#39;fqdn&#39;]}.key&quot;修改为# nginx[&#39;ssl_certificate_key&#39;] = &quot;/etc/gitlab/ssl/gitlab.example.com.key&quot; 将# nginx[&#39;ssl_dhparam&#39;] = nil 修改为# nginx[&#39;ssl_dhparam&#39;] = &quot;/etc/gitlab/ssl/dhparams.pem&quot; gitlab-ctl reconfigure vim /var/opt/gitlab/nginx/conf/gitlab-http.conf 在server_name下添加如下配置内容：rewrite ^(.*)$ https://$host$1 permanent; 重启gitlab，使配置生效,gitlab-ctl restart，如遇到访问错误直接等待启动完成 修改本地hosts文件 将gitlab服务器的地址添加 gitlab.example.com 初始化时修改管理员密码，root 12345678 git -c http.sslVerify=false clone https://gitlab.example.com/root/test-repo.git git add . #git config --global user.email &quot;admin@example.com&quot; #git config --global user.name &quot;admin&quot; git commit -m &#39;init&#39; git -c http.sslVerify=false pull origin master git -c http.sslVerify=false push origin master https://gitlab.example.com/admin/system_info 查看系统资源状态值 https://gitlab.example.com/admin/logs 其中application.log记录了git用户的记录，production.log实时查看所有的访问链接 https://gitlab.example.com/admin/users/new 创建用户https://gitlab.example.com/root/test-repo/project_members 修改指定项目的成员，在项目的manage access中，修改登录密码 rm -rf test-repo/ git -c http.sslVerify=false clone https://gitlab.example.com/root/test-repo.git dev 12345678 cd test-repo/ git checkout -b release-1.0 git add . git commit -m &#39;release-1.0&#39; git -c http.sslVerify=false push origin release-1.0 dev登录后create merge request lead登录将受到release-1.0的merge申请，点击merge后可以填写comment并提交 Skywalkingmkdir /data2/skywalking vim docker-compose.yml version: &#39;3.3&#39; services: elasticsearch: image: wutang/elasticsearch-shanghai-zone:6.3.2 container_name: elasticsearch restart: always ports: - 191:9200 - 192:9300 environment: cluster.name: elasticsearch docker-compose up -d https://mirrors.huaweicloud.com/apache/incubator/skywalking/6.0.0-beta/apache-skywalking-apm-incubating-6.0.0-beta.tar.gz tar zxf apache-skywalking-apm-incubating-6.0.0-beta.tar.gz cd apache-skywalking-apm-incubating vim apache-skywalking-apm-bin/config/application.yml 注释h2,打开es并修改clusterNodes地址 # h2: # driver: ${SW_STORAGE_H2_DRIVER:org.h2.jdbcx.JdbcDataSource} # url: ${SW_STORAGE_H2_URL:jdbc:h2:mem:skywalking-oap-db} # user: ${SW_STORAGE_H2_USER:sa} elasticsearch: nameSpace: ${SW_NAMESPACE:&quot;&quot;} clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:192.168.2.7:191} indexShardsNumber: ${SW_STORAGE_ES_INDEX_SHARDS_NUMBER:2} indexReplicasNumber: ${SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:0} # Batch process setting, refer to https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.5/java-docs-bulk-processor.html bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:2000} # Execute the bulk every 2000 requests bulkSize: ${SW_STORAGE_ES_BULK_SIZE:20} # flush the bulk every 20mb flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10} # flush the bulk every 10 seconds whatever the number of requests concurrentRequests: ${SW_STORAGE_ES_CONCURRENT_REQUESTS:2} # the number of concurrent requests apache-skywalking-apm-incubating/webapp/webapp.yml port 193 ./apache-skywalking-apm-incubating/bin/startup.shSentinelcd /data2/ &amp;&amp; git clone https://github.com/alibaba/Sentinel.git cd Sentinel/ mvn clean package -DskipTests cd sentinel-dashboard/target/ nohup java -Dserver.port=190 -Dcsp.sentinel.dashboard.server=localhost:190 -Dproject.name=sentinel-dashboard -jar sentinel-dashboard.jar &amp;安装黑体字体yum -y install fontconfig fc-list :lang=zh cd /usr/share/fonts &amp;&amp; mkdir chinese chmod -R 755 /usr/share/fonts/chinese cd chinese/ &amp;&amp; rz simhei.ttf fs_GB2312.ttf fzxbsjt.ttf yum -y install ttmkfdir ttmkfdir -e /usr/share/X11/fonts/encodings/encodings.dir vim /etc/fonts/fonts.conf &lt;dir&gt;/usr/share/fonts&lt;/dir&gt; &lt;dir&gt;/usr/share/fonts/chinese&lt;/dir&gt; &lt;dir&gt;/usr/share/X11/fonts/Type1&lt;/dir&gt; &lt;dir&gt;/usr/share/X11/fonts/TTF&lt;/dir&gt; &lt;dir&gt;/usr/local/share/fonts&lt;/dir&gt; &lt;dir prefix=&quot;xdg&quot;&gt;fonts&lt;/dir&gt; &lt;!-- the following element will be removed in the future --&gt; &lt;dir&gt;~/.fonts&lt;/dir&gt; fc-cache fc-list :lang=zh RocketMQmkdir /data2/RocketMQ vim docker-compose.yml version: &#39;3.5&#39; services: rmqnamesrv: image: foxiswho/rocketmq:server container_name: rmqnamesrv ports: - 9876:9876 volumes: - ./data/logs:/opt/logs - ./data/store:/opt/store networks: rmq: aliases: - rmqnamesrv rmqbroker: image: foxiswho/rocketmq:broker container_name: rmqbroker ports: - 10909:10909 - 10911:10911 volumes: - ./data/logs:/opt/logs - ./data/store:/opt/store - ./data/brokerconf/broker.conf:/etc/rocketmq/broker.conf environment: NAMESRV_ADDR: &quot;rmqnamesrv:9876&quot; JAVA_OPTS: &quot; -Duser.home=/opt&quot; JAVA_OPT_EXT: &quot;-server -Xms128m -Xmx128m -Xmn128m&quot; command: mqbroker -c /etc/rocketmq/broker.conf depends_on: - rmqnamesrv networks: rmq: aliases: - rmqbroker rmqconsole: image: styletang/rocketmq-console-ng container_name: rmqconsole ports: - 8088:8080 environment: JAVA_OPTS: &quot;-Drocketmq.namesrv.addr=rmqnamesrv:9876 -Dcom.rocketmq.sendMessageWithVIPChannel=false&quot; depends_on: - rmqnamesrv networks: rmq: aliases: - rmqconsole networks: rmq: name: rmq driver: bridge vim /data2/RocketMQ/data/brokerconf/broker.conf brokerClusterName=DefaultCluster brokerName=broker-a brokerId=0 brokerIP1=192.168.2.7 defaultTopicQueueNums=4 autoCreateTopicEnable=true autoCreateSubscriptionGroup=true listenPort=10911 deleteWhen=04 fileReservedTime=120 mapedFileSizeCommitLog=1073741824 mapedFileSizeConsumeQueue=300000 diskMaxUsedSpaceRatio=88 maxMessageSize=65536 brokerRole=ASYNC_MASTER flushDiskType=ASYNC_FLUSH docker-compose up -d docker-compose down http://192.168.2.7:8088/RabbitMQdocker-compose.yml version: &#39;3&#39; services: rabbitmq: image: rabbitmq:management-alpine container_name: rabbitmq environment: - RABBITMQ_DEFAULT_USER=admin - RABBITMQ_DEFAULT_PASS=admin restart: always ports: - &quot;15672:15672&quot; - &quot;5672:5672&quot; logging: driver: &quot;json-file&quot; options: max-size: &quot;200k&quot; max-file: &quot;10&quot;docker-compose up -d,5672为rabbitmq的服务端口，15672为rabbitmq的web管理界面端口。 jenkinsrpm -qa|grep jenkins 搜索已安装 rpm -e --nodeps jenkins-2.83-1.1.noarch find / -name jenkins*|xargs rm -rf rpm -ivh jdk-8u171-linux-x64.rpm 安装java http://mirrors.jenkins.io/redhat/jenkins-2.180-1.1.noarch.rpm rpm -ivh jenkins-2.180-1.1.noarch.rpm 安装jenkins新版本 vi /etc/sysconfig/jenkins JENKINS_USER=&quot;root&quot; JENKINS_PORT=&quot;181&quot; vim /etc/rc.d/init.d/jenkins 修改candidates /data2/jdk/bin/java systemctl daemon-reload systemctl restart jenkins http://192.168.2.7:181 cat /var/lib/jenkins/secrets/initialAdminPassword 初始密码串并安装默认的插件 vim /var/lib/jenkins/hudson.model.UpdateCenter.xml 必须在填写完密码后修改 改https为http或者改为http://mirror.xmission.com/jenkins/updates/update-center.json systemctl restart jenkins 若页面空白/var/lib/jenkins/config.xml &lt;authorizationStrategy class=&quot;hudson.security.AuthorizationStrategy$Unsecured&quot;/&gt; &lt;securityRealm class=&quot;hudson.security.SecurityRealm$None&quot;/&gt; 系统管理-全局工具配置:/data2/maven /data2/jdk /data2/maven/conf/settings.xml 安装插件：Maven Integration，GitHub plugin，Git plugin 新建任务时，丢弃旧的构建，保持构建的天数3，保持构建的最大个数5 定时删除none的docker镜像手动执行：docker rmi $(docker images -f “dangling=true” -q)定时构建语法： 每天凌晨2:00跑一次 H 2 * * * 每隔5分钟构建一次 H/5 * * * * 每两小时构建一次 H H/2 * * * 每天中午12点定时构建一次 H 12 * * * 或0 12 * * *（0这种写法也被H替代了） 每天下午18点前定时构建一次 H 18 * * * 每15分钟构建一次 H/15 * * * * 或*/5 * * * *(这种方式已经被第一种替代了，jenkins也不推荐这种写法了) 周六到周日，18点-23点，三小时构建一次 H 18-23/3 * * 6-7 shell脚本 echo ---------------Stop-Rm-Containers...------------------ docker stop `docker ps -a| grep expire | awk &#39;{print $1}&#39;`|xargs docker rm echo ---------------Clear-Images...------------------ clearImagesList=$(docker images -f &quot;dangling=true&quot; -q) if [ ! -n &quot;$clearImagesList&quot; ]; then echo &quot;no images need clean up.&quot; else docker rmi $(docker images -f &quot;dangling=true&quot; -q) echo &quot;clear success.&quot; fi cron 每隔5秒执行一次：*/5 * * * * ? 每隔1分钟执行一次：0 */1 * * * ? 每天23点执行一次：0 0 23 * * ? 每天凌晨1点执行一次：0 0 1 * * ? 每月1号凌晨1点执行一次：0 0 1 1 * ? 每月最后一天23点执行一次：0 0 23 L * ? 每周星期天凌晨1点实行一次：0 0 1 ? * L 在26分、29分、33分执行一次：0 26,29,33 * * * ? 每天的0点、13点、18点、21点都执行一次：0 0 0,13,18,21 * * ? toptop -d 2 -c -p 123456 //每隔2秒显示pid是12345的进程的资源使用情况，并显式该进程启动的命令行参数 M —根据驻留内存大小进行排序 P —根据CPU使用百分比大小进行排序 T —根据时间/累计时间进行排序 c —切换显示命令名称和完整命令行 t —切换显示进程和CPU信息 m —切换显示内存信息 l —切换显示平均负载和启动时间信息 o —改变显示项目的顺序 f —从当前显示中添加或删除项目 S —切换到累计模式 s —改变两次刷新之间的延迟时间。系统将提示用户输入新的时间，单位为s。如果有小数，就换算成ms。 q —退出top程序 i —忽略闲置和僵尸进程。这是一个开关式的命令 k —终止一个进程更改显示内容通过 f 键可以选择显示的内容。按 f 键之后会显示列的列表，按 a-z 即可显示或隐藏对应的列，最后按回车键确定。 Exampletop - 01:06:48 up 1:22, 1 user, load average: 0.06, 0.60, 0.48 Tasks: 29 total, 1 running, 28 sleeping, 0 stopped, 0 zombie Cpu(s): 0.3% us, 1.0% sy, 0.0% ni, 98.7% id, 0.0% wa, 0.0% hi, 0.0% si Mem: 191272k total, 173656k used, 17616k free, 22052k buffers Swap: 192772k total, 0k used, 192772k free, 123988k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND root 16 0 7976 2456 1980 S 0.7 1.3 0:11.03 sshd root 16 0 2128 980 796 R 0.7 0.5 0:02.72 top root 16 0 1992 632 544 S 0.0 0.3 0:00.90 init root 34 19 0 0 0 S 0.0 0.0 0:00.00 ksoftirqd/0 root RT 0 0 0 0 S 0.0 0.0 0:00.00 watchdog/0 统计信息区前五行是系统整体的统计信息。第一行是任务队列信息，同 uptime 命令的执行结果。其内容如下： 01:06:48 当前时间 up 1:22 系统运行时间，格式为时:分 1 user 当前登录用户数 load average: 0.06, 0.60, 0.48 系统负载，即任务队列的平均长度。三个数值分别为 1分钟、5分钟、15分钟前到现在的平均值。 第二、三行为进程和CPU的信息。当有多个CPU时，这些内容可能会超过两行。内容如下： total 进程总数 running 正在运行的进程数 sleeping 睡眠的进程数 stopped 停止的进程数 zombie 僵尸进程数 Cpu(s): 0.3% us 用户空间占用CPU百分比 1.0% sy 内核空间占用CPU百分比 0.0% ni 用户进程空间内改变过优先级的进程占用CPU百分比 98.7% id 空闲CPU百分比 0.0% wa 等待输入输出的CPU时间百分比 0.0%hi：硬件CPU中断占用百分比 0.0%si：软中断占用百分比 0.0%st：虚拟机占用百分比最后两行为内存信息。内容如下： Mem: 191272k total 物理内存总量 173656k used 使用的物理内存总量 17616k free 空闲内存总量 22052k buffers 用作内核缓存的内存量 Swap: 192772k total 交换区总量 0k used 使用的交换区总量 192772k free 空闲交换区总量 123988k cached 缓冲的交换区总量,内存中的内容被换出到交换区，而后又被换入到内存，但使用过的交换区尚未被覆盖，该数值即为这些内容已存在于内存中的交换区的大小,相应的内存再次被换出时可不必再对交换区写入。进程信息区统计信息区域的下方显示了各个进程的详细信息。首先来认识一下各列的含义 序号 列名 含义 a PID 进程id b PPID 父进程id c RUSER Real user name d UID 进程所有者的用户id e USER 进程所有者的用户名 f GROUP 进程所有者的组名 g TTY 启动进程的终端名。不是从终端启动的进程则显示为 ? h PR 优先级 i NI nice值。负值表示高优先级，正值表示低优先级 j P 最后使用的CPU，仅在多CPU环境下有意义 k %CPU 上次更新到现在的CPU时间占用百分比 l TIME 进程使用的CPU时间总计，单位秒 m TIME+ 进程使用的CPU时间总计，单位1/100秒 n %MEM 进程使用的物理内存百分比 o VIRT 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES p SWAP 进程使用的虚拟内存中，被换出的大小，单位kb。 q RES 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA r CODE 可执行代码占用的物理内存大小，单位kb s DATA 可执行代码以外的部分(数据段+栈)占用的物理内存大小，单位kb t SHR 共享内存大小，单位kb u nFLT 页面错误次数 v nDRT 最后一次写入到现在，被修改过的页面数。 w S 进程状态(D=不可中断的睡眠状态,R=运行,S=睡眠,T=跟踪/停止,Z=僵尸进程) x COMMAND 命令名/命令行 y WCHAN 若该进程在睡眠，则显示睡眠中的系统函数名 z Flags 任务标志，参考 sched.h VIRT：virtual memory usage。Virtual这个词很神，一般解释是：virtual adj.虚的, 实质的,[物]有效的, 事实上的。到底是虚的还是实的？让Google给Define之后，将就明白一点，就是这东西还是非物质的，但是有效果的，不发生在真实世界的，发生在软件世界的等等。这个内存使用就是一个应用占有的地址空间，只是要应用程序要求的，就全算在这里，而不管它真的用了没有。写程序怕出错，又不在乎占用的时候，多开点内存也是很正常的。RES：resident memory usage。常驻内存。这个值就是该应用程序真的使用的内存，但还有两个小问题，一是有些东西可能放在交换盘上了（SWAP），二是有些内存可能是共享的。SHR：shared memory。共享内存。就是说这一块内存空间有可能也被其他应用程序使用着；而Virt － Shr似乎就是这个程序所要求的并且没有共享的内存空间。DATA：数据占用的内存。如果top没有显示，按f键可以显示出来。这一块是真正的该程序要求的数据空间，是真正在运行中要使用的。SHR是一个潜在的可能会被共享的数字，如果只开一个程序，也没有别人共同使用它；VIRT里面的可能性更多，比如它可能计算了被许多X的库所共享的内存；RES应该是比较准确的，但不含有交换出去的空间；但基本可以说RES是程序当前使用的内存量。 Q1:-bash: fork: Cannot allocate memory进程数满了,echo 1000000 &gt; /proc/sys/kernel/pid_max,echo “kernel.pid_max=1000000 “ &gt;&gt; /etc/sysctl.conf,sysctl -ptop:展示进程视图，监控服务器进程数值默认进入top时，各进程是按照CPU的占用量来排序的,-f查看实际内存占用量]]></content>
      <categories>
        <category>系统</category>
      </categories>
      <tags>
        <tag>gitlab</tag>
        <tag>jenkins</tag>
        <tag>sentinel</tag>
        <tag>linux</tag>
        <tag>rocketmq</tag>
        <tag>rabbitmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker]]></title>
    <url>%2F2019%2F08%2F20%2Fdocker%2F</url>
    <content type="text"><![CDATA[docker安装使用及镜像制作 安装docker基本配置yum install -y vim lrzsz git setenforce 0 vim /etc/selinux/config SELINUX=disabled systemctl stop firewalld systemctl disable firewalldcentosyum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-selinux \ docker-engine-selinux \ docker-engine -y 移除旧版本 rm -rf /etc/systemd/system/docker.service.d /var/lib/docker /var/run/docker yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum makecache fast yum list docker-ce --showduplicates | sort -r yum install -y docker-ce-18.09.5 systemctl restart docker yum update -y nss curl libcurl curl -L https://github.com/docker/compose/releases/download/1.20.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose docker-compose --versionubuntuapt-get update apt-get -y install apt-transport-https ca-certificates curl software-properties-common # step 2: 安装GPG证书 curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add - # Step 3: 写入软件源信息 sudo add-apt-repository &quot;deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable&quot; # Step 4: 更新并安装 Docker-CE sudo apt-get -y update sudo apt-get -y install docker-ce # 安装指定版本的Docker-CE: # Step 1: 查找Docker-CE的版本: # apt-cache madison docker-ce # docker-ce | 17.03.1~ce-0~ubuntu-xenial | http://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages # docker-ce | 17.03.0~ce-0~ubuntu-xenial | http://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages # Step 2: 安装指定版本的Docker-CE: (VERSION 例如上面的 17.03.1~ce-0~ubuntu-xenial) # sudo apt-get -y install docker-ce=[VERSION]Docker配置及基本使用vim ~/.bashrc alias dops=&#39;docker ps --format &quot;table {{.ID}}\t{{.Names}}\t{{.Image}}\t&quot;&#39; alias dopsa=&#39;docker ps -a --format &quot;table {{.ID}}\t{{.Names}}\t{{.Image}}\t&quot;&#39; alias dolo=&#39;docker logs -ft&#39; vim /etc/docker/daemon.json { &quot;registry-mirrors&quot;: [&quot;https://3gki6pei.mirror.aliyuncs.com&quot;], &quot;storage-driver&quot;:&quot;devicemapper&quot;, &quot;insecure-registries&quot;:[&quot;192.168.2.7:5000&quot;], &quot;log-driver&quot;: &quot;json-file&quot;, &quot;log-opt&quot;: { &quot;max-size&quot;: &quot;10m&quot;, &quot;max-file&quot;: &quot;10&quot; } } systemctl daemon-reload systemctl restart docker.service service docker stop 删除所有镜像 rm -rf /var/lib/docker systemctl start docker.service --restart=always 设置重启docker自动启动容器 docker update --restart=always es-node2 docker run -itd myimage:test /bin/bash -c &quot;命令1;命令2&quot; 启动容器自动执行命令 docker export registry &gt; /home/registry.tar 将容器打成tar包 scp /home/registry.tar root@192.168.2.7:/root cat ~/registry.tar | docker import - registry/2.5 将tar包打成镜像 docker save jdk1.8 &gt; /home/java.tar.gz 导出镜像 docker load &lt; /home/java.tar.gz 导入镜像 docker stop sentine1 | xargs docker rm docker rm -f redis-slave1|true docker logs -f -t --since=&quot;2018-02-08&quot; --tail=100 CONTAINER_ID 查看指定时间后的日志，只显示最后100行 docker logs --since 30m CONTAINER_ID 查看最近30分钟的日志 /var/lib/docker/containers/contain id 下rm -rf *.log 删除docker日志 进入https://homenew.console.aliyun.com/ 搜索容器镜像服务，进入侧边栏的镜像加速器获取自己的Docker加速镜像地址。 安装Registry私服方案1docker run -di --name=registry -p 5000:5000 docker.io/registry访问 http://192.168.2.5:5000/v2/_catalog 方案2vim config.yml version: 0.1 log: fields: service: registry storage: cache: blobdescriptor: inmemory filesystem: rootdirectory: /var/lib/registry delete: enabled: true http: addr: :5000 headers: X-Content-Type-Options: [nosniff] health: storagedriver: enabled: true interval: 10s threshold: 3 vim /usr/local/docker/registry/docker-compose.yml version: &#39;3.1&#39; services: registry: image: registry restart: always container_name: registry ports: - 5000:5000 volumes: - ./data:/var/lib/registry - ./config.yml:/etc/docker/registry/config.yml frontend: image: konradkleine/docker-registry-frontend:v2 container_name: registry-frontend restart: always ports: - 184:80 volumes: - ./certs/frontend.crt:/etc/apache2/server.crt:ro - ./certs/frontend.key:/etc/apache2/server.key:ro environment: - ENV_DOCKER_REGISTRY_HOST=192.168.2.5 - ENV_DOCKER_REGISTRY_PORT=5000使用docker-compose up -d启动registry容器，http://192.168.2.5:184/ 访问私有镜像库 curl -I -H &quot;Accept: application/vnd.docker.distribution.manifest.v2+json&quot; 192.168.2.7:5000/v2/ht-micro-record-service-user/manifests/1.0.0-SNAPSHOT 查看镜像Etag curl -I -H &quot;Accept: application/vnd.docker.distribution.manifest.v2+json&quot; 192.168.2.5:5000/v2/ht-micro-record-commons/manifests/1.0.0-SNAPSHOT 查看镜像Etag curl -i -X DELETE 192.168.2.5:5000/v2/codewj-redis-cluster/manifests/sha256:d6d6fad1ac67310ee34adbaa72986c6b233bd713906013961c722ecb10a049e5 删除codewj-redis-cluster:latest镜像 curl -I -X DELETE http://192.168.2.7:5000/v2/ht-micro-record-service-user/manifests/sha256:a6d4e02fa593f0ae30476bda8d992dcb0fc2341e6fef85a9887444b5e4b75a04 删除user镜像 docker exec -it registry registry garbage-collect /etc/docker/registry/config.yml 垃圾回收blob docker exec registry rm -rf /var/lib/registry/docker/registry/v2/repositories/codewj-redis-cluster 强制删除 docker exec registry rm -rf /var/lib/registry/docker/registry/v2/repositories/ht-micro-record-service-user 强制删除JDK镜像制作环境配置vim /etc/docker/daemon.json { &quot;registry-mirrors&quot;: [&quot;https://3gki6pei.mirror.aliyuncs.com&quot;], &quot;storage-driver&quot;:&quot;devicemapper&quot;, &quot;insecure-registries&quot;:[&quot;192.168.2.7:5000&quot;] } vim /lib/systemd/system/docker.service 开放访问 ExecStart 新增 -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock \ systemctl daemon-reload systemctl restart docker docker start registry制作镜像mkdir -p /usr/local/dockerjdk8 &amp;&amp; cd /usr/local/dockerjdk8sz jdk-8u60-linux-x64.tar.gz 传到该目录下vim Dockerfile #依赖镜像名称和ID FROM docker.io/centos:7 #指定镜像创建者信息 MAINTAINER OneJane #切换工作目录 WORKDIR /usr RUN mkdir /usr/local/java #ADD 是相对路径jar,把java添加到容器中 ADD jdk-8u60-linux-x64.tar.gz /usr/local/java/ ENV LANG en_US.UTF-8 #配置java环境变量 ENV JAVA_HOME /usr/local/java/jdk1.8.0_60 ENV JRE_HOME $JAVA_HOME/jre ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib:$CLASSPATH ENV PATH $JAVA_HOME/bin:$PATHdocker build -t='onejane-jdk1.8' .安装中文环境 yum -y install fontconfig fc-list :lang=zh cd /usr/share/fonts &amp;&amp; mkdir chinese chmod -R 755 /usr/share/fonts/chinese cd chinese/ &amp;&amp; rz simhei.ttf fs_GB2312.ttf fzxbsjt.ttf yum -y install ttmkfdir ttmkfdir -e /usr/share/X11/fonts/encodings/encodings.dir vim /etc/fonts/fonts.conf &lt;dir&gt;/usr/share/fonts&lt;/dir&gt; &lt;dir&gt;/usr/share/fonts/chinese&lt;/dir&gt; &lt;dir&gt;/usr/share/X11/fonts/Type1&lt;/dir&gt; &lt;dir&gt;/usr/share/X11/fonts/TTF&lt;/dir&gt; &lt;dir&gt;/usr/local/share/fonts&lt;/dir&gt; &lt;dir prefix=&quot;xdg&quot;&gt;fonts&lt;/dir&gt; &lt;!-- the following element will be removed in the future --&gt; &lt;dir&gt;~/.fonts&lt;/dir&gt; fc-cache fc-list :lang=zh 上传到私服docker commit 6ea1085dfc2a pxc:v1.0 将镜像保存本地 docker commit -m &quot;容器说明&quot; -a &quot;OneJane&quot; [CONTAINER ID] [给新的镜像命名] 将容器打包成镜像 docker tag onejane-jdk1.8 192.168.2.7:5000/onejane-jdk1.8 docker push 192.168.2.7:5000/onejane-jdk1.8 将镜像推到仓库Redis镜像制作vim entrypoint.sh #!/bin/sh #只作用于当前进程,不作用于其创建的子进程 set -e #$0--Shell本身的文件名 $1--第一个参数 $@--所有参数列表 # allow the container to be started with `--user` if [ &quot;$1&quot; = &#39;redis-server&#39; -a &quot;$(id -u)&quot; = &#39;0&#39; ]; then sed -i &#39;s/REDIS_PORT/&#39;$REDIS_PORT&#39;/g&#39; /usr/local/etc/redis.conf chown -R redis . #改变当前文件所有者 exec gosu redis &quot;$0&quot; &quot;$@&quot; #gosu是sudo轻量级”替代品” fi exec &quot;$@&quot; vim redis.conf #端口 port REDIS_PORT #开启集群 cluster-enabled yes #配置文件 cluster-config-file nodes.conf cluster-node-timeout 5000 #更新操作后进行日志记录 appendonly yes #设置主服务的连接密码 # masterauth #设置从服务的连接密码 # requirepassvi Dockerfile #基础镜像 FROM redis #修复时区 RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime RUN echo &#39;Asia/Shanghai&#39; &gt;/etc/timezone #环境变量 ENV REDIS_PORT 8000 #ENV REDIS_PORT_NODE 18000 #暴露变量 EXPOSE $REDIS_PORT #EXPOSE $REDIS_PORT_NODE #复制 COPY entrypoint.sh /usr/local/bin/ COPY redis.conf /usr/local/etc/ #for config rewrite RUN chmod 777 /usr/local/etc/redis.conf RUN chmod +x /usr/local/bin/entrypoint.sh #入口 ENTRYPOINT [&quot;/usr/local/bin/entrypoint.sh&quot;] #命令 CMD [&quot;redis-server&quot;, &quot;/usr/local/etc/redis.conf&quot;]docker build -t codewj/redis-cluster:1.0 . 上传到私服docker tag codewj/redis-cluster:1.0 192.168.2.5:5000/codewj-redis-cluster docker push 192.168.2.5:5000/codewj-redis-cluster上传到docker hubdocker login 输入https://hub.docker.com/账户密码 docker tag onejane-jdk1.8 onejane/onejane-jdk1.8 docker push onejane/onejane-jdk1.8:latest docker logout Redisdocker run -d –privileged=true -p 6379:6379 -v $PWD/redis.conf:/etc/redis/redis.conf -v $PWD/data:/data –name redis redis redis-server /etc/redis/redis.conf –appendonly yeshttp://download.redis.io/redis-stable/redis.conf bind 0.0.0.0 port 6379 daemonize no appendonly yes protected-mode no 手动部署 mvn clean install deploy docker:build -DpushImage docker run -di --name=panchip -v /tmp/saas:/tmp/saas --net=host 192.168.2.7:5000/panchip:1.0.0-SNAPSHOT docker export panchip &gt; /home/panchip.tar 容器打成tar cat panchip.tar | docker import - panchip tar转镜像Q1 Failed to start Docker Application Container Engine的解决办法 vim /etc/docker/daemon.json vim /usr/lib/systemd/system/docker.service]]></content>
      <categories>
        <category>持续集成</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitLab+jenkins+docker]]></title>
    <url>%2F2019%2F08%2F20%2FGitLab-jenkins-docker%2F</url>
    <content type="text"><![CDATA[GitLab+jenkins+docker自动发布 http://192.168.2.5:181/view/all/newJob 构建一个maven项目ht-micro-record-service-note-provider添加jenkins主机公钥到gitlab，并生成全局凭据1.Username with password root/1234562.SSH Username with private key Enter Directly,添加gitlab服务器私钥 parent.relativePath修改为，发布单个服务时设定一个空值将始终从仓库中获取，不从本地路径获取 vim /usr/lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix://var/run/docker.sock 基础服务&lt;build&gt; &lt;finalName&gt;ht-micro-record-commons-domain&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;classifier&gt;exec&lt;/classifier&gt; &lt;!--被依赖的包加该配置--&gt; &lt;mainClass&gt;com.ht.micro.record.commons.CommonsMapperApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 微服务提供者 &lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;name&gt;Nexus Release Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;name&gt;Nexus Snapshot Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt; &lt;build&gt; &lt;finalName&gt;ht-micro-record-service-note-consumer&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;!--被其他服务引用必须增加该配置--&gt; &lt;classifier&gt;exec&lt;/classifier&gt; &lt;mainClass&gt;com.ht.micro.record.service.consumer.NoteConsumerServiceApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- docker的maven插件，官网 https://github.com/spotify/docker-maven-plugin --&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt; &lt;version&gt;0.4.13&lt;/version&gt; &lt;configuration&gt; &lt;imageName&gt;192.168.2.5:5000/${project.artifactId}:${project.version}&lt;/imageName&gt; &lt;baseImage&gt;jdk1.8&lt;/baseImage&gt; &lt;entryPoint&gt;[&quot;java&quot;, &quot;-jar&quot;,&quot;/${project.build.finalName}.jar&quot;]&lt;/entryPoint&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/&lt;/targetPath&gt; &lt;directory&gt;${project.build.directory}&lt;/directory&gt; &lt;include&gt;${project.build.finalName}.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;dockerHost&gt;http://192.168.2.5:2375&lt;/dockerHost&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; Jenkinsexport app_name=”ht-micro-record-service-note-consumer”export app_version=”1.0.0-SNAPSHOT”sh /usr/local/docker/ht-micro-record/deploy.sh #!/bin/bash # 判断项目是否在运行 DATE=`date +%s` last_app_name=`echo &quot;$app_name-expire-$DATE&quot;` if docker ps | grep $app_name;then docker stop $app_name docker rename $app_name $last_app_name docker run -di --name=$app_name -v /opt/template/:/opt/template/ -e JAVA_OPTS=&#39;-Xmx3g -Xms3g&#39; --net=host 192.168.2.7:5000/$app_name:$app_version # 判断项目是否存在 elif docker ps -a | grep $app_name;then docker start $app_name else docker run -di --name=$app_name -v /opt/template/:/opt/template/ -e JAVA_OPTS=&#39;-Xmx3g -Xms3g&#39; --net=host 192.168.2.7:5000/$app_name:$app_version fi vim /usr/local/bin/dokill docker images | grep -e $*|awk &#39;{print $3}&#39;|sed &#39;2p&#39;|xargs docker rmi chmod a+x /usr/local/bin/dokill dokill tensquare_recruit Build时必须clean，项目名必须小写 Q1:Failed to execute goal on project : Could not resolve dependencies for 对最父级项目clean install，再最子项目clean install Q2: repackage failed: Unable to find main class -&gt; [Help 1] 构建显示缺少主类 @SpringBootApplication public class CommonsApplication { public static void main(String[] args) { } }分布式构建192.168.2.7:181 访问jenkins，系统管理-节点管理-新建节点ln -s /data2/jdk/bin/java /usr/bin/java 分布式部署通过网关负载时，需要重新部署网关gateway服务。 免密登陆192.168.2.7 ssh-keygen -t rsa ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.2.5 ssh root@192.168.2.5 新建jenkins任务192.168.2.7ht-micro-record-service-user-2.5vim /data2/deploy2.5.sh #!/bin/bash echo &quot;------------------服务信息---------------&quot; echo $1 $2 echo &quot;------------------开始部署---------------&quot; ssh root@192.168.2.5 sh /data2/deploy.sh $1 $2 192.168.2.5vim /data2/deploy.sh #!/bin/bash # 判断项目是否在运行 if docker ps | grep $1;then docker stop $1|xargs docker rm docker rmi 192.168.2.7:5000/$1:$2 # 判断项目是否存在 elif docker ps -a | grep $1;then docker rm $1 docker rmi 192.168.2.7:5000/$1:$2 elif docker images|grep $1;then docker rmi 192.168.2.7:5000/$1:$2 fi docker run -di --name=$1 -v /opt/:/opt/ -e JAVA_OPTS=&#39;-Xmx3g -Xms3g&#39; --net=host 192.168.2.7:5000/$1:$2]]></content>
      <categories>
        <category>持续集成</category>
      </categories>
      <tags>
        <tag>gitlab</tag>
        <tag>jenkins</tag>
        <tag>docker</tag>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch]]></title>
    <url>%2F2019%2F08%2F20%2FElasticSearch%2F</url>
    <content type="text"><![CDATA[ElasticSearch基础及数据迁移 https://github.com/medcl/esm-abandoned/releases/download/v0.4.1/linux64-esm--2019.4.20.tar.gz 基本原理 写：发送一个请求到协调节点，对document进行hash路由到对应由primary shard的节点上，处理请求并同步到replica shard，协调节点检测同步后返回相应给客户端。读：发送一个请求到协调节点，根据doc id对document进行hash路由到对应node，通过随机轮询算法从primary和replica的shard中随机选择让读请求负载均衡，返回document给协调节点后给客户端。一个索引拆分为多个shard存储部分数据，每个shard由primary shard和replica shard组成，primary写入将同步到replica，类似kafka的partition副本。保证高可用。Es多个节点选举一个为master，管理和切换主副shard，若宕机则重新选举，并将宕机节点primary shard身份转移到其他机器的replica shard。重启将修改原primary为replica同步数据。 每个在文档上执行的写操作，包括删除，都会使其版本增加。 真正的删除时机： deleting a document doesn’t immediately remove the document from disk; it just marks it as deleted. Elasticsearch will clean up deleted documents in the background as you continue to index more data. 删除索引是会立即释放空间的，不存在所谓的“标记”逻辑。 删除文档的时候，是将新文档写入，同时将旧文档标记为已删除。 磁盘空间是否释放取决于新旧文档是否在同一个segment file里面，因此ES后台的segment merge在合并segment file的过程中有可能触发旧文档的物理删除。但因为一个shard可能会有上百个segment file，还是有很大几率新旧文档存在于不同的segment里而无法物理删除。想要手动释放空间，只能是定期做一下force merge，并且将max_num_segments设置为1。多机器集群搭建 vi /etc/sysctl.conf vm.max_map_count=262144 sysctl -pNode1 192.168.2.5最好挂载到大磁盘上 mkdir /usr/local/docker/ElasticSearch/data -p &amp;&amp; chmod 777 /usr/local/docker/ElasticSearch/data mkdir /usr/local/docker/ElasticSearch/config/ -p &amp;&amp; cd /usr/local/docker/ElasticSearch/config/ vi es.yml cluster.name: elasticsearch-cluster node.name: es-node1 network.bind_host: 0.0.0.0 network.publish_host: 192.168.2.5 http.port: 1800 transport.tcp.port: 1801 http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; node.master: true node.data: true discovery.zen.ping.unicast.hosts: [&quot;192.168.2.5:1801&quot;,&quot;192.168.2.6:1801&quot;] # 可配置多个 discovery.zen.minimum_master_nodes: 1 修改/usr/share/elasticsearch/config/jvm.options中-Xms10g -Xmx10g docker run -d -v /data3/elasticsearch/config/jvm.options:/usr/share/elasticsearch/config/jvm.options -v /data3/elasticsearch/config/es.yml:/usr/share/elasticsearch/config/elasticsearch.yml -v /data3/elasticsearch/data:/usr/share/elasticsearch/data --name es-node1 --net host --privileged elasticsearch:6.6.0 docker exec -it es-node1 bash elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.6.0/elasticsearch-analysis-ik-6.6.0.zip docker restart es-node1Node2 192.168.2.6mkdir /usr/local/docker/ElasticSearch/data -p &amp;&amp; chmod 777 /usr/local/docker/ElasticSearch/data mkdir /usr/local/docker/ElasticSearch/config/ -p &amp;&amp; cd /usr/local/docker/ElasticSearch/config/ vi es.yml cluster.name: elasticsearch-cluster node.name: es-node2 network.bind_host: 0.0.0.0 network.publish_host: 192.168.2.6 http.port: 1800 transport.tcp.port: 1801 http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; node.master: true node.data: true discovery.zen.ping.unicast.hosts: [&quot;192.168.2.5:1801&quot;,&quot;192.168.2.6:1801&quot;] # 可配置多个 discovery.zen.minimum_master_nodes: 1 # 集群最少需要有两个 node , 才能保证既可以不脑裂, 又可以高可用 修改/usr/share/elasticsearch/config/jvm.options中-Xms10g -Xmx10g docker run -d -v /data2/elasticsearch/config/jvm.options:/usr/share/elasticsearch/config/jvm.options -v /data2/elasticsearch/config/es.yml:/usr/share/elasticsearch/config/elasticsearch.yml -v /data3/elasticsearch/data:/usr/share/elasticsearch/data --name es-node2 --net host --privileged elasticsearch:6.6.0 docker exec -it es-node2 bash elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.6.0/elasticsearch-analysis-ik-6.6.0.zip docker restart es-node2http://192.168.2.5:1800/_cat/plugins 查看插件信息 esm数据迁移./esm -s http://192.168.2.6:9200 -d http://192.168.2.7:1800 -x p_answer_handle_alarm -w=5 -b=10 -c 10000; 同步数据 curl 192.168.2.7:1800/_cat/indices?v 查看所有索引信息 常见参数使用： ./esm -s http://192.168.2.6:9200 -d http://192.168.2.7:1800 -x t_record_analyze -y record_test --copy_settings --copy_mappings --shards=4 -w=5 -b=10 -c 10000;数据迁移完美方案通过两集群都建立同样索引，保证mappings和settings一致，再同步数据 curl -XGET 192.168.2.7:1800/p_answer_handle_alarm 查看索引信息 curl -XGET 192.168.2.7:1800/p_answer_handle_alarm/_search; curl -XGET http://192.168.2.5:1800/record_test/_settings?pretty 查看settings信息 PUT http://192.168.2.5:1800/record_set/_settings 修改副本数，片的信息分又重新做了调整，同curl -XPUT &#39;192.168.2.7:1800/record_set/_settings&#39; -d&#39;{&quot;index&quot;:{&quot;number_of_replicas&quot;:1}}&#39; { &quot;index&quot;:{ &quot;number_of_replicas&quot;:1 } } index.blocks.read_only //设置为 true 使索引和索引元数据为只读，false 为允许写入和元数据更改。 index.blocks.read // 设置为 true 可禁用对索引的读取操作 index.blocks.write //设置为 true 可禁用对索引的写入操作。 index.blocks.metadata // 设置为 true 可禁用索引元数据的读取和写入 index.mapping.total_fields.limit //1000 防止字段过多引起崩溃 curl -XGET http://192.168.2.5:1800/record_set/_mappings?pretty 查看mappings信息 http://192.168.2.5:1800/_cluster/settings?pretty 查看settings信息 PUT http:///192.168.2.5:1800/record_set/doc/_mapping 新增字段 { &quot;properties&quot;: { &quot;col1&quot;: { &quot;type&quot;: &quot;text&quot;, &quot;index&quot;: false } } } PUT 192.168.2.6:9200/test 新建索引 { &quot;mappings&quot;:{ &quot;doc&quot;:{ &quot;properties&quot;:{ &quot;jjbh&quot;:{&quot;type&quot;: &quot;keyword&quot;,&quot;index&quot;: &quot;true&quot;}, &quot;bccljg&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;index&quot;:&quot;true&quot;,&quot;analyzer&quot;: &quot;ik_max_word&quot;,&quot;search_analyzer&quot;: &quot;ik_max_word&quot;}, &quot;bjnr&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;index&quot;: &quot;true&quot;,&quot;analyzer&quot;:&quot;ik_max_word&quot;,&quot;search_analyzer&quot;:&quot;ik_max_word&quot;}, &quot;cjlb&quot;:{&quot;type&quot;: &quot;keyword&quot;,&quot;index&quot;: &quot;false&quot;} } } } } 192.168.2.7:1800/test/doc/{_id} 插入数据，_id不加默认随机字符串 { &quot;jjbh&quot;: &quot;4654132465&quot;, &quot;bccljg&quot;: &quot;我是一名合格的程序员&quot;, &quot;bjnr&quot;: &quot;今天天气真的好啊&quot;, &quot;cjlb&quot;: &quot;天上地下飞禽走兽&quot; } 192.168.2.7:1800/test/doc/{_id} 更新 { &quot;jjbh&quot;: &quot;11&quot;, &quot;bccljg&quot;: &quot;我是一名合格的程序员&quot;, &quot;bjnr&quot;: &quot;今天天气真的好啊&quot;, &quot;cjlb&quot;: &quot;天上地下飞禽走兽&quot; } PUT 192.168.2.5:1800/test 新建索引 { &quot;mappings&quot;:{ &quot;doc&quot;:{ &quot;properties&quot;:{ &quot;jjbh&quot;:{&quot;type&quot;: &quot;keyword&quot;,&quot;index&quot;: &quot;true&quot;}, &quot;bccljg&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;index&quot;:&quot;true&quot;,&quot;analyzer&quot;: &quot;ik_max_word&quot;,&quot;search_analyzer&quot;: &quot;ik_max_word&quot;}, &quot;bjnr&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;index&quot;: &quot;true&quot;,&quot;analyzer&quot;:&quot;ik_max_word&quot;,&quot;search_analyzer&quot;:&quot;ik_max_word&quot;}, &quot;cjlb&quot;:{&quot;type&quot;: &quot;keyword&quot;,&quot;index&quot;: &quot;false&quot;} } } } } ./esm -s http://192.168.2.6:9200 -d http://192.168.2.5:1800 -x test -y test -w=5 -b=10 -c 10000; 同步数据 POST 192.168.2.5:1800/test/doc 全局加ik分词器 { &quot;settings&quot;:{ &quot;analysis&quot;:{ &quot;analyzer&quot;:{ &quot;ik&quot;:{&quot;tokenizer&quot;: &quot;ik_max_keyword&quot;} } } } } POST 192.168.2.5:1800/test/doc/_search 查询 POST 192.168.2.5:1800/test/doc/_delete_by_query 删除 精确查询/删除 { &quot;query&quot;:{ &quot;term&quot;:{ &quot;bccljg.keyword&quot;:&quot;我是一名合格的程序员&quot; } } } 模糊查询/删除 { &quot;query&quot;: { &quot;match&quot;: { &quot;bccljg&quot;: &quot;合格&quot; } } } 正则模糊查询 { &quot;query&quot;: { &quot;regexp&quot;: { &quot;bccljg.keyword&quot;: &quot;.*我是.*&quot; } } } 文本开头查询 { &quot;query&quot;: { &quot;prefix&quot;: { &quot;bccljg.keyword&quot;: &quot;我是&quot; } } } Q1:若数据结果不一致，可能是磁盘不足。Q2:”caused_by”:{“type”:”search_context_missing_exception”,”reason”:”No search context found for id [69218326]”}} ./esm -s http://192.168.2.6:9200 -d http://192.168.2.7:1800 -x t_record_analyze -y t_record_analyze -w=5 -b=10 -c 10000; ./esm -s http://192.168.2.6:9200 -d http://192.168.2.7:1800 -x t_neo4j_data -y t_neo4j_data -w=5 -b=10 -c 10000; ./esm -s http://192.168.2.6:9200 -d http://192.168.2.7:1800 -x t_data_analysis -y t_data_analysis -w=5 -b=10 -c 10000; ./esm -s http://192.168.2.6:9200 -d http://192.168.2.7:1800 -x t_cjjxq -y t_cjjxq -w=5 -b=10 -c 10000; ./esm -s http://192.168.1.225:9200 -d http://192.168.2.7:1800 -x t_alarm_analysis_result -y t_alarm_analysis_result -w=5 -b=10 -c 10000; ./esm -s http://192.168.2.6:9200 -d http://192.168.2.7:1800 -x p_answer_handle_alarm -y p_answer_handle_alarm -t=10m -w=5 -b=10 -c 5000; ./esm -s http://192.168.2.6:9200 -d http://192.168.2.7:1800 -x city_answer_handle_alarm -y city_answer_handle_alarm -t=10m -w=5 -b=10 -c 5000;scroll time 超时，设置-t参数，默认是1m 同步后的数据结构 logstashES单节点安装docker run -di --name=tensquare_es -p 9200:9200 -p 9300:9300 --privileged docker.io/elasticsearch:6.6.0 docker cp tensquare_es:/usr/share/elasticsearch/config/elasticsearch.yml /usr/share/elasticsearch.yml docker stop tensquare_es docker rm tensquare_es vim /usr/share/elasticsearch.yml transport.host: 0.0.0.0 docker restart tensquare_es vim /etc/security/limits.conf * soft nofile 65536 * hard nofile 65536 nofile是单个进程允许打开的最大文件个数 soft nofile 是软限制 hard nofile是硬限制 vim /etc/sysctl.conf vm.max_map_count=655360 sysctl -p 修改内核参数立马生效 我们需要以文件挂载的 方式创建容器才行，这样我们就可以通过修改宿主机中的某个文件来实现对容器内配置 文件的修改 docker run -di --name=tensquare_es -p 9200:9200 -p 9300:9300 --privileged -v /usr/share/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml docker.io/elasticsearch:6.6.0 docker restart tensquare_es 才可以远程连接使用 同步mysqlhttps://www.elastic.co/cn/downloads/past-releases/logstash-6-6-0tar zxf logstash-6.6.0.tar.gz &amp;&amp; cd /root/logstash-6.6.0/binmkdir mysql &amp;&amp; cd mysql &amp;&amp; vim mysql.conf jdbc { # mysql jdbc connection string to our backup databse jdbc_connection_string =&gt; &quot;jdbc:mysql://192.168.2.5:185/ht_micro_record?characterEncoding=UTF8&quot; # the user we wish to excute our statement as jdbc_user =&gt; &quot;root&quot; jdbc_password =&gt; &quot;123456&quot; # the path to our downloaded jdbc driver jdbc_driver_library =&gt; &quot;/root/logstash-6.6.0/bin/mysql/mysql-connector-java-5.1.47.jar&quot; # the name of the driver class for mysql jdbc_driver_class =&gt; &quot;com.mysql.jdbc.Driver&quot; jdbc_paging_enabled =&gt; &quot;true&quot; jdbc_page_size =&gt; &quot;500000&quot; #以下对应着要执行的sql的绝对路径。 #statement_filepath =&gt; &quot;select id,title,content from tb_article&quot; statement =&gt; &quot;SELECT id,applyer_police_num,applyer_name FROM t_apply&quot; #定时字段 各字段含义（由左至右）分、时、天、月、年，全部为*默认含义为每分钟都更新（测试结果，不同的话请留言指出） schedule =&gt; &quot;* * * * *&quot; } } output { elasticsearch { #ESIP地址与端口 hosts =&gt; &quot;192.168.3.224:9200&quot; #ES索引名称（自己定义的） index =&gt; &quot;t_apply&quot; #自增ID编号 document_id =&gt; &quot;%{id}&quot; document_type =&gt; &quot;article&quot; } stdout { #以JSON格式输出 codec =&gt; json_lines } } nohup ./logstash -f mysql/mysql.conf &amp; 同步es/root/logstash-6.6.0/bin/es/es.conf input { elasticsearch { hosts =&gt; [&quot;192.168.2.5:1800&quot;,&quot;192.168.2.7:1800&quot;] index =&gt; &quot;t_neo4j_data&quot; size =&gt; 1000 scroll =&gt; &quot;1m&quot; codec =&gt; &quot;json&quot; docinfo =&gt; true } } filter { mutate { remove_field =&gt; [&quot;@timestamp&quot;, &quot;@version&quot;] } } output { elasticsearch { hosts =&gt; [&quot;192.168.3.224:9200&quot;] index =&gt; &quot;%{[@metadata][_index]}&quot; } stdout { codec =&gt; rubydebug { metadata =&gt; true } } } ./logstash -f es/es.conf –path.data ../logs/ elasticdumpwget https://nodejs.org/dist/v8.11.2/node-v8.11.2-linux-x64.tar.xz tar xf node-v8.11.2-linux-x64.tar.xz -C /usr/local/ ln -s /usr/local/node-v8.11.2-linux-x64/bin/npm /usr/local/bin/npm ln -s /usr/local/node-v8.11.2-linux-x64/bin/node /usr/local/bin/node npm install -g cnpm --registry=https://registry.npm.taobao.org ln -s /usr/local/node-v8.11.2-linux-x64/bin/cnpm /usr/local/bin/cnpm cnpm init -f cnpm install elasticdump cd node_modules/elasticdump/bin ./elasticdump \ --input=http://192.168.2.6:9200/t_record_analyze \ --output=http://192.168.3.224:9200/t_record_analyze \ --type=analyzer ./elasticdump \ --input=http://192.168.2.6:9200/t_record_analyze \ --output=http://192.168.3.224:9200/t_record_analyze \ --type=mapping ./elasticdump \ --input=http://192.168.2.6:9200/t_record_analyze \ --output=http://192.168.3.224:9200/t_record_analyze \ --type=data ./elasticdump \ --input=http://192.168.2.6:9200:9200/t_record_analyze \ --output=data.json \ --searchBody &#39;{&quot;query&quot;:{&quot;term&quot;:{&quot;username&quot;: &quot;admin&quot;}}} ./elasticdump \ --input=./data.json \ --output=http://192.168.2.7:1800整合springboot父pom &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.2.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt;子pom &lt;!--elasticsearch--&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.java.dev.jna&lt;/groupId&gt; &lt;artifactId&gt;jna&lt;/artifactId&gt; &lt;!-- &lt;version&gt;3.0.9&lt;/version&gt; --&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt; &lt;artifactId&gt;gson&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt;配置文件bootstrap.yml es: nodes: 192.168.2.5:1800,192.168.2.7:1800 host: 192.168.2.6,192.168.2.7 port: 1801,1801 types: doc clusterName: elasticsearch-cluster #笔录分析库 blDbName: t_record_analyze #接警信息库 jjDbName: p_answer_alarm #处警信息库 cjDbName: p_handle_alarm #警情分析结果信息库 jqfxDbName: t_alarm_analysis_result #标准化分析 cjjxqDbName: t_cjjxq #接处警整合库 jcjDbName: p_answer_handle_alarm #警情分析 daDbName: t_data_analysis #警情结果表(neo4j使用) neo4jData: t_neo4j_data #市局接处警表 cityDbName: city_answer_handle_alarm配置类com/ht/micro/record/service/dubbo/provider/utils/EsConfig.java @Service public class EsConfig { @Value(&quot;${es.nodes}&quot;) private String nodes; @Value(&quot;${es.host}&quot;) private String host; @Value(&quot;${es.port}&quot;) private String port; @Value(&quot;${es.blDbName}&quot;) private String blDbName; @Value(&quot;${es.jjDbName}&quot;) private String jjDbName; @Value(&quot;${es.cjDbName}&quot;) private String cjDbName; @Value(&quot;${es.jqfxDbName}&quot;) private String jqfxDbName; @Value(&quot;${es.clusterName}&quot;) private String clusterName; @Value(&quot;${es.jjDbName}&quot;) private String answerDbName; @Value(&quot;${es.cjDbName}&quot;) private String handleDbName; @Value(&quot;${es.cjjxqDbName}&quot;) private String cjjxqDbName; @Value(&quot;${es.jcjDbName}&quot;) private String jcjDbName; @Value(&quot;${es.daDbName}&quot;) private String daDbName; @Value(&quot;${es.daDbName}&quot;) private String fxDbName; @Value(&quot;${es.types}&quot;) private String types; @Value(&quot;${es.neo4jData}&quot;) private String neo4jData; @Value(&quot;${es.cityDbName}&quot;) private String cityDbName; } 配置类 com/ht/micro/record/service/dubbo/provider/utils/ElasticClientSingleton.java @Service public class ElasticClientSingleton { protected final Logger logger = LoggerFactory.getLogger(ElasticClientSingleton.class); private AtomicInteger atomicPass = new AtomicInteger(); // 0 未初始化, 1 已初始化 private TransportClient transportClient; private BulkRequestBuilder bulkRequest; public synchronized void init(EsConfig esConfig) { try { String ipArray = esConfig.getHost(); String portArray = esConfig.getPort(); String cluster = esConfig.getClusterName(); Settings settings = Settings.builder() .put(&quot;cluster.name&quot;, cluster) //连接的集群名 .put(&quot;client.transport.ignore_cluster_name&quot;, true) .put(&quot;client.transport.sniff&quot;, false)//如果集群名不对，也能连接 .build(); transportClient = new PreBuiltTransportClient(settings); String[] ips = ipArray.split(&quot;,&quot;); String[] ports = portArray.split(&quot;,&quot;); for (int i = 0; i &lt; ips.length; i++) { transportClient.addTransportAddress(new TransportAddress(InetAddress.getByName(ips[i]), Integer .parseInt(ports[i]))); } atomicPass.set(1); } catch (Exception e) { e.printStackTrace(); logger.error(e.getMessage()); atomicPass.set(0); destroy(); } } public void destroy() { if (transportClient != null) { transportClient.close(); transportClient = null; } } public BulkRequestBuilder getBulkRequest(EsConfig esConfig) { if (atomicPass.get() == 0) { // 初始化 init(esConfig); } bulkRequest = transportClient.prepareBulk(); return bulkRequest; } public TransportClient getTransportClient(EsConfig esConfig) { if (atomicPass.get() == 0) { // 初始化 init(esConfig); } return transportClient; } }配置类 com/ht/micro/record/service/dubbo/provider/utils/ElasticClient.java @Service public class ElasticClient { @Autowired private EsConfig esConfig; private static final Logger logger = LoggerFactory.getLogger(ElasticClient.class); private static BulkRequestBuilder bulkRequest; @Autowired private ElasticClientSingleton elasticClientSingleton; @PostConstruct public void init() { bulkRequest = elasticClientSingleton.getBulkRequest(esConfig); } public static String postRequest(String url, String query) { RestTemplate restTemplate = new RestTemplate(); MediaType type = MediaType.parseMediaType(&quot;application/json; charset=UTF-8&quot;); HttpHeaders headers = new HttpHeaders(); headers.setContentType(type); headers.add(&quot;Accept&quot;, MediaType.APPLICATION_JSON.toString()); HttpEntity&lt;String&gt; formEntity = new HttpEntity&lt;String&gt;(query, headers); String result = restTemplate.postForObject(url, formEntity, String.class); return result; } /** * action 提交操作 */ // public void action() { // int reqSize = bulkRequest.numberOfActions(); // //读不到数据了，默认已经全部读取 // if (reqSize == 0) { // bulkRequest.request().requests().clear(); // } // bulkRequest.setTimeout(new TimeValue(1000 * 60 * 5)); //超时30秒 // BulkResponse bulkResponse = bulkRequest.execute().actionGet(); // //持久化异常 // if (bulkResponse.hasFailures()) { // logger.error(bulkResponse.buildFailureMessage()); // bulkRequest.request().requests().clear(); // } // logger.info(&quot;import over....&quot; + bulkResponse.getItems().length); // } }测试com/ht/micro/record/service/dubbo/provider/controller/ProviderController.java @Autowired ElasticClientSingleton elasticClientSingleton; @Autowired EsConfig esConfig; @GetMapping(&quot;/test&quot;) public void test(){ SearchRequestBuilder srb = elasticClientSingleton.getTransportClient(esConfig).prepareSearch(esConfig.getCityDbName()).setTypes(esConfig.getTypes()); TermsQueryBuilder cjbsBuilder = QueryBuilders.termsQuery(&quot;cjbs&quot;, &quot;4&quot;); SearchResponse weekSearchResponse = srb.setQuery(cjbsBuilder).execute().actionGet(); System.out.println(weekSearchResponse.getHits().getTotalHits()); } ElasticSearch-SQL使用sql操作elasticsearch https://github.com/NLPchina/elasticsearch-sqlhttps://artifacts.elastic.co/maven/org/elasticsearch/client/x-pack-transport/6.6.0/x-pack-transport-6.6.0.jar 在github上查找相关案例，需要相关的jar并没有找到，后去elasticsearch-sql-6.6.0.0.zip插件包中找到elasticsearch-sql-6.6.0.0.jar，启动测试程序 Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/elasticsearch/xpack/client/PreBuiltXPackTransportClient at com.alibaba.druid.pool.ElasticSearchDruidDataSource.createPhysicalConnection(ElasticSearchDruidDataSource.java:686) at com.alibaba.druid.pool.ElasticSearchDruidDataSource.createPhysicalConnection(ElasticSearchDruidDataSource.java:632) at com.alibaba.druid.pool.ElasticSearchDruidDataSource.init(ElasticSearchDruidDataSource.java:579) at com.alibaba.druid.pool.ElasticSearchDruidDataSource.getConnection(ElasticSearchDruidDataSource.java:930) at com.alibaba.druid.pool.ElasticSearchDruidDataSource.getConnection(ElasticSearchDruidDataSource.java:926) at com.ht.micro.record.service.dubbo.provider.controller.Test.main(Test.java:20) Caused by: java.lang.ClassNotFoundException: org.elasticsearch.xpack.client.PreBuiltXPackTransportClient at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 6 more x-pack-transportx-pack-core后继续报错，陆续加入x-pack-core，unboundid-ldapsdk，bcpkix-jdk15on，bcprov-jdk15on等依赖才跑通流程。 安装elasticsearch-sqldocker exec -it es-node1 bash elasticsearch-plugin install https://github.com/NLPchina/elasticsearch-sql/releases/download/6.6.0.0/elasticsearch-sql-6.6.0.0.zip docker restart es-node1 本地安装jarmvn install:install-file -DgroupId=org.elasticsearch.plugin -DartifactId=x-pack-core -Dversion=6.6.0 -Dpackaging=jar -Dfile=x-pack-core-6.6.0.jarmvn install:install-file -DgroupId=org.elasticsearch.client -DartifactId=x-pack-transport -Dversion=6.6.0 -Dpackaging=jar -Dfile=x-pack-transport-6.6.0.jarmvn install:install-file -DgroupId=org.nlpcn -DartifactId=elasticsearch-sql -Dversion=6.6.0.0 -Dpackaging=jar -Dfile=elasticsearch-sql-6.6.0.0.jar 引入依赖&lt;dependency&gt; &lt;groupId&gt;org.nlpcn&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-sql&lt;/artifactId&gt; &lt;version&gt;6.6.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.15&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-client&lt;/artifactId&gt; &lt;version&gt;6.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;6.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;transport&lt;/artifactId&gt; &lt;version&gt;6.6.0&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.elasticsearch.plugin&lt;/groupId&gt; &lt;artifactId&gt;transport-netty4-client&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;artifactId&gt;elasticsearch-rest-client&lt;/artifactId&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.plugin&lt;/groupId&gt; &lt;artifactId&gt;transport-netty4-client&lt;/artifactId&gt; &lt;version&gt;6.6.0&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.38&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.unboundid&lt;/groupId&gt; &lt;artifactId&gt;unboundid-ldapsdk&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.bouncycastle&lt;/groupId&gt; &lt;artifactId&gt;bcprov-jdk15on&lt;/artifactId&gt; &lt;version&gt;1.58&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.bouncycastle&lt;/groupId&gt; &lt;artifactId&gt;bcpkix-jdk15on&lt;/artifactId&gt; &lt;version&gt;1.58&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;x-pack-transport&lt;/artifactId&gt; &lt;version&gt;6.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.plugin&lt;/groupId&gt; &lt;artifactId&gt;x-pack-core&lt;/artifactId&gt; &lt;version&gt;6.6.0&lt;/version&gt; &lt;/dependency&gt; 测试@RunWith(SpringRunner.class) @SpringBootTest(classes={DubboProviderApplication .class})// 指定启动类 public class ElasticSearchSql { @Test public void testselect() throws Exception { Properties properties = new Properties(); properties.put(&quot;url&quot;, &quot;jdbc:elasticsearch://192.168.2.7:1801,192.168.2.5:1801/&quot;); properties.put(PROP_CONNECTIONPROPERTIES, &quot;client.transport.ignore_cluster_name=true&quot;); DruidDataSource dds = (DruidDataSource) ElasticSearchDruidDataSourceFactory.createDataSource(properties); dds.setInitialSize(1); Connection connection = dds.getConnection(); String sql2 = &quot;select * FROM t_word_freq limit 10&quot;; PreparedStatement ps = connection.prepareStatement(sql2); ResultSet resultSet = ps.executeQuery(); while (resultSet.next()) { //sql对应输出 System.out.println(resultSet.getString(&quot;jjbh&quot;) ); } ps.close(); connection.close(); dds.close(); } } 由于springboot启动时自动加载druid autoconfigration,而低版本druid配合springboot2.x报错ClassNotFoundException Log4j2Filter，需要手动exclude druid包 &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-commons-service&lt;/artifactId&gt; &lt;version&gt;${project.parent.version}&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.16&lt;/version&gt; &lt;/dependency&gt; Q1:重启服务 停止所有index服务 执行curl -XPUT $url/_cluster/settings?pretty -d ‘{“transient” : {“cluster.routing.allocation.enable” : “none”}}’ 执行curl -XPOST $url/_flush/synced?pretty 重启ES集群 等待集群分片全部分配成功，执行curl -XPUT $url/_cluster/settings?pretty -d ‘{“transient” : {“cluster.routing.allocation.enable” : “all”}}’ 开启所有index服务 PUT http://192.168.2.7:1800/_cluster/settings 禁止分片分配。这一步阻止 Elasticsearch 再平衡缺失的分片，直到你告诉它可以进行了。 { &quot;transient&quot; : { &quot;cluster.routing.allocation.enable&quot; : &quot;none&quot; } }启动完毕后PUT http://192.168.2.7:1800/_cluster/settings 重启分片分配 { &quot;transient&quot; : { &quot;cluster.routing.allocation.enable&quot; : &quot;all&quot; } }Q2Elasticsearch默认安装后设置的内存是1GB，这是远远不够用于生产环境的。有两种方式修改Elasticsearch的堆内存： 设置环境变量：export ES_HEAP_SIZE=10g 在es启动时会读取该变量； 启动时作为参数传递给es： ./bin/elasticsearch -Xmx10g -Xms10gQ3 operations are blocked on license expiration. All data operations (read and curl -XPOST -u elastic:changeme &#39;http://192.168.2.5:1800/_xpack/license/start_basic?acknowledge=true&#39; -H &quot;Content-Type: application/json&quot; -d @one-jane-b9ac97b5-0b80-4d0d-9f1d-363b6fb3ce3c-v5.json]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>springboot</tag>
        <tag>elastisearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-Cluster]]></title>
    <url>%2F2019%2F08%2F20%2FRedis-Cluster%2F</url>
    <content type="text"><![CDATA[redis-cluster集群搭建使用 支撑多个master node，每个master挂载多个slave，master写对应slave读，每个master都有slave节点，若挂掉则将某个slave转为master。与相比Sentinel（哨兵）实现的高可用，集群（cluster）更多的是强调数据的分片或者是节点的伸缩性，如果在集群的主节点上加入对应的从节点，集群还可以自动故障转移。主从复制： 通过把这个RDB文件或AOF文件传给slave服务器，slave服务器重新加载RDB文件，来实现复制的功能！当建立一个从服务器后，从服务器会想主服务器发送一个SYNC的命令，主服务器接收到SYNC命令之后会执行BGSAVE，然后保存到RDB文件，然后发送到从服务器！收到RDB文件然后就载入到内存！ Redis 集群中内置了 16384 个哈希槽，当需要在 Redis 集群中放置一个 key-value 时，redis 先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点 集群中所有master参与,如果半数以上master节点与master节点通信超过(cluster-node-timeout),认为当前master节点挂掉.如果集群任意master挂掉,且当前master没有slave.集群进入fail状态,也可以理解成集群的slot映射[0-16383]不完成时进入fail状态如果集群超过半数以上master挂掉，无论是否有slave集群进入fail状态. Sentinel和Cluster区别 Redis-Sentinel(哨兵模式)是Redis官方推荐的高可用性(HA)解决方案，当用Redis做Master-slave的高可用方案时，假如master宕机了，Redis本身(包括它的很多客户端)都没有实现自动进行主备切换，而Redis-sentinel本身也是一个独立运行的进程，它能监控多个master-slave集群，发现master宕机后能进行自动切换。 Redis-Cluster当遇到单机内存、并发、流量等瓶颈时，可以采用Cluster架构达到负载均衡的目的。分布式集群首要解决把整个数据集按照分区规则映射到多个节点的问题，即把数据集划分到多个节点上，每个节点负责整个数据的一个子集。 多机集群redis-cluster，也叫分布式redis集群，可以有多个master，数据分片分布在这些master上。systemctl stop firewalld.servicesystemctl disable firewalld.service 192.168.2.5vim /usr/local/docker/redis/docker-compose.yml version: &#39;3&#39; services: redis1: image: publicisworldwide/redis-cluster container_name: redis1 network_mode: host restart: always volumes: - ./8001/data:/data environment: - REDIS_PORT=8001 redis2: image: publicisworldwide/redis-cluster container_name: redis2 network_mode: host restart: always volumes: - ./8002/data:/data environment: - REDIS_PORT=8002 redis3: image: publicisworldwide/redis-cluster container_name: redis3 network_mode: host restart: always volumes: - ./8003/data:/data environment: - REDIS_PORT=8003docker-compose up -d 192.168.2.7vim /usr/local/docker/redis/docker-compose.yml version: &#39;3&#39; services: redis1: image: publicisworldwide/redis-cluster network_mode: host container_name: redis4 restart: always volumes: - ./8004/data:/data environment: - REDIS_PORT=8004 redis2: image: publicisworldwide/redis-cluster network_mode: host container_name: redis5 restart: always volumes: - ./8005/data:/data environment: - REDIS_PORT=8005 redis3: image: publicisworldwide/redis-cluster network_mode: host container_name: redis6 restart: always volumes: - ./8006/data:/data environment: - REDIS_PORT=8006docker-compose up -d 启动方式直接启动docker run --rm -it inem0o/redis-trib create --replicas 1 192.168.2.5:8001 192.168.2.5:8002 192.168.2.5:8003 192.168.2.7:8004 192.168.2.7:8005 192.168.2.7:8006 docker exec -it redis1 redis-cli -h 127.0.0.1 -p 8001 -c set a 100 cluster info cluster nodes 自动指定master slave 指定masterdocker run --rm -it inem0o/redis-trib create 192.168.2.5:8001 192.168.2.5:8002 192.168.2.5:8003 docker run --rm -it inem0o/redis-trib add-node --slave --master-id 84c3b7ecbc4933e1368a6927f26c79ecc76810b3 192.168.2.7:8004 192.168.2.5:8001 docker run --rm -it inem0o/redis-trib add-node --slave --master-id 716f11f2971e9494183937abd61f7a4baf0b3959 192.168.2.7:8005 192.168.2.5:8002 docker run --rm -it inem0o/redis-trib add-node --slave --master-id c93060613a8f1531c82b97d97eeac402048f0b25 192.168.2.7:8006 192.168.2.5:8003 docker run --rm -it inem0o/redis-trib info 192.168.2.5:8001 docker run --rm -it inem0o/redis-trib help 若同一台宿主机，不想使用host模式同一台，也可以把network_mode去掉，但就要加ports映射。redis-cluster的节点端口共分为2种，一种是节点提供服务的端口，如6379；一种是节点间通信的端口，固定格式为：10000+6379。 docker-compose.yml version: &#39;3&#39; services: redis1: image: publicisworldwide/redis-cluster restart: always volumes: - /app/app/redis/8001/data:/data environment: - REDIS_PORT=8001 ports: - &#39;8001:8001&#39; - &#39;18001:18001&#39; redis2: image: publicisworldwide/redis-cluster restart: always volumes: - /app/app/redis/8002/data:/data environment: - REDIS_PORT=8002 ports: - &#39;8002:8002&#39; - &#39;18002:18002&#39; redis3: image: publicisworldwide/redis-cluster restart: always volumes: - /app/app/redis/8003/data:/data environment: - REDIS_PORT=8003 ports: - &#39;8003:8003&#39; - &#39;18003:18003&#39; redis4: image: publicisworldwide/redis-cluster restart: always volumes: - /app/app/redis/8004/data:/data environment: - REDIS_PORT=8004 ports: - &#39;8004:8004&#39; - &#39;18004:18004&#39; redis5: image: publicisworldwide/redis-cluster restart: always volumes: - /app/app/redis/8005/data:/data environment: - REDIS_PORT=8005 ports: - &#39;8005:8005&#39; - &#39;18005:18005&#39; redis6: image: publicisworldwide/redis-cluster restart: always volumes: - /app/app/redis/8006/data:/data environment: - REDIS_PORT=8006 ports: - &#39;8006:8006&#39; - &#39;18006:18006&#39;自定义Redis集群制作redis镜像vim entrypoint.sh #!/bin/sh #只作用于当前进程,不作用于其创建的子进程 set -e #$0--Shell本身的文件名 $1--第一个参数 $@--所有参数列表 # allow the container to be started with `--user` if [ &quot;$1&quot; = &#39;redis-server&#39; -a &quot;$(id -u)&quot; = &#39;0&#39; ]; then sed -i &#39;s/REDIS_PORT/&#39;$REDIS_PORT&#39;/g&#39; /usr/local/etc/redis.conf chown -R redis . #改变当前文件所有者 exec gosu redis &quot;$0&quot; &quot;$@&quot; #gosu是sudo轻量级”替代品” fi exec &quot;$@&quot;vim redis.conf #端口 port REDIS_PORT #开启集群 cluster-enabled yes #配置文件 cluster-config-file nodes.conf cluster-node-timeout 5000 #更新操作后进行日志记录 appendonly yes #设置主服务的连接密码 # masterauth #设置从服务的连接密码 # requirepassvi Dockerfile #基础镜像 FROM redis #修复时区 RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime RUN echo &#39;Asia/Shanghai&#39; &gt;/etc/timezone #环境变量 ENV REDIS_PORT 8000 #ENV REDIS_PORT_NODE 18000 #暴露变量 EXPOSE $REDIS_PORT #EXPOSE $REDIS_PORT_NODE #复制 COPY entrypoint.sh /usr/local/bin/ COPY redis.conf /usr/local/etc/ #for config rewrite RUN chmod 777 /usr/local/etc/redis.conf RUN chmod +x /usr/local/bin/entrypoint.sh #入口 ENTRYPOINT [&quot;/usr/local/bin/entrypoint.sh&quot;] #命令 CMD [&quot;redis-server&quot;, &quot;/usr/local/etc/redis.conf&quot;] docker build -t codewj/redis-cluster:1.0 .docker tag codewj/redis-cluster:1.0 192.168.2.5:5000/codewj-redis-clusterdocker push 192.168.2.5:5000/codewj-redis-cluster 192.168.2.5version: &#39;3&#39; services: redis1: image: 192.168.2.7:5000/onejane-redis-cluster container_name: redis1 network_mode: host restart: always volumes: - ./8001/data:/data environment: - REDIS_PORT=8001 redis2: image: 192.168.2.7:5000/onejane-redis-cluster container_name: redis2 network_mode: host restart: always volumes: - ./8002/data:/data environment: - REDIS_PORT=8002 redis3: image: 192.168.2.7:5000/onejane-redis-cluster container_name: redis3 network_mode: host restart: always volumes: - ./8003/data:/data environment: - REDIS_PORT=8003192.168.2.7version: &#39;3&#39; services: redis1: image: 192.168.2.7:5000/onejane-redis-cluster network_mode: host container_name: redis4 restart: always volumes: - ./8004/data:/data environment: - REDIS_PORT=8004 redis2: image: 192.168.2.7:5000/onejane-redis-cluster network_mode: host container_name: redis5 restart: always volumes: - ./8005/data:/data environment: - REDIS_PORT=8005 redis3: image: 192.168.2.7:5000/onejane-redis-cluster network_mode: host container_name: redis6 restart: always volumes: - ./8006/data:/data environment: - REDIS_PORT=8006]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>redis-cluster</tag>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven]]></title>
    <url>%2F2019%2F08%2F20%2Fmaven%2F</url>
    <content type="text"><![CDATA[maven基本用法 一个GroupId（项目）下面可以有很多个ArtifactId（模块），每个ArtifactId（模块）会有很多个Version（版本），每个Version（版本）一般被Packaging（打包）为jar、war、pom中的一种。 父模块的packaging必须为pom,packaging的默认值为jar module“组织”子模块,基于父模块pom的子模块相对目录名，不是子模块的artifactId 多模块deploy的时候，根据子模块的相互依赖关系整理一个build顺序，然后依次build，独立模块之间根据配置顺序build 子pom 会直接继承 父pom 中声明的属性 父pom 中仅仅使用dependencies 中dependency 依赖的包，会被 子pom 直接继承（不需要显式依赖） 父pom 中仅仅使用plugins 中plugin依赖的插件，会被 子pom 直接继承（不需要显式依赖） 父pom中可以使用dependecyManagement和pluginManagement来统一管理jar包和插件pugin，不会被子pom 直接继承。子pom如果希望继承该包或插件，则需要显式依赖，同时像 等配置项可以不被显式写出，默认从父pom继承,同pluginManagement mvn 命令对应着maven项目生命周期，顺序为compile、test、package、install、depoly，执行其中任一周期，都会把前面周期 统一执行 具有依赖传递性，而不具有依赖传递性 安装jar包到本地 mvn install:install-file -DgroupId=org.csource.fastdfs -DartifactId=fastdfs -Dversion=1.2 -Dpackaging=jar -Dfile=D:\Project\pinyougou\fastDFSdemo\src\main\webapp\WEB-INF\lib\fastdfs_client_v1.20.jar properties&lt;properties&gt; &lt;spring.version&gt;2.5&lt;/spring.version&gt; &lt;/properties&gt; &lt;version&gt;${spring.version}&lt;/version&gt; pluginManagement父 &lt;pluginManagement&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.8.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; 子 &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; 由于 Maven 内置了 maven-compiler-plugin 与生命周期的绑定，因此子模块就不再需要任何 maven-compiler-plugin 的配置了。 资源打包配置 &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;${project.build.directory}/classes&lt;/targetPath&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;includes&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;include&gt;**/*.yml&lt;/include&gt; &lt;include&gt;**/*.txt&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt; &lt;/resources&gt; dependencyManagement&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.juvenxu.sample&lt;/groupId&gt; &lt;artifactid&gt;sample-dependency-infrastructure&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; 优势 Parent的dependencyManagement节点的作用是管理整个项目所需要用到的第三方依赖。只要约定了第三方依赖的坐标（GroupId:ArtifactId:version），后代模块即可通过GroupId:ArtifactId进行依赖的引入。其它元素如 version 和 scope 都能通过继承父 POM 的 dependencyManagement 得到这样能够避免依赖版本的冲突。当然，这里只是进行约定，并不会真正地引用依赖。 依赖统一管理(parent中定义，需要变动dependency版本，只要修改一处即可)； 代码简洁(子model只需要指定groupId、artifactId即可) dependencyManagement只会影响现有依赖的配置，但不会引入依赖，即子model不会继承parent中dependencyManagement所有预定义的depandency，只引入需要的依赖即可，简单说就是“按需引入依赖”或者“按需继承”；因此，在parent中严禁直接使用depandencys预定义依赖，坏处是子model会自动继承depandencys中所有预定义依赖；劣势单继承：maven的继承跟java一样，单继承，也就是说子model中只能出现一个parent标签；parent模块中，dependencyManagement中预定义太多的依赖，造成pom文件过长，而且很乱； 通过父pom的parent继承的方法，只能继承一个parent。实际开发中，用户很可能需要继承自己公司的标准parent配置，这个时候可以使用 scope=import 来实现多继承。 解决：scope=import只能用在dependencyManagement里面,且仅用于type=pom的dependency,要继承多个，可以在dependencyManagement通过非继承的方式来引入这段依赖管理配置，添加依赖scope=import，type=pom &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;1.3.3.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.test.sample&lt;/groupId&gt; &lt;artifactid&gt;base-parent1&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;自己的项目里面就不需要继承SpringBoot的module了，而可以继承自己项目的module了。 Scope provided 被依赖项目理论上可以参与编译、测试、运行等阶段，相当于compile，但是再打包阶段做了exclude的动作。,servlet-api和jsp-api都是由tomcat等servlet容器负责提供的包,这个 jar 包已由应用服务器提供,这里引入只用于开发,不进行打包 runtime被依赖项目无需参与项目的编译，但是会参与到项目的测试和运行,在编译的时候我们不需要 JDBC API 的 jar 包，而在运行的时候我们才需要 JDBC 驱动包。 compile（默认）被依赖项目需要参与到当前项目的编译，测试，打包，运行等阶段。打包的时候通常会包含被依赖项目。 test 被依赖项目仅仅参与测试相关的工作，包括测试代码的编译，执行,Junit 测试。 system 被依赖项不会从 maven 仓库中查找，而是从本地系统中获取，systemPath 元素用于制定本地系统中 jar 文件的路径发布 &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;!-- ID 名称必须要与 settings.xml 中 Servers 配置的 ID 名称保持一致。--&gt; &lt;name&gt;Nexus Release Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.5:182/repository/maven-releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;name&gt;Nexus Snapshot Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.5:182/repository/maven-snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt; mvn deploy发布到私服,在项目 pom.xml 中设置的版本号添加 SNAPSHOT 标识的都会发布为 SNAPSHOT 版本，没有 SNAPSHOT 标识的都会发布为 RELEASE 版本Nexus 3.0 不支持页面上传，可使用 maven 命令： mvn deploy:deploy-file -DgroupId=com.github.axet -DartifactId=kaptcha -Dversion=0.0.9 -Dpackaging=jar -Dfile=E:\kaptcha-0.0.9.jar -Durl=http://192.168.2.5:182/repository/maven-releases/ -DrepositoryId=nexus-releases 要求jar的pom中的repository.id和settings.xml中一致 上传第三方jarproxy：即你可以设置代理，设置了代理之后，在你的nexus中找不到的依赖就会去配置的代理的地址中找hosted：你可以上传你自己的项目到这里面group：它可以包含前面两个，是一个聚合体。一般用来给客户一个访问nexus的统一地址。 你可以上传私有的项目到hosted，以及配置proxy以获取第三方的依赖（比如可以配置中央仓库的地址）。前面两个都 弄好了之后，在通过group聚合给客户提供统一的访问地址。 settings.xml&lt;server&gt; &lt;id&gt;3rdParty&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;123456&lt;/password&gt; &lt;/server&gt; &lt;repository&gt; &lt;id&gt;3rdParty&lt;/id&gt; &lt;url&gt;http://192.168.2.7:182/repository/3rdParty/&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/repository&gt; mvn deploy:deploy-file -DgroupId=org.nlpcn -DartifactId=elasticsearch-sql -Dversion=6.6.0.0 -Dpackaging=jar -Dfile=elasticsearch-sql-6.6.0.0.jar -Durl=http://192.168.2.7:182/repository/3rdParty/ -DrepositoryId=3rdParty pom.xml &lt;repository&gt; &lt;id&gt;3rdParty&lt;/id&gt; &lt;name&gt;3rdParty Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.7:182/repository/3rdParty/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; parent 必须放在group Id 上]]></content>
      <categories>
        <category>持续集成</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xxl-job]]></title>
    <url>%2F2019%2F08%2F20%2Fxxl-job%2F</url>
    <content type="text"><![CDATA[springboot整合xxl-job定时器 搭建wget https://github.com/xuxueli/xxl-job/archive/2.1.0.tar.gz tar zxf xxl-job-2.1.0.tar.gz docker cp xxl-job-2.1.0/doc/db/tables_xxl_job.sql ht-mysql-slave:/root/ docker exec -it ht-mysql-slave mysql -u root -p use xxl_job; source /root/tables_xxl_job.sql docker run -d --rm \ -e PARAMS=&quot;--spring.datasource.url=jdbc:mysql://192.168.2.7:186/xxl_job?Unicode=true&amp;characterEncoding=UTF-8 --spring.datasource.username=root --spring.datasource.password=123456&quot; \ -p 183:8080 \ --name xxl-job-admin xuxueli/xxl-job-admin:2.1.0http://192.168.2.7:183/xxl-job-admin/ admin 123456 开发&lt;dependency&gt; &lt;groupId&gt;com.xuxueli&lt;/groupId&gt; &lt;artifactId&gt;xxl-job-core&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt;ht-micro-record-service-job/pom.xml &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-service-job&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-service-job&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.xuxueli&lt;/groupId&gt; &lt;artifactId&gt;xxl-job-core&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt; &lt;artifactId&gt;jsr305&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt; &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rocketmq&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.objenesis&lt;/groupId&gt; &lt;artifactId&gt;objenesis&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;!-- Projects Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-commons-service&lt;/artifactId&gt; &lt;version&gt;${project.parent.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Projects End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;ht.micro.record.service.job.JobServiceApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;application.yaml spring: application: name: ht-micro-record-service-job datasource: druid: url: jdbc:mysql://192.168.2.88:189/ht-micro-record?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false username: root password: 123456 initial-size: 1 min-idle: 1 max-active: 20 test-on-borrow: true driver-class-name: com.mysql.jdbc.Driver cloud: nacos: discovery: server-addr: 192.168.2.5:8848,192.168.2.5:8849,192.168.2.5:8850 config: server-addr: 192.168.2.5:8848,192.168.2.5:8849,192.168.2.5:8850 sentinel: transport: port: 8719 dashboard: 192.168.2.5:190 server: port: 9701 xxl: job: executor: logpath: logs/xxl-job/jobhandler appname: xxl-job-executor port: 9999 logretentiondays: -1 ip: 192.168.3.233 admin: addresses: http://192.168.2.7:183/xxl-job-admin accessTokencom.ht.micro.record.service.job.JobServiceApplication @SpringBootApplication(scanBasePackages = &quot;com.ht.micro.record&quot;) @EnableDiscoveryClient @MapperScan(basePackages = &quot;com.ht.micro.record.commons.mapper&quot;) public class JobServiceApplication { public static void main(String[] args) { SpringApplication.run(JobServiceApplication.class, args); } }com.ht.micro.record.service.job.config.XxlJobConfig @Configuration @ComponentScan(basePackages = &quot;com.ht.micro.record.service.job.handler&quot;) public class XxlJobConfig { private Logger logger = LoggerFactory.getLogger(XxlJobConfig.class); @Value(&quot;${xxl.job.admin.addresses}&quot;) private String adminAddresses; @Value(&quot;${xxl.job.executor.appname}&quot;) private String appName; @Value(&quot;${xxl.job.executor.ip}&quot;) private String ip; @Value(&quot;${xxl.job.executor.port}&quot;) private int port; @Value(&quot;${xxl.job.accessToken}&quot;) private String accessToken; @Value(&quot;${xxl.job.executor.logpath}&quot;) private String logPath; @Value(&quot;${xxl.job.executor.logretentiondays}&quot;) private int logRetentionDays; @Bean(initMethod = &quot;start&quot;, destroyMethod = &quot;destroy&quot;) public XxlJobSpringExecutor xxlJobExecutor() { logger.info(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; xxl-job config init.&quot;); XxlJobSpringExecutor xxlJobSpringExecutor = new XxlJobSpringExecutor(); xxlJobSpringExecutor.setAdminAddresses(adminAddresses); xxlJobSpringExecutor.setAppName(appName); xxlJobSpringExecutor.setIp(ip); xxlJobSpringExecutor.setPort(port); xxlJobSpringExecutor.setAccessToken(accessToken); xxlJobSpringExecutor.setLogPath(logPath); xxlJobSpringExecutor.setLogRetentionDays(logRetentionDays); return xxlJobSpringExecutor; } }com.ht.micro.record.service.job.handler.TestJobHandler @JobHandler(value=&quot;testJobHandler&quot;) @Component public class TestJobHandler extends IJobHandler { @Override public ReturnT&lt;String&gt; execute(String param) throws Exception { XxlJobLogger.log(&quot;XXL-JOB, Hello World.&quot;); for (int i = 0; i &lt; 5; i++) { XxlJobLogger.log(&quot;beat at:&quot; + i); TimeUnit.SECONDS.sleep(2); } return SUCCESS; } }]]></content>
      <categories>
        <category>定时器</category>
      </categories>
      <tags>
        <tag>xxl-job</tag>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot2-multi-druid]]></title>
    <url>%2F2019%2F08%2F20%2Fspringboot2-multi-druid%2F</url>
    <content type="text"><![CDATA[基于springboot2+druid实现多数据源 配置application.yml server: port: 10205 spring: datasource: base: #监控统计拦截的filters filters: stat type: com.alibaba.druid.pool.DruidDataSource jdbc-url: jdbc:mysql://192.168.1.250:3306/htbl_test?useUnicode=true&amp;characterEncoding=UTF-8 username: root password: 123456 driver-class-name: com.mysql.jdbc.Driver max-idle: 10 max-wait: 10000 min-idle: 5 initial-size: 5 validation-query: SELECT 1 test-on-borrow: false test-while-idle: true time-between-eviction-runs-millis: 18800 dic: #监控统计拦截的filters filters: stat type: com.alibaba.druid.pool.DruidDataSource jdbc-url: jdbc:mysql://192.168.1.48:3306/ht_nlp?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false username: root password: Sonar@1234 driver-class-name: com.mysql.jdbc.Driver max-idle: 10 max-wait: 10000 min-idle: 5 initial-size: 5 validation-query: SELECT 1 test-on-borrow: false test-while-idle: true time-between-eviction-runs-millis: 18800数据源1配置BaseMybatisConfig @Configuration @MapperScan(basePackages = {&quot;com.ht.micro.record.commons.mapper.baseMapper&quot;}, sqlSessionTemplateRef = &quot;baseSqlSessionTemplate&quot;) public class BaseMybatisConfig { @Value(&quot;${spring.datasource.base.filters}&quot;) String filters; @Value(&quot;${spring.datasource.base.driver-class-name}&quot;) String driverClassName; @Value(&quot;${spring.datasource.base.username}&quot;) String username; @Value(&quot;${spring.datasource.base.password}&quot;) String password; @Value(&quot;${spring.datasource.base.jdbc-url}&quot;) String url; @Bean(name=&quot;baseDataSource&quot;) @Primary//必须加此注解，不然报错，下一个类则不需要添加 spring.datasource @ConfigurationProperties(prefix=&quot;spring.datasource.base&quot;)//prefix值必须是application.properteis中对应属性的前缀 public DataSource baseDataSource() throws SQLException { DruidDataSource druid = new DruidDataSource(); // 监控统计拦截的filters druid.setFilters(filters); // 配置基本属性 druid.setDriverClassName(driverClassName); druid.setUsername(username); druid.setPassword(password); druid.setUrl(url); return druid; } @Bean(name=&quot;baseSqlSessionFactory&quot;) @Primary public SqlSessionFactory baseSqlSessionFactory(@Qualifier(&quot;baseDataSource&quot;)DataSource dataSource)throws Exception{ // 创建Mybatis的连接会话工厂实例 SqlSessionFactoryBean bean=new SqlSessionFactoryBean(); bean.setDataSource(dataSource);//// 设置数据源bean //添加XML目录 ResourcePatternResolver resolver=new PathMatchingResourcePatternResolver(); try{ bean.setMapperLocations(resolver.getResources(&quot;classpath:baseMapper/*Mapper.xml&quot;));//// 设置mapper文件路径 return bean.getObject(); }catch(Exception e){ e.printStackTrace(); throw new RuntimeException(e); } } @Bean(name=&quot;baseSqlSessionTemplate&quot;) @Primary public SqlSessionTemplate baseSqlSessionTemplate(@Qualifier(&quot;baseSqlSessionFactory&quot;)SqlSessionFactory sqlSessionFactory)throws Exception{ SqlSessionTemplate template=new SqlSessionTemplate(sqlSessionFactory);//使用上面配置的Factory return template; } } 数据源2配置@Configuration @MapperScan(basePackages = {&quot;com.ht.micro.record.commons.mapper.dicMapper&quot;}, sqlSessionTemplateRef = &quot;dicSqlSessionTemplate&quot;) public class DicMybatisConfig { @Value(&quot;${spring.datasource.dic.filters}&quot;) String filters; @Value(&quot;${spring.datasource.dic.driver-class-name}&quot;) String driverClassName; @Value(&quot;${spring.datasource.dic.username}&quot;) String username; @Value(&quot;${spring.datasource.dic.password}&quot;) String password; @Value(&quot;${spring.datasource.dic.jdbc-url}&quot;) String url; @Bean(name = &quot;dicDataSource&quot;) @ConfigurationProperties(prefix=&quot;spring.datasource.dic&quot;) public DataSource dicDataSource() throws SQLException { DruidDataSource druid = new DruidDataSource(); // 监控统计拦截的filters // druid.setFilters(filters); // 配置基本属性 druid.setDriverClassName(driverClassName); druid.setUsername(username); druid.setPassword(password); druid.setUrl(url); /* //初始化时建立物理连接的个数 druid.setInitialSize(initialSize); //最大连接池数量 druid.setMaxActive(maxActive); //最小连接池数量 druid.setMinIdle(minIdle); //获取连接时最大等待时间，单位毫秒。 druid.setMaxWait(maxWait); //间隔多久进行一次检测，检测需要关闭的空闲连接 druid.setTimeBetweenEvictionRunsMillis(timeBetweenEvictionRunsMillis); //一个连接在池中最小生存的时间 druid.setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis); //用来检测连接是否有效的sql druid.setValidationQuery(validationQuery); //建议配置为true，不影响性能，并且保证安全性。 druid.setTestWhileIdle(testWhileIdle); //申请连接时执行validationQuery检测连接是否有效 druid.setTestOnBorrow(testOnBorrow); druid.setTestOnReturn(testOnReturn); //是否缓存preparedStatement，也就是PSCache，oracle设为true，mysql设为false。分库分表较多推荐设置为false druid.setPoolPreparedStatements(poolPreparedStatements); // 打开PSCache时，指定每个连接上PSCache的大小 druid.setMaxPoolPreparedStatementPerConnectionSize(maxPoolPreparedStatementPerConnectionSize); */ return druid; } @Bean(name = &quot;dicSqlSessionFactory&quot;) public SqlSessionFactory dicSqlSessionFactory(@Qualifier(&quot;dicDataSource&quot;)DataSource dataSource)throws Exception{ SqlSessionFactoryBean bean=new SqlSessionFactoryBean(); bean.setDataSource(dataSource); //添加XML目录 ResourcePatternResolver resolver=new PathMatchingResourcePatternResolver(); try{ bean.setMapperLocations(resolver.getResources(&quot;classpath:dicMapper/*Mapper.xml&quot;)); return bean.getObject(); }catch(Exception e){ e.printStackTrace(); throw new RuntimeException(e); } } @Bean(name=&quot;dicSqlSessionTemplate&quot;) public SqlSessionTemplate dicSqlSessionTemplate(@Qualifier(&quot;dicSqlSessionFactory&quot;)SqlSessionFactory sqlSessionFactory)throws Exception{ SqlSessionTemplate template=new SqlSessionTemplate(sqlSessionFactory);//使用上面配置的Factory return template; } }启动类NoteProviderServiceApplication @SpringBootApplication(scanBasePackages = &quot;com.ht.micro.record&quot;) @EnableDiscoveryClient @MapperScan(basePackages = &quot;com.ht.micro.record.commons.mapper&quot;) @EnableSwagger2 public class NoteProviderServiceApplication { public static void main(String[] args) { SpringApplication.run(NoteProviderServiceApplication.class, args); } }测试服务层WebDicController @RestController @RequestMapping(value = &quot;webdic&quot;) public class WebDicController extends AbstractBaseController&lt;TbUser&gt; { @Autowired private WebDicService webDicService; @ApiOperation(value = &quot;获取所有字典&quot;) @RequestMapping public List&lt;WebDic&gt; getAll() { return webDicService.getAll(); } }TApplyController @RestController @RequestMapping(value = &quot;apply&quot;) public class TApplyController extends AbstractBaseController { @Autowired private TApplyService tApplyService; /** http://localhost:10105/apply/asked/任大龙 * @param name * @return */ @ApiImplicitParams({ @ApiImplicitParam(name = &quot;askedName&quot;, value = &quot;被询问人名&quot;, required = true, paramType = &quot;path&quot;) }) @GetMapping(value = &quot;asked/{name}&quot;) public List&lt;TApply&gt; getByAskedName(@PathVariable String name){ return tApplyService.getByAskedName(name); } }WebDicService @Service public class WebDicService { @Autowired WebDicMapper webDicMapper; public List&lt;WebDic&gt; getAll() { return webDicMapper.selectAll(); } }TApplyService public interface TApplyService extends BaseCrudService&lt;TApply&gt; { List&lt;TApply&gt; getByAskedName(String name); }TApplyServiceImpl @Service public class TApplyServiceImpl extends BaseCrudServiceImpl&lt;TApply, TApplyMapper&gt; implements TApplyService { @Autowired private TApplyMapper tApplyMapper; public List&lt;TApply&gt; getByAskedName(String name) { return tApplyMapper.selectByAskedName(name); } }mapper层com.ht.micro.record.commons.mapper.baseMapper.TApplyMapper public interface TApplyMapper extends MyMapper&lt;TApply&gt; { /** * 根据名称查找 * @param name * @return */ List&lt;TApply&gt; selectByAskedName(String name); }com.ht.micro.record.commons.mapper.dicMapper.WebDicMapper public interface WebDicMapper extends MyMapper&lt;WebDic&gt; { }tk.mybatis.mapper.MyMapper public interface MyMapper&lt;T&gt; extends Mapper&lt;T&gt;, MySqlMapper&lt;T&gt; { }baseMapper/TApplyMapper.xml &lt;mapper namespace=&quot;com.ht.micro.record.commons.mapper.baseMapper.TApplyMapper&quot;&gt; &lt;resultMap id=&quot;BaseResultMap&quot; type=&quot;com.ht.micro.record.commons.domain.TApply&quot;&gt; &lt;!-- WARNING - @mbg.generated --&gt; &lt;id column=&quot;id&quot; jdbcType=&quot;INTEGER&quot; property=&quot;id&quot; /&gt; &lt;result column=&quot;applyer_police_num&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;applyerPoliceNum&quot; /&gt; &lt;result column=&quot;applyer_name&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;applyerName&quot; /&gt; &lt;result column=&quot;asked_name&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;askedName&quot; /&gt; &lt;result column=&quot;applyer_id&quot; jdbcType=&quot;INTEGER&quot; property=&quot;applyerId&quot; /&gt; &lt;result column=&quot;unit_id&quot; jdbcType=&quot;INTEGER&quot; property=&quot;unitId&quot; /&gt; &lt;result column=&quot;case_info_ids&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;caseInfoIds&quot; /&gt; &lt;result column=&quot;case_count&quot; jdbcType=&quot;INTEGER&quot; property=&quot;caseCount&quot; /&gt; &lt;result column=&quot;apply_time&quot; jdbcType=&quot;TIMESTAMP&quot; property=&quot;applyTime&quot; /&gt; &lt;result column=&quot;apply_state&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;applyState&quot; /&gt; &lt;result column=&quot;apply_goal&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;applyGoal&quot; /&gt; &lt;result column=&quot;approve_time&quot; jdbcType=&quot;TIMESTAMP&quot; property=&quot;approveTime&quot; /&gt; &lt;result column=&quot;approve_state&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;approveState&quot; /&gt; &lt;result column=&quot;approver_id&quot; jdbcType=&quot;INTEGER&quot; property=&quot;approverId&quot; /&gt; &lt;result column=&quot;approve_prop&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;approveProp&quot; /&gt; &lt;/resultMap&gt; &lt;select id=&quot;selectByAskedName&quot; parameterType=&quot;string&quot; resultMap=&quot;BaseResultMap&quot;&gt; select * from t_apply where asked_name = #{name,jdbcType=VARCHAR} &lt;/select&gt; &lt;/mapper&gt;dicMapper/WebDicMapper.xml &lt;mapper namespace=&quot;com.ht.micro.record.commons.mapper.dicMapper&quot;&gt; &lt;resultMap id=&quot;BaseResultMap&quot; type=&quot;com.ht.micro.record.commons.domain.WebDic&quot;&gt; &lt;!-- WARNING - @mbg.generated --&gt; &lt;id column=&quot;id&quot; jdbcType=&quot;INTEGER&quot; property=&quot;id&quot; /&gt; &lt;result column=&quot;name&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;name&quot; /&gt; &lt;result column=&quot;type&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;type&quot; /&gt; &lt;/resultMap&gt; &lt;/mapper&gt;]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>springboot</tag>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[etcd+overlay+pxc集群搭建]]></title>
    <url>%2F2019%2F08%2F20%2Fetcd-overlay-pxc%2F</url>
    <content type="text"><![CDATA[etcd+overlay+pxc实现基于swarm自定义网络数据库高可用 curl -L https://github.com/coreos/etcd/releases/download/v2.2.1/etcd-v2.2.1-linux-amd64.tar.gz -o etcd-v2.2.1-linux-amd64.tar.gz etcd集群tar xzvf etcd-v2.2.1-linux-amd64.tar.gz &amp;&amp; cd etcd-v2.2.1-linux-amd64 {NODE_NAME}:etcd节点名称，需要和命令中的-initial-cluster的对应的{NODE1_NAME}或{NODE2_NAME}对应 {NODE_IP}/{NODE1_IP}/{NODE2_NAME}：节点的IP ./etcd -name {NODE_NAME} -initial-advertise-peer-urls [http://{NODE_IP}:2380](http://NODE_IP:2380) \ -listen-peer-urls &lt;http://0.0.0.0:2380&gt; \ -listen-client-urls [http://0.0.0.0:2379,http://127.0.0.1:4001](http://0.0.0.0:2379,http:/127.0.0.1:4001) \ -advertise-client-urls &lt;http://0.0.0.0:2379&gt; \ -initial-cluster-token etcd-cluster \ -initial-cluster {NODE1_NAME}=http://{NODE1_IP}:2380,{NODE2_NAME}=http://{NODE2_IP}:2380 \ -initial-cluster-state new 单机tar xzvf etcd-v2.2.1-linux-amd64.tar.gz &amp;&amp; cd etcd-v2.2.1-linux-amd64 nohup ./etcd --advertise-client-urls &#39;http://192.168.3.226:2379&#39; --listen-client-urls &#39;http://0.0.0.0:2379&#39; &amp; ./etcdctl member list 查看启动情况 ./etcdctl mk name OneJane ./etcdctl get name 其他主机 ./etcdctl -endpoint http://192.168.3.226:2379 get name ./etcdctl -endpoint http://192.168.3.226:2379 mk age 22 swarmdocker配置192.168.3.224 vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --cluster-store=etcd://192.168.3.224:2379 --cluster-advertise=192.168.3.224:2375 systemctl daemon-reload service docker start 192.168.3.227 vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --cluster-store=etcd://192.168.3.224:2379 --cluster-advertise=192.168.3.227:2375 systemctl daemon-reload service docker start overlay192.168.3.224 docker swarm init --advertise-addr 192.168.3.224 192.168.3.227 docker swarm join --token SWMTKN-1-4qdkodh0g0c73iw5oehhn4rmsxxxca1cdfushtujspjsn1i827-3x1zyoo4tm4d4qm9x8f4o658q 192.168.3.227:2377 192.168.3.224 docker network create --driver overlay --attachable overnet docker run -itd --name=worker-1 --net=overnet ubuntu docker exec worker-1 apt-get update docker exec worker-1 apt-get install net-tools docker exec worker-1 ifconfig 10.0.0.5 192.168.3.227 docker run -itd --name=worker-2 --net=overnet ubuntu docker exec worker-2 apt-get update docker exec worker-2 apt-get install net-tools docker exec worker-2 apt-get install -y inetutils-ping docker exec worker-2 ifconfig docker exec worker-2 ping 10.0.0.5 PXC多机器集群vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock systemctl daemon-reload service docker start 192.168.3.224 docker swarm init --advertise-addr 192.168.3.224 192.168.3.227 docker swarm join --token SWMTKN-1-4qdkodh0g0c73iw5oehhn4rmsxxxca1cdfushtujspjsn1i827-3x1zyoo4tm4d4qm9x8f4o658q 192.168.3.227:2377 192.168.3.224 docker network create --driver overlay --attachable overnet 192.168.3.224 docker volume create v01 docker run -d \ -p 3306:3306 \ -e MYSQL_ROOT_PASSWORD=123456 \ -e CLUSTER_NAME=PXC \ -e XTRABACKUP_PASSWORD=123456 \ -v v01:/var/lib/mysql \ --privileged \ --name=node1 \ --net=overnet \ percona/percona-xtradb-cluster:5.6 192.168.3.227 docker volume create v02 docker run -d \ -p 3306:3306 \ -e MYSQL_ROOT_PASSWORD=123456 \ -e CLUSTER_NAME=PXC \ -e XTRABACKUP_PASSWORD=123456 \ -e CLUSTER_JOIN=node1 \ -v v02:/var/lib/mysql \ --name=node2 \ --net=overnet \ percona/percona-xtradb-cluster:5.6]]></content>
      <categories>
        <category>高可用</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>pxc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux-partition]]></title>
    <url>%2F2019%2F08%2F20%2Flinux-partition%2F</url>
    <content type="text"><![CDATA[parted实现linux大磁盘分区 已挂载磁盘分区分区需先把磁盘unmount /dev/mapper/centos_ht05-homeQ1:Error: /dev/dm-2: unrecognised disk labelmklabel gpt 转成gpt格式再分区 开始分区du -h –max-depth=1 查看各文件夹大小 parted /dev/dm-2 查看/dev/mapper/centos_ht05-home是l文件属性，链接到dm-2,分区dm-2磁盘 (parted) mkpart Partition name? []? data1 File system type? [ext2]? ext4 Start? 0 End? 3584GB (parted) mkpart Partition name? []? data2 File system type? [ext2]? ext4 Start? 3584GB End? 7168GB (parted) mkpart Partition name? []? data3 File system type? [ext2]? ext4 Start? 7168GB End? -1 (parted) p Model: Linux device-mapper (linear) (dm) Disk /dev/dm-2: 11.9TB Sector size (logical/physical): 512B/512B Partition Table: gpt Disk Flags: Number Start End Size File system Name Flags 1 17.4kB 3584GB 3584GB data1 2 3584GB 7168GB 3584GB data2 3 7168GB 11.9TB 4745GB data3 若分区错误：rm 1 挂载目录mkdir /data1 /data2 /data3 ll -t /dev/dm-* 发现 dm-2 dm-17 dm-28 dm-27最新 mkfs -t ext4 /dev/dm-2 mkfs -t ext4 /dev/dm-17 mkfs -t ext4 /dev/dm-27 mkfs -t ext4 /dev/dm-28 fdisk -l 查看/dev/mapper/centos_ht05-home3 /dev/mapper/centos_ht05-home2 /dev/mapper/centos_ht05-home1生成 mount /dev/mapper/centos_ht05-home1 /data1 mount /dev/mapper/centos_ht05-home2 /data2 mount /dev/mapper/centos_ht05-home3 /data3 开机自动挂载vi /etc/fstab /dev/mapper/centos_ht05-home1 /data1 ext4 defaults 0 0 /dev/mapper/centos_ht05-home2 /data2 ext4 defaults 0 0 /dev/mapper/centos_ht05-home3 /data3 ext4 defaults 0 0 df -hl 查看磁盘分区挂载情况]]></content>
      <categories>
        <category>系统</category>
      </categories>
      <tags>
        <tag>partition</tag>
        <tag>sentinel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis-sentinel]]></title>
    <url>%2F2019%2F08%2F20%2Fredis-sentinel%2F</url>
    <content type="text"><![CDATA[基于redis-sentinel实现redis哨兵集群 redis-sentinel，只有一个master，各实例数据保持一致； 单个redis-sentinel进程来监控redis集群是不可靠的，由于redis-sentinel本身也有single-point-of-failure-problem(单点问题)，当出现问题时整个redis集群系统将无法按照预期的方式切换主从。官方推荐：一个健康的集群部署，至少需要3个Sentinel实例。另外，redis-sentinel只需要配置监控redis master，而集群之间可以通过master相互通信。 首先Sentinel是集群部署的，Client可以链接任何一个Sentinel服务所获的结果都是一致的。其次，所有的Sentinel服务都会对Redis的主从服务进行监控，当监控到Master服务无响应的时候，Sentinel内部进行仲裁，从所有的 Slave选举出一个做为新的Master。并且把其他的slave作为新的Master的Slave。 name ip port redis-master 192.168.2.5 6300 redis-slave1 192.168.2.7 6301 redis-slave2 192.168.2.7 6302 sentinel1 192.168.2.5 26000 sentinel2 192.168.2.7 26001 sentinel3 192.168.2.7 26002 Redis部署在192.168.2.5上运行 docker run -it --name redis-master --network host -d redis --appendonly yes --port 6300 在192.168.2.7上运行 docker run -it --name redis-slave1 --network host -d redis --appendonly yes --port 6301 --slaveof 192.168.2.5 6300 docker run -it --name redis-slave2 --network host -d redis --appendonly yes --port 6302 --slaveof 192.168.2.5 6300 Sentinel部署wget http://download.redis.io/redis-stable/sentinel.conf mkdir /app/{sentine1,sentine2,sentine3}/{data,conf} -p 并复制sentinel1.conf，sentinel2.conf，sentinel3.conf /app├── sentine1│ ├── conf│ └── data├── sentine2│ ├── conf│ └── data└── sentine3 ├── conf └── data 192.168.2.5主节点chmod a+w -R /app/ port 26000 pidfile /var/run/redis-sentinel.pid logfile &quot;&quot; daemonize no dir /tmp sentinel monitor mymaster 192.168.2.5 6300 2 sentinel down-after-milliseconds mymaster 30000 sentinel parallel-syncs mymaster 1 sentinel failover-timeout mymaster 180000 sentinel deny-scripts-reconfig yes创建主节点 docker run -d --network host --name sentine1 \ -v /app/sentine1/data:/var/redis/data \ -v /app/sentine1/conf/sentinel.conf:/usr/local/etc/redis/sentinel.conf \ redis /usr/local/etc/redis/sentinel.conf --sentinel 192.168.2.7 从节点sentinel2.conf port 26001 daemonize no dir /tmp sentinel monitor mymaster 192.168.2.5 6300 2 sentinel down-after-milliseconds mymaster 30000 sentinel parallel-syncs mymaster 1 sentinel failover-timeout mymaster 180000 sentinel deny-scripts-reconfig yes创建从节点1 docker run -d --network host --name sentine2 \ -v /app/sentine2/data:/var/redis/data \ -v /app/sentine2/conf/sentinel.conf:/usr/local/etc/redis/sentinel.conf \ redis /usr/local/etc/redis/sentinel.conf --sentinel 192.168.2.7 从节点sentinel3.conf port 26002 daemonize no dir /tmp sentinel monitor mymaster 192.168.2.5 6300 2 sentinel down-after-milliseconds mymaster 30000 sentinel parallel-syncs mymaster 1 sentinel failover-timeout mymaster 180000 sentinel deny-scripts-reconfig yes创建从节点2 docker run -d --network host --name sentine3 \ -v /app/sentine3/data:/var/redis/data \ -v /app/sentine3/conf/sentinel.conf:/usr/local/etc/redis/sentinel.conf \ redis /usr/local/etc/redis/sentinel.conf --sentinel 测试[root@centoss2 app]# redis-cli -p 26000 127.0.0.1:26000&gt; sentinel master mymaster]]></content>
      <categories>
        <category>高可用</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>sentinel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Replication+Haproxy+Keepalived]]></title>
    <url>%2F2019%2F08%2F20%2FReplication-Haproxy-Keepalived%2F</url>
    <content type="text"><![CDATA[基于Replication+Haproxy+Keepalived实现数据库高可用 Haproxy 负载均衡haproxy提供负载均衡，并自动切换故障容器vim /usr/local/docker/mysql/haproxy/haproxy.cfg 编写配置文件 global #工作目录 chroot /usr/local/etc/haproxy #日志文件，使用rsyslog服务中local5日志设备（/var/log/local5），等级info log 127.0.0.1 local5 info #守护进程运行 daemon defaults log global mode http #日志格式 option httplog #日志中不记录负载均衡的心跳检测记录 option dontlognull #连接超时（毫秒） timeout connect 5000 #客户端超时（毫秒） timeout client 50000 #服务器超时（毫秒） timeout server 50000 #监控界面 listen admin_stats #监控界面的访问的IP和端口 bind 0.0.0.0:8888 #访问协议 mode http #URI相对地址 stats uri /dbs #统计报告格式 stats realm Global\ statistics #登陆帐户信息 stats auth admin:abc123456 #数据库负载均衡 listen proxy-mysql #访问的IP和端口 bind 0.0.0.0:185 #网络协议 mode tcp #负载均衡算法（轮询算法） #轮询算法：roundrobin #权重算法：static-rr #最少连接算法：leastconn #请求源IP算法：source balance roundrobin #日志格式 option tcplog #在MySQL中创建一个没有权限的haproxy用户，密码为空。Haproxy使用这个账户对MySQL数据库心跳检测 option mysql-check user haproxy server MySQL_1 192.168.3.226:3317 check weight 1 maxconn 2000 server MySQL_2 192.168.3.225:3318 check weight 1 maxconn 2000 #使用keepalive检测死链 option tcpka 在两台Replication组建的mysql集群同时创建mysql_cluster的haproxy容器，形成集群。 docker run -itd -v /data2/haproxy:/usr/local/etc/haproxy --name mysql-haproxy --privileged --net host haproxy docker exec -it ht-mysql-master mysql -u root -p drop user &#39;haproxy&#39;@&#39;%&#39;; create user &#39;haproxy&#39;@&#39;%&#39; IDENTIFIED BY &#39;&#39;;http://192.168.3.226:4001/dbs admin abc123456 实时查看haproxy监控页面 admin:abc123456192.168.3.226 185 root 123456 访问数据库，与Replication数据同步一致。 Keepalived双机热备高可用应用程序向宿主机65的发起请求，宿主机的Keepalived路由到docker内部的虚拟IP15。Haproxy容器内Keepalived抢占虚拟IP，接收到所有数据库请求将被转发到抢占虚拟IP的Haproxy，keepalived互相心跳检测，一旦主服务器挂了，备用服务器将有权抢到虚拟ip，再通过负载均衡分发到某一个PXC节点，并通过主从复制实现数据同步。 192.168.3.226docker exec -it mysql_cluster bash 进入h1 echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty main restricted&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty-updates main restricted&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty universe&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty-updates universe&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty multiverse&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty-updates multiverse&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty-backports main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list echo &quot;# deb http://security.ubuntu.com/ubuntu zesty-security main restricted&quot; &gt;&gt; /etc/apt/sources.list echo &quot;# deb http://security.ubuntu.com/ubuntu zesty-security universe&quot; &gt;&gt; /etc/apt/sources.list echo &quot;# deb http://security.ubuntu.com/ubuntu zesty-security multiverse&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://archive.canonical.com/ubuntu zesty partner&quot; &gt;&gt; /etc/apt/sources.list apt-get update apt-get install gnupg -y --allow-unauthenticated apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 3B4FE6ACC0B21F32 apt-get update apt-get install keepalived vim -y vim /etc/keepalived/keepalived.conf vrrp_instance VI_1 { state MASTER interface enp1s0f0 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 123456 } virtual_ipaddress { 192.168.2.88 } } virtual_server 192.168.2.88 3306 { delay_loop 3 lb_algo rr lb_kind NAT persistence_timeout 50 protocol TCP real_server 192.168.2.5 185 { weight 1 } } service keepalived start ping 192.168.2.88192.168.3.225docker exec -it mysql_cluster bash 进入h2 echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty main restricted&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty-updates main restricted&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty universe&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty-updates universe&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty multiverse&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty-updates multiverse&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty-backports main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list echo &quot;# deb http://security.ubuntu.com/ubuntu zesty-security main restricted&quot; &gt;&gt; /etc/apt/sources.list echo &quot;# deb http://security.ubuntu.com/ubuntu zesty-security universe&quot; &gt;&gt; /etc/apt/sources.list echo &quot;# deb http://security.ubuntu.com/ubuntu zesty-security multiverse&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://archive.canonical.com/ubuntu zesty partner&quot; &gt;&gt; /etc/apt/sources.list apt-get update apt-get install gnupg -y --allow-unauthenticated apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 3B4FE6ACC0B21F32 apt-get update apt-get install keepalived vim -y vim /etc/keepalived/keepalived.conf vrrp_instance VI_1 { state BACKUP interface enp1s0f0 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 123456 } virtual_ipaddress { 192.168.2.88 } } virtual_server 192.168.2.88 3306 { delay_loop 3 lb_algo rr lb_kind NAT persistence_timeout 50 protocol TCP real_server 192.168.2.7 186 { weight 1 } } service keepalived start ping 192.168.2.88访问192.168.3.222 189 root 123456]]></content>
      <categories>
        <category>高可用</category>
      </categories>
      <tags>
        <tag>haproxy</tag>
        <tag>replication</tag>
        <tag>keepalived</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Replication主从复制]]></title>
    <url>%2F2019%2F08%2F20%2FReplication%2F</url>
    <content type="text"><![CDATA[基于Replication实现mysql主从复制 简单介绍本文具体讲述mysql基于多机器的数据库高可用的一些解决方案。 主从复制：常见方案有PXC以及Replication。 Replication的主从在主库中操作，速度较快，弱一致性，单向异步，一旦stop slave将无法同步；PXC集群速度慢，强一致性，高价值数据，双向同步。 负载均衡：Nginx更适用于HTTP协议的应用负载，刚刚支持TCP；Haproxy提供负载，故障自动切换。 双机热备：Keepalived通过虚拟IP将请求分发，让抢占到虚拟IP的Haproxy通过负载分发给某一数据库节点。 Replication单机Masterdocker run -di --name=mysql_master -p 3300:3306 -e MYSQL_ROOT_PASSWORD=123456 mysql docker cp mysql_master:/etc/mysql/my.cnf /usr/share/mysql/my_master.cnf docker cp mysql_master:/etc/mysql/my.cnf /usr/share/mysql/my_slaver.cnf docker stop mysql_master docker rm mysql_master docker run -di --name=mysql_master -p 3300:3306 -e MYSQL_ROOT_PASSWORD=123456 -v /usr/share/mysql/my_master.cnf:/etc/mysql/my.cnf mysql:5.7.25 mkdir -p /usr/local/mysql_master chown -R 777 /usr/local/mysql_master/ 以上主要取出配置文件模板类型 vim /usr/share/mysql/my_master.cnf basedir = /usr/local/mysql_master port = 3306 server_id = 98 log_bin=zlinux01 docker restart mysql_master docker exec -it mysql_master /bin/bash mysql -u root -p ALTER USER &#39;root&#39;@&#39;%&#39; IDENTIFIED WITH mysql_native_password BY &#39;123456&#39;; GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39; WITH GRANT OPTION; FLUSH PRIVILEGES; grant replication slave on *.* to &#39;root&#39;@&#39;192.168.12.98&#39; identified by &#39;123456&#39;; flush tables with read lock; show master status; +-----------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +-----------------+----------+--------------+------------------+-------------------+ | zlinux01.000004 | 154 | | | | +-----------------+----------+--------------+------------------+-------------------+ Slavedocker run -di --name=mysql_slaver -p 3301:3306 -e MYSQL_ROOT_PASSWORD=123456 -v /usr/share/mysql/my_slaver.cnf:/etc/mysql/my.cnf mysql:5.7.25 mkdir -p /usr/local/mysql_slaver chown -R mysql.mysql /usr/local/mysql_slaver/ vim /usr/share/mysql/my_slaver.cnf basedir = /usr/local/mysql_slaver port = 3306 server_id = 89 log_bin=zlinux02 docker restart mysql_slaver docker exec -it mysql_slaver /bin/bash mysql -u root -p ALTER USER &#39;root&#39;@&#39;%&#39; IDENTIFIED WITH mysql_native_password BY &#39;123456&#39;; stop slave; change master to master_host=&#39;192.168.12.98&#39;,master_user=&#39;root&#39;, master_password=&#39;123456&#39;,master_port=3300, master_log_file=&#39;zlinux01.000004&#39;,master_log_pos=154; start slave; show slave status\G Slave_IO_Running: Yes Slave_SQL_Running: Yes则成功单机集群在实际应用中毫无意义，仅供参考。 多机一主多从Master 192.168.3.226mkdir -p /usr/local/mysql_master chown -R 777 /usr/local/mysql_master docker run -di --name=mysql_master -p 3300:3306 -e MYSQL_ROOT_PASSWORD=123456 mysql docker cp mysql_master:/etc/mysql/my.cnf /usr/share/mysql/my_master.cnf 并修改 basedir = /usr/local/mysql_master port = 3306 server_id = 98 log_bin=zlinux01 docker stop mysql_master docker rm mysql_master docker run -di --name=mysql_master -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -v /usr/share/mysql/my_master.cnf:/etc/mysql/my.cnf mysql:5.7.25 docker exec -it mysql_master /bin/bash mysql -u root -p ALTER USER &#39;root&#39;@&#39;%&#39; IDENTIFIED WITH mysql_native_password BY &#39;123456&#39;; GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39; WITH GRANT OPTION; FLUSH PRIVILEGES; grant replication slave on *.* to &#39;root&#39;@&#39;192.168.3.225&#39; identified by &#39;123456&#39;; flush tables with read lock; show master status; +-----------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +-----------------+----------+--------------+------------------+-------------------+ | zlinux01.000004 | 1135 | | | | +-----------------+----------+--------------+------------------+-------------------+Slaver 192.168.3.225mkdir -p /usr/local/mysql_slaver chown -R 777 /usr/local/mysql_slaver docker cp mysql_master:/etc/mysql/my.cnf /usr/share/mysql/my_slaver.cnf 并修改 basedir = /usr/local/mysql_slaver port = 3306 server_id = 89 log_bin=zlinux02 docker run -di --name=mysql_slaver -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -v /usr/share/mysql/my_slaver.cnf:/etc/mysql/my.cnf mysql:5.7.25 docker exec -it mysql_slaver /bin/bash mysql -u root -p ALTER USER &#39;root&#39;@&#39;%&#39; IDENTIFIED WITH mysql_native_password BY &#39;123456&#39;; stop slave; change master to master_host=&#39;192.168.3.226&#39;,master_user=&#39;root&#39;, master_password=&#39;123456&#39;,master_port=3306, master_log_file=&#39;zlinux01.000004&#39;,ster_logmaster_log_pos=1135; start slave; show slave status\G 证明主从复制实现 Slave_IO_Running: Yes Slave_SQL_Running: Yes以上即Replication的主从复制，单向复制，只可作为热备使用。 最终方案Master_1 192.168.3.226/root └── test └── mysql_test1 ├── haproxy │ └── haproxy.cfg ├── log ├── mone │ ├── conf │ │ └── my.cnf │ └── data └── mtwo ├── conf │ └── my.cnf └── data mkdir test/mysql_test1/{mone,mtwo}/{data,conf} -p vim test/mysql_test1/mone/conf/my.cnf [mysqld] server_id = 1 log-bin= mysql-bin replicate-ignore-db=mysql replicate-ignore-db=sys replicate-ignore-db=information_schema replicate-ignore-db=performance_schema read-only=0 relay_log=mysql-relay-bin log-slave-updates=on auto-increment-offset=1 auto-increment-increment=2 !includedir /etc/mysql/conf.d/ !includedir /etc/mysql/mysql.conf.d/ vim test/mysql_test1/mysql/mtwo/conf/my.cnf [mysqld] server_id = 2 log-bin= mysql-bin replicate-ignore-db=mysql replicate-ignore-db=sys replicate-ignore-db=information_schema replicate-ignore-db=performance_schema read-only=0 relay_log=mysql-relay-bin log-slave-updates=on auto-increment-offset=2 auto-increment-increment=2 !includedir /etc/mysql/conf.d/ !includedir /etc/mysql/mysql.conf.d/ scp -r test root@192.168.3.225:/root/ docker run --name monemysql -d -p 3317:3306 -e MYSQL_ROOT_PASSWORD=root -v ~/test/mysql_test1/mone/data:/var/lib/mysql -v ~/test/mysql_test1/mone/conf/my.cnf:/etc/mysql/my.cnf mysql:5.7 docker exec -it monemysql mysql -u root -p 输入root stop slave; GRANT REPLICATION SLAVE ON *.* to &#39;slave&#39;@&#39;%&#39; identified by &#39;123456&#39;; 创建一个slave同步账号slave，允许访问的IP地址为%，%表示通配符用来同步数据 show master status; +------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+-------------------+ | mysql-bin.000003 | 443 | | | | +------------------+----------+--------------+------------------+-------------------+ docker inspect monemysql | grep IPA 查看容器ip &quot;SecondaryIPAddresses&quot;: null, &quot;IPAddress&quot;: &quot;172.17.0.2&quot;, &quot;IPAMConfig&quot;: null, &quot;IPAddress&quot;: &quot;172.17.0.2&quot;,Master_2 192.168.3.225docker run --name mtwomysql -d -p 3318:3306 -e MYSQL_ROOT_PASSWORD=root -v ~/test/mysql_test1/mtwo/data:/var/lib/mysql -v ~/test/mysql_test1/mtwo/conf/my.cnf:/etc/mysql/my.cnf mysql:5.7 docker exec -it mtwomysql mysql -u root -p 输入root stop slave; change master to master_host=&#39;192.168.3.226&#39;,master_user=&#39;slave&#39;, master_password=&#39;123456&#39;,master_log_file=&#39;mysql-bin.000003&#39;, master_log_pos=443,master_port=3317; GRANT REPLICATION SLAVE ON *.* to &#39;slave&#39;@&#39;%&#39; identified by &#39;123456&#39;; 创建一个用户来同步数据 start slave ; 启动同步 show master status; 查看状态 +------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+-------------------+ | mysql-bin.000003 | 443 | | | | +------------------+----------+--------------+------------------+-------------------+双向同步Master_1 192.168.3.226 stop slave; change master to master_host=&#39;192.168.3.225&#39;,master_user=&#39;slave&#39;, master_password=&#39;123456&#39;,master_log_file=&#39;mysql-bin.000003&#39;, master_log_pos=443,master_port=3318; start slave ; 在两个容器中查看 show slave status\G; Slave_IO_Running: Yes Slave_SQL_Running: Yes 双向验证，数据同步 实例 映射路径data为空 2.7docker run --name ht-mysql-master -d -p 3317:3306 -e MYSQL_ROOT_PASSWORD=123456 -v /data3/replication/data:/var/lib/mysql -v /data2/replication/conf/my.cnf:/etc/mysql/my.cnf mysql:5.7 docker exec -it ht-mysql-master mysql -u root -p GRANT REPLICATION SLAVE ON *.* TO &#39;repl_user&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; FLUSH PRIVILEGES; reset master; #清空master的binlog，平时慎用，可选 flush tables with read lock; #只读 flush logs; show master status; 2.5docker run --name ht-mysql-slave -d -p 3318:3306 -e MYSQL_ROOT_PASSWORD=123456 -v /data3/replication/data:/var/lib/mysql -v /data3/replication/conf/my.cnf:/etc/mysql/my.cnf mysql:5.7 docker exec -it ht-mysql-slave mysql -u root -p GRANT REPLICATION SLAVE ON *.* TO &#39;repl_user&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; FLUSH PRIVILEGES; stop slave; CHANGE MASTER TO MASTER_HOST=&#39;192.168.2.7&#39;,MASTER_PORT=3317, MASTER_USER=&#39;repl_user&#39;, MASTER_PASSWORD=&#39;123456&#39;, MASTER_LOG_FILE=&#39;mysql-bin.000002&#39;, MASTER_LOG_POS=154; start slave; show slave status\G reset master;(清空master的binlog，平时慎用，可选) flush tables with read lock; flush logs; show master status;2.7unlock stop slave; CHANGE MASTER TO MASTER_HOST=&#39;192.168.2.5&#39;,MASTER_PORT=3318, MASTER_USER=&#39;repl_user&#39;, MASTER_PASSWORD=&#39;123456&#39;, MASTER_LOG_FILE=&#39;mysql-bin.000002&#39;, MASTER_LOG_POS=154; start slave; show slave status\G2.5unlock tables;Semisync半同步配置nodeA和nodeB上执行： mysql&gt; INSTALL PLUGIN rpl_semi_sync_master SONAME &#39;semisync_master.so&#39;; mysql&gt; INSTALL PLUGIN rpl_semi_sync_slave SONAME &#39;semisync_slave.so&#39;; mysql&gt; show variables like &#39;%semi%&#39;; rpl_semi_sync_master_timeout=10000 表示主库在某次事务中，如果等待时间超过10秒，则降级为普通模式，不再等待备库。如果主库再次探测到备库恢复了，则会自动再次回到semisync模式。 rpl_semi_sync_master_wait_point=AFTER_SYNCAFTER_SYNC工作流程： 客户端提交一个事务，master将事务写入binlog并刷新到磁盘，发送到slave，master等待slave反馈。 slave接收master的binlog，写到本地的relaylog里。发送确认信息给master。 当接收到slave反馈，master提交事务并返回结果给客户端。这样就保证了主从数据一致。 mysql&gt; SET GLOBAL rpl_semi_sync_master_enabled = 1; mysql&gt; SET GLOBAL rpl_semi_sync_slave_enabled = 1; mysql&gt; stop slave;start slave; mysql&gt; show status like &#39;%semi%&#39;; +--------------------------------------------+-------+ | Variable_name | Value | +--------------------------------------------+-------+ | Rpl_semi_sync_master_clients | 1 | | Rpl_semi_sync_master_net_avg_wait_time | 0 | | Rpl_semi_sync_master_net_wait_time | 0 | | Rpl_semi_sync_master_net_waits | 0 | | Rpl_semi_sync_master_no_times | 0 | | Rpl_semi_sync_master_no_tx | 0 | | Rpl_semi_sync_master_status | ON | (master同步） | Rpl_semi_sync_master_timefunc_failures | 0 | | Rpl_semi_sync_master_tx_avg_wait_time | 0 | | Rpl_semi_sync_master_tx_wait_time | 0 | | Rpl_semi_sync_master_tx_waits | 0 | | Rpl_semi_sync_master_wait_pos_backtraverse | 0 | | Rpl_semi_sync_master_wait_sessions | 0 | | Rpl_semi_sync_master_yes_tx | 0 | | Rpl_semi_sync_slave_status | ON |（从同步） +--------------------------------------------+-------+ 15 rows in set (0.00 sec)并修改my.cnf，添加下面两行： rpl_semi_sync_master_enabled = 1 rpl_semi_sync_slave_enabled = 1]]></content>
      <categories>
        <category>高可用</category>
      </categories>
      <tags>
        <tag>replication</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[个人博客]]></title>
    <url>%2F2019%2F08%2F20%2Fblog%2F</url>
    <content type="text"><![CDATA[基于Nexmoe的搭建的个人博客 Hexohttps://nodejs.org/download/release/v10.15.3/ 安装node npm install -g cnpm --registry=https://registry.npm.taobao.org 安装cnpm npm config set registry https://registry.npm.taobao.org 使用npm淘宝源 npm install -g hexo-cli hexo init blog cd blog npm installNexmoecd themes git clone https://github.com/nexmoe/hexo-theme-nexmoe.git nexmoe cd nexmoe git checkout master npm i --save hexo-wordcount npm i hexo-deployer-git --save npm i -g gulp --save cp -i _config.example.yml _config.ymlvim package.json &quot;scripts&quot;: { &quot;build&quot;: &quot;hexo clean &amp;&amp; hexo g &amp;&amp; gulp &amp;&amp; hexo d &amp; git add * &amp; git commit -m &#39;加油&#39; &amp; git push origin master&quot;, &quot;test&quot;: &quot;hexo clean &amp;&amp; hexo g &amp;&amp; gulp &amp;&amp; hexo s&quot;, &quot;dev&quot;: &quot;hexo clean &amp;&amp; hexo g &amp;&amp; hexo s&quot; },默认自动开启github pages 使用npm install gulp npm install --save-dev gulp npm install gulp-htmlclean gulp-htmlmin gulp-minify-css gulp-uglify gulp-imagemin --savegulpfile.js var gulp = require(&#39;gulp&#39;); var minifycss = require(&#39;gulp-minify-css&#39;); var uglify = require(&#39;gulp-uglify&#39;); var htmlmin = require(&#39;gulp-htmlmin&#39;); var htmlclean = require(&#39;gulp-htmlclean&#39;); var imagemin = require(&#39;gulp-imagemin&#39;); // 压缩html gulp.task(&#39;minify-html&#39;, function() { return gulp.src(&#39;./public/**/*.html&#39;) .pipe(htmlclean()) .pipe(htmlmin({ removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, })) .pipe(gulp.dest(&#39;./public&#39;)) }); // 压缩css gulp.task(&#39;minify-css&#39;, function() { return gulp.src(&#39;./public/**/*.css&#39;) .pipe(minifycss({ compatibility: &#39;ie8&#39; })) .pipe(gulp.dest(&#39;./public&#39;)); }); // 压缩js gulp.task(&#39;minify-js&#39;, function() { return gulp.src(&#39;./public/js/**/*.js&#39;) .pipe(uglify()) .pipe(gulp.dest(&#39;./public&#39;)); }); // 压缩图片 gulp.task(&#39;minify-images&#39;, function() { return gulp.src(&#39;./public/images/**/*.*&#39;) .pipe(imagemin( [imagemin.gifsicle({&#39;optimizationLevel&#39;: 3}), imagemin.jpegtran({&#39;progressive&#39;: true}), imagemin.optipng({&#39;optimizationLevel&#39;: 7}), imagemin.svgo()], {&#39;verbose&#39;: true})) .pipe(gulp.dest(&#39;./public/images&#39;)) }); // 默认任务 gulp.task(&#39;default&#39;, [ &#39;minify-html&#39;,&#39;minify-css&#39;,&#39;minify-js&#39;,&#39;minify-images&#39; ]);hexo g &amp;&amp; gulp _config.ymltitle: OneJane subtitle: 码农养成记 description: keywords: Spring Cloud,Docker,Dubbo author: OneJane language: zh-CN timezone: Hongkong url: https://onejane.github.io/ theme: nexmoe deploy: type: git repo: github: git@github.com:OneJane/OneJane.github.io.git branch: master message: github highlight: enable: false line_number: true auto_detect: false tab_replace: themes/nexmoe/_config.ymlavatar: https://i.loli.net/2019/08/20/UIhTqdQiPasxLtr.jpg # 网站 Logo background: https://i.loli.net/2019/01/13/5c3aec85a4343.jpg # 既是博客的背景，又是文章默认头图 favicon: href: /img/a.ico # 网站图标 type: image/png # 图标类型，可能的值有(image/png, image/vnd.microsoft.icon, image/x-icon, image/gif) social: zhihu: - https://www.zhihu.com/people/codewj/activities - icon-zhihu - rgb(231, 106, 141) - rgba(231, 106, 141, .15) GitHub: - https://github.com/OneJane - icon-github - rgb(25, 23, 23) - rgba(25, 23, 23, .15) analytics: la_site_id: 20279757 comment: gitment gitment: owner: onejane # 持有该 repo 的 GitHub username repo: onejane.github.io # 存放评论的 issue 所在的 repo clientID: e677e59382e1c7a468fd # GitHub Client ID clientSecret: 717d041bc4ab749f069314862232cfb6ec8adc15 # GitHub Client Secret 打赏themes/nexmoe/layout/_partial/donate.ejs &lt;! -- 添加捐赠图标 --&gt; &lt;div class =&quot;post-donate&quot;&gt; &lt;div id=&quot;donate_board&quot; class=&quot;donate_bar center&quot;&gt; &lt;a id=&quot;btn_donate&quot; class=&quot;btn_donate&quot; href=&quot;javascript:;&quot; title=&quot;打赏&quot;&gt;&lt;/a&gt; &lt;span class=&quot;donate_txt&quot;&gt; ↑&lt;br&gt; &lt;%=theme.donate_message%&gt; &lt;/span&gt; &lt;br&gt; &lt;/div&gt; &lt;div id=&quot;donate_guide&quot; class=&quot;donate_bar center hidden&quot; &gt; ![](/images/alipay.jpg) ![](/images/alipay.jpg) &lt;!-- 支付宝打赏图案 --&gt; &lt;img src=&quot;&lt;%- theme.root_url %&gt;/images/alipay.jpg&quot; alt=&quot;支付宝打赏&quot;&gt; 666 &lt;!-- 微信打赏图案 --&gt; &lt;img src=&quot;&lt;%- theme.root_url %&gt;/images/wechatpay.png&quot; alt=&quot;微信打赏&quot;&gt; &lt;/div&gt; &lt;script type=&quot;text/javascript&quot;&gt; document.getElementById(&#39;btn_donate&#39;).onclick = function(){ $(&#39;#donate_board&#39;).addClass(&#39;hidden&#39;); $(&#39;#donate_guide&#39;).removeClass(&#39;hidden&#39;); } &lt;/script&gt; &lt;/div&gt; &lt;! -- 添加捐赠图标 --&gt; themes/nexmoe/source/css/_partial/donate.styl .donate_bar { text-align: center; margin-top: 5% } .donate_bar a.btn_donate { display: inline-block; width: 82px; height: 82px; margin-left: auto; margin-right: auto; background: url(http://img.t.sinajs.cn/t5/style/images/apps_PRF/e_media/btn_reward.gif)no-repeat; -webkit-transition: background 0s; -moz-transition: background 0s; -o-transition: background 0s; -ms-transition: background 0s; transition: background 0s } .donate_bar a.btn_donate:hover { background-position: 0 -82px } .donate_bar .donate_txt { display: block; color: #9d9d9d; font: 14px/2 &quot;Microsoft Yahei&quot; } .donate_bar.hidden{ display: none } .post-donate{ margin-top: 80px; } #donate_guide{ height: 210px; width: 420px; margin: 0 auto; } #donate_guide img{ height: 200px; height: 200px; } 在source\css\style.styl中添加@import ‘_partial/donate’themes/nexmoe/layout/post.ejs &lt;% if (theme.donate){ %&gt; &lt;%- partial(&#39;donate&#39;) %&gt; &lt;% } %&gt; themes/nexmoe/_config.yml #是否开启打赏功能 donate: true #打赏文案 donate_message: 欣赏此文？求鼓励，求支持！404插入音乐hexo new page 404 生成source/404.md --- title: 404 permalink: /404 cover: https://i.loli.net/2019/08/20/DMuWHOGTq4AR1iQ.png --- &lt;div class=&quot;aplayer&quot; data-id=&quot;439625244&quot; data-server=&quot;netease&quot; data-type=&quot;song&quot; data-autoplay=&quot;true&quot; data-mode=&quot;single&quot;&gt;&lt;/div&gt;themes/nexmoe/layout/layout.ejs &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.css&quot;&gt; &lt;script src=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.js&quot;&gt;&lt;/script&gt; ... &lt;script src=&quot;https://cdn.jsdelivr.net/npm/meting@1.1.0/dist/Meting.min.js&quot;&gt;&lt;/script&gt; windows右键复制路径Windows Registry Editor Version 5.00 [HKEY_CLASSES_ROOT\Directory\shell\copypath] @=&quot;copy path of dir&quot; [HKEY_CLASSES_ROOT\Directory\shell\copypath\command] @=&quot;mshta vbscript:clipboarddata.setdata(\&quot;text\&quot;,\&quot;%1\&quot;)(close)&quot; [HKEY_CLASSES_ROOT\*\shell\copypath] @=&quot;copy path of file&quot; [HKEY_CLASSES_ROOT\*\shell\copypath\command] @=&quot;mshta vbscript:clipboarddata.setdata(\&quot;text\&quot;,\&quot;%1\&quot;)(close)&quot;windows定时任务此电脑–&gt;管理–&gt;任务计划管理–&gt;任务计划程序 –&gt;任务计划程序库–&gt;Microsoft –&gt;windows vim C:\Users\codewj\Desktop\build-blog.bat E: &amp;&amp; cd E:\Project\blog &amp;&amp; npm run build robots.txtUser-agent: * Disallow: Disallow: /bin/ Sitemap: https://onejane.github.io/sitemap.txt 加入blog\themes\nexmoe\source google收录 https://search.google.com/search-console 添加资源 将google验证文件放入blog\themes\nexmoe\source，发布校验 站点地图 将生成的站点地图放入blog\themes\nexmoe\source bing收录https://www.bing.com/toolbox/webmaster/ 登陆后进入https://www.bing.com/webmaster/home/mysites#https://www.bing.com/webmaster/home/dashboard?url=https%3A%2F%2Fonejane.github.io%2F 使用小书匠小书匠 新建github reository为blog，并初始化 新建token:https://github.com/settings/tokens/new 并Generate token得到129483a01745abe39ed1ac109ec09f1d71b9e8c3数据存储&amp;图床服务 QuestionAssertionError [ERR_ASSERTION]: Task function must be specified?&quot;devDependencies&quot;: { &quot;gulp&quot;: &quot;^3.9.1&quot; } npm install GulpUglifyError:unable to minify JavaScriptnpm install gulp-util –save-dev var gutil = require(&#39;gulp-util&#39;); // 压缩public目录下的所有js gulp.task(&#39;minify-js&#39;, function() { return gulp.src(&#39;./public/**/*.js&#39;) .pipe(uglify()) .on(&#39;error&#39;, function (err) { gutil.log(gutil.colors.red(&#39;[Error]&#39;), err.toString()); }) //增加这一行 .pipe(gulp.dest(&#39;./public&#39;)); }); gittalk Error：validation failedgitalk.ejs id: window.location.pathname改为id: decodeURI(window.location.pathname)]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>nexmoe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单机Nacos集群]]></title>
    <url>%2F2019%2F08%2F20%2Fnacos%2F</url>
    <content type="text"><![CDATA[基于docker的单机nacos集群安装配置 环境搭建yum update -y nss curl libcurl curl -L https://github.com/docker/compose/releases/download/1.20.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose docker-compose --version git clone https://github.com/nacos-group/nacos-docker.git cd nacos-docker 映射cluster-hostname启动脚本 - ../docker-startup.sh:/home/nacos/bin/docker-startup.sh 修改docker-startup.sh中环境变量 JAVA_OPT=&quot;${JAVA_OPT} -server -Xms512m -Xmx512m -Xmn256m -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m&quot; vim ~/.bashrc alias dofo=&#39;docker ps --format &quot;table {{.Names}}\t{{.Ports}}\t{{.Image}}\t&quot;&#39; alias dolo=&#39;docker logs -ft&#39; source ~/.bashrc #docker-compose up -d #docker-compose logs -ft #docker-compose down docker-compose -f example/cluster-hostname.yaml up -d docker stop nacos1 nacos2 nacos3 docker start nacos1 nacos2 nacos3 docker-compose -f example/cluster-hostname.yaml downhttp://192.168.2.7:9503/actuator/nacos-discoveryhttp://192.168.2.5:8848/nacos http://192.168.2.5:8849/nacos http://192.168.2.5:8850/nacos nacos/nacos Nginx负载均衡mkdir /usr/local/docker/nginx/nacos -p vim n1.conf upstream nacos { # 配置负载均衡 server 192.168.2.5:8848; server 192.168.2.5:8849; server 192.168.2.5:8850; } server { listen 8841; server_name 192.168.2.5; location / { proxy_pass http://nacos; index index.html index.htm; } vim n2.conf upstream nacos { # 配置负载均衡 server 192.168.2.5:8848; server 192.168.2.5:8849; server 192.168.2.5:8850; } server { listen 8842; server_name 192.168.2.5; location / { proxy_pass http://nacos; index index.html index.htm; } docker run -it -d –name nginx2 -v /usr/local/docker/nginx/nacos/n2.conf:/etc/nginx/nginx.conf –net=host –privileged nginxdocker run -it -d –name nginx1 -v /usr/local/docker/nginx/nacos/n1.conf:/etc/nginx/nginx.conf –net=host –privileged nginx http://192.168.2.5:8841/nacos/#/login http://192.168.2.5:8842/nacos/#/login docker pause nacos1 测试页面维持访问 Keepalive双机热备docker exec -it nginx1 bash mv /etc/apt/sources.list sources.list.bak echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list apt-get update apt-get install gnupg -y --allow-unauthenticated apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 3B4FE6ACC0B21F32 apt-get update apt-get install keepalived vim -y vim /etc/keepalived/keepalived.conf vrrp_instance VI_1 { state MASTER interface docker0 # 填写docker网卡名 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 123456 } virtual_ipaddress { 172.17.0.100 # 定义该网卡下的虚拟ip地址段地址 } } service keepalived start docker exec -it nginx2 bash mv /etc/apt/sources.list sources.list.bak echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list apt-get update apt-get install gnupg -y --allow-unauthenticated apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 3B4FE6ACC0B21F32 apt-get update apt-get install keepalived vim -y vim /etc/keepalived/keepalived.conf vrrp_instance VI_1 { state BACKUP interface docker0 # 填写docker网卡名 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 123456 } virtual_ipaddress { 172.17.0.100 # 定义虚拟ip地址段地址 } } service keepalived start 由于docker内的虚拟ip不能被外界访问借助宿主机keepalived映射外网可以访问的虚拟ip， exit yum install keepalived -y mv /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak vim /etc/keepalived/keepalived.conf vrrp_instance VI_1 { state MASTER interface enp1s0f0 # 宿主机网卡 virtual_router_id 51 # 保持一致 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.2.155 # 宿主机虚拟ip } } virtual_server 192.168.2.155 183 { # 宿主机虚拟ip及开放端口 delay_loop 3 lb_algo rr lb_kind NAT persistence_timeout 50 protocol TCP real_server 172.17.0.100 8841 { # nginx1容器虚拟ip及开放nacos端口 weight 1 } } virtual_server 192.168.2.155 183 { delay_loop 3 lb_algo rr lb_kind NAT persistence_timeout 50 protocol TCP real_server 172.17.0.100 8842 { # nginx2容器虚拟ip及开放nacos端口 weight 1 } } #apt-get --purge remove keepalived -y #/sbin/ip addr del 192.168.12.100/32 dev enp1s0f0 删除虚拟ip 如docker0没有则重启docker，再次检查创建systemctl daemon-reloadsystemctl restart docker 访问 http://192.168.2.155:183/nacos/#/login nacos nacos 关闭nacos心跳日志logging: level: com.alibaba.nacos.client.naming: error]]></content>
      <categories>
        <category>注册中心</category>
      </categories>
      <tags>
        <tag>spring cloud alibaba</tag>
        <tag>docker</tag>
      </tags>
  </entry>
</search>
