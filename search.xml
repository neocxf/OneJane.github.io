<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[OpenCV与TensorFlow手写数字刷脸识别]]></title>
    <url>%2F2019%2F09%2F03%2FOpenCV%E4%B8%8ETensorFlow%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E5%88%B7%E8%84%B8%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[OpenCV与TensorFlow手写数字刷脸识别 http://yann.lecun.com/exdb/mnist/ 手写数字识别KNN最近领域# 1 重要 # 2 KNN最近领域 CNN卷积神经网络 2种 # 3 样本 # 4 旧瓶装新酒 ：数字识别的不同 # 4.1 网络 4。2 每一级 4.3 先原理 后代码 # 本质：knn test 样本 K个 max4 3个1 -》1 # 1 load Data 1.1 随机数 1.2 4组 训练 测试 （图片 和 标签） # 2 knn test train distance 5*500 = 2500 784=28*28 # 3 knn k个最近的图片5 500 1-》500train （4） # 4 k个最近的图片-&gt; parse centent label # 5 label -》 数字 p9 测试图片-》数据 # 6 检测概率统计 import tensorflow as tf import numpy as np import random from tensorflow.examples.tutorials.mnist import input_data # load data 2 one_hot : 1 0000 1 fileName mnist = input_data.read_data_sets(&#39;MNIST_data&#39;,one_hot=True) # 属性设置 trainNum = 55000 testNum = 10000 trainSize = 500 testSize = 5 k = 4 # data 分解 1 trainSize 2范围0-trainNum 3 replace=False trainIndex = np.random.choice(trainNum,trainSize,replace=False) testIndex = np.random.choice(testNum,testSize,replace=False) trainData = mnist.train.images[trainIndex]# 训练图片 trainLabel = mnist.train.labels[trainIndex]# 训练标签 testData = mnist.test.images[testIndex] testLabel = mnist.test.labels[testIndex] # 28*28 = 784 print(&#39;trainData.shape=&#39;,trainData.shape)#500*784 1 图片个数 2 784? print(&#39;trainLabel.shape=&#39;,trainLabel.shape)#500*10 print(&#39;testData.shape=&#39;,testData.shape)#5*784 print(&#39;testLabel.shape=&#39;,testLabel.shape)#5*10 print(&#39;testLabel=&#39;,testLabel)# 4 :testData [0] 3:testData[1] 6 # tf input 784-&gt;image trainDataInput = tf.placeholder(shape=[None,784],dtype=tf.float32) trainLabelInput = tf.placeholder(shape=[None,10],dtype=tf.float32) testDataInput = tf.placeholder(shape=[None,784],dtype=tf.float32) testLabelInput = tf.placeholder(shape=[None,10],dtype=tf.float32) #knn distance 5*785. 5*1*784 # 5测试图片 500训练图片 784 (3D) 2500*784 f1 = tf.expand_dims(testDataInput,1) # 维度扩展 f2 = tf.subtract(trainDataInput,f1)# 784 sum(784) f3 = tf.reduce_sum(tf.abs(f2),reduction_indices=2)# 完成数据累加 784 abs # 5*500 f4 = tf.negative(f3)# 取反 f5,f6 = tf.nn.top_k(f4,k=4) # 选取f4 最大的四个值 # f3 最小的四个值 # f6 index-&gt;trainLabelInput f7 = tf.gather(trainLabelInput,f6) # f8 num reduce_sum reduction_indices=1 &#39;竖直&#39; f8 = tf.reduce_sum(f7,reduction_indices=1) # tf.argmax 选取在某一个最大的值 index f9 = tf.argmax(f8,dimension=1) # f9 -&gt; test5 image -&gt; 5 num with tf.Session() as sess: # f1 &lt;- testData 5张图片 p1 = sess.run(f1,feed_dict={testDataInput:testData[0:5]}) print(&#39;p1=&#39;,p1.shape)# p1= (5, 1, 784) p2 = sess.run(f2,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) print(&#39;p2=&#39;,p2.shape)#p2= (5, 500, 784) (1,100) p3 = sess.run(f3,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) print(&#39;p3=&#39;,p3.shape)#p3= (5, 500) print(&#39;p3[0,0]=&#39;,p3[0,0]) #130.451 knn distance p3[0,0]= 155.812 p4 = sess.run(f4,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) print(&#39;p4=&#39;,p4.shape) print(&#39;p4[0,0]&#39;,p4[0,0]) p5,p6 = sess.run((f5,f6),feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) #p5= (5, 4) 每一张测试图片（5张）分别对应4张最近训练图片 #p6= (5, 4) print(&#39;p5=&#39;,p5.shape) print(&#39;p6=&#39;,p6.shape) print(&#39;p5[0,0]&#39;,p5[0]) print(&#39;p6[0,0]&#39;,p6[0])# p6 index p7 = sess.run(f7,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel}) print(&#39;p7=&#39;,p7.shape)#p7= (5, 4, 10) print(&#39;p7[]&#39;,p7) p8 = sess.run(f8,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel}) print(&#39;p8=&#39;,p8.shape) print(&#39;p8[]=&#39;,p8) p9 = sess.run(f9,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel}) print(&#39;p9=&#39;,p9.shape) print(&#39;p9[]=&#39;,p9) p10 = np.argmax(testLabel[0:5],axis=1) print(&#39;p10[]=&#39;,p10) j = 0 for i in range(0,5): if p10[i] == p9[i]: j = j+1 print(&#39;ac=&#39;,j*100/5) CNN卷积神经网络#cnn : 1 卷积 # ABC # A: 激励函数+矩阵 乘法加法 # A CNN : pool（激励函数+矩阵 卷积 加法） # C：激励函数+矩阵 乘法加法（A-》B） # C：激励函数+矩阵 乘法加法（A-》B） + softmax（矩阵 乘法加法） # loss：tf.reduce_mean(tf.square(y-layer2)) # loss：code #1 import import tensorflow as tf import numpy as np from tensorflow.examples.tutorials.mnist import input_data # 2 load data mnist = input_data.read_data_sets(&#39;MNIST_data&#39;,one_hot = True) # 3 input imageInput = tf.placeholder(tf.float32,[None,784]) # 28*28 labeInput = tf.placeholder(tf.float32,[None,10]) # 10 列数 # 4 data reshape # [None,784]-&gt;M*28*28*1 2D-&gt;4D 28*28 wh 1 channel imageInputReshape = tf.reshape(imageInput,[-1,28,28,1]) # 5 卷积 w0 : 卷积内核 5*5 out:32 in:1 w0 = tf.Variable(tf.truncated_normal([5,5,1,32],stddev = 0.1)) b0 = tf.Variable(tf.constant(0.1,shape=[32])) # 6 # layer1：激励函数+卷积运算 # imageInputReshape : M*28*28*1 w0:5,5,1,32 layer1 = tf.nn.relu(tf.nn.conv2d(imageInputReshape,w0,strides=[1,1,1,1],padding=&#39;SAME&#39;)+b0) # M*28*28*32 # pool 采样 数据量减少很多M*28*28*32 =&gt; M*7*7*32 layer1_pool = tf.nn.max_pool(layer1,ksize=[1,4,4,1],strides=[1,4,4,1],padding=&#39;SAME&#39;) # [1 2 3 4]-&gt;[4] # 7 layer2 out : 激励函数+乘加运算： softmax（激励函数 + 乘加运算） # [7*7*32,1024] w1 = tf.Variable(tf.truncated_normal([7*7*32,1024],stddev=0.1)) b1 = tf.Variable(tf.constant(0.1,shape=[1024])) h_reshape = tf.reshape(layer1_pool,[-1,7*7*32])# M*7*7*32 -&gt; N*N1 # [N*7*7*32] [7*7*32,1024] = N*1024 h1 = tf.nn.relu(tf.matmul(h_reshape,w1)+b1) # 7.1 softMax w2 = tf.Variable(tf.truncated_normal([1024,10],stddev=0.1)) b2 = tf.Variable(tf.constant(0.1,shape=[10])) pred = tf.nn.softmax(tf.matmul(h1,w2)+b2)# N*1024 1024*10 = N*10 # N*10( 概率 )N1【0.1 0.2 0.4 0.1 0.2 。。。】 # label。 【0 0 0 0 1 0 0 0.。。】 loss0 = labeInput*tf.log(pred) loss1 = 0 # 7.2 for m in range(0,500):# test 100 for n in range(0,10): loss1 = loss1 - loss0[m,n] loss = loss1/500 # 8 train train = tf.train.GradientDescentOptimizer(0.01).minimize(loss) # 9 run with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for i in range(100): images,labels = mnist.train.next_batch(500) sess.run(train,feed_dict={imageInput:images,labeInput:labels}) pred_test = sess.run(pred,feed_dict={imageInput:mnist.test.images,labeInput:labels}) acc = tf.equal(tf.arg_max(pred_test,1),tf.arg_max(mnist.test.labels,1)) acc_float = tf.reduce_mean(tf.cast(acc,tf.float32)) acc_result = sess.run(acc_float,feed_dict={imageInput:mnist.test.images,labeInput:mnist.test.labels}) print(acc_result) 刷脸识别爬虫获取样本import urllib from bs4 import BeautifulSoup html = urllib.request.urlopen( &#39;https://www.duitang.com/album/?id=69001447&#39;).read() # parse url data 1 html 2 &#39;html.parser&#39; 3 &#39;utf-8&#39; soup = BeautifulSoup(html, &#39;html.parser&#39;, from_encoding=&#39;utf-8&#39;) # img images = soup.findAll(&#39;img&#39;) print(images) imageName = 0 for image in images: link = image.get(&#39;src&#39;) print(&#39;link=&#39;, link) fileFormat = link[-3:] if fileFormat == &#39;png&#39; or fileFormat == &#39;jpg&#39;: fileSavePath = &#39;C:/Users/codewj/AnacondaProjects/5刷脸识别/images/&#39; + str(imageName) + &#39;.jpg&#39; imageName = imageName + 1 urllib.request.urlretrieve(link, fileSavePath) ffmpegffmpeg -i input.mp4 -r 1 -q:v 2 -f image2 pic-%03d.jpeg 视频提取帧 ffmpeg -i input.mp4 -ss 00:00:20 -t 10 -r 1 -q:v 2 -f image2 pic-%03d.jpeg ffmpeg会从input.mp4的第20s时间开始，往下10s，即20~30s这10秒钟之间，每隔1s就抓一帧，总共会抓10帧。 ffmpeg -i input.avi output.mp4 视频转格式 ffmpeg -i a.mp4 -acodec copy -vn a.aac 视频提取音频 ffmpeg -i input.mp4 -vcodec copy -an output.mp4 视频提取音频 ffmpeg -ss 00:00:15 -t 00:00:05 -i input.mp4 -vcodec copy -acodec copy output.mp4 视频剪切 ffmpeg -i input.mp4 -b:v 2000k -bufsize 2000k -maxrate 2500k output.mp4 码率控制 ffmpeg -i input.mp4 -vcodec mpeg4 output.mp4 视频编码格式转换mpeg4 ffmpeg -i input.mp4 -vf scale=960:540 output.mp4 将输入视频缩小到960x540输出 ffmpeg -i input.mp4 -i iQIYI_logo.png -filter_complex overlay output.mp4 视频添加logo opencv预处理# 1 load xml 2 load jpg 3 haar gray 4 detect 5 draw import cv2 import numpy as np # load xml 1 file name face_xml = cv2.CascadeClassifier(&#39;haarcascade_frontalface_default.xml&#39;) eye_xml = cv2.CascadeClassifier(&#39;haarcascade_eye.xml&#39;) # load jpg img = cv2.imread(&#39;face.jpg&#39;) cv2.imshow(&#39;src&#39;,img) # haar gray gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) # detect faces 1 data 2 scale 3 5 faces = face_xml.detectMultiScale(gray,1.3,5) print(&#39;face=&#39;,len(faces)) # draw index = 0 for (x,y,w,h) in faces: cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) roi_face = gray[y:y+h,x:x+w] roi_color = img[y:y+h,x:x+w] fileName = str(index)+&#39;.jpg&#39; cv2.imwrite(fileName,roi_color) index = index + 1 # 1 gray eyes = eye_xml.detectMultiScale(roi_face) print(&#39;eye=&#39;,len(eyes)) #for (e_x,e_y,e_w,e_h) in eyes: #cv2.rectangle(roi_color,(e_x,e_y),(e_x+e_w,e_y+e_h),(0,255,0),2) cv2.imshow(&#39;dst&#39;,img) cv2.waitKey(0) 某个人脸识别# 1 数据yale 2 准备train label-》train # 3 cnn 4 检测 import tensorflow as tf import numpy as np import scipy.io as sio f = open(&#39;Yale_64x64.mat&#39;,&#39;rb&#39;) mdict = sio.loadmat(f) # fea gnd train_data = mdict[&#39;fea&#39;] train_label = mdict[&#39;gnd&#39;] # 数据无序排列 train_data = np.random.permutation(train_data) train_label = np.random.permutation(train_label) test_data = train_data[0:64] test_label = train_label[0:64] np.random.seed(100) test_data = np.random.permutation(test_data) np.random.seed(100) test_label = np.random.permutation(test_label) # train [0-9] [10*N] [15*N] [0 0 1 0 0 0 0 0 0 0] -&gt; 2 train_data = train_data.reshape(train_data.shape[0],64,64,1).astype(np.float32)/255 train_labels_new = np.zeros((165,15))# 165 image 15 for i in range(0,165): j = int(train_label[i,0])-1 # 1-15 0-14 train_labels_new[i,j] = 1 test_data_input = test_data.reshape(test_data.shape[0],64,64,1).astype(np.float32)/255 test_labels_input = np.zeros((64,15))# 165 image 15 for i in range(0,64): j = int(test_label[i,0])-1 # 1-15 0-14 test_labels_input[i,j] = 1 # cnn acc tf.nn tf.layer data_input = tf.placeholder(tf.float32,[None,64,64,1]) label_input = tf.placeholder(tf.float32,[None,15]) layer1 = tf.layers.conv2d(inputs=data_input,filters=32,kernel_size=2,strides=1,padding=&#39;SAME&#39;,activation=tf.nn.relu) layer1_pool = tf.layers.max_pooling2d(layer1,pool_size=2,strides=2) layer2 = tf.reshape(layer1_pool,[-1,32*32*32]) layer2_relu = tf.layers.dense(layer2,1024,tf.nn.relu) output = tf.layers.dense(layer2_relu,15) loss = tf.losses.softmax_cross_entropy(onehot_labels=label_input,logits=output) train = tf.train.GradientDescentOptimizer(0.01).minimize(loss) accuracy = tf.metrics.accuracy(labels=tf.argmax(label_input,axis=1),predictions=tf.argmax(output,axis=1))[1] # run acc init = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer()) with tf.Session() as sess: sess.run(init) for i in range(0,200): train_data_input = np.array(train_data) train_label_input = np.array(train_labels_new) sess.run([train,loss],feed_dict={data_input:train_data_input,label_input:train_label_input}) acc = sess.run(accuracy,feed_dict={data_input:test_data_input,label_input:test_labels_input}) print(&#39;acc:%.2f&#39;,acc)]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenCV与TensorFlow 入门人工智能图像处理]]></title>
    <url>%2F2019%2F09%2F02%2FOpenCV%E4%B8%8ETensorFlow%20%E5%85%A5%E9%97%A8%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[OpenCV与TensorFlow 入门人工智能图像处理 AnacondaAnaconda Navigator新建Env环境,添加channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/tensorflow,opencv,numpy,matplotlib,beautifulsoup4,urllib3,scipy，并进入Home安装Jupyter notebook点击进入Jupyter Launch 基本命令conda update conda conda info --envs conda create --name tensorflow36 activate tensorflow36 conda deactivate conda config --set show_channel_urls yes 设置搜索时显示通道地址 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --remove channels https://mirrors.tuna.tsinghua.edu.cn/tensorflow/linux/cpu/ conda listopencv入门opencv图片读取与展示 # 引入opencv2 import cv2 # 文件读取-封装格式解析-数据解码-数据加载 img = cv2.imread(&#39;a.jpg&#39;,1) cv2.imshow(&#39;image&#39;,img) # jpg png是文件封装格式，文件头（数据解码信息，附加信息，解码器根据附加信息将文件数据还原成最原始的数据）+文件数据（非文件原始数据，是压缩编码后的数据） # stop cv2.waitKey (0) opencv图片写入import cv2 img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imwrite(&#39;image1.jpg&#39;,img) # 1 name 2 data opencv图像质量jpg有损压缩import cv2 img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imwrite(&#39;imageTest.jpg&#39;,img,[cv2.IMWRITE_JPEG_QUALITY,50]) # 1M 100k 10k 0-100 # jpg RGB颜色分量组成 1.14M=720*547*3*8 bit/8 (b)=1.14M png无损压缩# 透明度属性 import cv2 img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imwrite(&#39;imageTest.png&#39;,img,[cv2.IMWRITE_PNG_COMPRESSION,0]) # jpg 0 压缩比高0-100 png 0 压缩比低0-9 # png RGB alpha opencv像素操作import cv2 img = cv2.imread(&#39;image0.jpg&#39;,1) (b,g,r) = img[100,100] print(b,g,r)# bgr #10 100 --- 110 100 for i in range(1,100): img[10+i,100] = (255,0,0) cv2.imshow(&#39;image&#39;,img) cv2.waitKey(0) #1000 ms tensorflow入门 tf_常量变量所有变量必须初始化完成 #opencv tensorflow #类比 语法 api 原理 #基础数据类型 运算符 流程 字典 数组 import tensorflow as tf data1 = tf.constant(2,dtype=tf.int32) data2 = tf.Variable(10,name=&#39;var&#39;) print(data1) print(data2) &#39;&#39;&#39; sess = tf.Session() print(sess.run(data1)) init = tf.global_variables_initializer() sess.run(init) print(sess.run(data2)) sess.close() # 本质 tf = tensor + 计算图 # tensor 数据 # op # graphs 数据操作 # session &#39;&#39;&#39; init = tf.global_variables_initializer() sess = tf.Session() with sess: sess.run(init) print(sess.run(data2)) tf_四则运算常量import tensorflow as tf data1 = tf.constant(6) data2 = tf.constant(2) dataAdd = tf.add(data1,data2) dataMul = tf.multiply(data1,data2) dataSub = tf.subtract(data1,data2) dataDiv = tf.divide(data1,data2) with tf.Session() as sess: print(sess.run(dataAdd)) print(sess.run(dataMul)) print(sess.run(dataSub)) print(sess.run(dataDiv)) print(&#39;end!&#39;) 变量import tensorflow as tf data1 = tf.constant(6) data2 = tf.Variable(2) dataAdd = tf.add(data1,data2) dataCopy = tf.assign(data2,dataAdd)# dataAdd -&gt;data2 dataMul = tf.multiply(data1,data2) dataSub = tf.subtract(data1,data2) dataDiv = tf.divide(data1,data2) init = tf.global_variables_initializer() with tf.Session() as sess: sess.run(init) # 所有变量必须完成初始化 print(sess.run(dataAdd)) print(sess.run(dataMul)) print(sess.run(dataSub)) print(sess.run(dataDiv)) print(&#39;sess.run(dataCopy)&#39;,sess.run(dataCopy))#8-&gt;data2 print(&#39;dataCopy.eval()&#39;,dataCopy.eval())#8+6-&gt;14-&gt;data2 = 14 print(&#39;tf.get_default_session()&#39;,tf.get_default_session().run(dataCopy)) print(&#39;end!&#39;) tf矩阵基础1占位#placehold import tensorflow as tf data1 = tf.placeholder(tf.float32) data2 = tf.placeholder(tf.float32) dataAdd = tf.add(data1,data2) with tf.Session() as sess: print(sess.run(dataAdd,feed_dict={data1:6,data2:2})) # 1 dataAdd 2 data (feed_dict = {1:6,2:2}) print(&#39;end!&#39;) 矩阵打印#类比 数组 M行N列 [] 内部[] [里面 列数据] [] 中括号整体 行数 #[[6,6]] [[6,6]] import tensorflow as tf data1 = tf.constant([[6,6]]) data2 = tf.constant([[2], [2]]) data3 = tf.constant([[3,3]]) data4 = tf.constant([[1,2], [3,4], [5,6]]) print(data4.shape)# 维度 with tf.Session() as sess: print(sess.run(data4)) #打印整体 print(sess.run(data4[0]))# 打印某一行 print(sess.run(data4[:,0]))#打印某列 print(sess.run(data4[0,1]))# 1 1 MN = 0 32 = M012 N01 矩阵计算import tensorflow as tf data1 = tf.constant([[6,6]]) data2 = tf.constant([[2], [2]]) data3 = tf.constant([[3,3]]) data4 = tf.constant([[1,2], [3,4], [5,6]]) matMul = tf.matmul(data1,data2) matMul2 = tf.multiply(data1,data2) matAdd = tf.add(data1,data3) with tf.Session() as sess: print(sess.run(matMul))#1 维 M=1 N2. 1X2(MK) 2X1(KN) = 1 print(sess.run(matAdd))#1行2列 print(sess.run(matMul2))# 1x2 2x1 = 2x2 print(sess.run([matMul,matAdd])) 矩阵定义import tensorflow as tf mat0 = tf.constant([[0,0,0],[0,0,0]]) mat1 = tf.zeros([2,3]) mat2 = tf.ones([3,2]) mat3 = tf.fill([2,3],15) with tf.Session() as sess: #print(sess.run(mat0)) #print(sess.run(mat1)) #print(sess.run(mat2)) print(sess.run(mat3)) mat4 = tf.constant([[2],[3],[4]]) mat5 = tf.zeros_like(mat1) mat6 = tf.linspace(0.0,2.0,11) mat7 = tf.random_uniform([2,3],-1,2) with tf.Session() as sess: print(sess.run(mat5)) print(sess.run(mat6)) print(sess.run(mat7)) tf模块Numpy的使用#CURD import numpy as np data1 = np.array([1,2,3,4,5]) print(data1) data2 = np.array([[1,2], [3,4]]) print(data2) #维度 print(data1.shape,data2.shape) # zero ones print(np.zeros([2,3]),np.ones([2,2])) # 改查 data2[1,0] = 5 print(data2) print(data2[1,1]) # 基本运算 data3 = np.ones([2,3]) print(data3*2)#对应相乘 print(data3/3) print(data3+2) # 矩阵+* data4 = np.array([[1,2,3],[4,5,6]]) print(data3+data4) print(data3*data4) tf模块matplotlib的使用import numpy as np import matplotlib.pyplot as plt # 折线 x = np.array([1,2,3,4,5,6,7,8]) y = np.array([3,5,7,6,2,6,10,15]) plt.plot(x,y,&#39;r&#39;)# 折线 1 x 2 y 3 color plt.plot(x,y,&#39;g&#39;,lw=10)# 4 line w # 柱状 x = np.array([1,2,3,4,5,6,7,8]) y = np.array([13,25,17,36,21,16,10,15]) plt.bar(x,y,0.2,alpha=1,color=&#39;b&#39;)# 5 color 4 透明度 3 0.9 plt.show() 绘制股票k线import tensorflow as tf import numpy as np import matplotlib.pyplot as plt date = np.linspace(1,15,15) endPrice = np.array([2511.90,2538.26,2510.68,2591.66,2732.98,2701.69,2701.29,2678.67,2726.50,2681.50,2739.17,2715.07,2823.58,2864.90,2919.08] ) beginPrice = np.array([2438.71,2500.88,2534.95,2512.52,2594.04,2743.26,2697.47,2695.24,2678.23,2722.13,2674.93,2744.13,2717.46,2832.73,2877.40]) print(date) # 定义绘图 plt.figure() # 数据装载 for i in range(0,15): # 1 柱状图 dateOne = np.zeros([2]) dateOne[0] = i; dateOne[1] = i; priceOne = np.zeros([2]) priceOne[0] = beginPrice[i] priceOne[1] = endPrice[i] if endPrice[i]&gt;beginPrice[i]: plt.plot(dateOne,priceOne,&#39;r&#39;,lw=8) else: plt.plot(dateOne,priceOne,&#39;g&#39;,lw=8) plt.show() 神经网络逼近股票收盘均价# layer1：激励函数+乘加运算 import tensorflow as tf import numpy as np import matplotlib.pyplot as plt date = np.linspace(1,15,15) endPrice = np.array([2511.90,2538.26,2510.68,2591.66,2732.98,2701.69,2701.29,2678.67,2726.50,2681.50,2739.17,2715.07,2823.58,2864.90,2919.08] ) beginPrice = np.array([2438.71,2500.88,2534.95,2512.52,2594.04,2743.26,2697.47,2695.24,2678.23,2722.13,2674.93,2744.13,2717.46,2832.73,2877.40]) print(date) plt.figure() for i in range(0,15): # 1 柱状图 dateOne = np.zeros([2]) dateOne[0] = i; dateOne[1] = i; priceOne = np.zeros([2]) priceOne[0] = beginPrice[i] priceOne[1] = endPrice[i] if endPrice[i]&gt;beginPrice[i]: plt.plot(dateOne,priceOne,&#39;r&#39;,lw=8) else: plt.plot(dateOne,priceOne,&#39;g&#39;,lw=8) #plt.show() # 输入矩阵A:15*1 隐藏层矩阵B:15*10 数据矩阵C:15*1 # A(15x1)*w1(1x10)+b1(1*10) = B(15x10) A-&gt;B # B(15x10)*w2(10x1)+b2(15x1) = C(15x1) B-&gt;C # 1次循环 A-|w1 w2 b1 b2|-&gt;C 与真实值相差，在2次循环时，梯度下降修改|w1 w2 b1 b2|减少2次误差..... # 1 A B C dateNormal = np.zeros([15,1]) priceNormal = np.zeros([15,1]) for i in range(0,15): dateNormal[i,0] = i/14.0; priceNormal[i,0] = endPrice[i]/3000.0; x = tf.placeholder(tf.float32,[None,1]) y = tf.placeholder(tf.float32,[None,1]) # B w1 = tf.Variable(tf.random_uniform([1,10],0,1)) b1 = tf.Variable(tf.zeros([1,10])) wb1 = tf.matmul(x,w1)+b1 layer1 = tf.nn.relu(wb1) # 激励函数 # C w2 = tf.Variable(tf.random_uniform([10,1],0,1)) b2 = tf.Variable(tf.zeros([15,1])) wb2 = tf.matmul(layer1,w2)+b2 layer2 = tf.nn.relu(wb2) loss = tf.reduce_mean(tf.square(y-layer2))#y 真实 layer2 计算 train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) # 梯度下降缩小loss with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for i in range(0,10000): sess.run(train_step,feed_dict={x:dateNormal,y:priceNormal}) # w1w2 b1b2 A + wb --&gt;layer2 pred = sess.run(layer2,feed_dict={x:dateNormal}) predPrice = np.zeros([15,1]) for i in range(0,15): predPrice[i,0]=(pred*3000)[i,0] plt.plot(date,predPrice,&#39;b&#39;,lw=1) plt.show() 计算机视觉加强之几何变换图片缩放API图片缩放# 1 load 2 info 3 resize 4 check import cv2 # 1表示彩色图片 img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape print(imgInfo) height = imgInfo[0] width = imgInfo[1] mode = imgInfo[2] # 1 放大 缩小 2 等比例 非 2:3 dstHeight = int(height*0.5) dstWidth = int(width*0.5) #最近临域插值 双线性插值 像素关系重采样 立方插值 dst = cv2.resize(img,(dstWidth,dstHeight)) cv2.imshow(&#39;image&#39;,dst) cv2.waitKey(0) 最近临域插值 原理 src 1020 dst 5*10 dst&lt;-src (1,2) &lt;- (2,4) dst x 1 -&gt; src x 2 newX newX = x(src 行/目标 行) newX = 1（10/5） = 2 newY = y(src 列/目标 列) newY = 2*（20/10）= 4 12.3 = 12 双线性插值 原理 A1 = 20% 上+80%下 A2 B1 = 30% 左+70%右 B2 1 最终点 = A1 30% + A2 70% 2 最终点 = B1 20% + B2 80% 实质：矩阵运算 源码图片缩放# 1 info 2 空白模版 3 xy import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] dstHeight = int(height/2) dstWidth = int(width/2) dstImage = np.zeros((dstHeight,dstWidth,3),np.uint8)#0-255 for i in range(0,dstHeight):#行 for j in range(0,dstWidth):#列 iNew = int(i*(height*1.0/dstHeight)) jNew = int(j*(width*1.0/dstWidth)) dstImage[i,j] = img[iNew,jNew] cv2.imshow(&#39;dst&#39;,dstImage) cv2.waitKey(0) # 1 opencv API resize 2 算法原理 3 源码 源码图片剪切#100 -》200 x #100-》300 y import cv2 img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape dst = img[100:200,100:300] cv2.imshow(&#39;image&#39;,dst) cv2.waitKey(0) 矩阵图片移位# 1 API 2 算法原理 3 源代码 import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] #### matShift = np.float32([[1,0,100],[0,1,200]])# 2*3 dst = cv2.warpAffine(img,matShift,(height,width))#1 data 2 mat 3 info # 移位 矩阵 cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) [1,0,100],[0,1,200] 22 21 组成[[1,0],[0,1]] 22 A[[100],[200]] 21 Bxy CAC+B = [[1x+0y],[0x+1*y]]+[[100],[200]] = [[x+100],[y+200]](10,20)-&gt;(110,120) 矩阵图片缩放#[[A1 A2 B1],[A3 A4 B2]] # [[A1 A2],[A3 A4]] [[B1],[B2]] # newX = A1*x + A2*y+B1 # newY = A3*x +A4*y+B2 # x-&gt;x*0.5 y-&gt;y*0.5 # newX = 0.5*x import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] matScale = np.float32([[0.5,0,0],[0,0.5,0]]) dst = cv2.warpAffine(img,matScale,(int(width/2),int(height/2))) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 源码图片移位import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape dst = np.zeros(img.shape,np.uint8) height = imgInfo[0] width = imgInfo[1] for i in range(0,height): for j in range(0,width-100): dst[i,j+100]=img[i,j] cv2.imshow(&#39;image&#39;,dst) cv2.waitKey(0) 源码图片镜像import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] deep = imgInfo[2] newImgInfo = (height*2,width,deep) dst = np.zeros(newImgInfo,np.uint8)#uint8 for i in range(0,height): for j in range(0,width): dst[i,j] = img[i,j] #x y = 2*h - y -1 dst[height*2-i-1,j] = img[i,j] for i in range(0,width): dst[height,i] = (0,0,255)#BGR cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 仿射变换import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] #src 3-&gt;dst 3 (左上角 左下角 右上角) matSrc = np.float32([[0,0],[0,height-1],[width-1,0]]) matDst = np.float32([[50,50],[300,height-200],[width-300,100]]) #组合 定义仿射变换矩阵 matAffine = cv2.getAffineTransform(matSrc,matDst)# mat 1 src 2 dst dst = cv2.warpAffine(img,matAffine,(width,height)) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) API图片旋转import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] # 2*3 matRotate = cv2.getRotationMatrix2D((height*0.5,width*0.5),45,1)# mat rotate 1 center 2 angle 3 scale #100*100 25 dst = cv2.warpAffine(img,matRotate,(height,width)) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 计算机视觉加强之图像特效API灰度处理#imread #方法1 imread import cv2 img0 = cv2.imread(&#39;image0.jpg&#39;,0) img1 = cv2.imread(&#39;image0.jpg&#39;,1) print(img0.shape) print(img1.shape) cv2.imshow(&#39;src&#39;,img0) cv2.waitKey(0) #方法2 cvtColor img = cv2.imread(&#39;image0.jpg&#39;,1) dst = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)# 颜色空间转换 1 data 2 BGR gray cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 源码灰度处理import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] # RGB R=G=B = gray (R+G+B)/3 dst = np.zeros((height,width,3),np.uint8) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] gray = (int(b)+int(g)+int(r))/3 dst[i,j] = np.uint8(gray) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) #方法4 gray = r*0.299+g*0.587+b*0.114 dst = np.zeros((height,width,3),np.uint8) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] b = int(b) g = int(g) r = int(r) gray = r*0.299+g*0.587+b*0.114 dst[i,j] = np.uint8(gray) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0)算法优化# 1 灰度 最重要 2 基础 3 实时性 # 定点-》浮点 +- */ &gt;&gt; # r*0.299+g*0.587+b*0.114 import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] # RGB R=G=B = gray (R+G+B)/3 dst = np.zeros((height,width,3),np.uint8) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] b = int(b) g = int(g) r = int(r) #gray = (r*1+g*2+b*1)/4 gray = (r+(g&lt;&lt;1)+b)&gt;&gt;2 dst[i,j] = np.uint8(gray) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 图片颜色反转#0-255 255-当前 灰度图片颜色反转 import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) # 1表示一个像素一种颜色 dst = np.zeros((height,width,1),np.uint8) for i in range(0,height): for j in range(0,width): grayPixel = gray[i,j] dst[i,j] = 255-grayPixel cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) #RGB 255-R=newR 彩色图片颜色反转 dst = np.zeros((height,width,3),np.uint8) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] dst[i,j] = (255-b,255-g,255-r) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 马赛克import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] for m in range(100,300): for n in range(100,200): # pixel -&gt;10*10 所有像素点用一个点替代 if m%10 == 0 and n%10==0: # for循环填充小矩形 for i in range(0,10): for j in range(0,10): (b,g,r) = img[m,n] img[i+m,j+n] = (b,g,r) cv2.imshow(&#39;dst&#39;,img) cv2.waitKey(0) 毛玻璃import cv2 import numpy as np import random img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] dst = np.zeros((height,width,3),np.uint8) mm = 8 ## -mm防止当前矩阵越界 for m in range(0,height-mm): for n in range(0,width-mm): index = int(random.random()*8)#0-8 (b,g,r) = img[m+index,n+index] dst[m,n] = (b,g,r) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 图片融合# dst = src1*a+src2*(1-a) 两张图片都需要大于第一张图片宽和高度的一半 import cv2 import numpy as np img0 = cv2.imread(&#39;image0.jpg&#39;,1) img1 = cv2.imread(&#39;image1.jpg&#39;,1) imgInfo = img0.shape height = imgInfo[0] width = imgInfo[1] # ROI roiH = int(height/2) roiW = int(width/2) img0ROI = img0[0:roiH,0:roiW] img1ROI = img1[0:roiH,0:roiW] # dst dst = np.zeros((roiH,roiW,3),np.uint8) dst = cv2.addWeighted(img0ROI,0.5,img1ROI,0.5,0)#add src1*a+src2*(1-a) # 1 src1 2 a 3 src2 4 1-a cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 图片边缘检测import cv2 import numpy as np import random img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] cv2.imshow(&#39;src&#39;,img) #canny 1 gray 2 高斯 3 canny gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) imgG = cv2.GaussianBlur(gray,(3,3),0) dst = cv2.Canny(img,50,50) #图片卷积——》th cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 源码边缘检测import cv2 import numpy as np import random import math img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] cv2.imshow(&#39;src&#39;,img) # sobel 1 算子模版 2 图片卷积 3 阈值判决 # [1 2 1 [ 1 0 -1 # 0 0 0 2 0 -2 # -1 -2 -1 ] 1 0 -1 ] # 四个点 [1 2 3 4] 计算模板 [a b c d] 卷积后 a*1+b*2+c*3+d*4 = dst # sqrt(a*a+b*b) = f&gt;th 则为边缘 gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) dst = np.zeros((height,width,1),np.uint8) for i in range(0,height-2): for j in range(0,width-2): # 竖直方向梯度 gy = gray[i,j]*1+gray[i,j+1]*2+gray[i,j+2]*1-gray[i+2,j]*1-gray[i+2,j+1]*2-gray[i+2,j+2]*1 # 水平方向梯度 gx = gray[i,j]+gray[i+1,j]*2+gray[i+2,j]-gray[i,j+2]-gray[i+1,j+2]*2-gray[i+2,j+2] # 计算梯度 grad = math.sqrt(gx*gx+gy*gy) if grad&gt;50: dst[i,j] = 255 else: dst[i,j] = 0 cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 浮雕效果import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) # newP = gray0-gray1+150 dst = np.zeros((height,width,1),np.uint8) for i in range(0,height): for j in range(0,width-1): grayP0 = int(gray[i,j]) grayP1 = int(gray[i,j+1]) newP = grayP0-grayP1+150 if newP &gt; 255: newP = 255 if newP &lt; 0: newP = 0 dst[i,j] = newP cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 颜色风格import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] #rgb -》RGB new “蓝色” # b=b*1.5 # g = g*1.3 dst = np.zeros((height,width,3),np.uint8) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] b = b*1.5 g = g*1.3 if b&gt;255: b = 255 if g&gt;255: g = 255 dst[i,j]=(b,g,r) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 油画特效# 1 gray # 2 7*7 10*10 3 0-255 256 4 64 0-63 64-127 # 3 10 0-63 99 64-127 # 4 count 5 dst = result import cv2 import numpy as np img = cv2.imread(&#39;image00.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) dst = np.zeros((height,width,3),np.uint8) for i in range(4,height-4): for j in range(4,width-4): array1 = np.zeros(8,np.uint8) # 定义8*8小方块 for m in range(-4,4): for n in range(-4,4): # 灰度等级划分8个段 每段32，/32知道p1投影在哪个灰度等级段 p1 = int(gray[i+m,j+n]/32) # 当前像素值完成累加 array1[p1] = array1[p1]+1 currentMax = array1[0] # l定义某一段 l = 0 for k in range(0,8): if currentMax&lt;array1[k]: currentMax = array1[k] l = k # 简化或者均值 for m in range(-4,4): for n in range(-4,4): if gray[i+m,j+n]&gt;=(l*32) and gray[i+m,j+n]&lt;=((l+1)*32): (b,g,r) = img[i+m,j+n] dst[i,j] = (b,g,r) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 线段绘制import cv2 import numpy as np newImageInfo = (500,500,3) dst = np.zeros(newImageInfo,np.uint8) # line # 绘制线段 1 dst 2 begin 3 end 4 color cv2.line(dst,(100,100),(400,400),(0,0,255)) # 5 line w cv2.line(dst,(100,200),(400,200),(0,255,255),20) # 6 line type cv2.line(dst,(100,300),(400,300),(0,255,0),20,cv2.LINE_AA) cv2.line(dst,(200,150),(50,250),(25,100,255)) cv2.line(dst,(50,250),(400,380),(25,100,255)) cv2.line(dst,(400,380),(200,150),(25,100,255)) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 矩形圆形绘制import cv2 import numpy as np newImageInfo = (500,500,3) dst = np.zeros(newImageInfo,np.uint8) # 1 2 左上角 3 右下角 4 5 fill -1 &gt;0 line w cv2.rectangle(dst,(50,100),(200,300),(255,0,0),5) # 2 center 3 r cv2.circle(dst,(250,250),(50),(0,255,0),2) # 2 center 3 轴 4 angle 5 begin 6 end 7 cv2.ellipse(dst,(256,256),(150,100),0,0,180,(255,255,0),-1) points = np.array([[150,50],[140,140],[200,170],[250,250],[150,50]],np.int32) print(points.shape) points = points.reshape((-1,1,2)) print(points.shape) cv2.polylines(dst,[points],True,(0,255,255)) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 文字图片绘制import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) font = cv2.FONT_HERSHEY_SIMPLEX cv2.rectangle(img,(200,100),(500,400),(0,255,0),3) # 1 dst 2 文字内容 3 坐标 4 5 字体大小 6 color 7 粗细 8 line type cv2.putText(img,&#39;this is flow&#39;,(100,300),font,1,(200,100,255),2,cv2.LINE_AA) cv2.imshow(&#39;src&#39;,img) cv2.waitKey(0) height = int(img.shape[0]*0.2) width = int(img.shape[1]*0.2) imgResize = cv2.resize(img,(width,height)) for i in range(0,height): for j in range(0,width): img[i+200,j+350] = imgResize[i,j] cv2.imshow(&#39;src&#39;,img) cv2.waitKey(0) 计算机视觉加强之图像美化灰度直方图源码# 1 0-255 2 概率 # 本质：统计每个像素灰度 出现的概率 0-255 p import cv2 import numpy as np import matplotlib.pyplot as plt img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) count = np.zeros(256,np.float) for i in range(0,height): for j in range(0,width): pixel = gray[i,j] index = int(pixel) count[index] = count[index]+1 for i in range(0,255): count[i] = count[i]/(height*width) x = np.linspace(0,255,256) y = count plt.bar(x,y,0.9,alpha=1,color=&#39;b&#39;) plt.show() cv2.waitKey(0)彩色直方图源码# 本质：统计每个像素灰度 出现的概率 0-255 p import cv2 import numpy as np import matplotlib.pyplot as plt img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] count_b = np.zeros(256,np.float) count_g = np.zeros(256,np.float) count_r = np.zeros(256,np.float) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] index_b = int(b) index_g = int(g) index_r = int(r) count_b[index_b] = count_b[index_b]+1 count_g[index_g] = count_g[index_g]+1 count_r[index_r] = count_r[index_r]+1 for i in range(0,256): count_b[i] = count_b[i]/(height*width) count_g[i] = count_g[i]/(height*width) count_r[i] = count_r[i]/(height*width) x = np.linspace(0,255,256) y1 = count_b plt.figure() plt.bar(x,y1,0.9,alpha=1,color=&#39;b&#39;) y2 = count_g plt.figure() plt.bar(x,y2,0.9,alpha=1,color=&#39;g&#39;) y3 = count_r plt.figure() plt.bar(x,y3,0.9,alpha=1,color=&#39;r&#39;) plt.show() cv2.waitKey(0) 灰度直方图均衡化# 本质：统计每个像素灰度 出现的概率 0-255 p # 累计概率 # 1 0.2 0.2 # 2 0.3 0.5 # 3 0.1 0.6 # 256 # 100 0.5 255*0.5 = new import cv2 import numpy as np import matplotlib.pyplot as plt img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) cv2.imshow(&#39;src&#39;,gray) count = np.zeros(256,np.float) for i in range(0,height): for j in range(0,width): pixel = gray[i,j] index = int(pixel) count[index] = count[index]+1 for i in range(0,255): count[i] = count[i]/(height*width) #计算累计概率 sum1 = float(0) for i in range(0,256): sum1 = sum1+count[i] count[i] = sum1 #print(count) # 计算映射表 map1 = np.zeros(256,np.uint16) for i in range(0,256): map1[i] = np.uint16(count[i]*255) # 映射 for i in range(0,height): for j in range(0,width): pixel = gray[i,j] gray[i,j] = map1[pixel] cv2.imshow(&#39;dst&#39;,gray) cv2.waitKey(0)彩色直方图均衡化# 本质：统计每个像素灰度 出现的概率 0-255 p # 累计概率 # 1 0.2 0.2 # 2 0.3 0.5 # 3 0.1 0.6 # 256 # 100 0.5 255*0.5 = new # 1 统计每个颜色出现的概率 2 累计概率 1 3 0-255 255*p # 4 pixel import cv2 import numpy as np import matplotlib.pyplot as plt img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] count_b = np.zeros(256,np.float) count_g = np.zeros(256,np.float) count_r = np.zeros(256,np.float) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] index_b = int(b) index_g = int(g) index_r = int(r) count_b[index_b] = count_b[index_b]+1 count_g[index_g] = count_g[index_g]+1 count_r[index_r] = count_r[index_r]+1 for i in range(0,255): count_b[i] = count_b[i]/(height*width) count_g[i] = count_g[i]/(height*width) count_r[i] = count_r[i]/(height*width) #计算累计概率 sum_b = float(0) sum_g = float(0) sum_r = float(0) for i in range(0,256): sum_b = sum_b+count_b[i] sum_g = sum_g+count_g[i] sum_r = sum_r+count_r[i] count_b[i] = sum_b count_g[i] = sum_g count_r[i] = sum_r #print(count) # 计算映射表 map_b = np.zeros(256,np.uint16) map_g = np.zeros(256,np.uint16) map_r = np.zeros(256,np.uint16) for i in range(0,256): map_b[i] = np.uint16(count_b[i]*255) map_g[i] = np.uint16(count_g[i]*255) map_r[i] = np.uint16(count_r[i]*255) # 映射 dst = np.zeros((height,width,3),np.uint8) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] b = map_b[b] g = map_g[g] r = map_r[r] dst[i,j] = (b,g,r) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 彩色直方图APIimport cv2 import numpy as np def ImageHist(image,type): color = (255,255,255) windowName = &#39;Gray&#39; if type == 31: color = (255,0,0) windowName = &#39;B Hist&#39; elif type == 32: color = (0,255,0) windowName = &#39;G Hist&#39; elif type == 33: color = (0,0,255) windowName = &#39;R Hist&#39; # 计算图片直方图1 image 2 [0]灰度直方图 3 mask None蒙版 4 256 5 0-255 hist = cv2.calcHist([image],[0],None,[256],[0.0,255.0]) # 获取像素值中最大最小值及各自下标 归一化处理 minV,maxV,minL,maxL = cv2.minMaxLoc(hist) histImg = np.zeros([256,256,3],np.uint8) for h in range(256): #处理完后绘制结果 intenNormal = int(hist[h]*256/maxV) cv2.line(histImg,(h,256),(h,256-intenNormal),color) cv2.imshow(windowName,histImg) return histImg img = cv2.imread(&#39;image0.jpg&#39;,1) channels = cv2.split(img)# RGB - R G B for i in range(0,3): ImageHist(channels[i],31+i) cv2.waitKey(0) 直方图均衡化#灰度 直方图均衡化 import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) cv2.imshow(&#39;src&#39;,gray) dst = cv2.equalizeHist(gray) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) #彩色 直方图均衡化 import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) (b,g,r) = cv2.split(img)#通道分解 bH = cv2.equalizeHist(b) gH = cv2.equalizeHist(g) rH = cv2.equalizeHist(r) result = cv2.merge((bH,gH,rH))# 通道合成 cv2.imshow(&#39;dst&#39;,result) cv2.waitKey(0) #YUV 直方图均衡化 import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgYUV = cv2.cvtColor(img,cv2.COLOR_BGR2YCrCb) cv2.imshow(&#39;src&#39;,img) channelYUV = cv2.split(imgYUV) channelYUV[0] = cv2.equalizeHist(channelYUV[0]) channels = cv2.merge(channelYUV) result = cv2.cvtColor(channels,cv2.COLOR_YCrCb2BGR) cv2.imshow(&#39;dst&#39;,result) cv2.waitKey(0) 图片修补生成坏图import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) for i in range(200,300): img[i,200] = (255,255,255) img[i,200+1] = (255,255,255) img[i,200-1] = (255,255,255) for i in range(150,250): img[250,i] = (255,255,255) img[250+1,i] = (255,255,255) img[250-1,i] = (255,255,255) cv2.imwrite(&#39;damaged.jpg&#39;,img) cv2.imshow(&#39;image&#39;,img) cv2.waitKey(0)修补#1 坏图 2 array 3 inpaint import cv2 import numpy as np img = cv2.imread(&#39;damaged.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] paint = np.zeros((height,width,1),np.uint8) # 描绘坏的部分的数组 for i in range(200,300): paint[i,200] = 255 paint[i,200+1] = 255 paint[i,200-1] = 255 for i in range(150,250): paint[250,i] = 255 paint[250+1,i] = 255 paint[250-1,i] = 255 cv2.imshow(&#39;paint&#39;,paint) #1 src 2 mask imgDst = cv2.inpaint(img,paint,3,cv2.INPAINT_TELEA) cv2.imshow(&#39;image&#39;,imgDst) cv2.waitKey(0) 亮度增强p = p+40import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] cv2.imshow(&#39;src&#39;,img) dst = np.zeros((height,width,3),np.uint8) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] bb = int(b)+40 gg = int(g)+40 rr = int(r)+40 if bb&gt;255: bb = 255 if gg&gt;255: gg = 255 if rr&gt;255: rr = 255 dst[i,j] = (bb,gg,rr) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) p = p*1.2+40import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] cv2.imshow(&#39;src&#39;,img) dst = np.zeros((height,width,3),np.uint8) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] bb = int(b*1.3)+10 gg = int(g*1.2)+15 if bb&gt;255: bb = 255 if gg&gt;255: gg = 255 dst[i,j] = (bb,gg,r) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 磨皮美白### 双边滤波 import cv2 img = cv2.imread(&#39;1.png&#39;,1) cv2.imshow(&#39;src&#39;,img) dst = cv2.bilateralFilter(img,15,35,35) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 高斯滤波import cv2 import numpy as np img = cv2.imread(&#39;image11.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) dst = cv2.GaussianBlur(img,(5,5),1.5) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 均值滤波#均值 6*6 1 。 * 【6*6】/36 = mean -》P import cv2 import numpy as np img = cv2.imread(&#39;image11.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] dst = np.zeros((height,width,3),np.uint8) for i in range(3,height-3): for j in range(3,width-3): sum_b = int(0) sum_g = int(0) sum_r = int(0) for m in range(-3,3):#-3 -2 -1 0 1 2 for n in range(-3,3): (b,g,r) = img[i+m,j+n] sum_b = sum_b+int(b) sum_g = sum_g+int(g) sum_r = sum_r+int(r) b = np.uint8(sum_b/36) g = np.uint8(sum_g/36) r = np.uint8(sum_r/36) dst[i,j] = (b,g,r) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 中值滤波# 中值滤波 3*3 import cv2 import numpy as np img = cv2.imread(&#39;image11.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] img = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY) cv2.imshow(&#39;src&#39;,img) dst = np.zeros((height,width,3),np.uint8) collect = np.zeros(9,np.uint8) for i in range(1,height-1): for j in range(1,width-1): k = 0 for m in range(-1,2): for n in range(-1,2): gray = img[i+m,j+n] collect[k] = gray k = k+1 # 0 1 2 3 4 5 6 7 8 # 1 for k in range(0,9): p1 = collect[k] for t in range(k+1,9): if p1&lt;collect[t]: mid = collect[t] collect[t] = p1 p1 = mid dst[i,j] = collect[4] cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0)]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenCV与TensorFlow 机器学习]]></title>
    <url>%2F2019%2F09%2F02%2FOpenCV%E4%B8%8ETensorFlow%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[OpenCV与TensorFlow 机器学习 视频分解图片# 1 load 2 info 3 parse 4 imshow imwrite import cv2 cap = cv2.VideoCapture(&quot;1.mp4&quot;)# 获取一个视频打开cap 1 file name isOpened = cap.isOpened# 判断是否打开‘ print(isOpened) fps = cap.get(cv2.CAP_PROP_FPS)#帧率 width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))#w h height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) print(fps,width,height) i = 0 while(isOpened): if i == 10: break else: i = i+1 (flag,frame) = cap.read()# 读取每一张 flag frame fileName = &#39;image&#39;+str(i)+&#39;.jpg&#39; print(fileName) if flag == True: cv2.imwrite(fileName,frame,[cv2.IMWRITE_JPEG_QUALITY,100]) print(&#39;end!&#39;) 图片合成视频import cv2 img = cv2.imread(&#39;image1.jpg&#39;) imgInfo = img.shape size = (imgInfo[1],imgInfo[0]) print(size) videoWrite = cv2.VideoWriter(&#39;2.mp4&#39;,-1,5,size)# 写入对象 # 1 file name 2 编码器 3 帧率 4 size for i in range(1,11): fileName = &#39;image&#39;+str(i)+&#39;.jpg&#39; img = cv2.imread(fileName) videoWrite.write(img)# 写入方法 1 jpg data print(&#39;end!&#39;) 基于Haar+Adaboost人脸识别# 1 load xml 2 load jpg 3 haar gray 4 detect 5 draw import cv2 import numpy as np # load xml 1 file name face_xml = cv2.CascadeClassifier(&#39;haarcascade_frontalface_default.xml&#39;) eye_xml = cv2.CascadeClassifier(&#39;haarcascade_eye.xml&#39;) # load jpg img = cv2.imread(&#39;face.jpg&#39;) cv2.imshow(&#39;src&#39;,img) # haar gray gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) # detect faces 1 data 2 scale 3 5 faces = face_xml.detectMultiScale(gray,1.3,5) print(&#39;face=&#39;,len(faces)) # draw for (x,y,w,h) in faces: cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) roi_face = gray[y:y+h,x:x+w] roi_color = img[y:y+h,x:x+w] # 1 gray eyes = eye_xml.detectMultiScale(roi_face) print(&#39;eye=&#39;,len(eyes)) #for (e_x,e_y,e_w,e_h) in eyes: #cv2.rectangle(roi_color,(e_x,e_y),(e_x+e_w,e_y+e_h),(0,255,0),2) cv2.imshow(&#39;dst&#39;,img) cv2.waitKey(0) SVM身高体重预测# 1 思想 分类器 # 2 如何？ 寻求一个最优的超平面 分类 # 3 核：line # 4 数据：样本 # 5 训练 SVM_create train predict # svm本质 寻求一个最优的超平面 分类 # svm 核: line # 身高体重 训练 预测 import cv2 import numpy as np import matplotlib.pyplot as plt #1 准备data rand1 = np.array([[155,48],[159,50],[164,53],[168,56],[172,60]]) rand2 = np.array([[152,53],[156,55],[160,56],[172,64],[176,65]]) # 2 label label = np.array([[0],[0],[0],[0],[0],[1],[1],[1],[1],[1]]) # 3 data data = np.vstack((rand1,rand2)) data = np.array(data,dtype=&#39;float32&#39;) # svm 所有的数据都要有label # [155,48] -- 0 女生 [152,53] ---1 男生 # 监督学习 0 负样本 1 正样本 # 4 训练 svm = cv2.ml.SVM_create() # ml 机器学习模块 SVM_create() 创建 # 属性设置 svm.setType(cv2.ml.SVM_C_SVC) # svm type svm.setKernel(cv2.ml.SVM_LINEAR) # line svm.setC(0.01) # 训练 result = svm.train(data,cv2.ml.ROW_SAMPLE,label) # 预测 pt_data = np.vstack([[167,55],[162,57]]) #0 女生 1男生 pt_data = np.array(pt_data,dtype=&#39;float32&#39;) print(pt_data) (par1,par2) = svm.predict(pt_data) print(par2) Hog+SVM小狮子识别# 训练 # 1 参数 2hog 3 svm 4 computer hog 5 label 6 train 7 pred 8 draw import cv2 import numpy as np import matplotlib.pyplot as plt # 1 par PosNum = 820 # 正样本个数 NegNum = 1931 # 负样本个数 winSize = (64,128) blockSize = (16,16)# 105 blockStride = (8,8)#4 cell cellSize = (8,8) nBin = 9#9 bin 3780 # 2 hog create hog 1 win 2 block 3 blockStride 4 cell 5 bin hog = cv2.HOGDescriptor(winSize,blockSize,blockStride,cellSize,nBin) # 3 svm svm = cv2.ml.SVM_create() # 4 computer hog 特征提取存储 标签标识完成 featureNum = int(((128-16)/8+1)*((64-16)/8+1)*4*9) #3780 特征维度 featureArray = np.zeros(((PosNum+NegNum),featureNum),np.float32) # 用于装在特征，行正负样本个数，列就是特征维度 labelArray = np.zeros(((PosNum+NegNum),1),np.int32) # 标签 # svm 监督学习 样本 标签 svm -》image hog for i in range(0,PosNum): fileName = &#39;pos/&#39;+str(i+1)+&#39;.jpg&#39; img = cv2.imread(fileName) hist = hog.compute(img,(8,8))# 3780 for j in range(0,featureNum): featureArray[i,j] = hist[j] # featureArray hog [1,:] hog1 [2,:]hog2 labelArray[i,0] = 1 # 正样本 label 1 for i in range(0,NegNum): fileName = &#39;neg/&#39;+str(i+1)+&#39;.jpg&#39; img = cv2.imread(fileName) hist = hog.compute(img,(8,8))# 3780 for j in range(0,featureNum): featureArray[i+PosNum,j] = hist[j] labelArray[i+PosNum,0] = -1 # 负样本 label -1 svm.setType(cv2.ml.SVM_C_SVC) svm.setKernel(cv2.ml.SVM_LINEAR) svm.setC(0.01) # 6 train ret = svm.train(featureArray,cv2.ml.ROW_SAMPLE,labelArray) # 7 myHog ：《-myDetect # myDetect-《resultArray rho # myHog-》detectMultiScale # 7 检测 核心：create Hog -》 myDetect—》array-》 # resultArray-》resultArray = -1*alphaArray*supportVArray # rho-》svm-〉svm.train alpha = np.zeros((1),np.float32) rho = svm.getDecisionFunction(0,alpha) print(rho) print(alpha) alphaArray = np.zeros((1,1),np.float32) # 支持向量机数组 supportVArray = np.zeros((1,featureNum),np.float32) resultArray = np.zeros((1,featureNum),np.float32) alphaArray[0,0] = alpha resultArray = -1*alphaArray*supportVArray # detect检测创建 myDetect = np.zeros((3781),np.float32) for i in range(0,3780): myDetect[i] = resultArray[0,i] myDetect[3780] = rho[0] # rho svm （判决） myHog = cv2.HOGDescriptor() myHog.setSVMDetector(myDetect) # load 1表示彩色图片 imageSrc = cv2.imread(&#39;Test2.jpg&#39;,1) # (8,8) win 检测目标 objs = myHog.detectMultiScale(imageSrc,0,(8,8),(32,32),1.05,2) # xy wh 三维 最后一维 x = int(objs[0][0][0]) y = int(objs[0][0][1]) w = int(objs[0][0][2]) h = int(objs[0][0][3]) # 绘制展示 图片 起始位置 终止位置 颜色 线条宽度 cv2.rectangle(imageSrc,(x,y),(x+w,y+h),(255,0,0),2) cv2.imshow(&#39;dst&#39;,imageSrc) cv2.waitKey(0)]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow与Flask结合打造手写体数字识别]]></title>
    <url>%2F2019%2F08%2F30%2FTensorFlow%E4%B8%8EFlask%E7%BB%93%E5%90%88%E6%89%93%E9%80%A0%E6%89%8B%E5%86%99%E4%BD%93%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[TensorFlow与Flask结合打造手写体数字识别 AnacondaAnaconda Navigator新建Env环境,添加channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/tensorflow和opencv，并进入Home安装Jupyter notebook 定义模型modelmnist_testdemo/mnist/model.py 线性模型import tensorflow as tf # Y=W*x+b 线性模型 def regression(x): W = tf.Variable(tf.zeros([784, 10]), name=&quot;W&quot;) b = tf.Variable(tf.zeros([10]), name=&quot;b&quot;) y = tf.nn.softmax(tf.matmul(x, W) + b) return y, [W, b] 卷积模型# 卷积模型 def convolutional(x, keep_prob): # 卷积层 def conv2d(x, W): return tf.nn.conv2d(x, W, [1, 1, 1, 1], padding=&#39;SAME&#39;) # 池化层 def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&#39;SAME&#39;) # 定义权重 def weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial) # 边 def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) x_image = tf.reshape(x, [-1, 28, 28, 1]) W_conv1 = weight_variable([5, 5, 1, 32]) b_conv1 = bias_variable([32]) h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) h_pool1 = max_pool_2x2(h_conv1) W_conv2 = weight_variable([5, 5, 32, 64]) b_conv2 = bias_variable([64]) h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) h_pool2 = max_pool_2x2(h_conv2) # full connection W_fc1 = weight_variable([7 * 7 * 64, 1024]) b_fc1 = bias_variable([1024]) h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64]) h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) W_fc2 = weight_variable([1024, 10]) b_fc2 = bias_variable([10]) y = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) return y, [W_conv1, b_conv1, W_conv2, b_conv2, W_fc1, b_fc1, W_fc2, b_fc2] 定义数据mnist_testdemo/mnist/input_data.py from __future__ import absolute_import from __future__ import division from __future__ import print_function import gzip import os import tempfile import numpy from six.moves import urllib from six.moves import xrange import tensorflow as tf from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets 训练线性模型import os import input_data import model import tensorflow as tf # 从input_data中下载数据到MNIST_data data = input_data.read_data_sets(&#39;MNIST_data&#39;, one_hot=True) # create model with tf.variable_scope(&quot;regression&quot;): # 用户输入占位符 x = tf.placeholder(tf.float32, [None, 784]) y, variables = model.regression(x) # train y_ = tf.placeholder(&quot;float&quot;, [None, 10]) cross_entropy = -tf.reduce_sum(y_ * tf.log(y)) # 训练步骤 train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy) # 预测 correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) # 准确度 accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # 保存训练变量参数 saver = tf.train.Saver(variables) # 开始训练 with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for _ in range(20000): batch_xs, batch_ys = data.train.next_batch(100) sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys}) # 打印测试集和训练集的精准度 print((sess.run(accuracy, feed_dict={x:data.test.images, y_:data.test.labels}))) # 保存训练好的模型 path = saver.save( sess,os.path.join(os.path.dirname(__file__),&#39;data&#39;,&#39;regression.ckpt&#39;), write_meta_graph=False,write_state=False) print(&quot;Saved:&quot;, path) 生成mnist_testdemo/mnist/data/regression.ckpt.data-00000-of-00001和mnist_testdemo/mnist/data/regression.ckpt.index 训练卷积模型import os import model import tensorflow as tf import input_data data = input_data.read_data_sets(&#39;MNIST_data&#39;, one_hot=True) #model with tf.variable_scope(&quot;convolutional&quot;): x = tf.placeholder(tf.float32, [None, 784], name=&#39;x&#39;) keep_prob = tf.placeholder(tf.float32) y, variables = model.convolutional(x, keep_prob) #train y_ = tf.placeholder(tf.float32, [None, 10], name=&#39;y&#39;) cross_entropy = -tf.reduce_sum(y_ * tf.log(y)) # 随机梯度下降 train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) saver = tf.train.Saver(variables) with tf.Session() as sess: merged_summary_op = tf.summary.merge_all() summay_writer = tf.summary.FileWriter(&#39;./mnist_log/1&#39;, sess.graph) summay_writer.add_graph(sess.graph) sess.run(tf.global_variables_initializer()) for i in range(20000): batch = data.train.next_batch(50) if i % 100 == 0: train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0}) print(&quot;step %d, training accuracy %g&quot; % (i, train_accuracy)) sess.run(train_step, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5}) print(sess.run(accuracy, feed_dict={x: data.test.images, y_: data.test.labels, keep_prob: 1.0})) path = saver.save( sess, os.path.join(os.path.dirname(__file__), &#39;data&#39;, &#39;convalutional.ckpt&#39;), write_meta_graph=False, write_state=False) print(&quot;Saved:&quot;, path) 生成 mnist_testdemo/mnist/data/convalutional.ckpt.data-00000-of-00001和mnist_testdemo/mnist/data/convalutional.ckpt.index 集成flaskmnist_testdemo/main.py # -*- coding:utf-8 -*- import numpy as np import tensorflow as tf from flask import Flask, jsonify, render_template, request import pprint from mnist import model x = tf.placeholder(&quot;float&quot;, [None, 784]) sess = tf.Session() # 取出训练好的线性模型 with tf.variable_scope(&quot;regression&quot;): y1, variables = model.regression(x) saver = tf.train.Saver(variables) saver.restore(sess, &quot;mnist/data/regression.ckpt&quot;) # 取出训练好的卷积模型 with tf.variable_scope(&quot;convolutional&quot;): keep_prob = tf.placeholder(&quot;float&quot;) y2, variables = model.convolutional(x, keep_prob) saver = tf.train.Saver(variables) saver.restore(sess, &quot;mnist/data/convalutional.ckpt&quot;) # 根据输入调用线性模型并返回识别结果 def regression(input): return sess.run(y1, feed_dict={x: input}).flatten().tolist() # 根据输入调用卷积模型并返回识别结果 def convolutional(input): return sess.run(y2, feed_dict={x: input, keep_prob: 1.0}).flatten().tolist() app = Flask(__name__) @app.route(&#39;/api/mnist&#39;, methods=[&#39;POST&#39;]) def mnist(): # pprint.pprint(request.json) input = ((255 - np.array(request.json, dtype=np.uint8)) / 255.0).reshape(1, 784) output1 = regression(input) output2 = convolutional(input) pprint.pprint(output1) pprint.pprint(output2) return jsonify(results=[output1, output2]) @app.route(&#39;/&#39;) def main(): return render_template(&#39;index.html&#39;) if __name__ == &#39;__main__&#39;: app.debug = True app.run(host=&#39;0.0.0.0&#39;, port=8889) js核心代码drawInput() { var ctx = this.input.getContext(&#39;2d&#39;); var img = new Image(); img.onload = () =&gt; { var inputs = []; var small = document.createElement(&#39;canvas&#39;).getContext(&#39;2d&#39;); small.drawImage(img, 0, 0, img.width, img.height, 0, 0, 28, 28); var data = small.getImageData(0, 0, 28, 28).data; for (var i = 0; i &lt; 28; i++) { for (var j = 0; j &lt; 28; j++) { var n = 4 * (i * 28 + j); inputs[i * 28 + j] = (data[n + 0] + data[n + 1] + data[n + 2]) / 3; ctx.fillStyle = &#39;rgb(&#39; + [data[n + 0], data[n + 1], data[n + 2]].join(&#39;,&#39;) + &#39;)&#39;; ctx.fillRect(j * 5, i * 5, 5, 5); } } if (Math.min(...inputs) === 255) { return; } $.ajax({ url: &#39;/api/mnist&#39;, type: &#39;POST&#39;, contentType: &#39;application/json&#39;, data: JSON.stringify(inputs), success: (data) =&gt; { data = JSON.parse(data); for (let i = 0; i &lt; 2; i++) { var max = 0; var max_index = 0; for (let j = 0; j &lt; 10; j++) { var value = Math.round(data.results[i][j] * 1000); if (value &gt; max) { max = value; max_index = j; } var digits = String(value).length; for (var k = 0; k &lt; 3 - digits; k++) { value = &#39;0&#39; + value; } var text = &#39;0.&#39; + value; if (value &gt; 999) { text = &#39;1.000&#39;; } $(&#39;#output tr&#39;).eq(j + 1).find(&#39;td&#39;).eq(i).text(text); } for (let j = 0; j &lt; 10; j++) { if (j === max_index) { $(&#39;#output tr&#39;).eq(j + 1).find(&#39;td&#39;).eq(i).addClass(&#39;success&#39;); } else { $(&#39;#output tr&#39;).eq(j + 1).find(&#39;td&#39;).eq(i).removeClass(&#39;success&#39;); } } } } }); }; img.src = this.canvas.toDataURL(); } 前端将数据inputs以json传入/api/mnist regression(input)和convolutional(input)调用模型feed_dict喂参数返回结果]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nlp依存句法和语义依存分析]]></title>
    <url>%2F2019%2F08%2F27%2Fnlp%E4%BE%9D%E5%AD%98%E5%8F%A5%E6%B3%95%E5%92%8C%E8%AF%AD%E4%B9%89%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[nlp依存句法和语义依存分析 依存句法分析 依存语法 (Dependency Parsing, DP) 通过分析语言单位内成分之间的依存关系揭示其句法结构。 直观来讲,依存句法分析识别句子中的“主谓宾”、“定状补”这些语法成分,并分析各成分之间的关系。 依存句法分析标注关系 (共14种) 及含义如下:主谓关系 SBV subject-verb 我送她一束花 (我 &lt;– 送) 动宾关系 VOB 直接宾语,verb-object 我送她一束花 (送 –&gt; 花) 间宾关系 IOB 间接宾语,indirect-object 我送她一束花 (送 –&gt; 她) 前置宾语 FOB 前置宾语,fronting-object 他什么乢都读 (乢 &lt;– 读) 兼语 DBL double 他请我吃饭 (请 –&gt; 我) 定中关系 ATT attribute 红苹果 (红 &lt;– 苹果) 状中结构 ADV adverbial 非常美丽 (非常 &lt;– 美丽) 动补结构 CMP complement 做完了作业 (做 –&gt; 完) 并列关系 COO coordinate 大山和大海 (大山 –&gt; 大海) 介宾关系 POB preposition-object 在贸易区内 (在 –&gt; 内) 左附加关系 LAD left adjunct 大山和大海 (和 &lt;– 大海) 右附加关系 RAD right adjunct 孩子们 (孩子 –&gt; 们) 独立结构 IS independent structure 两个单句在结构上彼此独立 核心关系 HED head 指整个句子的核心 依存句法树解析recursionSearch.py # encoding=utf8 import re, os, json from stanfordParse import pos from stanfordParse import parse_sentence from recursionSearch import search def split_long_sentence_by_pos(text): del_flag = [&#39;DEC&#39;, &#39;AD&#39;, &#39;DEG&#39;, &#39;DER&#39;, &#39;DEV&#39;, &#39;SP&#39;, &#39;AS&#39;, &#39;ETC&#39;, &#39;SP&#39;, &#39;MSP&#39;, &#39;IJ&#39;, &#39;ON&#39;, &#39;JJ&#39;, &#39;FW&#39;, &#39;LB&#39;, &#39;SB&#39;, &#39;BA&#39;, &#39;AD&#39;, &#39;PN&#39;, &#39;RB&#39;] pos_tag = pos(text) new_str = &#39;&#39; for apos in pos_tag: if apos[1] not in del_flag: new_str += apos[0] return new_str def extract_parallel(text): parallel_text = [] pattern = re.compile(&#39;[，,][\u4e00-\u9fa5]{2,4}[，,]&#39;) search_obj = pattern.search(text) if search_obj: start_start, end = search_obj.span() rep = text[start_start:end - 2] rep1 = text[start_start:end - 1] if &#39;，&#39; in rep1: rep1.replace(&#39;，&#39;, &#39;、&#39;) if &#39;,&#39; in rep1: rep1.replace(&#39;,&#39;, &#39;、&#39;) text.replace(rep1, text) parallel_text.append(rep[1:]) text_leave = text.replace(rep, &#39;&#39;) while pattern.search(text_leave): start, end = pattern.search(text_leave).span() rep = text_leave[start:end - 2] rep1 = text[start_start:end - 1] if &#39;，&#39; in rep1: rep1.replace(&#39;，&#39;, &#39;、&#39;) if &#39;,&#39; in rep1: rep1.replace(&#39;,&#39;, &#39;、&#39;) text.replace(rep1, text) text_leave = text_leave.replace(rep, &#39;&#39;) parallel_text.append(rep[1:]) return parallel_text, text else: return None, text def split_long_sentence_by_sep(text): segment = [] if &#39;。&#39; or &#39;.&#39; or &#39;!&#39; or &#39;！&#39; or &#39;?&#39; or &#39;？&#39; or &#39;;&#39; or &#39;；&#39; in text: text = re.split(r&#39;[。.!！?？;；]&#39;, text) for seg in text: if seg == &#39;&#39; or seg == &#39; &#39;: continue para, seg = extract_parallel(seg) if len(seg) &gt; 19: seg = split_long_sentence_by_pos(seg) if len(seg) &gt; 19: seg = re.split(&#39;[，,]&#39;, seg) if isinstance(seg, list) and &#39;&#39; in seg: seg = seg.remove(&#39;&#39;) if isinstance(seg, list) and &#39; &#39; in seg: seg = seg.remove(&#39; &#39;) segment.append(seg) return segment def read_data(path): return open(path, &quot;r&quot;, encoding=&quot;utf8&quot;) def get_np_words(t): noun_phrase_list = [] for tree in t.subtrees(lambda t: t.height() == 3): if tree.label() == &#39;NP&#39; and len(tree.leaves()) &gt; 1: noun_phrase = &#39;&#39;.join(tree.leaves()) noun_phrase_list.append(noun_phrase) return noun_phrase_list def get_n_v_pair(t): for tree in t.subtrees(lambda t: t.height() == 3): if tree.label() == &#39;NP&#39; and len(tree.leaves()) &gt; 1: noun_phrase = &#39;&#39;.join(tree.leaves()) if __name__ == &quot;__main__&quot;: out = open(&quot;dependency.txt&quot;, &#39;w&#39;, encoding=&#39;utf8&#39;) itera = read_data(&#39;text.txt&#39;) for it in itera: s = parse_sentence(it) # 通过Stanfordnlp依存句法分析得到一个句法树 用nltk包装成树的结构 res = search(s) # 使用nltk遍历树，然后把短语合并 print(res) stanfordParse.py # encoding=utf8 from stanfordcorenlp import StanfordCoreNLP from nltk import Tree, ProbabilisticTree nlp = StanfordCoreNLP(&#39;E:/stanford-corenlp-full-2018-10-05&#39;, lang=&#39;zh&#39;) import nltk, re grammer = &quot;NP: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN&gt;}&quot; cp = nltk.RegexpParser(grammer) # 生成规则 pattern = re.compile(u&#39;[^a-zA-Z\u4E00-\u9FA5]&#39;) pattern_del = re.compile(&#39;(\a-zA-Z0-9+)&#39;) def _replace_c(text): &quot;&quot;&quot; 将英文标点符号替换成中文标点符号，并去除html语言的一些标志等噪音 :param text: :return: &quot;&quot;&quot; intab = &quot;,?!()&quot; outtab = &quot;，？！（）&quot; deltab = &quot; \n&lt;li&gt;&lt; li&gt;+_-.&gt;&lt;li \U0010fc01 _&quot; trantab = text.maketrans(intab, outtab, deltab) return text.translate(trantab) def parse_sentence(text): text = _replace_c(text) # 文本去噪 try: if len(text.strip()) &gt; 6: # 判断，文本是否大于6个字，小于6个字的我们认为不是句子 return Tree.fromstring( nlp.parse(text.strip())) # nlp.parse(text.strip())：是将句子变成依存句法树 Tree.fromstring是将str类型的树转换成nltk的结构的树 except: pass def pos(text): text = _replace_c(text) if len(text.strip()) &gt; 6: return nlp.pos_tag(text) else: return False def denpency_parse(text): return nlp.dependency_parse(text) from nltk.chunk.regexp import * sentenceSplit_host.py # encoding=utf8 import re, os, json from stanfordParse import pos from stanfordParse import parse_sentence from recursionSearch import search def split_long_sentence_by_pos(text): del_flag = [&#39;DEC&#39;, &#39;AD&#39;, &#39;DEG&#39;, &#39;DER&#39;, &#39;DEV&#39;, &#39;SP&#39;, &#39;AS&#39;, &#39;ETC&#39;, &#39;SP&#39;, &#39;MSP&#39;, &#39;IJ&#39;, &#39;ON&#39;, &#39;JJ&#39;, &#39;FW&#39;, &#39;LB&#39;, &#39;SB&#39;, &#39;BA&#39;, &#39;AD&#39;, &#39;PN&#39;, &#39;RB&#39;] pos_tag = pos(text) new_str = &#39;&#39; for apos in pos_tag: if apos[1] not in del_flag: new_str += apos[0] return new_str def extract_parallel(text): parallel_text = [] pattern = re.compile(&#39;[，,][\u4e00-\u9fa5]{2,4}[，,]&#39;) search_obj = pattern.search(text) if search_obj: start_start, end = search_obj.span() rep = text[start_start:end - 2] rep1 = text[start_start:end - 1] if &#39;，&#39; in rep1: rep1.replace(&#39;，&#39;, &#39;、&#39;) if &#39;,&#39; in rep1: rep1.replace(&#39;,&#39;, &#39;、&#39;) text.replace(rep1, text) parallel_text.append(rep[1:]) text_leave = text.replace(rep, &#39;&#39;) while pattern.search(text_leave): start, end = pattern.search(text_leave).span() rep = text_leave[start:end - 2] rep1 = text[start_start:end - 1] if &#39;，&#39; in rep1: rep1.replace(&#39;，&#39;, &#39;、&#39;) if &#39;,&#39; in rep1: rep1.replace(&#39;,&#39;, &#39;、&#39;) text.replace(rep1, text) text_leave = text_leave.replace(rep, &#39;&#39;) parallel_text.append(rep[1:]) return parallel_text, text else: return None, text def split_long_sentence_by_sep(text): segment = [] if &#39;。&#39; or &#39;.&#39; or &#39;!&#39; or &#39;！&#39; or &#39;?&#39; or &#39;？&#39; or &#39;;&#39; or &#39;；&#39; in text: text = re.split(r&#39;[。.!！?？;；]&#39;, text) for seg in text: if seg == &#39;&#39; or seg == &#39; &#39;: continue para, seg = extract_parallel(seg) if len(seg) &gt; 19: seg = split_long_sentence_by_pos(seg) if len(seg) &gt; 19: seg = re.split(&#39;[，,]&#39;, seg) if isinstance(seg, list) and &#39;&#39; in seg: seg = seg.remove(&#39;&#39;) if isinstance(seg, list) and &#39; &#39; in seg: seg = seg.remove(&#39; &#39;) segment.append(seg) return segment def read_data(path): return open(path, &quot;r&quot;, encoding=&quot;utf8&quot;) def get_np_words(t): noun_phrase_list = [] for tree in t.subtrees(lambda t: t.height() == 3): if tree.label() == &#39;NP&#39; and len(tree.leaves()) &gt; 1: noun_phrase = &#39;&#39;.join(tree.leaves()) noun_phrase_list.append(noun_phrase) return noun_phrase_list def get_n_v_pair(t): for tree in t.subtrees(lambda t: t.height() == 3): if tree.label() == &#39;NP&#39; and len(tree.leaves()) &gt; 1: noun_phrase = &#39;&#39;.join(tree.leaves()) if __name__ == &quot;__main__&quot;: out = open(&quot;dependency.txt&quot;, &#39;w&#39;, encoding=&#39;utf8&#39;) itera = read_data(&#39;text.txt&#39;) for it in itera: s = parse_sentence(it) # 通过Stanfordnlp依存句法分析得到一个句法树 用nltk包装成树的结构 res = search(s) # 使用nltk遍历树，然后把短语合并 print(res) 语义依存分析 语义依存分析：分析句子各个语言单位之间的语义关联,并将语义关联以依存结构呈现。使用语义依存刻画句子语义,好处在于丌需要去抽象词汇本身,而是通过词汇所承受的语义框架来描述该词汇,而论元的数目相对词汇来说数量总是少了很多的。语义依存分析目标是跨越句子表层句法结构的束缚,直接获取深层的语义信息。 例如以下三个句子,用不同的表达方式表达了同一个语义信息,即张三实施了一个吃的动作,吃的动作是对苹果实施的。 • 语义依存分析不受句法结构的影响,将具有直接语义关联的语言单元直接连接依存弧并标记上相应的语义关系。这也是语义依存分析不句法依存分析的重要区别。 • 语义依存关系分为三类,分别是主要语义角色,每一种语义角色对应存在一个嵌套关系和反关系;事件关系,描述两个事件间的关系;语义依附标记,标记说话者语气等依附性信息。 语义依存分析标注关系及含义如下:关系类型 Tag Description Example 施事关系 Agt Agent 我送她一束花 (我 &lt;-- 送) 当事关系 Exp Experiencer 我跑得快 (跑 --&gt; 我) 感事关系 Aft Affection 我思念家乡 (思念 --&gt; 我) 领事关系 Poss Possessor 他有一本好读 (他 &lt;-- 有) 受事关系 Pat Patient 他打了小明 (打 --&gt; 小明) 客事关系 Cont Content 他听到鞭炮声 (听 --&gt; 鞭炮声) 成事关系 Prod Product 他写了本小说 (写 --&gt; 小说) 源事关系 Orig Origin 我军缴获敌人四辆坦克 (缴获 --&gt; 坦克) 涉事关系 Datv Dative 他告诉我个秘密 ( 告诉 --&gt; 我 ) 比较角色 Comp Comitative 他成绩比我好 (他 --&gt; 我) 属事角色 Belg Belongings 老赵有俩女儿 (老赵 &lt;-- 有) 类事角色 Clas Classification 他是中学生 (是 --&gt; 中学生) 依据角色 Accd According 本庭依法宣判 (依法 &lt;-- 宣判) 缘故角色 Reas Reason 他在愁女儿婚事 (愁 --&gt; 婚事) 。。。。。。 名词短语块挖掘# encoding=utf8 import os, json, nltk, re from jpype import * from tokenizer import cut_hanlp huanhang = set([&#39;。&#39;, &#39;？&#39;, &#39;！&#39;, &#39;?&#39;]) keep_pos = &quot;q,qg,qt,qv,s,t,tg,g,gb,gbc,gc,gg,gm,gp,mg,Mg,n,an,ude1,nr,ns,nt,nz,nb,nba,nbc,nbp,nf,ng,nh,nhd,o,nz,nx,ntu,nts,nto,nth,ntch,ntcf,ntcb,ntc,nt,nsf,ns,nrj,nrf,nr2,nr1,nr,nnt,nnd,nn,nmc,nm,nl,nit,nis,nic,ni,nhm,nhd&quot; keep_pos_nouns = set(keep_pos.split(&quot;,&quot;)) keep_pos_v = &quot;v,vd,vg,vf,vl,vshi,vyou,vx,vi,vn&quot; keep_pos_v = set(keep_pos_v.split(&quot;,&quot;)) keep_pos_p = set([&#39;p&#39;, &#39;pbei&#39;, &#39;pba&#39;]) merge_pos = keep_pos_p | keep_pos_v keep_flag = set( [&#39;：&#39;, &#39;，&#39;, &#39;？&#39;, &#39;。&#39;, &#39;！&#39;, &#39;；&#39;, &#39;、&#39;, &#39;-&#39;, &#39;.&#39;, &#39;!&#39;, &#39;,&#39;, &#39;:&#39;, &#39;;&#39;, &#39;?&#39;, &#39;(&#39;, &#39;)&#39;, &#39;（&#39;, &#39;）&#39;, &#39;&lt;&#39;, &#39;&gt;&#39;, &#39;《&#39;, &#39;》&#39;]) drop_pos_set = set( [&#39;xu&#39;, &#39;xx&#39;, &#39;y&#39;, &#39;yg&#39;, &#39;wh&#39;, &#39;wky&#39;, &#39;wkz&#39;, &#39;wp&#39;, &#39;ws&#39;, &#39;wyy&#39;, &#39;wyz&#39;, &#39;wb&#39;, &#39;u&#39;, &#39;ud&#39;, &#39;ude1&#39;, &#39;ude2&#39;, &#39;ude3&#39;, &#39;udeng&#39;, &#39;udh&#39;]) def getNodes(parent, model_tagged_file): # 使用for循环遍历树 text = &#39;&#39; for node in parent: if type(node) is nltk.Tree: # 如果是NP或者VP的合并分词 if node.label() == &#39;NP&#39;: text += &#39;&#39;.join(node_child[0].strip() for node_child in node.leaves()) + &quot;/NP&quot; + 3 * &quot; &quot; if node.label() == &#39;VP&#39;: text += &#39;&#39;.join(node_child[0].strip() for node_child in node.leaves()) + &quot;/VP&quot; + 3 * &quot; &quot; else: # 不是树的，就是叶子节点，我们直接表解词PP或者其他O if node[1] in keep_pos_p: text += node[0].strip() + &quot;/PP&quot; + 3 * &quot; &quot; if node[0] in huanhang: text += node[0].strip() + &quot;/O&quot; + 3 * &quot; &quot; if node[1] not in merge_pos: text += node[0].strip() + &quot;/O&quot; + 3 * &quot; &quot; # print(&quot;hh&quot;) model_tagged_file.write(text + &quot;\n&quot;) def grammer(sentence, model_tagged_file): # {内/f 训/v 师/ng 单/b 柜/ng} &quot;&quot;&quot; input sentences shape like :[(&#39;工作&#39;, &#39;vn&#39;), (&#39;描述&#39;, &#39;v&#39;), (&#39;：&#39;, &#39;w&#39;), (&#39;我&#39;, &#39;rr&#39;), (&#39;曾&#39;, &#39;d&#39;), (&#39;在&#39;, &#39;p&#39;)] &quot;&quot;&quot; # 定义名词块 “&lt; &gt;”:一个单元 “*”：匹配零次或多次 “+”：匹配一次或多次 “&lt;ude1&gt;?”： “的”出现零次或一次 grammar1 = r&quot;&quot;&quot;NP: {&lt;m|mg|Mg|mq|q|qg|qt|qv|s|&gt;*&lt;a|an|ag&gt;*&lt;s|g|gb|gbc|gc|gg|gm|gp|n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|o|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd&gt;+&lt;f&gt;?&lt;ude1&gt;?&lt;g|gb|gbc|gc|gg|gm|gp|n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|o|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd&gt;+} {&lt;n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd&gt;+&lt;cc&gt;+&lt;n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd&gt;+} {&lt;m|mg|Mg|mq|q|qg|qt|qv|s|&gt;*&lt;q|qg|qt|qv&gt;*&lt;f|b&gt;*&lt;vi|v|vn|vg|vd&gt;+&lt;ude1&gt;+&lt;n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd&gt;+} {&lt;g|gb|gbc|gc|gg|gm|gp|n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd&gt;+&lt;vi&gt;?} VP:{&lt;v|vd|vg|vf|vl|vshi|vyou|vx|vi|vn&gt;+} &quot;&quot;&quot; # 动词短语块 cp = nltk.RegexpParser(grammar1) try: result = cp.parse(sentence) # nltk的依存语法分析，输出是以grammer设置的名词块为单位的树 except: pass else: getNodes(result, model_tagged_file) # 使用 getNodes 遍历树【这个是使用for循环，上一个是使用栈动态添加】 def data_read(): fout = open(&#39;nvp.txt&#39;, &#39;w&#39;, encoding=&#39;utf8&#39;) for line in open(&#39;text.txt&#39;, &#39;r&#39;, encoding=&#39;utf8&#39;): line = line.strip() grammer(cut_hanlp(line), fout) # 先进行hanlp进行分词，在使用grammer进行合并短语 fout.close() if __name__ == &#39;__main__&#39;: data_read() 自定义语法与CFG什么是语法解析? • 在自然语言学习过程中,每个人一定都学过语法,例如句子可以用主语、谓语、宾语来表示。在自然语言的处理过程中,有许多应用场景都需要考虑句子的语法,因此研究语法解析变得非常重要。 • 语法解析有两个主要的问题,其一是句子语法在计算机中的表达与存储方法,以及语料数据集;其二是语法解析的算法。 句子语法在计算机中的表达与存储方法• 对于第一个问题,我们可以用树状结构图来表示,如下图所示,S表示句子;NP、VP、PP是名词、动词、介词短语(短语级别);N、V、P分别是名词、动词、介词。 语法解析的算法上下文无关语法(Context-Free Grammer)• 为了生成句子的语法树,我们可以定义如下的一套上下文无关语法。 • 1)N表示一组非叶子节点的标注,例如{S、NP、VP、N...} • 2)Σ表示一组叶子结点的标注,例如{boeing、is...} • 3)R表示一组觃则,每条规则可以表示为 • 4)S表示语法树开始的标注 • 举例来说,语法的一个语法子集可以表示为下图所示。 当给定一个句子时,我们便可以按照从左到右的顺序来解析语法。 例如,句子the man sleeps就可以表示为(S (NP (DT the) (NN man)) (VP sleeps))。 概率分布的上下文无关语法(Probabilistic Context-Free Grammar)• 上下文无关的语法可以很容易的推导出一个句子的语法结构,但是缺点是推导出的结构可能存在二义性。 • 由于语法的解析存在二义性,我们就需要找到一种方法从多种可能的语法树中找出最可能的一棵树。 一种常见的方法既是PCFG (Probabilistic Context-Free Grammar)。 如下图所示,除了常见的语法规则以外,我们还对每一条规则赋予了一个概率。 对于每一棵生成的语法树,我们将其中所有规则的概率的乘积作为语法树的出现概率。 当我们获得多颗语法树时,我们可以分别计算每颗语法树的概率p(t),出现概率最大的那颗语法树就是我们希望得到的结果,即arg max p(t)。 训练算法• 我们已经定义了语法解析的算法,而这个算法依赖于CFG中对于N、Σ、 R、S的定义以及PCFG中的p(x)。上文中我们提到了Penn Treebank通 过手工的方法已经提供了一个非常大的语料数据集,我们的任务就是从 语料库中训练出PCFG所需要的参数。 • 1)统计出语料库中所有的N与Σ; • 2)利用语料库中的所有规则作为R; • 3)针对每个规则A -&gt; B,从语料库中估算p(x) = p(A -&gt; B) / p(A); • 在CFG的定义的基础上,我们重新定义一种叫Chomsky的语法格式。 这种格式要求每条规则只能是X -&gt; Y1 Y2或者X -&gt; Y的格式。实际上 Chomsky语法格式保证生产的语法树总是二叉树的格式,同时任意一 棵语法树总是能够转化成Chomsky语法格式。语法树预测算法• 假设我们已经有一个PCFG的模型,包含N、Σ、R、S、p(x)等参数,并 且语法树总是Chomsky语法格式。当输入一个句子x1, x2, ... , xn时, 我们要如何计算句子对应的语法树呢? • 第一种方法是暴力遍历的方法,每个单词x可能有m = len(N)种取值, 句子长度是n,每种情况至少存在n个规则,所以在时间复杂度O(m n n) 的情况下,我们可以判断出所有可能的语法树并计算出最佳的那个。 • 第二种方法当然是动态规划,我们定义w[i, j, X]是第i个单词至第j个单 词由标注X来表示的最大概率。直观来讲,例如xi, xi+1, ... , xj,当 X=PP时,子树可能是多种解释方式,如(P NP)或者(PP PP),但是w[i, j, PP]代表的是继续往上一层递归时,我们只选择当前概率最大的组合 方式。 语法解析按照上述的算法过程便完成了。虽说PCFG也有一些缺点,例如:1)缺乏词法信息;2)连续短语(如名词、介词)的处理等。但总体来讲它给语法解析提供了一种非常有效的实现方法。 # encoding=utf8 def exec_cmd(cmd): p = subprocess.Popen( cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, env=ENVIRON) out, err = p.communicate() return out, err import nltk, os, jieba from nltk.tree import Tree from nltk.draw import TreeWidget from nltk.draw.tree import TreeView from nltk.draw.util import CanvasFrame from nltk.parse import RecursiveDescentParser class Cfg(): &#39;&#39;&#39; &#39;&#39;&#39; def setUp(self): pass def tearDown(self): pass def test_sample(self): print(&quot;test_sample&quot;) # This is a CFG grammar, where: # Start Symbol : S # Nonterminal : NP,VP,DT,NN,VB # Terminal : &quot;I&quot;, &quot;a&quot; ,&quot;saw&quot; ,&quot;dog&quot; grammar = nltk.grammar.CFG.fromstring(&quot;&quot;&quot; S -&gt; NP VP NP -&gt; DT NN | NN VP -&gt; VB NP DT -&gt; &quot;a&quot; NN -&gt; &quot;I&quot; | &quot;dog&quot; VB -&gt; &quot;saw&quot; &quot;&quot;&quot;) sentence = &quot;I saw a dog&quot;.split() parser = RecursiveDescentParser(grammar) final_tree = parser.parse(sentence) for i in final_tree: print(i) def test_nltk_cfg_qtype(self): print(&quot;test_nltk_cfg_qtype&quot;) gfile = os.path.join( curdir, os.path.pardir, &quot;config&quot;, &quot;grammar.question-type.cfg&quot;) question_grammar = nltk.data.load(&#39;file:%s&#39; % gfile) def get_missing_words(grammar, tokens): &quot;&quot;&quot; Find list of missing tokens not covered by grammar &quot;&quot;&quot; missing = [tok for tok in tokens if not grammar._lexical_index.get(tok)] return missing sentence = &quot;what is your name&quot; sent = sentence.split() missing = get_missing_words(question_grammar, sent) target = [] for x in sent: if x in missing: continue target.append(x) rd_parser = RecursiveDescentParser(question_grammar) result = [] print(&quot;target: &quot;, target) for tree in rd_parser.parse(target): result.append(x) print(&quot;Question Type\n&quot;, tree) if len(result) == 0: print(&quot;Not Question Type&quot;) def cfg_en(self): print(&quot;test_nltk_cfg_en&quot;) # 定义英文语法规则 grammar = nltk.CFG.fromstring(&quot;&quot;&quot; S -&gt; NP VP VP -&gt; V NP | V NP PP V -&gt; &quot;saw&quot; | &quot;ate&quot; NP -&gt; &quot;John&quot; | &quot;Mary&quot; | &quot;Bob&quot; | Det N | Det N PP Det -&gt; &quot;a&quot; | &quot;an&quot; | &quot;the&quot; | &quot;my&quot; N -&gt; &quot;dog&quot; | &quot;cat&quot; | &quot;cookie&quot; | &quot;park&quot; PP -&gt; P NP P -&gt; &quot;in&quot; | &quot;on&quot; | &quot;by&quot; | &quot;with&quot; &quot;&quot;&quot;) sent = &quot;Mary saw Bob&quot;.split() rd_parser = RecursiveDescentParser(grammar) result = [] for i, tree in enumerate(rd_parser.parse(sent)): result.append(tree) assert len(result) &gt; 0, &quot; CFG tree parse fail.&quot; print(result) def cfg_zh(self): grammar = nltk.CFG.fromstring(&quot;&quot;&quot; S -&gt; N VP VP -&gt; V NP | V NP | V N V -&gt; &quot;尊敬&quot; N -&gt; &quot;我们&quot; | &quot;老师&quot; &quot;&quot;&quot;) sent = &quot;我们 尊敬 老师&quot;.split() rd_parser = RecursiveDescentParser(grammar) result = [] for i, tree in enumerate(rd_parser.parse(sent)): result.append(tree) print(&quot;Tree [%s]: %s&quot; % (i + 1, tree)) assert len(result) &gt; 0, &quot;Can not recognize CFG tree.&quot; if len(result) == 1: print(&quot;Draw tree with Display ...&quot;) result[0].draw() else: print(&quot;WARN: Get more then one trees.&quot;) print(result) if __name__ == &#39;__main__&#39;: cfg = Cfg() cfg.cfg_en() cfg.cfg_zh()]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nlp分词词性标注及命名实体]]></title>
    <url>%2F2019%2F08%2F27%2Fnlp%E5%88%86%E8%AF%8D%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8%E5%8F%8A%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%2F</url>
    <content type="text"><![CDATA[nlp分词词性标注及命名实体 分词==中文分词==(Chinese Word Segmentation) 指的是将一个汉字序列切分成一个一个单独的词。分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。 词性标注==词性标注==(Part-of-Speech tagging 或POS tagging) 又称词类标注或者简称标注,是指为分词结果中的每个单词标注一个正确的词性的程 序,也即确定每个词是名词、动词、形容词或其他词性的过程。在汉语中,词性标注比较简单,因为汉语词汇词性多变的情况比较少见,大多词语只有一个词性,或者出现频次最高的词性远远高于第二位的词性。据说,只需选取最高频词性,即可实现80%准确率的中文词性标注程序。 命名实体识别==命名实体识别==(Named Entity Recognition,简称NER) 又称作“专名识别”,是指识别文本中具有特定意义的实体,主要包括人名、地名、机构名、专有名词等。一般来说,命名实体识别的任务就是识别出待处理文本中三大类(实体类、时间类和数字类)、七小类(人名、机构名、地名、时间、日期、货币和百分比)命名实体。 在不同的顷目中,命名实体类别具有不同的定义。 准确分词之加载自定义字典分词当分词工具分词不准确时,该怎么办? 加载自定义字典?该如何加载?cut_data.py # -*- coding=utf8 -*- import jieba import re from tokenizer import cut_hanlp # 加载字典 jieba.load_userdict(&quot;dict.txt&quot;) # 交叉拼接list，把FLAG*替换成*期 def merge_two_list(a, b): c = [] len_a, len_b = len(a), len(b) minlen = min(len_a, len_b) for i in range(minlen): c.append(a[i]) c.append(b[i]) if len_a &gt; len_b: for i in range(minlen, len_a): c.append(a[i]) else: for i in range(minlen, len_b): c.append(b[i]) return c if __name__ == &quot;__main__&quot;: fp = open(&quot;text.txt&quot;, &quot;r&quot;, encoding=&quot;utf8&quot;) fout = open(&quot;result_cut.txt&quot;, &quot;w&quot;, encoding=&quot;utf8&quot;) # 特殊符号字典无法分离，正则区分 regex1 = u&#39;(?:[^\u4e00-\u9fa5（）*&amp;……%￥$，,。.@! ！]){1,5}期&#39; # 非汉字xxx期 regex2 = r&#39;(?:[0-9]{1,3}[.]?[0-9]{1,3})%&#39; # xx.xx% p1 = re.compile(regex1) p2 = re.compile(regex2) for line in fp.readlines(): # 逐行读取 result1 = p1.findall(line) # 返回匹配到的list if result1: regex_re1 = result1 line = p1.sub(&quot;FLAG1&quot;, line) # 将匹配到的替换成FLAG1 result2 = p2.findall(line) if result2: line = p2.sub(&quot;FLAG2&quot;, line) words = jieba.cut(line) # 结巴分词，type(word)返回一个generator object result = &quot; &quot;.join(words) # 结巴分词结果 本身是一个generator object，所以使用 “ ”.join() 拼接起来 # E:\hanlp\data\dictionary\custom\resume_nouns.txt在E:\hanlp\hanlp.properties配置 # CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; 现代汉语补充词库.txt; 全国地名大全.txt ns; 人名词典.txt; 机构名词典.txt; resume_nouns.txt; 上海地名.txt ns;data/dictionary/person/nrf.txt nrf; words1 = cut_hanlp(line) # hanlp分词结果，返回的是str if &quot;FLAG1&quot; in result: result = result.split(&quot;FLAG1&quot;) result = merge_two_list(result, result1) ss = result result = &quot;&quot;.join(result) # 本身是个list，我们需要的是str，所以使用 &quot;&quot;.join() 拼接起来 if &quot;FLAG2&quot; in result: result = result.split(&quot;FLAG2&quot;) result = merge_two_list(result, result2) result = &quot;&quot;.join(result) # print(result) fout.write(&quot;jieba:&quot; + result) fout.write(&quot;hanlp:&quot; + words1) fout.close() tokenizer.py # encoding=utf8 import os, gc, re, sys from jpype import * root_path = &quot;E:/hanlp&quot; djclass_path = &quot;-Djava.class.path=&quot; + root_path + os.sep + &quot;hanlp-1.7.4.jar;&quot; + root_path startJVM(getDefaultJVMPath(), djclass_path, &quot;-Xms1g&quot;, &quot;-Xmx1g&quot;) Tokenizer = JClass(&#39;com.hankcs.hanlp.tokenizer.StandardTokenizer&#39;) def to_string(sentence, return_generator=False): if return_generator: return (word_pos_item.toString().split(&#39;/&#39;) for word_pos_item in Tokenizer.segment(sentence)) else: return &quot; &quot;.join([word_pos_item.toString().split(&#39;/&#39;)[0] for word_pos_item in Tokenizer.segment(sentence)]) # 这里的“”.split(&#39;/&#39;)可以将string拆分成list 如：&#39;ssfa/fsss&#39;.split(&#39;/&#39;) =&gt; [&#39;ssfa&#39;, &#39;fsss&#39;] def seg_sentences(sentence, with_filter=True, return_generator=False): segs = to_string(sentence, return_generator=return_generator) if with_filter: g = [word_pos_pair[0] for word_pos_pair in segs if len(word_pos_pair) == 2 and word_pos_pair[0] != &#39; &#39; and word_pos_pair[1] not in drop_pos_set] else: g = [word_pos_pair[0] for word_pos_pair in segs if len(word_pos_pair) == 2 and word_pos_pair[0] != &#39; &#39;] return iter(g) if return_generator else g def cut_hanlp(raw_sentence, return_list=True): if len(raw_sentence.strip()) &gt; 0: return to_string(raw_sentence) if return_list else iter(to_string(raw_sentence)) 准确分词之动态调整词频和字典顺序当分词字典的词冲突,相互影响该怎么办? 调整词频和字典顺序。cut_data.py # -*- coding=utf8 -*- import jieba import re from tokenizer import cut_hanlp jieba.load_userdict(&quot;dict.txt&quot;) # # 设置高词频：一个 # jieba.suggest_freq(&#39;台中&#39;,tune=True) # 设置高词频：dict.txt中的每一行都设置一下 # fp=open(&quot;dict.txt&quot;, &#39;r&#39;, encoding=&#39;utf8&#39;) # for line in fp: # line = line.strip() # jieba.suggest_freq(line, tune=True) # # 设置高词频：dict.txt中的每一行都设置一下快速方法 [jieba.suggest_freq(line.strip(), tune=True) for line in open(&quot;dict.txt&quot;, &#39;r&#39;, encoding=&#39;utf8&#39;)] if __name__ == &quot;__main__&quot;: string = &quot;台中正确应该不会被切开。&quot; # 通过调整词频 suggest_freq(line, tune=True) words_jieba = &quot; &quot;.join(jieba.cut(string, HMM=False)) # 通过排序sort_dict_by_lenth，优先按照长的字典项匹配 words_hanlp = cut_hanlp(string) print(&quot;words_jieba:&quot; + words_jieba, &#39;\n&#39;, &quot;words_hanlp:&quot; + words_hanlp) sort_dict_by_lenth.py # encoding=utf8 import os dict_file = open( &quot;E:&quot; + os.sep + &quot;hanlp&quot; + os.sep + &quot;data&quot; + os.sep + &quot;dictionary&quot; + os.sep + &quot;custom&quot; + os.sep + &quot;resume_nouns.txt&quot;, &#39;r&#39;, encoding=&#39;utf8&#39;) d = {} [d.update({line: len(line.split(&quot; &quot;)[0])}) for line in dict_file] # 读取源字典文件并从长到短排序 优先匹配长字典项 f = sorted(d.items(), key=lambda x: x[1], reverse=True) dict_file = open( &quot;E:&quot; + os.sep + &quot;hanlp&quot; + os.sep + &quot;data&quot; + os.sep + &quot;dictionary&quot; + os.sep + &quot;custom&quot; + os.sep + &quot;resume_nouns1.txt&quot;, &#39;w&#39;, encoding=&#39;utf8&#39;) [dict_file.write(item[0]) for item in f] dict_file.close() 词性标注代码实现及信息提取extract_data.py # -*- coding=utf8 -*- import jieba import re from tokenizer import seg_sentences fp = open(&quot;text.txt&quot;, &#39;r&#39;, encoding=&#39;utf8&#39;) fout = open(&quot;out.txt&quot;, &#39;w&#39;, encoding=&#39;utf8&#39;) for line in fp: line = line.strip() if len(line) &gt; 0: fout.write(&#39; &#39;.join(seg_sentences(line)) + &quot;\n&quot;) fout.close() if __name__ == &quot;__main__&quot;: pass tokenizer.py # encoding=utf8 import os, gc, re, sys from jpype import * root_path = &quot;E:/hanlp&quot; djclass_path = &quot;-Djava.class.path=&quot; + root_path + os.sep + &quot;hanlp-1.7.4.jar;&quot; + root_path startJVM(getDefaultJVMPath(), djclass_path, &quot;-Xms1g&quot;, &quot;-Xmx1g&quot;) Tokenizer = JClass(&#39;com.hankcs.hanlp.tokenizer.StandardTokenizer&#39;) keep_pos = &quot;q,qg,qt,qv,s,t,tg,g,gb,gbc,gc,gg,gm,gp,m,mg,Mg,mq,n,an,vn,ude1,nr,ns,nt,nz,nb,nba,nbc,nbp,nf,ng,nh,nhd,o,nz,nx,ntu,nts,nto,nth,ntch,ntcf,ntcb,ntc,nt,nsf,ns,nrj,nrf,nr2,nr1,nr,nnt,nnd,nn,nmc,nm,nl,nit,nis,nic,ni,nhm,nhd&quot; keep_pos_nouns = set(keep_pos.split(&quot;,&quot;)) keep_pos_v = &quot;v,vd,vg,vf,vl,vshi,vyou,vx,vi&quot; keep_pos_v = set(keep_pos_v.split(&quot;,&quot;)) keep_pos_p = set([&#39;p&#39;, &#39;pbei&#39;, &#39;pba&#39;]) drop_pos_set = set( [&#39;xu&#39;, &#39;xx&#39;, &#39;y&#39;, &#39;yg&#39;, &#39;wh&#39;, &#39;wky&#39;, &#39;wkz&#39;, &#39;wp&#39;, &#39;ws&#39;, &#39;wyy&#39;, &#39;wyz&#39;, &#39;wb&#39;, &#39;u&#39;, &#39;ud&#39;, &#39;ude1&#39;, &#39;ude2&#39;, &#39;ude3&#39;, &#39;udeng&#39;, &#39;udh&#39;, &#39;p&#39;, &#39;rr&#39;, &#39;w&#39;]) han_pattern = re.compile(r&#39;[^\dA-Za-z\u3007\u4E00-\u9FCB\uE815-\uE864]+&#39;) HanLP = JClass(&#39;com.hankcs.hanlp.HanLP&#39;) def to_string(sentence, return_generator=False): if return_generator: return (word_pos_item.toString().split(&#39;/&#39;) for word_pos_item in Tokenizer.segment(sentence)) else: return [(word_pos_item.toString().split(&#39;/&#39;)[0], word_pos_item.toString().split(&#39;/&#39;)[1]) for word_pos_item in Tokenizer.segment(sentence)] def seg_sentences(sentence, with_filter=True, return_generator=False): segs = to_string(sentence, return_generator=return_generator) if with_filter: g = [word_pos_pair[0] for word_pos_pair in segs if len(word_pos_pair) == 2 and word_pos_pair[0] != &#39; &#39; and word_pos_pair[1] not in drop_pos_set] else: g = [word_pos_pair[0] for word_pos_pair in segs if len(word_pos_pair) == 2 and word_pos_pair[0] != &#39; &#39;] return iter(g) if return_generator else g TextRank算法原理介绍tex_rank.py # -*- coding=utf8 -*- from jieba import analyse # 引入TextRank关键词抽取接口 textrank = analyse.textrank # 原始文本 text = &quot;非常线程是程序执行时的最小单位，它是进程的一个执行流，\ 是CPU调度和分派的基本单位，一个进程可以由很多个线程组成，\ 线程间共享进程的所有资源，每个线程有自己的堆栈和局部变量。\ 线程由CPU独立调度执行，在多CPU环境下就允许多个线程同时运行。\ 同样多线程也可以实现并发操作，每个请求分配一个线程来处理。&quot; print(&quot;\nkeywords by textrank:&quot;) # 基于TextRank算法进行关键词抽取 keywords = textrank(text, topK=10, withWeight=True, allowPOS=(&#39;ns&#39;, &#39;n&#39;)) # 输出抽取出的关键词 f words = [keyword for keyword, w in keywords if w &gt; 0.2] print(&#39; &#39;.join(words) + &quot;\n&quot;) jieba 词性标注 标注 含义 来源 Ag 形语素 形容词性语素形容词代码为 a,语素代码g前面置以A a 形容词 取英语形容词 adjective的第1个字母 ad 副形词 直接作状语的形容词形容词代码 a和副词代码d并在一起 an 名形词 具有名词功能的形容词形容词代码 a和名词代码n并在一起 b 区别词 取汉字“别”的声母 c 连词 取英语连词 conjunction的第1个字母 dg 副语素 副词性语素副词代码为 d,语素代码g前面置以D d 副词 取 adverb的第2个字母,因其第1个字母已用于形容词 e 叹词 取英语叹词 exclamation的第1个字母 f 方位词 取汉字“方” g 语素 绝大多数语素都能作为合成词的“词根”,取汉字“根”的声母 h 前接成分 取英语 head的第1个字母 i 成语 取英语成语 idiom的第1个字母 j 简称略语 取汉字“简”的声母 k 后接成分 l 习用语 习用语尚未成为成语,有点“临时性”,取“临”的声母 m 数词 取英语 numeral的第3个字母,n,u已有他用 Ng 名语素 名词性语素名词代码为 n,语素代码g前面置以N n 名词 取英语名词 noun的第1个字母 nr 人名 名词代码 n和“人(ren)”的声母并在一起 ns 地名 名词代码 n和处所词代码s并在一起 nt 机构团体 “团”的声母为 t,名词代码n和t并在一起 nz 其他丏名 “丏”的声母的第 1个字母为z,名词代码n和z并在一起 o 拟声词 取英语拟声词 onomatopoeia的第1个字母 p 介词 取英语介词 prepositional的第1个字母 q 量词 取英语 quantity的第1个字母 r 代词 取英语代词 pronoun的第2个字母,因p已用于介词 s 处所词 取英语 space的第1个字母 tg 时语素 时间词性语素时间词代码为 t,在语素的代码g前面置以T t 时间词 取英语 time的第1个字母 u 助词 取英语助词 auxiliary vg 动语素 动词性语素动词代码为 v在语素的代码g前面置以V v 动词 取英语动词 verb的第一个字母 vd 副动词 直接作状语的动词动词和副词的代码并在一起 vn 名动词 指具有名词功能的动词动词和名词的代码并在一起 w 标点符号 x 非语素字 非语素字只是一个符号,字母 x通常用于代表未知数、符号 y 语气词 取汉字“语”的声母 z 状态词 取汉字“状”的声母的前一个字母 un 未知词 不可识别词及用户自定义词组取英文Unkonwn首两个字母(非北大标准,CSW分词中定义) hanlp词性标注 标注 含义 a 形容词 ad 副形词 ag 形容词性语素 al 形容词性惯用语 an 名形词 b 区别词 begin 仅用于始##始 bg 区别语素 bl 区别词性惯用语 c 连词 cc 并列连词 d 副词 dg 辄,俱,复之类的副词 dl 连语 e 叹词 end 仅用于终##终 f 方位词 g 学术词汇 gb 生物相关词汇 gbc 生物类别 gc 化学相关词汇 gg 地理地质相关词汇 gi 计算机相关词汇 gm 数学相关词汇 gp 物理相关词汇 h 前缀 i 成语 j 简称略语 k 后缀 l 习用语 m 数词 mg 数语素 Mg 甲乙丙丁之类的数词 mq 数量词 n 名词 nb 生物名 nba 动物名 nbc 动物纲目 nbp 植物名 nf 食品，比如“薯片” ng 名词性语素 nh 医药疾病等健康相关名词 nhd 疾病 nhm 药品 ni 机构相关（不是独立机构名） nic 下属机构 nis 机构后缀 nit 教育相关机构 nl 名词性惯用语 nm 物品名 nmc 化学品名 nn 工作相关名词 nnd 职业 nnt 职务职称 nr 人名 nr1 复姓 nr2 蒙古姓名 nrf 音译人名 nrj 日语人名 ns 地名 nsf 音译地名 nt 机构团体名 ntc 公司名 ntcb 银行 ntcf 工厂 ntch 酒店宾馆 nth 医院 nto 政府机构 nts 中小学 ntu 大学 nx 字母专名 nz 其他专名 o 拟声词 p 介词 pba 介词“把” pbei 介词“被” q 量词 qg 量词语素 qt 时量词 qv 动量词 r 代词 rg 代词性语素 Rg 古汉语代词性语素 rr 人称代词 ry 疑问代词 rys 处所疑问代词 ryt 时间疑问代词 ryv 谓词性疑问代词 rz 指示代词 rzs 处所指示代词 rzt 时间指示代词 rzv 谓词性指示代词 s 处所词 t 时间词 tg 时间词性语素 u 助词 ud 助词 ude1 的 底 ude2 地 ude3 得 udeng 等 等等 云云 udh 的话 ug 过 uguo 过 uj 助词 ul 连词 ule 了 喽 ulian 连 （“连小学生都会”） uls 来讲 来说 而言 说来 usuo 所 uv 连词 uyy 一样 一般 似的 般 uz 着 uzhe 着 uzhi 之 v 动词 vd 副动词 vf 趋向动词 vg 动词性语素 vi 不及物动词（内动词） vl 动词性惯用语 vn 名动词 vshi 动词“是” vx 形式动词 vyou 动词“有” w 标点符号 wb 百分号千分号，全角：％ ‰ 半角：% wd 逗号，全角：， 半角：, wf 分号，全角：； 半角： ; wh 单位符号，全角：￥ ＄ ￡ ° ℃ 半角：$ wj 句号，全角：。 wky 右括号，全角：） 〕 ］ ｝ 》 】 〗 〉 半角： ) ] { &gt; wkz 左括号，全角：（ 〔 ［ ｛ 《 【 〖 〈 半角：( [ { &lt; wm 冒号，全角：： 半角： : wn 顿号，全角：、 wp 破折号，全角：—— －－ ——－ 半角：— —- ws 省略号，全角：…… … wt 叹号，全角：！ ww 问号，全角：？ wyy 右引号，全角：” ’ 』 wyz 左引号，全角：“ ‘ 『 x 字符串 xu 网址URL xx 非语素字 y 语气词(delete yg) yg 语气语素 z 状态词 zg 状态词]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nlp基础]]></title>
    <url>%2F2019%2F08%2F26%2Fnlp%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[nlp基础 简介 NLP( Natural Language Processing ) 是 自然 语言 处理 的 简称,是研究人与计算机交互的语言问题的一门学科。机器理解并解释人类写作与说话方式的能力。近年来,深度学习技术在自然语言处理方面的研究和应用也取得了显著的成果。 提问和回答、知识工程、语言生成、语音识别,语音合成,自动分词,句法分析,语法纠错,关键词提取,文本分类/聚类,文本自动摘要,信息检索(ES,Solr),信息抽取,知识图谱,机器翻译,人机对话,机器写作,情感分析,文字识别,阅读理解,推荐系统,高考机器人等。 环境搭建Anaconda3-5.1.0-Windows-x86_64.exe将Anaconda加入系统环境变量 常用开发包numpy numpy系统是Python的一种开源的数值计算包。 包括：1、一个强大的N维数组对象Array；2、比较成熟的（广播）函数库；3、用于整合C/C++和Fortran代码的工具包；4、实用的线性代数、傅里叶变换和随机数生成函数。numpy和稀疏矩阵运算包scipy配合使用更加方便。 conda install numpy NLTK Natural Language Toolkit，自然语言处理工具包，在NLP领域中， 最常使用的一个Python库。 conda install nltk Gensim Gensim是一个占内存低，接口简单，免费的Python库，它可以用来从文档中自动提取语义主题。它包含了很多非监督学习算法如：TF/IDF，潜在语义分析（Latent Semantic Analysis，LSA）、隐含狄利克雷分配（Latent Dirichlet Allocation，LDA），层次狄利克雷过程 （Hierarchical Dirichlet Processes，HDP）等。 Gensim支持Word2Vec,Doc2Vec等模型。 conda install gensimpip install gensim如不可安装python库gensim‑3.8.0‑cp36‑cp36m‑win_amd64.whl下载后pip install Tensorflow TensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统。TensorFlow可被用于语音识别或图像识别等多项机器学习和深度学习领域。TensorFlow是一个采用数据流图（data flow graphs），用于数值计算的开源软件库。节点（Nodes）在图中表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor）。它灵活的架构让你可以在多种平台上展开计算，例如台式计算机中的一个或多个CPU（戒GPU），服务器，移动设备等等。TensorFlow 最初由Google大脑小组（隶属于Google机器智能研究机构）的研究员和工程师们开发出来，用于机器学习和深度神经网络方面的研究，但这个系统的通用 性使其也可广泛用于其他计算领域。 conda install tensorflowpip install tensorflow python库pip install tensorflow-1.9.0-cp36-cp36m-win_amd64.whl下载后pip install jieba “结巴”中文分词：是广泛使用的中文分词工具，具有以下特点： 1）三种分词模式：精确模式，全模式和搜索引擎模式 2）词性标注和返回词语在原文的起止位置（ Tokenize） 3）可加入自定义字典 4）代码对 Python 2/3 均兼容 5）支持多种语言，支持简体繁体 项目地址 pip install jieba demo# encoding=utf-8 import jieba import jieba.posseg as pseg print(&quot;\njieba分词全模式：&quot;) seg_list = jieba.cut(&quot;我来到北京清华大学&quot;, cut_all=True) print(&quot;Full Mode: &quot; + &quot;/ &quot;.join(seg_list)) # 全模式 print(&quot;\njieba分词精确模式：&quot;) seg_list = jieba.cut(&quot;我来到北京清华大学&quot;, cut_all=False) print(&quot;Default Mode: &quot; + &quot;/ &quot;.join(seg_list)) # 精确模式 print(&quot;\njieba默认分词是精确模式：&quot;) seg_list = jieba.cut(&quot;他来到了网易杭研大厦&quot;) # 默认是精确模式 print(&quot;, &quot;.join(seg_list)) print(&quot;\njiba搜索引擎模式：&quot;) seg_list = jieba.cut_for_search(&quot;小明硕士毕业于中国科学院计算所，后在日本京都大学深造&quot;) # 搜索引擎模式 print(&quot;, &quot;.join(seg_list)) strings=&quot;是广泛使用的中文分词工具，具有以下特点：&quot; words = pseg.cut(strings) print(&quot;\njieba词性标注：&quot;) for word, flag in words: print(&#39;%s %s&#39; % (word, flag)) Stanford NLP Stanford NLP提供了一系列自然语言分析工具。它能够给出基本的 词形，词性，不管是公司名还是人名等，格式化的日期，时间，量词， 并且能够标记句子的结构，语法形式和字词依赖，指明那些名字指向同 样的实体，指明情绪，提取发言中的开放关系等。 1.一个集成的语言分析工具集； 2.进行快速，可靠的任意文本分析； 3.整体的高质量的文本分析; 4.支持多种主流语言; 5.多种编程语言的易用接口; 6.方便的简单的部署web服务。 Python 版本stanford nlp 安装 • 1)安装stanford nlp自然语言处理包: pip install stanfordcorenlp • 2)下载Stanford CoreNLP文件 https://stanfordnlp.github.io/CoreNLP/download.html • 3)下载中文模型jar包,https://nlp.stanford.edu/software/stanford-chinese-corenlp-2018-10-05-models.jar • 4)把下载的stanford-chinese-corenlp-2018-10-05-models.jar放在解压后的Stanford CoreNLP文件夹中，改Stanford CoreNLP文件夹名为stanfordnlp（可选） • 5)在Python中引用模型: • from stanfordcorenlp import StanfordCoreNLP • nlp = StanfordCoreNLP(r‘path&#39;, lang=&#39;zh&#39;) 例如： nlp = StanfordCoreNLP(r&#39;/home/kuo/NLP/module/stanfordnlp/&#39;, lang=&#39;zh&#39;) demo# -*-encoding=utf8-*- from stanfordcorenlp import StanfordCoreNLP nlp = StanfordCoreNLP(r&#39;E:\stanford-corenlp-full-2018-10-05&#39;, lang=&#39;zh&#39;) fin = open(&#39;news.txt&#39;, &#39;r&#39;, encoding=&#39;utf8&#39;) fner = open(&#39;ner.txt&#39;, &#39;w&#39;, encoding=&#39;utf8&#39;) ftag = open(&#39;pos_tag.txt&#39;, &#39;w&#39;, encoding=&#39;utf8&#39;) for line in fin: line = line.strip() # 去掉空行 if len(line) &lt; 1: continue # 命名实体识别 fner.write(&quot; &quot;.join([each[0] + &quot;/&quot; + each[1] for each in nlp.ner(line) if len(each) == 2]) + &quot;\n&quot;) # 词性识别 ftag.write(&quot; &quot;.join([each[0] + &quot;/&quot; + each[1] for each in nlp.pos_tag(line) if len(each) == 2]) + &quot;\n&quot;) fner.close() ftag.close() print(&quot;okkkkk&quot;) sentence = &#39;清华大学位于北京。&#39; print(nlp.word_tokenize(sentence)) print(nlp.pos_tag(sentence)) print(nlp.ner(sentence)) print(nlp.parse(sentence)) print(nlp.dependency_parse(sentence)) Hanlp HanLP是由一系列模型与算法组成的Java工具包，目标是普及自然 语言处理在生产环境中的应用。HanLP具备功能完善、性能高效、架构 清晰、语料时新、可自定义的特点。 功能：中文分词 词性标注 命名实体识别 依存句法分析 关键词提取 新词发现 短语提取 自动摘要 文本分类 拼音简繁 • 1、安装Java:我装的是Java 1.8 • 2、安裝Jpype, conda install -c conda-forge jpype1=0.7 [或者]pip install jpype1 • 3、测试是否按照成功: from jpype import * startJVM(getDefaultJVMPath(), &quot;-ea&quot;) java.lang.System.out.println(&quot;Hello World&quot;) shutdownJVM() • 比如data目录是root=E:/hanlp/data,那么root=root=E:/hanlp • 1、https://github.com/hankcs/HanLP/releases 下载hanlp-1.7.4-release.zip包，data-for-1.7.4.zip包,解压后重命名为hanlp• 2、配置文件• 示例配置文件:hanlp.properties• 配置文件的作用是告诉HanLP数据包的位置,只需修改第一行:root=E:/hanlp demo#-*- coding:utf-8 -*- from jpype import * startJVM(getDefaultJVMPath(), &quot;-Djava.class.path=E:\hanlp\hanlp-1.7.4.jar;E:\hanlp&quot;, &quot;-Xms1g&quot;, &quot;-Xmx1g&quot;) # 启动JVM，Linux需替换分号;为冒号: print(&quot;=&quot; * 30 + &quot;HanLP分词&quot; + &quot;=&quot; * 30) HanLP = JClass(&#39;com.hankcs.hanlp.HanLP&#39;) # 中文分词 print(HanLP.segment(&#39;你好，欢迎在Python中调用HanLP的API&#39;)) print(&quot;-&quot; * 70) print(&quot;=&quot; * 30 + &quot;标准分词&quot; + &quot;=&quot; * 30) StandardTokenizer = JClass(&#39;com.hankcs.hanlp.tokenizer.StandardTokenizer&#39;) print(StandardTokenizer.segment(&#39;你好，欢迎在Python中调用HanLP的API&#39;)) print(&quot;-&quot; * 70) # NLP分词NLPTokenizer会执行全部命名实体识别和词性标注 print(&quot;=&quot; * 30 + &quot;NLP分词&quot; + &quot;=&quot; * 30) NLPTokenizer = JClass(&#39;com.hankcs.hanlp.tokenizer.NLPTokenizer&#39;) print(NLPTokenizer.segment(&#39;中国科学院计算技术研究所的宗成庆教授正在教授自然语言处理课程&#39;)) print(&quot;-&quot; * 70) print(&quot;=&quot; * 30 + &quot;索引分词&quot; + &quot;=&quot; * 30) IndexTokenizer = JClass(&#39;com.hankcs.hanlp.tokenizer.IndexTokenizer&#39;) termList = IndexTokenizer.segment(&quot;主副食品&quot;); for term in termList: print(str(term) + &quot; [&quot; + str(term.offset) + &quot;:&quot; + str(term.offset + len(term.word)) + &quot;]&quot;) print(&quot;-&quot; * 70) print(&quot;=&quot; * 30 + &quot; CRF分词&quot; + &quot;=&quot; * 30) print(&quot;-&quot; * 70) print(&quot;=&quot; * 30 + &quot; 极速词典分词&quot; + &quot;=&quot; * 30) SpeedTokenizer = JClass(&#39;com.hankcs.hanlp.tokenizer.SpeedTokenizer&#39;) print(NLPTokenizer.segment(&#39;江西鄱阳湖干枯，中国最大淡水湖变成大草原&#39;)) print(&quot;-&quot; * 70) print(&quot;=&quot; * 30 + &quot; 自定义分词&quot; + &quot;=&quot; * 30) CustomDictionary = JClass(&#39;com.hankcs.hanlp.dictionary.CustomDictionary&#39;) CustomDictionary.add(&#39;攻城狮&#39;) CustomDictionary.add(&#39;单身狗&#39;) HanLP = JClass(&#39;com.hankcs.hanlp.HanLP&#39;) print(HanLP.segment(&#39;攻城狮逆袭单身狗，迎娶白富美，走上人生巅峰&#39;)) print(&quot;-&quot; * 70) print(&quot;=&quot; * 20 + &quot;命名实体识别与词性标注&quot; + &quot;=&quot; * 30) NLPTokenizer = JClass(&#39;com.hankcs.hanlp.tokenizer.NLPTokenizer&#39;) print(NLPTokenizer.segment(&#39;中国科学院计算技术研究所的宗成庆教授正在教授自然语言处理课程&#39;)) print(&quot;-&quot; * 70) document = &quot;水利部水资源司司长陈明忠9月29日在国务院新闻办举行的新闻发布会上透露，&quot; \ &quot;根据刚刚完成了水资源管理制度的考核，有部分省接近了红线的指标，&quot; \ &quot;有部分省超过红线的指标。对一些超过红线的地方，陈明忠表示，对一些取用水项目进行区域的限批，&quot; \ &quot;严格地进行水资源论证和取水许可的批准。&quot; print(&quot;=&quot; * 30 + &quot;关键词提取&quot; + &quot;=&quot; * 30) print(HanLP.extractKeyword(document, 8)) print(&quot;-&quot; * 70) print(&quot;=&quot; * 30 + &quot;自动摘要&quot; + &quot;=&quot; * 30) print(HanLP.extractSummary(document, 3)) print(&quot;-&quot; * 70) text = r&quot;算法工程师\n 算法（Algorithm）是一系列解决问题的清晰指令，也就是说，能够对一定规范的输入，在有限时间内获得所要求的输出。如果一个算法有缺陷，或不适合于某个问题，执行这个算法将不会解决这个问题。不同的算法可能用不同的时间、空间或效率来完成同样的任务。一个算法的优劣可以用空间复杂度与时间复杂度来衡量。算法工程师就是利用算法处理事物的人。\n \n 1职位简介\n 算法工程师是一个非常高端的职位；\n 专业要求：计算机、电子、通信、数学等相关专业；\n 学历要求：本科及其以上的学历，大多数是硕士学历及其以上；\n 语言要求：英语要求是熟练，基本上能阅读国外专业书刊；\n 必须掌握计算机相关知识，熟练使用仿真工具MATLAB等，必须会一门编程语言。\n\n2研究方向\n 视频算法工程师、图像处理算法工程师、音频算法工程师 通信基带算法工程师\n \n 3目前国内外状况\n 目前国内从事算法研究的工程师不少，但是高级算法工程师却很少，是一个非常紧缺的专业工程师。算法工程师根据研究领域来分主要有音频/视频算法处理、图像技术方面的二维信息算法处理和通信物理层、雷达信号处理、生物医学信号处理等领域的一维信息算法处理。\n 在计算机音视频和图形图像技术等二维信息算法处理方面目前比较先进的视频处理算法：机器视觉成为此类算法研究的核心；另外还有2D转3D算法(2D-to-3D conversion)，去隔行算法(de-interlacing)，运动估计运动补偿算法(Motion estimation/Motion Compensation)，去噪算法(Noise Reduction)，缩放算法(scaling)，锐化处理算法(Sharpness)，超分辨率算法(Super Resolution),手势识别(gesture recognition),人脸识别(face recognition)。\n 在通信物理层等一维信息领域目前常用的算法：无线领域的RRM、RTT，传送领域的调制解调、信道均衡、信号检测、网络优化、信号分解等。\n 另外数据挖掘、互联网搜索算法也成为当今的热门方向。\n&quot; print(&quot;=&quot; * 30 + &quot;短语提取&quot; + &quot;=&quot; * 30) print(HanLP.extractPhrase(text, 10)) print(&quot;-&quot; * 70) shutdownJVM()]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
        <tag>nlp</tag>
        <tag>numpy</tag>
        <tag>NLTK</tag>
        <tag>Gensim</tag>
        <tag>Stanford NLP</tag>
        <tag>Hanlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jmeter基础]]></title>
    <url>%2F2019%2F08%2F26%2Fjmeter%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[jmeter基本环境搭建及基础操作 压力测试工具对比 loadrunner 性能稳定，压测结果及细粒度大，可以自定义脚本进行压测，但是太过于重大，功能比较繁多 apache ab(单接口压测最方便) 模拟多线程并发请求,ab命令对发出负载的计算机要求很低，既不会占用很多CPU，也不会占用太多的内存，但却会给目标服务器造成巨大的负载, 简单DDOS攻击等 webbench webbench首先fork出多个子进程，每个子进程都循环做web访问测试。子进程把访问的结果通过pipe告诉父进程，父进程做最终的统计结果。 jmeter 压测不同的协议和应用 1) Web - HTTP, HTTPS (Java, NodeJS, PHP, ASP.NET, …) 2) SOAP / REST Webservices 3) FTP 4) Database via JDBC 5) LDAP 轻量目录访问协议 6) Message-oriented middleware (MOM) via JMS 7) Mail - SMTP(S), POP3(S) and IMAP(S) 8) TCP等等 使用场景及优点 1）功能测试 2）压力测试 3）分布式压力测试 4）纯java开发 5）上手容易，高性能 4）提供测试数据分析 5）各种报表数据图形展示环境搭建 需要安装JDK8。或者JDK9,JDK10 快速下载 windows： https://archive.apache.org/dist/jmeter/binaries/apache-jmeter-4.0.zip mac或者linux：https://archive.apache.org/dist/jmeter/binaries/apache-jmeter-4.0.tgz 目录 bin:核心可执行文件，包含配置 jmeter.bat: windows启动文件： jmeter: mac或者linux启动文件： jmeter-server：mac或者Liunx分布式压测使用的启动文件 jmeter-server.bat：mac或者Liunx分布式压测使用的启动文件 jmeter.properties: 核心配置文件 extras：插件拓展的包 lib:核心的依赖包 ext:核心包 junit:单元测试包 修改界面语言 1、控制台修改 menu -&gt; options -&gt; choose language 2、配置文件修改 bin目录 -&gt; jmeter.properties 默认 #language=en 改为 language=zh_CN 基本测试 java -jar gs-spring-boot-0.1.0.jar 添加-&gt;threads-&gt;线程组（控制总体并发） 线程数：虚拟用户数。一个虚拟用户占用一个进程或线程准备时长（Ramp-Up Period(in seconds)）：全部线程启动的时长，比如100个线程，20秒，则表示20秒内100个线程都要启动完成，每秒启动5个线程循环次数：每个线程发送的次数，假如值为5，100个线程，则会发送500次请求，可以勾选永远循环 线程组-&gt;添加-&gt; Sampler(采样器) -&gt; Http （一个线程组下面可以增加几个Sampler） 名称：采样器名称 注释：对这个采样器的描述web服务器： 默认协议是http 默认端口是80 服务器名称或IP ：请求的目标服务器名称或IP地址路径：服务器URLUse multipart/from-data for HTTP POST ：当发送POST请求时，使用Use multipart/from-data方法发送，默认不选中。 线程组-&gt;添加-&gt;监听器-&gt;察看结果树 该结果树属于全局，可已针对每一个请求设置结果树 线程组 -&gt; 添加 -&gt; 断言 -&gt; 响应断言 apply to(应用范围): Main sample only: 仅当前父取样器 进行断言，一般一个请求，如果发一个请求会触发多个，则就有sub sample（比较少用）要测试的响应字段： 响应文本：即响应的数据，比如json等文本 响应代码：http的响应状态码，比如200，302，404这些 响应信息：http响应代码对应的响应信息，例如：OK, Found Response Header: 响应头模式匹配规则： 包括：包含在里面就成功 匹配：响应内容完全匹配，不区分大小写 equals：完全匹配，区分大小写 线程组-&gt; 添加 -&gt; 监听器 -&gt; 断言结果 里面的内容是sampler采样器的名称断言失败，查看结果树任务结果颜色标红(通过结果数里面双击不通过的记录，可以看到错误信息)每个sample下面可以加单独的结果树，然后同时加多个断言，最外层可以加个结果树进行汇总 线程组-&gt;添加-&gt;监听器-&gt;聚合报告（Aggregate Report） lable: sampler的名称 Samples: 一共发出去多少请求,例如10个用户，循环10次，则是 100 Average: 平均响应时间 Median: 中位数，也就是 50％ 用户的响应时间 90% Line : 90％ 用户的响应不会超过该时间 （90% of the samples took no more than this time. The remaining samples at least as long as this） 95% Line : 95％ 用户的响应不会超过该时间 99% Line : 99％ 用户的响应不会超过该时间 min : 最小响应时间 max : 最大响应时间 Error%：错误的请求的数量/请求的总数 Throughput： 吞吐量——默认情况下表示每秒完成的请求数（Request per Second) 可类比为qps,并发数提高，qps不涨则瓶颈 KB/Sec: 每秒接收数据量 启动测试http请求 变量实操很多变量在全局中都有使用，或者测试数据更改，可以在一处定义，四处使用，比如服务器地址 线程组-&gt;add -&gt; Config Element(配置原件)-&gt; User Definde Variable（用户定义的变量） 通过${xx}调用 线程组-&gt;add -&gt; Config Element(配置原件)-&gt; CSV data set config (CSV数据文件设置) csv变量使用csv_name txt多变量使用csv_name csv_pwd 数据库test1 Add directory or jar to classpath添加mysql-connector-java-5.1.30.jarThread Group -&gt; add -&gt; sampler -&gt; jdbc request 无参查询 有参查询 预编译 更新 预编译 JDBC request-&gt;add -&gt; config element -&gt; JDBC connection configuration Variable Name for created pool同JDBC request 的Variable Name for created pool declared in JDBC connection configuration Thread Group -&gt; add -&gt; sampler -&gt; debug sampler variable name of pool declared in JDBC connection configuration（和配置文件同名）Query Type 查询类型parameter values 参数值parameter types 参数类型variable names sql执行结果变量名result variable names 所有结果当做一个对象存储query timeouts 查询超时时间handle results 处理结果集 分布式压测]]></content>
      <categories>
        <category>测试</category>
      </categories>
      <tags>
        <tag>jmeter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy爬取ssr链接]]></title>
    <url>%2F2019%2F08%2F23%2Fscrapy%E7%88%AC%E5%8F%96ssr%E9%93%BE%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[基于scrapy爬取ssr链接 环境搭建python3.5 虚拟环境virtualenvpip install virtualenv 提示pip版本太低 python -m pip install --upgrade pip pip install -i https://pypi.doubanio.com/simple/ --trusted-host pypi.doubanio.com django 使用豆瓣源加速 pip uninstall django 卸载django virtualenv scrapytest 默认环境创建虚拟环境 cd scrapytest/Scripts &amp;&amp; activate.bat &amp;&amp; python 进入3.5虚拟环境 virtualenv -p D:\Python27\python.exe scrapytest cd scrapytest/Scripts &amp;&amp; activate.bat &amp;&amp; python 进入2.7虚拟环境 deactivate.bat 退出虚拟环境 apt-get install python-virtualenv 安装虚拟环境 virtualenv py2 &amp;&amp; cd py2 &amp;&amp; cd bin &amp;&amp; source activate &amp;&amp; python 进入2.7虚拟环境 virtualenv -p /usr/bin/python3 py3 &amp;&amp; &amp;&amp; cd py3 &amp;&amp; cd bin &amp;&amp; source activate &amp;&amp; python 进入3.5虚拟环境虚拟环境virtualenvwrapperpip install virtualenvwrapper pip install virtualenvwrapper-win 解决workon不是内部指令 workon 列出所有虚拟环境 新建环境变量 WORKON_HOME=E:\envs mkvirtualenv py3scrapy 新建并进入虚拟环境 deactivate 退出虚拟环境 workon py3scrapy 进入指定虚拟环境 pip install -i https://pypi.douban.com/simple scrapy 安装scrapy源 若缺少lxml出错https://www.lfd.uci.edu/~gohlke/pythonlibs/寻找对应版本的lxml的whl源 python -m pip install --upgrade pip 更新pip pip install lxml-4.1.1-cp35-cp35m-win_amd64.whl 若缺少Twisted出错http://www.lfd.uci.edu/~gohlke/pythonlibs/#lxml搜对应版本Twisted pip install Twisted‑17.9.0‑cp35‑cp35m‑win_amd64.whl mkvirtualenv --python=D:\Python27\python.exe py2scrapy 一般不会出问题 pip install -i https://pypi.douban.com/simple scrapy pip install virtualenvwrapper find / -name virualenvwrapper.sh vim ~/.bashrc export WORKON_HOME=$HOME/.virtualenvs source /home/wj/.local/bin/virtualenvwrapper.sh source ~/.bashrc mkvirtualenv py2scrapy 指向生成~/.virtualenv deactivate 退出虚拟环境 mkdirtualenv --python=/usr/bin/python3 py3scrapy项目实战项目搭建pip install virtualenvwrapper-win mkvirtualenv --python=F:\Python\Python35\python.exe ssr pip install Twisted-17.9.0-cp35-cp35m-win_amd64.whl pip install -i https://pypi.douban.com/simple/ scrapy scrapy startproject ssr cd ssr scrapy genspider ssr https://freevpn-ss.tk/category/technology/ scrapy genspider --list scrapy genspider -t crawl lagou www.lagou.com 使用crawl模板 pycharm--新建项目---Pure Python---Interpreter为E:\envs\ssr\Scripts\python.exe pycharm--打开---ssr,修改settings--project Interpreter为D:\Envs\ss pip list pip install -i https://pypi.douban.com/simple pypiwin32 pillow requests redis fake-useragent pip install mysqlclient-1.4.4-cp35-cp35m-win_amd64.whl 或者pip install -i https://pypi.douban.com/simple mysqlclient 出错apt-get install libmysqlclient-dev 或者yum install python-devel mysql-devel scrapy crawl jobbole 修改settings.py ROBOTSTXT_OBEY = False scrapy shell http://blog.jobbole.com/ 可以在脚本中调试xpath或者chrome浏览器右键copy xpath,chrome浏览器右键copy selector scrapy shell -s USER_AGENT=&quot;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:51.0) Gecko/20100101 Firefox/51.0&quot; https://www.zhihu.com/question/56320032 pip freeze &gt; requirements.txt 生成依赖到文件 pip install -r requirements.txt 一键安装依赖爬虫开发1scrapy shell https://freevpn-ss.tk/category/technology/ shell中查看节点 response.css(&quot;.posts-list .panel a::attr(href)&quot;).extract_first() response.css(&quot;.posts-list .panel a img::attr(src)&quot;).extract_first() response.xpath(&quot;//*[@id=&#39;container&#39;]/div/ul/li/article/a/img/@src&quot;).extract_first()启动类main.py from scrapy.cmdline import execute import sys import os sys.path.append(os.path.dirname(os.path.abspath(__file__))) execute([&quot;scrapy&quot;, &quot;crawl&quot;, &quot;freevpn-ss.tk&quot;]) 基础配置ssr/settings.py import os BOT_NAME = &#39;ssr&#39; SPIDER_MODULES = [&#39;ssr.spiders&#39;] NEWSPIDER_MODULE = &#39;ssr.spiders&#39; # Crawl responsibly by identifying yourself (and your website) on the user-agent #USER_AGENT = &#39;ssr (+http://www.yourdomain.com)&#39; # Obey robots.txt rules ROBOTSTXT_OBEY = False import sys BASE_DIR = os.path.dirname(os.path.abspath(os.path.dirname(__file__))) sys.path.insert(0, os.path.join(BASE_DIR, &#39;ssr&#39;)) MYSQL_HOST = &quot;127.0.0.1&quot; MYSQL_DBNAME = &quot;scrapy&quot; MYSQL_USER = &quot;root&quot; MYSQL_PASSWORD = &quot;&quot; ITEM_PIPELINES = { &#39;ssr.pipelines.MysqlTwistedPipline&#39;: 2,#连接池异步插入 &#39;ssr.pipelines.JsonExporterPipleline&#39;: 1,#连接池异步插入 } ssr/pipelines.py from scrapy.exporters import JsonItemExporter from scrapy.pipelines.images import ImagesPipeline import codecs import json import MySQLdb import MySQLdb.cursors from twisted.enterprise import adbapi from ssr.utils.common import DateEncoder class SsrPipeline(object): def process_item(self, item, spider): return item class SsrImagePipeline(ImagesPipeline): def item_completed(self, results, item, info): if &quot;front_image_url&quot; in item: for ok, value in results: image_file_path = value[&quot;path&quot;] # 填充自定义路径 item[&quot;front_image_path&quot;] = image_file_path return item class JsonWithEncodingPipeline(object): # 自定义json文件的导出 def __init__(self): self.file = codecs.open(&#39;article.json&#39;, &#39;w&#39;, encoding=&quot;utf-8&quot;) def process_item(self, item, spider): # 序列化，ensure_ascii利于中文,json没法序列化date格式，需要新写函数 lines = json.dumps(dict(item), ensure_ascii=False, cls=DateEncoder) + &quot;\n&quot; self.file.write(lines) return item def spider_closed(self, spider): self.file.close() class JsonExporterPipleline(object): # 调用scrapy提供的json export导出json文件 def __init__(self): self.file = open(&#39;ssr.json&#39;, &#39;wb&#39;) self.exporter = JsonItemExporter(self.file, encoding=&quot;utf-8&quot;, ensure_ascii=False) self.exporter.start_exporting() def close_spider(self, spider): self.exporter.finish_exporting() self.file.close() def process_item(self, item, spider): self.exporter.export_item(item) return item class MysqlPipeline(object): # 采用同步的机制写入mysql def __init__(self): self.conn = MySQLdb.connect(&#39;127.0.0.1&#39;, &#39;root&#39;, &#39;123456&#39;, &#39;scrapy&#39;, charset=&quot;utf8&quot;, use_unicode=True) self.cursor = self.conn.cursor() def process_item(self, item, spider): insert_sql = &quot;&quot;&quot; insert into ssr(url, ip,ssr, port,password,secret) VALUES (%s, %s, %s, %s, %s) &quot;&quot;&quot; self.cursor.execute(insert_sql, (item[&quot;url&quot;],item[&quot;ssr&quot;], item[&quot;ip&quot;], item[&quot;port&quot;], item[&quot;password&quot;], item[&quot;secret&quot;])) self.conn.commit() class MysqlTwistedPipline(object): # 异步连接池插入数据库，不会阻塞 def __init__(self, dbpool): self.dbpool = dbpool @classmethod def from_settings(cls, settings):# 初始化时即被调用静态方法 dbparms = dict( host = settings[&quot;MYSQL_HOST&quot;],#setttings中定义 db = settings[&quot;MYSQL_DBNAME&quot;], user = settings[&quot;MYSQL_USER&quot;], passwd = settings[&quot;MYSQL_PASSWORD&quot;], charset=&#39;utf8&#39;, cursorclass=MySQLdb.cursors.DictCursor, use_unicode=True, ) dbpool = adbapi.ConnectionPool(&quot;MySQLdb&quot;, **dbparms) return cls(dbpool) def process_item(self, item, spider): #使用twisted将mysql插入变成异步执行 query = self.dbpool.runInteraction(self.do_insert, item) query.addErrback(self.handle_error, item, spider) #处理异常 def handle_error(self, failure, item, spider): #处理异步插入的异常 print (failure) def do_insert(self, cursor, item): #执行具体的插入，不具体的如MysqlPipeline.process_item() #根据不同的item 构建不同的sql语句并插入到mysql中 insert_sql, params = item.get_insert_sql() cursor.execute(insert_sql, params) 实体类ssr/items.py import scrapy from scrapy.loader import ItemLoader from scrapy.loader.processors import MapCompose, TakeFirst, Join import re import datetime from w3lib.html import remove_tags def date_convert(value): try: create_date = datetime.datetime.strptime(value, &quot;%Y/%m/%d&quot;).date() except Exception as e: create_date = datetime.datetime.now().date() return create_date def get_nums(value): match_re = re.match(&quot;.*?(\d+).*&quot;, value) if match_re: nums = int(match_re.group(1)) else: nums = 0 return nums def return_value(value): return value class SsrItemLoader(ItemLoader): # 自定义itemloader default_output_processor = TakeFirst() class SsrItem(scrapy.Item): url = scrapy.Field() ip = scrapy.Field( input_processor=MapCompose(return_value),#传递进来可以预处理 ) port = scrapy.Field() ssr = scrapy.Field() front_image_url = scrapy.Field() password = scrapy.Field() secret = scrapy.Field() def get_insert_sql(self): insert_sql = &quot;&quot;&quot; insert into ssr(url,ssr, ip, port, password,secret) VALUES (%s, %s,%s, %s, %s,%s) ON DUPLICATE KEY UPDATE ssr=VALUES(ssr) &quot;&quot;&quot; params = (self[&quot;url&quot;],self[&quot;ssr&quot;], self[&quot;ip&quot;],self[&quot;port&quot;], self[&quot;password&quot;],self[&quot;secret&quot;]) return insert_sql, params 核心代码ssr/spiders/freevpn_ss_tk.py # -*- coding: utf-8 -*- import time from datetime import datetime from urllib import parse import scrapy from scrapy.http import Request from ssr.items import SsrItemLoader, SsrItem class FreevpnSsTkSpider(scrapy.Spider): name = &#39;freevpn-ss.tk&#39; # 必须一级域名 allowed_domains = [&#39;freevpn-ss.tk&#39;] start_urls = [&#39;https://freevpn-ss.tk/category/technology/&#39;] custom_settings = { # 优先并覆盖项目，避免被重定向 &quot;COOKIES_ENABLED&quot;: False, # 关闭cookies &quot;DOWNLOAD_DELAY&quot;: 1, &#39;DEFAULT_REQUEST_HEADERS&#39;: { &#39;Accept&#39;: &#39;application/json, text/javascript, */*; q=0.01&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate, br&#39;, &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.8&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;, &#39;Cookie&#39;: &#39;&#39;, &#39;Host&#39;: &#39;freevpn-ss.tk&#39;, &#39;Origin&#39;: &#39;https://freevpn-ss.tk/&#39;, &#39;Referer&#39;: &#39;https://freevpn-ss.tk/&#39;, &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36&#39;, } } def parse(self, response): # post_nodes = response.css(&quot;.posts-list .panel&gt;a&quot;) # for post_node in post_nodes: # image_url = post_node.css(&quot;img::attr(src)&quot;).extract_first(&quot;&quot;) # post_url = post_node.css(&quot;::attr(href)&quot;).extract_first(&quot;&quot;) # yield Request(url=parse.urljoin(response.url, post_url), meta={&quot;front_image_url&quot;: image_url},callback=self.parse_detail) # response获取meta # # next_url = response.css(&quot;.next-page a::attr(href)&quot;).extract_first(&quot;&quot;) # if next_url: # print(next_url) # yield Request(url=parse.urljoin(response.url, next_url), callback=self.parse) post_node = response.css(&quot;.posts-list .panel&gt;a&quot;)[0] image_url = post_node.css(&quot;img::attr(src)&quot;).extract_first(&quot;&quot;) post_url = post_node.css(&quot;::attr(href)&quot;).extract_first(&quot;&quot;) yield Request(url=parse.urljoin(response.url, post_url), meta={&quot;front_image_url&quot;: image_url}, callback=self.parse_detail) # response获取meta def parse_detail(self, response): # 通过item loader加载item front_image_url = response.meta.get(&quot;front_image_url&quot;, &quot;&quot;) # 文章封面图 ssr_nodes = response.css(&quot;table tbody tr&quot;) with open(datetime.now().strftime(&#39;%Y-%m-%d&#39;), &#39;a&#39;) as file_object: for ssr in ssr_nodes: item_loader = SsrItemLoader(item=SsrItem(), response=response) # 默认ItemLoader是一个list，自定义TakeFirst() print(ssr.xpath(&quot;td[4]/text()&quot;).extract_first(&quot;&quot;)) item_loader.add_value(&quot;url&quot;, response.url) item_loader.add_value(&quot;ssr&quot;, ssr.css(&quot;td:nth-child(1)&gt;a::attr(href)&quot;).extract_first(&quot;&quot;)) item_loader.add_value(&quot;ip&quot;, ssr.css(&quot;td:nth-child(2)::text&quot;).extract_first(&quot;&quot;)) item_loader.add_value(&quot;front_image_url&quot;, front_image_url) item_loader.add_value(&quot;port&quot;, ssr.xpath(&quot;td[3]/text()&quot;).extract_first(&quot;&quot;)) item_loader.add_value(&quot;password&quot;, ssr.xpath(&quot;td[4]/text()&quot;).extract_first(&quot;&quot;)) item_loader.add_value(&quot;secret&quot;, ssr.xpath(&quot;td[5]/text()&quot;).extract_first(&quot;&quot;)) ssr_item = item_loader.load_item() file_object.write(ssr.css(&quot;td:nth-child(1)&gt;a::attr(href)&quot;).extract_first(&quot;&quot;)+&quot;\n&quot;) yield ssr_item # 将传到piplines中 爬虫开发2# -*- coding: utf-8 -*- import time from datetime import datetime from urllib import parse import scrapy from scrapy.http import Request from ssr.items import SsrItemLoader, SsrItem class FanQiangSpider(scrapy.Spider): name = &#39;fanqiang.network&#39; # 必须一级域名 allowed_domains = [&#39;fanqiang.network&#39;] start_urls = [&#39;https://fanqiang.network/免费ssr&#39;] def parse(self, response): post_nodes = response.css(&quot;.post-content table tbody tr&quot;) item_loader = SsrItemLoader(item=SsrItem(), response=response) with open(datetime.now().strftime(&#39;%Y-%m-%d&#39;), &#39;a&#39;) as file_object: for post_node in post_nodes: item_loader.add_value(&quot;url&quot;, response.url) item_loader.add_value(&quot;ssr&quot;, post_node.css(&quot;td:nth-child(1)&gt;a::attr(href)&quot;).extract_first(&quot;&quot;).replace(&quot;http://freevpn-ss.tk/&quot;, &quot;&quot;)) item_loader.add_value(&quot;ip&quot;, post_node.css(&quot;td:nth-child(2)::text&quot;).extract_first(&quot;&quot;)) item_loader.add_value(&quot;port&quot;, post_node.xpath(&quot;td[3]/text()&quot;).extract_first(&quot;&quot;)) item_loader.add_value(&quot;password&quot;, post_node.xpath(&quot;td[4]/text()&quot;).extract_first(&quot;&quot;)) item_loader.add_value(&quot;secret&quot;, post_node.xpath(&quot;td[5]/text()&quot;).extract_first(&quot;&quot;)) ssr_item = item_loader.load_item() file_object.write(post_node.css(&quot;td:nth-child(1)&gt;a::attr(href)&quot;).extract_first(&quot;&quot;).replace(&quot;http://freevpn-ss.tk/&quot;, &quot;&quot;) + &quot;\n&quot;) yield ssr_item # 将传到piplines中 多爬虫同时运行settings.py COMMANDS_MODULE = &#39;ssr.commands&#39; ssr/commands/crawlall.py import os from scrapy.commands import ScrapyCommand from scrapy.utils.conf import arglist_to_dict from scrapy.utils.python import without_none_values from scrapy.exceptions import UsageError class Command(ScrapyCommand): requires_project = True def syntax(self): return &quot;[options] &lt;spider&gt;&quot; def short_desc(self): return &quot;Run all spider&quot; def add_options(self, parser): ScrapyCommand.add_options(self, parser) parser.add_option(&quot;-a&quot;, dest=&quot;spargs&quot;, action=&quot;append&quot;, default=[], metavar=&quot;NAME=VALUE&quot;, help=&quot;set spider argument (may be repeated)&quot;) parser.add_option(&quot;-o&quot;, &quot;--output&quot;, metavar=&quot;FILE&quot;, help=&quot;dump scraped items into FILE (use - for stdout)&quot;) parser.add_option(&quot;-t&quot;, &quot;--output-format&quot;, metavar=&quot;FORMAT&quot;, help=&quot;format to use for dumping items with -o&quot;) def process_options(self, args, opts): ScrapyCommand.process_options(self, args, opts) try: opts.spargs = arglist_to_dict(opts.spargs) except ValueError: raise UsageError(&quot;Invalid -a value, use -a NAME=VALUE&quot;, print_help=False) if opts.output: if opts.output == &#39;-&#39;: self.settings.set(&#39;FEED_URI&#39;, &#39;stdout:&#39;, priority=&#39;cmdline&#39;) else: self.settings.set(&#39;FEED_URI&#39;, opts.output, priority=&#39;cmdline&#39;) feed_exporters = without_none_values( self.settings.getwithbase(&#39;FEED_EXPORTERS&#39;)) valid_output_formats = feed_exporters.keys() if not opts.output_format: opts.output_format = os.path.splitext(opts.output)[1].replace(&quot;.&quot;, &quot;&quot;) if opts.output_format not in valid_output_formats: raise UsageError(&quot;Unrecognized output format &#39;%s&#39;, set one&quot; &quot; using the &#39;-t&#39; switch or as a file extension&quot; &quot; from the supported list %s&quot; % (opts.output_format, tuple(valid_output_formats))) self.settings.set(&#39;FEED_FORMAT&#39;, opts.output_format, priority=&#39;cmdline&#39;) def run(self, args, opts): # 获取爬虫列表 spd_loader_list = self.crawler_process.spider_loader.list() # 获取所有的爬虫文件。 print(spd_loader_list) # 遍历各爬虫 for spname in spd_loader_list or args: self.crawler_process.crawl(spname, **opts.spargs) print(&#39;此时启动的爬虫为：&#39; + spname) self.crawler_process.start()main.py from scrapy import cmdline from scrapy.cmdline import execute import sys import os sys.path.append(os.path.dirname(os.path.abspath(__file__))) # execute([&quot;scrapy&quot;, &quot;crawl&quot;, &quot;freevpn-ss.tk&quot;]) # execute([&quot;scrapy&quot;, &quot;crawl&quot;, &quot;fanqiang.network&quot;]) cmdline.execute(&quot;scrapy crawlall&quot;.split()) 防反爬随机uapip install -i https://pypi.doubanio.com/simple/ –trusted-host pypi.doubanio.com scrapy-fake-useragent DOWNLOADER_MIDDLEWARES = { &#39;scrapy_fake_useragent.middleware.RandomUserAgentMiddleware&#39;: 1, } 报错socket.timeout: timed out，查看F:/Anaconda3/Lib/site-packages/fake_useragent/settings.py __version__ = &#39;0.1.11&#39; DB = os.path.join( tempfile.gettempdir(), &#39;fake_useragent_{version}.json&#39;.format( version=__version__, ), ) CACHE_SERVER = &#39;https://fake-useragent.herokuapp.com/browsers/{version}&#39;.format( version=__version__, ) BROWSERS_STATS_PAGE = &#39;https://www.w3schools.com/browsers/default.asp&#39; BROWSER_BASE_PAGE = &#39;http://useragentstring.com/pages/useragentstring.php?name={browser}&#39; # noqa BROWSERS_COUNT_LIMIT = 50 REPLACEMENTS = { &#39; &#39;: &#39;&#39;, &#39;_&#39;: &#39;&#39;, } SHORTCUTS = { &#39;internet explorer&#39;: &#39;internetexplorer&#39;, &#39;ie&#39;: &#39;internetexplorer&#39;, &#39;msie&#39;: &#39;internetexplorer&#39;, &#39;edge&#39;: &#39;internetexplorer&#39;, &#39;google&#39;: &#39;chrome&#39;, &#39;googlechrome&#39;: &#39;chrome&#39;, &#39;ff&#39;: &#39;firefox&#39;, } OVERRIDES = { &#39;Edge/IE&#39;: &#39;Internet Explorer&#39;, &#39;IE/Edge&#39;: &#39;Internet Explorer&#39;, } HTTP_TIMEOUT = 5 HTTP_RETRIES = 2 HTTP_DELAY = 0.1 http://useragentstring.com/pages/useragentstring.php?name=Chrome 打开超时报错，其中CACHE_SERVER是存储了所有UserAgent的json数据，再次观察其中DB这个变量，结合fake_useragent\fake.py中的逻辑，判断这个变量应该是存储json数据的，所以大体逻辑应该是，首次初始化时，会自动爬取CACHE_SERVER中的json数据，然后将其存储到本地，所以我们直接将json存到指定路径下，再次初始化时，应该就不会报错 &gt;&gt;&gt; import tempfile &gt;&gt;&gt; print(tempfile.gettempdir()) C:\Users\codewj\AppData\Local\Temp 将CACHE_SERVER的json数据保存为fake_useragent_0.1.11.json,并放到目录C:\Users\codewj\AppData\Local\Temp中 &gt;&gt;&gt; import fake_useragent &gt;&gt;&gt; ua = fake_useragent.UserAgent() &gt;&gt;&gt; ua.data_browsers[&#39;chrome&#39;][0] &#39;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36&#39; 注：如果CACHE_SERVER不是https://fake-useragent.herokuapp.com/browsers/0.1.11，请更新一下库pip install –upgrade fake_useragent]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
        <tag>selenium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[好客租房mybatisplus]]></title>
    <url>%2F2019%2F08%2F22%2F%E5%A5%BD%E5%AE%A2%E7%A7%9F%E6%88%BFmybatisplus%2F</url>
    <content type="text"><![CDATA[基于dubbo react mybatisplus elk实战整合开发 mysqldocker run -di --name=mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 docker.io/mysql mysql -u root -p ALTER USER &#39;root&#39;@&#39;%&#39; IDENTIFIED WITH mysql_native_password BY &#39;123456&#39;; docker logs -fn 500 mysql MybatisPlus入门执行建表 haoke.sql mybatis-plus/pom.xml &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--mybatis-plus的springboot支持--&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.0.5&lt;/version&gt; &lt;/dependency&gt; &lt;!--mysql驱动--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; application.properties spring.application.name = itcast-mybatis-plus spring.datasource.driver-class-name=com.mysql.jdbc.Driver spring.datasource.url=jdbc:mysql://192.168.3.237:3306/haoke?useUnicode=true&amp;characterEncoding=utf8&amp;autoReconnect=true&amp;allowMultiQueries=true&amp;useSSL=false spring.datasource.username=root spring.datasource.password=123456 com/onejane/mybatisplus/pojo/User.java @Data public class User { @TableId(value = &quot;ID&quot;, type = IdType.AUTO) private Long id; private String name; private Integer age; private String email; } com/onejane/mybatisplus/mapper/UserMapper.java public interface UserMapper extends BaseMapper&lt;User&gt; { } com/onejane/mybatisplus/MyApplication.java @MapperScan(&quot;com.onejane.mybatisplus.mapper&quot;) //设置mapper接口的扫描包 @SpringBootApplication public class MyApplication { /** * 分页插件 */ @Bean public PaginationInterceptor paginationInterceptor() { return new PaginationInterceptor(); } public static void main(String[] args) { SpringApplication.run(MyApplication.class, args); } } com/onejane/mybatisplus/mapper/UserMaperTest.java @RunWith(SpringRunner.class) @SpringBootTest public class UserMaperTest { @Autowired private UserMapper userMapper; @Test public void testSelect(){ List&lt;User&gt; users = this.userMapper.selectList(null); for (User user : users) { System.out.println(user); } } @Test public void testSelectById(){ User user = this.userMapper.selectById(3L); System.out.println(user); } @Test public void testSelectByLike(){ QueryWrapper&lt;User&gt; wrapper = new QueryWrapper&lt;User&gt;(); wrapper.like(&quot;name&quot;, &quot;o&quot;); List&lt;User&gt; list = this.userMapper.selectList(wrapper); for (User user : list) { System.out.println(user); } } @Test public void testSelectByLe(){ QueryWrapper&lt;User&gt; wrapper = new QueryWrapper&lt;User&gt;(); wrapper.le(&quot;age&quot;, 20); List&lt;User&gt; list = this.userMapper.selectList(wrapper); for (User user : list) { System.out.println(user); } } @Test public void testSave(){ User user = new User(); user.setAge(25); user.setEmail(&quot;zhangsan@qq.com&quot;); user.setName(&quot;zhangsan&quot;); int count = this.userMapper.insert(user); System.out.println(&quot;新增数据成功! count =&gt; &quot; + count); } @Test public void testDelete(){ this.userMapper.deleteById(7L); System.out.println(&quot;删除成功!&quot;); } @Test public void testUpdate(){ User user = new User(); user.setId(6L); user.setName(&quot;lisi&quot;); this.userMapper.updateById(user); System.out.println(&quot;修改成功!&quot;); } @Test public void testSelectPage() { Page&lt;User&gt; page = new Page&lt;&gt;(2, 2); IPage&lt;User&gt; userIPage = this.userMapper.selectPage(page, null); System.out.println(&quot;总条数 ------&gt; &quot; + userIPage.getTotal()); System.out.println(&quot;当前页数 ------&gt; &quot; + userIPage.getCurrent()); System.out.println(&quot;当前每页显示数 ------&gt; &quot; + userIPage.getSize()); List&lt;User&gt; records = userIPage.getRecords(); for (User user : records) { System.out.println(user); } } } 兼容配置application.properties ## 指定全局配置文件 mybatis-plus.config-location = classpath:mybatis-config.xml # 指定mapper.xml文件 mybatis-plus.mapper-locations = classpath*:mybatis/*.xml Lombok&lt;!--简化代码的工具包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;version&gt;1.18.4&lt;/version&gt; &lt;/dependency&gt; 安装idea Lombok插件 @Data：注解在类上；提供类所有属性的 getting 和 setting 方法，此外还提供了equals、canEqual、hashCode、toString 方法 @Setter setting 方法 @Getter：注解在属性上；为属性提供 getting 方法 @Slf4j：注解在类上；为类提供一个 属性名为log 的 slf4j日志对象 @NoArgsConstructor：注解在类上；为类提供一个无参的构造方法 @AllArgsConstructor：注解在类上；为类提供一个全参的构造方法 @Builder：使用Builder模式构建对象 @Slf4j @Data @AllArgsConstructor @Builder public class Item { private Long id; private String title; private Long price; public Item() { log.info(&quot;写日志。。。。。&quot;); } public static void main(String[] args) { Item item1 = new Item(1L,&quot;哈哈哈&quot;,10L); Item item2 = Item.builder().price(100L)title(&quot;hello&quot;).id(1L).build(); System.out.println(item1.getId()); } } 搭建后台服务系统haoke-manage├─haoke-manage-api-server├─haoke-manage-dubbo-server│ ├─haoke-manage-dubbo-server-ad│ ├─haoke-manage-dubbo-server-common│ ├─haoke-manage-dubbo-server-generator MybatisPlus的AutoGenerator插件生成代码文件│ ├─haoke-manage-dubbo-server-house-resources│ │ ├─haoke-manage-dubbo-server-house-resources-dubbo-interface 对外提供的sdk包 只提供pojo实体以及接口，不提供实现类│ │ ├─haoke-manage-dubbo-server-house-resources-dubbo-service 具体实现 haoke-manage&lt;!--spring boot的支持放在groupId上面--&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.0.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;!--springboot 测试支持--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--dubbo的springboot支持--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;0.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--dubbo框架--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;dubbo&lt;/artifactId&gt; &lt;version&gt;2.6.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;3.7&lt;/version&gt; &lt;/dependency&gt; &lt;!--zk依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.13&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.sgroschupf&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; haoke-manage-api-serverpom.xml &lt;dependencies&gt; &lt;!--springboot的web支持--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.onejane&lt;/groupId&gt; &lt;artifactId&gt;haoke-manage-dubbo-server-house-resources-dubbo-interface&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; application.properties # Spring boot application spring.application.name = haoke-manage-api-server server.port = 18080 #logging.level.root=DEBUG # dubbo的应用名称 dubbo.application.name = dubbo-consumer-haoke-manage # zk注册中心 dubbo.registry.address = zookeeper://192.168.3.237:2181 dubbo.registry.client = zkclient Pagination @Data @AllArgsConstructor public class Pagination { private Integer current; private Integer pageSize; private Integer total; }TableResult @Data @AllArgsConstructor public class TableResult&lt;T&gt; { private List&lt;T&gt; list; private Pagination pagination; } HouseResourcesService @Service public class HouseResourcesService { @Reference(version = &quot;1.0.0&quot;) private ApiHouseResourcesService apiHouseResourcesService; public boolean save(HouseResources houseResources) { int result = this.apiHouseResourcesService.saveHouseResources(houseResources); return result == 1; } public TableResult&lt;HouseResources&gt; queryList(HouseResources houseResources, Integer currentPage, Integer pageSize) { PageInfo&lt;HouseResources&gt; pageInfo = this.apiHouseResourcesService. queryHouseResourcesList(currentPage, pageSize, houseResources); return new TableResult&lt;&gt;(pageInfo.getRecords(), new Pagination(currentPage, pageSize, pageInfo.getTotal())); } /** * 根据id查询房源数据 * * @param id * @return */ public HouseResources queryHouseResourcesById(Long id){ // 调用dubbo中的服务进行查询数据 return this.apiHouseResourcesService.queryHouseResourcesById(id); } public boolean update(HouseResources houseResources) { return this.apiHouseResourcesService.updateHouseResources(houseResources); } }HouseResourcesController @Controller @RequestMapping(&quot;house/resources&quot;) public class HouseResourcesController { @Autowired private HouseResourcesService houseResourcesService; /** * 新增房源 * * @param houseResources json数据 * @return */ @PostMapping @ResponseBody public ResponseEntity&lt;Void&gt; save(@RequestBody HouseResources houseResources) { try { boolean bool = this.houseResourcesService.save(houseResources); if (bool) { return ResponseEntity.status(HttpStatus.CREATED).build(); } } catch (Exception e) { e.printStackTrace(); } return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).build(); } /** * 查询房源列表 * * @param houseResources * @param currentPage * @param pageSize * @return */ @GetMapping @ResponseBody public ResponseEntity&lt;TableResult&gt; list(HouseResources houseResources, @RequestParam(name = &quot;currentPage&quot;, defaultValue = &quot;1&quot;) Integer currentPage, @RequestParam(name = &quot;pageSize&quot;, defaultValue = &quot;10&quot;) Integer pageSize) { return ResponseEntity.ok(this.houseResourcesService.queryList(houseResources, currentPage, pageSize)); } /** * 修改房源 * * @param houseResources json数据 * @return */ @PutMapping @ResponseBody public ResponseEntity&lt;Void&gt; update(@RequestBody HouseResources houseResources) { try { boolean bool = this.houseResourcesService.update(houseResources); if (bool) { return ResponseEntity.status(HttpStatus.NO_CONTENT).build(); } } catch (Exception e) { e.printStackTrace(); } return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).build(); } } DubboApiApplication @SpringBootApplication public class DubboApiApplication { public static void main(String[] args) { SpringApplication.run(DubboApiApplication.class, args); } }haoke-manage-dubbo-serverpom.xml &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; haoke-manage-dubbo-server-generatorpom.xml &lt;dependencies&gt; &lt;!-- freemarker 模板引擎 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.freemarker&lt;/groupId&gt; &lt;artifactId&gt;freemarker&lt;/artifactId&gt; &lt;version&gt;2.3.28&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.0.5&lt;/version&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt; CodeGenerator 配置数据源及账户密码，运行即可生成对应文件，并将pojo类拷贝到工程中备用 public class CodeGenerator { /** * &lt;p&gt; * 读取控制台内容 * &lt;/p&gt; */ public static String scanner(String tip) { Scanner scanner = new Scanner(System.in); StringBuilder help = new StringBuilder(); help.append(&quot;请输入&quot; + tip + &quot;：&quot;); System.out.println(help.toString()); if (scanner.hasNext()) { String ipt = scanner.next(); if (StringUtils.isNotEmpty(ipt)) { return ipt; } } throw new MybatisPlusException(&quot;请输入正确的&quot; + tip + &quot;！&quot;); } public static void main(String[] args) { // 代码生成器 AutoGenerator mpg = new AutoGenerator(); // 全局配置 GlobalConfig gc = new GlobalConfig(); String projectPath = System.getProperty(&quot;user.dir&quot;); gc.setOutputDir(projectPath + &quot;/src/main/java&quot;); gc.setAuthor(&quot;onejane&quot;); gc.setOpen(false); mpg.setGlobalConfig(gc); // 数据源配置 DataSourceConfig dsc = new DataSourceConfig(); dsc.setUrl(&quot;jdbc:mysql://192.168.3.237:3306/haoke?useUnicode=true&amp;characterEncoding=utf8&amp;autoReconnect=true&amp;allowMultiQueries=true&quot;); // dsc.setSchemaName(&quot;public&quot;); dsc.setDriverName(&quot;com.mysql.jdbc.Driver&quot;); dsc.setUsername(&quot;root&quot;); dsc.setPassword(&quot;123456&quot;); mpg.setDataSource(dsc); // 包配置 PackageConfig pc = new PackageConfig(); pc.setModuleName(scanner(&quot;模块名&quot;)); pc.setParent(&quot;com.onejane.haoke.dubbo.server&quot;); mpg.setPackageInfo(pc); // 自定义配置 InjectionConfig cfg = new InjectionConfig() { @Override public void initMap() { // to do nothing } }; List&lt;FileOutConfig&gt; focList = new ArrayList&lt;&gt;(); focList.add(new FileOutConfig(&quot;/templates/mapper.xml.ftl&quot;) { @Override public String outputFile(TableInfo tableInfo) { // 自定义输入文件名称 return projectPath + &quot;/src/main/resources/mapper/&quot; + pc.getModuleName() + &quot;/&quot; + tableInfo.getEntityName() + &quot;Mapper&quot; + StringPool.DOT_XML; } }); cfg.setFileOutConfigList(focList); mpg.setCfg(cfg); mpg.setTemplate(new TemplateConfig().setXml(null)); // 策略配置 StrategyConfig strategy = new StrategyConfig(); strategy.setNaming(NamingStrategy.underline_to_camel); strategy.setColumnNaming(NamingStrategy.underline_to_camel); strategy.setSuperEntityClass(&quot;com.onejane.haoke.dubbo.server.pojo.BasePojo&quot;); strategy.setEntityLombokModel(true); strategy.setRestControllerStyle(true); strategy.setSuperControllerClass(&quot;com.baomidou.ant.common.BaseController&quot;); strategy.setInclude(scanner(&quot;表名&quot;)); strategy.setSuperEntityColumns(&quot;id&quot;); strategy.setControllerMappingHyphenStyle(true); strategy.setTablePrefix(pc.getModuleName() + &quot;_&quot;); mpg.setStrategy(strategy); mpg.setTemplateEngine(new FreemarkerTemplateEngine()); mpg.execute(); } } 测试 haoke-manage-dubbo-server-house-resourcespom.xml &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.onejane&lt;/groupId&gt; &lt;artifactId&gt;haoke-manage-dubbo-server-common&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;!--需要注意：传递依赖中，如果需要使用，请显示引入--&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.0.5&lt;/version&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt; haoke-manage-dubbo-server-house-resources-dubbo-interfaceApiHouseResourcesService // dubbo service public interface ApiHouseResourcesService { /** * 新增房源 * * @param houseResources * @return -1:输入的参数不符合要求，0：数据插入数据库失败，1：成功 */ int saveHouseResources(HouseResources houseResources); /** * 分页查询房源列表 * * @param page 当前页 * @param pageSize 页面大小 * @param queryCondition 查询条件 * @return */ PageInfo&lt;HouseResources&gt; queryHouseResourcesList(int page, int pageSize, HouseResources queryCondition); /** * 根据id查询房源数据 * * @param id * @return */ HouseResources queryHouseResourcesById(Long id); boolean updateHouseResources(HouseResources houseResources); } HouseResources @Data @Accessors(chain = true) @TableName(&quot;tb_house_resources&quot;) public class HouseResources extends BasePojo { private static final long serialVersionUID = 779152022777511825L; @TableId(value = &quot;id&quot;, type = IdType.AUTO) private Long id; /** * 房源标题 */ private String title; /** * 楼盘id */ private Long estateId; /** * 楼号（栋） */ private String buildingNum; /** * 单元号 */ private String buildingUnit; /** * 门牌号 */ private String buildingFloorNum; /** * 租金 */ private Integer rent; /** * 租赁方式，1-整租，2-合租 */ private Integer rentMethod; /** * 支付方式，1-付一押一，2-付三押一，3-付六押一，4-年付押一，5-其它 */ private Integer paymentMethod; /** * 户型，如：2室1厅1卫 */ private String houseType; /** * 建筑面积 */ private String coveredArea; /** * 使用面积 */ private String useArea; /** * 楼层，如：8/26 */ private String floor; /** * 朝向：东、南、西、北 */ private String orientation; /** * 装修，1-精装，2-简装，3-毛坯 */ private Integer decoration; /** * 配套设施， 如：1,2,3 */ private String facilities; /** * 图片，最多5张 */ private String pic; /** * 描述 */ private String houseDesc; /** * 联系人 */ private String contact; /** * 手机号 */ private String mobile; /** * 看房时间，1-上午，2-中午，3-下午，4-晚上，5-全天 */ private Integer time; /** * 物业费 */ private String propertyCost; } haoke-manage-dubbo-server-house-resources-dubbo-servicepom.xml &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.onejane&lt;/groupId&gt; &lt;artifactId&gt;haoke-manage-dubbo-server-house-resources-dubbo-interface&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; application.properties # Spring boot application spring.application.name = haoke-manage-dubbo-server-house-resources # 数据库 spring.datasource.driver-class-name=com.mysql.jdbc.Driver spring.datasource.url=jdbc:mysql://192.168.3.237:3306/haoke?useUnicode=true&amp;characterEncoding=utf8&amp;autoReconnect=true&amp;allowMultiQueries=true&amp;useSSL=false spring.datasource.username=root spring.datasource.password=123456 # 服务的扫描包 dubbo.scan.basePackages = com.onejane.haoke.dubbo.server.api # 应用名称 dubbo.application.name = dubbo-provider-house-resources # 协议以及端口 dubbo.protocol.name = dubbo dubbo.protocol.port = 20880 # zk注册中心 dubbo.registry.address = zookeeper://192.168.3.237:2181 dubbo.registry.client = zkclient MybatisConfig @MapperScan(&quot;com.onejane.haoke.dubbo.server.mapper&quot;) @Configuration public class MybatisConfig { } HouseResourcesMapper public interface HouseResourcesMapper extends BaseMapper&lt;HouseResources&gt; { } HouseResourcesService // spring service public interface HouseResourcesService { /** * @param houseResources * * @return -1:输入的参数不符合要求，0：数据插入数据库失败，1：成功 */ int saveHouseResources(HouseResources houseResources); PageInfo&lt;HouseResources&gt; queryHouseResourcesList(int page, int pageSize, HouseResources queryCondition); /** * 根据房源id查询房源数据 * * @param id * @return */ HouseResources queryHouseResourcesById(Long id); /** * 更新房源数据 * * @param houseResources * @return */ boolean updateHouseResources(HouseResources houseResources); }HouseResourcesServiceImpl @Transactional @Service public class HouseResourcesServiceImpl extends BaseServiceImpl&lt;HouseResources&gt; implements HouseResourcesService { /** * @param houseResources * @return -1:输入的参数不符合要求，0：数据插入数据库失败，1：成功 */ @Override public int saveHouseResources(HouseResources houseResources) { // 添加校验或者是其他的一些逻辑 if (StringUtils.isBlank(houseResources.getTitle())) { // 不符合要求 return -1; } return super.save(houseResources); } @Override public PageInfo&lt;HouseResources&gt; queryHouseResourcesList(int page, int pageSize, HouseResources queryCondition) { QueryWrapper queryWrapper = new QueryWrapper(); // 根据数据的更新时间做倒序排序 queryWrapper.orderByDesc(&quot;updated&quot;); IPage iPage = super.queryPageList(queryWrapper, page, pageSize); return new PageInfo&lt;HouseResources&gt;(Long.valueOf(iPage.getTotal()).intValue(), page, pageSize, iPage.getRecords()); } public HouseResources queryHouseResourcesById(Long id) { return super.queryById(id); } @Override public boolean updateHouseResources(HouseResources houseResources) { return super.update(houseResources) == 1; } } DubboProvider @SpringBootApplication public class DubboProvider { /** * 分页插件 */ @Bean public PaginationInterceptor paginationInterceptor() { return new PaginationInterceptor(); } public static void main(String[] args) { new SpringApplicationBuilder(DubboProvider.class) .web(WebApplicationType.NONE) // 非 Web 应用 .run(args); } } ApiHouseResourcesServiceImpl @Service(version = &quot;1.0.0&quot;) // 分离dubbo服务和spring服务，易于扩展 public class ApiHouseResourcesServiceImpl implements ApiHouseResourcesService { @Autowired private HouseResourcesService houseResourcesService; @Override public int saveHouseResources(HouseResources houseResources) { return this.houseResourcesService.saveHouseResources(houseResources); } @Override public PageInfo&lt;HouseResources&gt; queryHouseResourcesList(int page, int pageSize, HouseResources queryCondition) { return this.houseResourcesService.queryHouseResourcesList(page, pageSize, queryCondition); } public HouseResources queryHouseResourcesById(Long id) { return this.houseResourcesService.queryHouseResourcesById(id); } @Override public boolean updateHouseResources(HouseResources houseResources) { return this.houseResourcesService.updateHouseResources(houseResources); } } 启动DubboAdmin中显示 测试 整合前端src/pages/haoke/House/AddResource.js添加标题及修改表单提交地址 &lt;FormItem {...formItemLayout} label=&quot;房源标题&quot;&gt; {getFieldDecorator(&#39;title&#39;,{rules:[{ required: true, message:&quot;此项为必填项&quot; }]})(&lt;Input style={{ width: '100%' }} /&gt;)} &lt;/FormItem&gt; dispatch({ type: &#39;house/submitHouseForm&#39;, payload: values, }); src/pages/haoke/House/models/form.js增加model import { routerRedux } from &#39;dva/router&#39;; import { message } from &#39;antd&#39;; import { addHouseResource } from &#39;@/services/haoke&#39;; export default { namespace: &#39;house&#39;, state: { }, effects: { *submitHouseForm({ payload }, { call }) { yield call(addHouseResource, payload); message.success(&#39;提交成功&#39;); } }, reducers: { saveStepFormData(state, { payload }) { return { ...state }; }, }, }; src/services/haoke.js增加服务，请求服务并且处理业务逻辑 import request from &#39;@/utils/request&#39;; export async function addHouseResource(params) { return request(&#39;/haoke/house/resources&#39;, { method: &#39;POST&#39;, body: params }); } 由于我们前端系统8000和后台服务系统18080的端口不同，会导致跨域问题，我们通过umi提供的反向代理功能解决这个问题。haoke-manage-web/config/config.js proxy: { &#39;/haoke/&#39;: { target: &#39;http://127.0.0.1:18080/&#39;, changeOrigin: true, pathRewrite: { &#39;^/haoke/&#39;: &#39;&#39; } } }, 代理效果是这样的：以haoke开头的请求都会被代理请求：http://localhost:8000/haoke/house/resources实际：http://127.0.0.1:18080/house/resources 测试启动前端：itcast-haoke-manage-web&gt;tyarn start启动提供者：itcast-haoke-manage-dubbo-server-house-resources-dubbo-service/DubboProvider启动消费者：tcast-haoke-manage/itcast-haoke-manage-api-server/DubboApiApplication]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>mybatisplus</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[好客租房dubbo]]></title>
    <url>%2F2019%2F08%2F21%2F%E5%A5%BD%E5%AE%A2%E7%A7%9F%E6%88%BFdubbo%2F</url>
    <content type="text"><![CDATA[基于dubbo react elk实战整合开发 整体架构 后端架构：SpringBoot+StringMVC+Dubbo+Mybatis+ELK+区块链 前端架构：React.js+html5+百度地图+微信小程序前端初始化haoke-manage-web tyarn install tyarn start 修改logo及版权信息src/layouts/BasicLayout.js 全局的布局文件 &lt;SiderMenu![itcast-haoke-manage-web](./attachments/itcast-haoke-manage-web.zip) logo={logo} Authorized={Authorized} theme={navTheme} onCollapse={this.handleMenuCollapse} menuData={menuData} isMobile={isMobile} {...this.props} /&gt; src/components/SiderMenu/index.js &lt;SiderMenu {...props} flatMenuKeys={getFlatMenuKeys(menuData)} collapsed={isMobile ? false : collapsed} /&gt; src/components/SiderMenu/SiderMenu.js &lt;div className={styles.logo} id=&quot;logo&quot;&gt; &lt;Link to=&quot;/&quot;&gt; {/* &lt;img src={logo} alt=&quot;logo&quot; /&gt; */} &lt;h1&gt;好客租房 · 管理系统&lt;/h1&gt; &lt;/Link&gt; &lt;/div&gt; src/layouts/Footer.js &lt;Footer style={{ padding: 0 }}&gt; &lt;GlobalFooter copyright={ &lt;Fragment&gt; Copyright &lt;Icon type=&quot;copyright&quot; /&gt; 2018 黑马程序员 博学谷 出品 &lt;/Fragment&gt; } /&gt; &lt;/Footer&gt; 新增房源config/router.config.js { path: &#39;/&#39;, redirect: &#39;/house/resource&#39; }, // 进入系统默认打开房源管理 { //房源管理 path: &#39;/house&#39;, name: &#39;house&#39;, icon: &#39;home&#39;, routes: [ { path: &#39;/house/resource&#39;, name: &#39;resource&#39;, component: &#39;./haoke/House/Resource&#39; }, { path: &#39;/house/addResource&#39;, name: &#39;addResource&#39;, component: &#39;./haoke/House/AddResource&#39; }, { path: &#39;/house/kanfang&#39;, name: &#39;kanfang&#39;, component: &#39;./haoke/House/KanFang&#39; }, { path: &#39;/house/zufang&#39;, name: &#39;zufang&#39;, component: &#39;./haoke/House/ZuFang&#39; } ] }, src/pages/haoke/House/AddResource.js @Form.create() 对页面进行了包装，包装之后，会在this.props中增加form对象,将拥有getFieldDecorator 双向绑定等功能,经过 getFieldDecorator 包装的控件，表单控件会自动添加 value （或 valuePropName 指定的其他属性） onChange （或 trigger 指定的其他属性），数据同步将被 Form 接管 你不再需要也不应该用 onChange 来做同步，但还是可以继续监听 onChange 等事件。 你不能用控件的 value defaultValue 等属性来设置表单域的值，默认值可以用getFieldDecorator 里的 initialValue,利用rule进行参数规则校验 你不应该用 setState ，可以使用 this.props.form.setFieldsValue 来动态改变表单值。 表单提交&lt;Button type=&quot;primary&quot; htmlType=&quot;submit&quot; loading={submitting}&gt; &lt;Form onSubmit={this.handleSubmit} hideRequiredMark style={{ marginTop: 8 }}&gt; 进行提交拦截 handleSubmit = e =&gt; { 通过form.validateFieldsAndScroll()对表单进行校验，通过values获取表单中输入的值。通过dispatch()调用model中定义的方法。 const { dispatch, form } = this.props; e.preventDefault(); console.log(this.state.fileList); form.validateFieldsAndScroll((err, values) =&gt; { if (!err) { if(values.facilities){ values.facilities = values.facilities.join(&quot;,&quot;); } if(values.floor_1 &amp;&amp; values.floor_2){ values.floor = values.floor_1 + &quot;/&quot; + values.floor_2; } values.houseType = values.houseType_1 + &quot;室&quot; + values.houseType_2 + &quot;厅&quot; + values.houseType_3 + &quot;卫&quot; + values.houseType_4 + &quot;厨&quot; + values.houseType_2 + &quot;阳台&quot;; delete values.floor_1; delete values.floor_2; delete values.houseType_1; delete values.houseType_2; delete values.houseType_3; delete values.houseType_4; delete values.houseType_5; dispatch({ type: &#39;form/submitRegularForm&#39;, payload: values, }); } }); }; 自动完成const estateMap = new Map([ [&#39;中远两湾城&#39;,&#39;1001|上海市,上海市,普陀区,远景路97弄&#39;], [&#39;上海康城&#39;,&#39;1002|上海市,上海市,闵行区,莘松路958弄&#39;], [&#39;保利西子湾&#39;,&#39;1003|上海市,上海市,松江区,广富林路1188弄&#39;], [&#39;万科城市花园&#39;,&#39;1004|上海市,上海市,闵行区,七莘路3333弄2区-15区&#39;], [&#39;上海阳城&#39;,&#39;1005|上海市,上海市,闵行区,罗锦路888弄&#39;] ]); &lt;AutoComplete style={{ width: '100%' }} dataSource={this.state.estateDataSource} placeholder=&quot;搜索楼盘&quot; onSelect={(value, option)=&gt;{ let v = estateMap.get(value); this.setState({ estateAddress: v.substring(v.indexOf(&#39;|&#39;)+1), estateId : v.substring(0,v.indexOf(&#39;|&#39;)) }); }} onSearch={this.handleSearch} filterOption={(inputValue, option) =&gt; option.props.children.toUpperCase().indexOf(inputValue.toUpperCase()) !== -1} /&gt; 通过onSearch进行动态设置数据源，这里使用的数据是静态数据 handleSearch = (value)=&gt;{ let arr = new Array(); if(value.length &gt; 0 ){ estateMap.forEach((v, k) =&gt; { if(k.startsWith(value)){ arr.push(k); } }); } this.setState({ estateDataSource: arr }); } ; 通过onSelect设置 选中楼盘后，在楼盘地址中填写地址数据 onSelect={(value, option)=&gt;{ let v = estateMap.get(value); this.setState({ estateAddress: v.substring(v.indexOf(&#39;|&#39;)+1), estateId : v.substring(0,v.indexOf(&#39;|&#39;)) }); }} 图片上传父组件通过属性的方式进行引用子组件，自组件在bind方法中改变this的引用为父组件 &lt;FormItem {...formItemLayout} label=&quot;上传室内图&quot;&gt; &lt;PicturesWall handleFileList={this.handleFileList.bind(this)}/&gt; &lt;/FormItem&gt; 父组件中获取数据 handleFileList = (obj)=&gt;{ console.log(obj, &quot;图片列表&quot;); } src/pages/haoke/Utils/PicturesWall.js 在子组件中通过this.props获取父组件方法传入的函数，进行调用，即可把数据传递到父组件中 handleChange = ({ fileList }) =&gt; { this.setState({ fileList }); this.props.handleFileList(this.state.fileList); } &lt;Upload action=&quot;1111111&quot; listType=&quot;picture-card&quot; fileList={fileList} onPreview={this.handlePreview} onChange={this.handleChange} &gt; {fileList.length &gt;= 5 ? null : uploadButton} &lt;/Upload&gt; 后端后台系统服务采用RPC+微服务的架构思想，RPC采用dubbo架构作为服务治理框架，对外接口采用RESTFul+GraphQL接口方式。 单一应用架构当网站流量很小时，只需一个应用，将所有功能都部署在一起，以减少部署节点和成本。此时，用于简化增删改查工作量的数据访问框架(ORM)是关键。 垂直应用架构当访问量逐渐增大，单一应用增加机器带来的加速度越来越小，将应用拆成互不相干的几个应用，以提升效率。此时，用于加速前端页面开发的Web框架(MVC)是关键。 分布式服务架构当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。此时，用于提高业务复用及整合的分布式服务框架(RPC)是关键。 流动计算架构当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。此时，用于提高机器利用率的资源调度和治理中心(SOA)是关键。调用关系说明 服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。流程说明： 服务提供者启动时: 向 /dubbo/com.foo.BarService/providers 目录下写入自己的 URL 地址 服务消费者启动时: 订阅 /dubbo/com.foo.BarService/providers 目录下的提供者 URL 地址。并向/dubbo/com.foo.BarService/consumers 目录下写入自己的 URL 地址 监控中心启动时: 订阅 /dubbo/com.foo.BarService 目录下的所有提供者和消费者 URL 地址。支持以下功能： 当提供者出现断电等异常停机时，注册中心能自动删除提供者信息 当注册中心重启时，能自动恢复注册数据，以及订阅请求 当会话过期时，能自动恢复注册数据，以及订阅请求 当设置 &lt;dubbo:registry check=”false” /&gt; 时，记录失败注册和订阅请求，后台定时重试 可通过 &lt;dubbo:registry username=”admin” password=”1234” /&gt; 设置 zookeeper 登录信息 可通过 &lt;dubbo:registry group=”dubbo” /&gt; 设置 zookeeper 的根节点，不设置将使用无根树 支持 * 号通配符 &lt;dubbo:reference group=”*” version=”*” /&gt; ，可订阅服务的所有分组和所有版本的提供者 zk安装apt-get install --reinstall systemd -y apt-get install -y docker.io systemctl start docker docker create --name zk -p 2181:2181 zookeeper:3.5 docker start zk 可以通过ZooInspector连接查看 yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum makecache fast yum list docker-ce --showduplicates | sort -r yum install -y docker-ce-18.09.5 systemctl restart docker docker create --name zk -p 2181:2181 zookeeper:3.5 docker start zk ZooInspector执行ZooInspector\build\start.bat查看zk信息 服务提供方dubbo/pom.xml &lt;!--添加SpringBoot parent支持--&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.0.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;!--添加SpringBoot测试--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--添加dubbo的springboot依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;0.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--添加dubbo依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;dubbo&lt;/artifactId&gt; &lt;version&gt;2.6.4&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;!--添加springboot的maven插件--&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; dubbo/dubbo-service/pom.xml &lt;dependencies&gt; &lt;!--添加springboot依赖，非web项目--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.13&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.sgroschupf&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; com/onejane/dubbo/pojo/User.java // 使用dubbo要求传输的对象必须实现序列化接口 public class User implements java.io.Serializable { private static final long serialVersionUID = -7341603933521593227L; private Long id; private String username; private String password; private Integer age; } com/onejane/dubbo/service/UserService.java public interface UserService { List&lt;User&gt; queryAll(); } com/onejane/dubbo/service/impl/UserServiceImpl.java @Service(version = &quot;${dubbo.service.version}&quot;) //声明这是一个dubbo服务 public class UserServiceImpl implements UserService { public List&lt;User&gt; queryAll() { List&lt;User&gt; list = new ArrayList&lt;User&gt;(); for (int i = 0; i &lt; 10; i++) { User user = new User(); user.setAge(10 + i); user.setId(Long.valueOf(i + 1)); user.setPassword(&quot;123456&quot;); user.setUsername(&quot;username_&quot; + i); list.add(user); } System.out.println(&quot;---------Service 3------------&quot;); return list; } } application.properties # Spring boot application spring.application.name = dubbo-service server.port = 9090 # Service version dubbo.service.version = 1.0.0 # 服务的扫描包 dubbo.scan.basePackages =com.onejane.dubbo.service # 应用名称 dubbo.application.name = dubbo-provider-demo # 协议以及端口 dubbo.protocol.name = dubbo dubbo.protocol.port = 20882 # zk注册中心 dubbo.registry.address = zookeeper://192.168.3.237:2181 dubbo.registry.client = zkclient com/onejane/dubbo/DubboProvider.java @SpringBootApplication public class DubboProvider { public static void main(String[] args) { new SpringApplicationBuilder(DubboProvider.class) .web(WebApplicationType.NONE) // 非 Web 应用 .run(args); } } 服务消费方dubbo/dubbo-comsumer/pom.xml &lt;dependencies&gt; &lt;!--添加springboot依赖，非web项目--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.13&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.sgroschupf&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.1&lt;/version&gt; &lt;/dependency&gt; &lt;!--引入service的依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;com.onejane&lt;/groupId&gt; &lt;artifactId&gt;dubbo-service&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; application.properties # Spring boot application spring.application.name = dubbo-consumer server.port = 9091 # 应用名称 dubbo.application.name = dubbo-consumer-demo # zk注册中心 dubbo.registry.address = zookeeper://192.168.3.237:2181 dubbo.registry.client = zkclient com/onejane/dubbo/UserServiceTest.java @RunWith(SpringRunner.class) @SpringBootTest public class UserServiceTest { // 负载均衡策略 默认随机 测试时启动dubbo.protocol.port多个不同端口的userService服务，并修改打印值进行区分 // loadbalance = &quot;roundrobin&quot;设置负载均衡策略 @Reference(version = &quot;1.0.0&quot;, loadbalance = &quot;roundrobin&quot;) private UserService userService; @Test public void testQueryAll() { for (int i = 0; i &lt; 100; i++) { System.out.println(&quot;开始调用远程服务 &gt;&gt;&gt;&gt;&gt;&quot; + i); List&lt;User&gt; users = this.userService.queryAll(); for (User user : users) { System.out.println(user); } try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } } } }启动服务后，即可测试 Dubbo Admintar zxf incubator-dubbo-ops.tar.gz -C /usr/local/ tar zxf apache-maven-3.6.0-bin.tar.gz -C /usr/local/ vim incubator-dubbo-ops/dubbo-admin-backend/src/main/resources/application.properties dubbo.registry.address=zookeeper://192.168.3.237:2181 vim /etc/profile 如误操作导致基础命令丢失，export PATH=/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin export MAVEN_HOME=/usr/local/apache-maven-3.6.0 export JAVA_HOME=/usr/local/jdk export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$MAVEN_HOME/bin:$PATH source /etc/profile cd /usr/local/incubator-dubbo-ops &amp;&amp; mvn clean install vim dubbo-admin-backend/src/main/resources/application.properties server.port=8888 mvn --projects dubbo-admin-backend spring-boot:run http://192.168.3.237:8888Dubbo 缺省协议不适合传送大数据量的服务，比如传文件，传视频等，除非请求量很低。Dubbo 缺省协议dubbo:// 协议采用单一长连接和 NIO 异步通讯，适合于小数据量大并发的服务调用，以及服务消费者机器数远大于服务提供者机器数的情况。 Transporter （传输）: mina, netty, grizzySerialization（序列化）: dubbo, hessian2, java, jsonDispatcher（分发调度）: all, direct, message, execution, connectionThreadPool（线程池）: fixed, cached 连接个数：单连接 连接方式：长连接 传输协议：TCP 传输方式：NIO 异步传输 序列化：Hessian 二进制序列化 适用范围：传入传出参数数据包较小（建议小于100K），消费者比提供者个数多，单一消费者无法压满提供 者，尽量不要用 dubbo 协议传输大文件或超大字符串。 适用场景：常规远程服务方法调用]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
        <tag>ant design pro</tag>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[好客租房react]]></title>
    <url>%2F2019%2F08%2F21%2F%E5%A5%BD%E5%AE%A2%E7%A7%9F%E6%88%BFreact%2F</url>
    <content type="text"><![CDATA[基于spring cloud alibaba实战整合开发 React入门mock dvareact/config/config.js export default { plugins:[ [&#39;umi-plugin-react&#39;,{ dva: true, //开启dva进行数据分层管理 }] ] }; react/mock/MockListData.js export default { &#39;get /ds/list&#39;: function (req, res) { //模拟请求返回数据 res.json({ data: [1, 2, 3, 4, 5], maxNum: 5 }); } } react/src/util/request.js // import fetch from &#39;dva/fetch&#39;; function checkStatus(response) { if (response.status &gt;= 200 &amp;&amp; response.status &lt; 300) { return response; } const error = new Error(response.statusText); error.response = response; throw error; } /** * Requests a URL, returning a promise. * * @param {string} url The URL we want to request * @param {object} [options] The options we want to pass to &quot;fetch&quot; * @return {object} An object containing either &quot;data&quot; or &quot;err&quot; */ export default async function request(url, options) { const response = await fetch(url, options); checkStatus(response); return await response.json(); } react/src/models/ListData.js import request from &#39;../util/request&#39;; export default { namespace: &#39;list&#39;, state: { data: [], maxNum: 1 }, reducers : { // 定义的一些函数 addNewData : function (state, result) { // state：指的是更新之前的状态数据, result: 请求到的数据 if(result.data){ //如果state中存在data数据，直接返回，在做初始化的操作 return result.data; } let maxNum = state.maxNum + 1; let newArr = [...state.data, maxNum]; return { data : newArr, maxNum : maxNum } //通过return 返回更新后的数据 } }, effects: { //新增effects配置，用于异步加载数据 *initData(params, sagaEffects) { //定义异步方法 const {call, put} = sagaEffects; //获取到call、put方法 const url = &quot;/ds/list&quot;; // 定义请求的url let data = yield call(request, url); //执行请求 yield put({ // 调用reducers中的方法 type : &quot;addNewData&quot;, //指定方法名 data : data //传递ajax回来的数据 }); } } } react/src/pages/List.js import React from &#39;react&#39;; import { connect } from &#39;dva&#39;; const namespace = &quot;list&quot;; // 说明：第一个回调函数，作用：将page层和model层进行链接，返回modle中的数据,并且将返回的数据，绑定到this.props // 接收第二个函数，这个函数的作用：将定义的函数绑定到this.props中，调用model层中定义的函数 @connect((state) =&gt; { return { dataList : state[namespace].data, maxNum : state[namespace].maxNum } }, (dispatch) =&gt; { // dispatch的作用：可以调用model层定义的函数 return { // 将返回的函数，绑定到this.props中 add : function () { dispatch({ //通过dispatch调用modle中定义的函数,通过type属性，指定函数命名，格式：namespace/函数名 type : namespace + &quot;/addNewData&quot; }); }, init : () =&gt; { dispatch({ //通过dispatch调用modle中定义的函数,通过type属性，指定函数命名，格式：namespace/函数名 type : namespace + &quot;/initData&quot; }); } } }) class List extends React.Component{ componentDidMount(){ //初始化的操作 this.props.init(); } render(){ return ( &lt;div&gt; &lt;ul&gt; { this.props.dataList.map((value,index)=&gt;{ return &lt;li key={index}&gt;{value}&lt;/li&gt; }) } &lt;/ul&gt; &lt;button onClick={() =&gt; { this.props.add(); }}&gt;点我&lt;/button&gt; &lt;/div&gt; ); } } export default List; umi dev Ant Design 入门react/config/config.js export default { plugins:[ [&#39;umi-plugin-react&#39;,{ dva: true, //开启dva进行数据分层管理 antd: true // 开启Ant Design功能 }] ], routes: [{ path: &#39;/&#39;, component: &#39;../layouts&#39;, //配置布局路由 routes: [ { path: &#39;/&#39;, component: &#39;./index&#39; }, { path: &#39;/myTabs&#39;, component: &#39;./myTabs&#39; }, { path: &#39;/user&#39;, routes: [ { path: &#39;/user/list&#39;, component: &#39;./user/UserList&#39; }, { path: &#39;/user/add&#39;, component: &#39;./user/UserAdd&#39; } ] } ] }] }; react/mock/MockListData.js &#39;get /ds/user/list&#39;: function (req, res) { res.json([{ key: &#39;1&#39;, name: &#39;张三1&#39;, age: 32, address: &#39;上海市&#39;, tags: [&#39;程序员&#39;, &#39;帅气&#39;], }, { key: &#39;2&#39;, name: &#39;李四2&#39;, age: 42, address: &#39;北京市&#39;, tags: [&#39;屌丝&#39;], }, { key: &#39;3&#39;, name: &#39;王五3&#39;, age: 32, address: &#39;杭州市&#39;, tags: [&#39;高富帅&#39;, &#39;富二代&#39;], }]); react/src/models/UserListData.js import request from &quot;../util/request&quot;; export default { namespace: &#39;userList&#39;, state: { list: [] }, effects: { *initData(params, sagaEffects) { const {call, put} = sagaEffects; const url = &quot;/ds/user/list&quot;; let data = yield call(request, url); yield put({ type : &quot;queryList&quot;, data : data }); } }, reducers: { queryList(state, result) { let data = [...result.data]; return { //更新状态值 list: data } } } } react/src/layouts/index.js import React from &#39;react&#39;; import { Layout, Menu, Icon } from &#39;antd&#39;; import Link from &#39;umi/link&#39;; const { Header, Footer, Sider, Content } = Layout; const SubMenu = Menu.SubMenu; // layouts/index.js文件将被作为全 局的布局文件。 class BasicLayout extends React.Component{ constructor(props){ super(props); this.state = { collapsed: true, } } render(){ return ( &lt;Layout&gt; &lt;Sider width={256} style={{minHeight: '100vh', color: 'white'}}&gt; &lt;div style={{ height: '32px', background: 'rgba(255,255,255,.2)', margin: '16px'}}/&gt; &lt;Menu defaultSelectedKeys={[&#39;1&#39;]} defaultOpenKeys={[&#39;sub1&#39;]} mode=&quot;inline&quot; theme=&quot;dark&quot; inlineCollapsed={this.state.collapsed} &gt; &lt;SubMenu key=&quot;sub1&quot; title={&lt;span&gt;&lt;Icon type=&quot;user&quot;/&gt;&lt;span&gt;用户管理&lt;/span&gt;&lt;/span&gt;}&gt; &lt;Menu.Item key=&quot;1&quot;&gt;&lt;Link to=&quot;/user/add&quot;&gt;新增用户&lt;/Link&gt;&lt;/Menu.Item&gt; &lt;Menu.Item key=&quot;2&quot;&gt;&lt;Link to=&quot;/user/list&quot;&gt;新增列表&lt;/Link&gt;&lt;/Menu.Item&gt; &lt;/SubMenu&gt; &lt;/Menu&gt; &lt;/Sider&gt; &lt;Layout&gt; &lt;Header style={{ background: '#fff', textAlign: 'center', padding: 0 }}&gt;Header&lt;/Header&gt; &lt;Content style={{ margin: '24px 16px 0' }}&gt; &lt;div style={{ padding: 24, background: '#fff', minHeight: 360 }}&gt; { this.props.children } &lt;/div&gt; &lt;/Content&gt; &lt;Footer style={{ textAlign: 'center' }}&gt;后台系统&lt;/Footer&gt; &lt;/Layout&gt; &lt;/Layout&gt; ) } } export default BasicLayout; react/src/pages/MyTabs.js import React from &#39;react&#39;; import { Tabs } from &#39;antd&#39;; // 第一步，导入需要使用的组件 const TabPane = Tabs.TabPane; function callback(key) { console.log(key); } class MyTabs extends React.Component{ render(){ return ( &lt;Tabs defaultActiveKey=&quot;1&quot; onChange={callback}&gt; &lt;TabPane tab=&quot;Tab 1&quot; key=&quot;1&quot;&gt;hello antd wo de 第一个 tabs&lt;/TabPane&gt; &lt;TabPane tab=&quot;Tab 2&quot; key=&quot;2&quot;&gt;Content of Tab Pane 2&lt;/TabPane&gt; &lt;TabPane tab=&quot;Tab 3&quot; key=&quot;3&quot;&gt;Content of Tab Pane 3&lt;/TabPane&gt; &lt;/Tabs&gt; ) } } export default MyTabs;react/src/pages/user/UserList.js import React from &#39;react&#39;; import { connect } from &#39;dva&#39;; import {Table, Divider, Tag, Pagination } from &#39;antd&#39;; const {Column} = Table; const namespace = &#39;userList&#39;; @connect((state)=&gt;{ return { data : state[namespace].list } }, (dispatch) =&gt; { return { initData : () =&gt; { dispatch({ type: namespace + &quot;/initData&quot; }); } } }) class UserList extends React.Component { componentDidMount(){ this.props.initData(); } render() { return ( &lt;div&gt; &lt;Table dataSource={this.props.data} pagination={{position:"bottom",total:500,pageSize:10, defaultCurrent:3}}&gt; &lt;Column title=&quot;姓名&quot; dataIndex=&quot;name&quot; key=&quot;name&quot; /&gt; &lt;Column title=&quot;年龄&quot; dataIndex=&quot;age&quot; key=&quot;age&quot; /&gt; &lt;Column title=&quot;地址&quot; dataIndex=&quot;address&quot; key=&quot;address&quot; /&gt; &lt;Column title=&quot;标签&quot; dataIndex=&quot;tags&quot; key=&quot;tags&quot; render={tags =&gt; ( &lt;span&gt; {tags.map(tag =&gt; &lt;Tag color=&quot;blue&quot; key={tag}&gt;{tag}&lt;/Tag&gt;)} &lt;/span&gt; )} /&gt; &lt;Column title=&quot;操作&quot; key=&quot;action&quot; render={(text, record) =&gt; ( &lt;span&gt; &lt;a href=&quot;javascript:;&quot;&gt;编辑&lt;/a&gt; &lt;Divider type=&quot;vertical&quot;/&gt; &lt;a href=&quot;javascript:;&quot;&gt;删除&lt;/a&gt; &lt;/span&gt; )} /&gt; &lt;/Table&gt; &lt;/div&gt; ); } } export default UserList; react/src/pages/user/UserAdd.js import React from &#39;react&#39; class UserAdd extends React.Component{ render(){ return ( &lt;div&gt;新增用户&lt;/div&gt; ); } } export default UserAdd; react/src/pages/index.js import React from &#39;react&#39; class Index extends React.Component { render(){ return &lt;div&gt;首页&lt;/div&gt; } } export default Index; // http://localhost:8000/Ant Design Pro 入门https://github.com/ant-design/ant-design-pro├── config # umi 配置，包含路由，构建等配置├── mock # 本地模拟数据├── public│ └── favicon.png # Favicon├── src│ ├── assets # 本地静态资源│ ├── components # 业务通用组件│ ├── e2e # 集成测试用例│ ├── layouts # 通用布局│ ├── models # 全局 dva model│ ├── pages # 业务页面入口和常用模板│ ├── services # 后台接口服务│ ├── utils # 工具库│ ├── locales # 国际化资源│ ├── global.less # 全局样式│ └── global.js # 全局 JS├── tests # 测试工具├── README.md└── package.json tyarn install #安装相关依赖 tyarn start #启动服务 http://localhost:8000/dashboard/analysis 测试新增路由config/router.config.js 默认配置两套路由 { path: &#39;/new&#39;, name: &#39;new&#39;, icon: &#39;user&#39;, routes: [ { path: &#39;/new/analysis&#39;, name: &#39;analysis&#39;, component: &#39;./New/NewAnalysis&#39;, }, { path: &#39;/new/monitor&#39;, name: &#39;monitor&#39;, component: &#39;./Dashboard/Monitor&#39;, }, { path: &#39;/new/workplace&#39;, name: &#39;workplace&#39;, component: &#39;./Dashboard/Workplace&#39;, }, ], }, src/pages/New/NewAnalysis.js import React from &#39;react&#39; class NewAnalysis extends React.Component { render() { return (&lt;div&gt;NewAnalysis&lt;/div&gt;); } } export default NewAnalysis; src/locales/zh-CN.js &#39;menu.new&#39;: &#39;New Dashboard&#39;, &#39;menu.new.analysis&#39;: &#39;New 分析页&#39;, &#39;menu.new.monitor&#39;: &#39;New 监控页&#39;, &#39;menu.new.workplace&#39;: &#39;New 工作台&#39;, model执行流程http://localhost:8000/list/table-listsrc/pages/List/TableList.js Table组件生成表格，数据源是data &lt;StandardTable selectedRows={selectedRows} loading={loading} data={data} columns={this.columns} onSelectRow={this.handleSelectRows} onChange={this.handleStandardTableChange} /&gt; data数据从构造方法的props中获取 const { rule: { data }, loading, } = this.props; rule数据由@connect装饰器获取，{ rule, loading }是解构表达式，props从connect中获取数据 @connect(({ rule, loading }) =&gt; ({ rule, loading: loading.models.rule, })) src/pages/List/models/rule.js 生成数据rule reducers: { save(state, action) { return { ...state, data: action.payload, }; }, src/pages/List/TableList.js 组件加载完成后加载数据 componentDidMount() { const { dispatch } = this.props; dispatch({ type: &#39;rule/fetch&#39;, }); } src/pages/List/models/rule.js 从rule.js中reducers加载save方法数据 *fetch({ payload }, { call, put }) { const response = yield call(queryRule, payload); yield put({ type: &#39;save&#39;, payload: response, }); }, queryRule是在/services/api中进行了定义 export async function queryRule(params) { return request(`/api/rule?${stringify(params)}`); } 数据的mock在mock/rule.js中完成 export default { &#39;GET /api/rule&#39;: getRule, &#39;POST /api/rule&#39;: postRule, }; git commit -m “ant-design-pro 下载” –no-verify]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>ant design pro</tag>
        <tag>react</tag>
        <tag>ant design</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[好客租房es6语法]]></title>
    <url>%2F2019%2F08%2F21%2F%E5%A5%BD%E5%AE%A2%E7%A7%9F%E6%88%BFes6%2F</url>
    <content type="text"><![CDATA[基于dubbo react elk实战整合开发 基础语法&lt;!DOCTYPE html&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;script&gt; // var 定义全局变量 for (var i = 0; i &lt; 5; i++) { console.log(i); } console.log(&quot;循环外：&quot; + i); // let 控制变量作用域 for (let j = 0; j &lt; 5; j++) { console.log(j); } console.log(&quot;循环外：&quot; + j); // const声明常量 不可修改 const a = 1; console.log(&quot;a=&quot;, a); a = 2; console.log(&quot;a=&quot;, a) // 字符串拓展函数 console.log(&quot;hello world&quot;.includes(&quot;hello&quot;)); console.log(&quot;hello world&quot;.startsWith(&quot;hello&quot;)); console.log(&quot;hello world&quot;.endsWith(&quot;world&quot;)); // 字符串模板保留换行源格式 let str = ` hello world `; console.log(str); // 新定义数组解构获取顺序获取值 let arr = [1, 2, 3] const [x, y, z] = arr; console.log(x, y, z) const [m] = arr; console.log(m) // 新定义对象结构顺序获取属性值 const person = { name: &#39;wj&#39;, age: 21, language: [&#39;java&#39;, &#39;js&#39;, &#39;php&#39;] } const {name, age, language} = person console.log(name, age, language) const {name: n, age: a, language: l} = person; console.log(n) // 函数默认值 function add(a, b = 1) { // b = b || 1; return a + b; } console.log(add(10)) // 箭头函数 var print = obj =&gt; (console.log(obj)) //一个参数 var sum = (a, b) =&gt; a + b // 多个参数 var sayHello1 = () =&gt; console.log(&quot;hello&quot;) // 没有参数 var sayHello2 = (a, b) =&gt; { // 多个函数 console.log(&quot;hello&quot;) console.log(&quot;world&quot;) return a + b; } // 对象函数属性简写 let persons = { name: &quot;jack&quot;, eat: food =&gt; console.log(persons.name + &quot;再吃&quot; + food), // 这里拿不到this eat1(food) { console.log(this.name + &quot;再吃&quot; + food) }, eat2(food) { console.log(this.name + &quot;再吃&quot; + food) } }; persons.eat(&quot;西瓜&quot;) // 箭头函数结合解构表达式 var hi = ({name}) =&gt; console.log(&quot;hello,&quot; + name) hi(persons) // map reduce let array = [&#39;1&#39;, &#39;2&#39;].map(s =&gt; parseInt(s)); //将原数组所有元素用函数处理哇年后放入新数组返回 let array1 = arr.map(function (s) { return parseInt(s); }) console.log(array) let num = [1, 20, 6, 5]; console.log(num.reduce((a, b) =&gt; a + b)) // 从左到友依次用reduce处理，并把结果作为下次reduce的第一个参数 let result = arr.reduce((a, b) =&gt; { return a + b; }, 1); //接受函数必须，初始值可选 // 扩展运算符 console.log(...[2, 3], ...[1, 20, 6, 5], 0) // 数组合并 let add1 = (x, y) =&gt; x + y; console.log(add1(...[1, 2])) const [f, ...l] = [1, 2, 3, 4, 5] //结合结构表达式 console.log(f, l) console.log(...&#39;heool&#39;) //字符串转数组 // promise 异步执行 const p = new Promise((resolve, reject) =&gt; { setTimeout(() =&gt; { const num = Math.random(); // 随机返回成功或失败 if (num &lt; 0.5) { resolve(&quot;成功 num=&quot; + num) } else { reject(&quot;失败 num=&quot; + num) } }, 300) }) p.then(function (msg) { console.log(msg) }).catch(function (msg) { console.log(msg) }) // set map let set = new Set() set.add(1) // clear delete has forEach(function(){}) size set.forEach(value =&gt; console.log(value)) let set2 = new Set([1, 2, 2, 1, 1]) // map为&lt;object,object&gt; const map = new Map([ [&#39;key1&#39;, &#39;value1&#39;], [&#39;value2&#39;, &#39;value2&#39;] ]) const set3 = new Set([ [&#39;t&#39;, &#39;t&#39;], [&#39;h&#39;, &#39;h&#39;] ]) const map2 = new Map(set3) const map3 = new Map(map) map.set(&#39;z&#39;, &#39;z&#39;) // clear delete(key) has(key) forEach(function(value,key){}) size values keys entries for (let key of map.keys()) { console.log(key) } console.log(...map.values()) // 类的基本用法 class User { constructor(name, age = 20) { this.name = name; this.age = age; } sayHi() { return &quot;hi&quot; } static isAdult(age) { if (age &gt;= 18) { return &quot;成年人&quot; } return &quot;未成年人&quot; } } class zhangsan extends User { // 类的继承 constructor() { super(&quot;张三&quot;, 10) this.address = &quot;上海&quot; } test(){ return &quot;name=&quot;+this.name; } } let user = new User(&quot;张三&quot;) let zs = new zhangsan() console.log(user) console.log(user.sayHi()) console.log(User.isAdult(20)) console.log(zs.name, zs.address) console.log(zs.sayHi()) // Generator函数 function* hello() { yield &quot;h&quot; yield &quot;e&quot; return &quot;a&quot; } let h = hello(); for (let obj of h) { //循环遍历或next遍历 console.log(&quot;===&quot; + obj) } console.log(h.next()) console.log(h.next()) console.log(h.next()) // 修饰器 修改类的行为 @T class Animal { constructor(name,age=20){ this.name=name; this.age=age; } } function T(target){ console.log(target); target.contry = &quot;china&quot; //通过修饰器添加的属性是静态属性 } console.log(Animal.contry) // 无法运行，需要转码：将ES6活ES2017转为ES5使用（将箭头函数转为普通函数） &lt;/script&gt; &lt;/html&gt;umi转码 node -v v8.12.0 npm i yarn tyarn -g tyarn使用淘宝源 tyarn -v 1.16.0 若报错通过yarn global bin获取路径加入Path tyarn global add umi umi tyarn init -y 多一个package.json umi g page index 生成page文件夹 编辑index.js index.js // 修饰器 function T(target) { console.log(target); target.country=&quot;中国&quot; } @T class People{ constructor(name,age=20){ this.name=name; this.age=age; } } console.log(People.country); import Util from &#39;./Util&#39;; console.log(Util.sum(10, 5)); Util.js class Util { static sum = (a,b) =&gt; { return a + b; } } export default Util; umi dev,通过http://localhost:8000/ 查看控制台 reactjs tyarn init -y tyarn add umi –dev tyarn add umi-plugin-react –dev umi jsxreact/config/config.js export default {}; react/src/pages/HelloWorld.js export default ()=&gt;{ const t=()=&gt;&quot;pig&quot; return ( &lt;div&gt;hello world {t()}&lt;/div&gt; ); } umi build 转码生成文件 dist\umi.jsumi dev 访问http://localhost:8000 todolistreact/src/pages/HelloWorld.js import React from &#39;react&#39; class HelloWorld extends React.Component{ render() { // this.props.name接受属性，this.props.children接受标签内容 return &lt;div&gt;hello name={this.props.name},say={this.props.children}&lt;/div&gt; } } export default HelloWorld; react/src/pages/Show.js import React from &#39;react&#39; import HelloWorld from &#39;./HelloWorld&#39; class Show extends React.Component{ render() { return &lt;HelloWorld name=&quot;zhansan&quot;&gt;haha&lt;/HelloWorld&gt; } } export default Show; react/src/pages/List.js import React from &#39;react&#39;; class List extends React.Component{ constructor(props){ super(props); this.state = { dataList : [1,2,3], maxNum : 3 }; } /*this.state值在构造参数中完成，要修改this.state的值，需要调用this.setState()完成*/ render(){ return ( &lt;div&gt; &lt;ul&gt; { this.state.dataList.map((value,index)=&gt;{ return &lt;li key={index}&gt;{value}&lt;/li&gt; }) } &lt;/ul&gt; &lt;button onClick={() =&gt; { let maxNum = this.state.maxNum + 1; let list = [...this.state.dataList,maxNum]; this.setState({ dataList: list, maxNum: maxNum }); }}&gt;点我&lt;/button&gt; &lt;/div&gt; ); } } export default List; LifeCycleimport React from &#39;react&#39;; //第一步，导入React class LifeCycle extends React.Component { constructor(props) { super(props); //构造方法 console.log(&quot;constructor()&quot;); } componentDidMount() { //组件挂载后调用 console.log(&quot;componentDidMount()&quot;); } componentWillUnmount() { //在组件从 DOM 中移除之前立刻被调用。 console.log(&quot;componentWillUnmount()&quot;); } componentDidUpdate() { //在组件完成更新后立即调用。在初始化时不会被调用。 console.log(&quot;componentDidUpdate()&quot;); } shouldComponentUpdate(nextProps, nextState){ // 每当this.props或this.state有变化，在render方法执行之前，就会调用这个方法。 // 该方法返回一个布尔值，表示是否应该继续执行render方法，即如果返回false，UI 就不会更新，默认返回true。 // 组件挂载时，render方法的第一次执行，不会调用这个方法。 console.log(&quot;shouldComponentUpdate()&quot;); } render() { return ( &lt;div&gt; &lt;h1&gt;React Life Cycle!&lt;/h1&gt; &lt;/div&gt; ); } } export default LifeCycle;]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>react</tag>
        <tag>umi</tag>
        <tag>es6</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Alibaba网关服务]]></title>
    <url>%2F2019%2F08%2F21%2FAlibaba%E7%BD%91%E5%85%B3%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[基于spring cloud alibaba实战整合开发 ht-micro-record-service-gateway&lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-service-gateway&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-service-gateway&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt; &lt;artifactId&gt;jsr305&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt; &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;!-- Commons Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Commons Begin --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;!-- docker的maven插件，官网 https://github.com/spotify/docker-maven-plugin --&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt; &lt;version&gt;0.4.13&lt;/version&gt; &lt;configuration&gt; &lt;imageName&gt;192.168.2.7:5000/${project.artifactId}:${project.version}&lt;/imageName&gt; &lt;baseImage&gt;onejane-jdk1.8 &lt;/baseImage&gt; &lt;entryPoint&gt;[&quot;java&quot;, &quot;-jar&quot;,&quot;/${project.build.finalName}.jar&quot;]&lt;/entryPoint&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/&lt;/targetPath&gt; &lt;directory&gt;${project.build.directory}&lt;/directory&gt; &lt;include&gt;${project.build.finalName}.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;dockerHost&gt;http://192.168.2.7:2375&lt;/dockerHost&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;com.ht.micro.record.service.gateway.GatewayServiceApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; bootstrap.properties spring.application.name=ht-micro-record-service-gateway-config spring.main.allow-bean-definition-overriding=true spring.cloud.nacos.config.file-extension=yaml spring.cloud.nacos.config.server-addr=192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 nacos配置 spring: application: name: ht-micro-record-service-gateway jackson: date-format: yyyy-MM-dd HH:mm:ss time-zone: GMT+8 default-property-inclusion: non_null cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8721 dashboard: 192.168.2.7:190 gateway: discovery: locator: enabled: true routes: http://localhost:9000/api/user/user/20 访问user服务 - id: HT-MICRO-RECORD-SERVICE-USER uri: lb://ht-micro-record-service-user predicates: - Path=/api/user/** filters: - StripPrefix=2 server: port: 9000 feign: sentinel: enabled: true management: endpoints: web: exposure: include: &quot;*&quot; logging: level: org.springframework.cloud.gateway: debug GatewayServiceApplication.java @SpringBootApplication @EnableDiscoveryClient @EnableFeignClients public class GatewayServiceApplication { // ----------------------------- 解决跨域 Begin ----------------------------- private static final String ALL = &quot;*&quot;; private static final String MAX_AGE = &quot;18000L&quot;; @Bean public RouteDefinitionLocator discoveryClientRouteDefinitionLocator(DiscoveryClient discoveryClient, DiscoveryLocatorProperties properties) { return new DiscoveryClientRouteDefinitionLocator(discoveryClient, properties); } @Bean public ServerCodecConfigurer serverCodecConfigurer() { return new DefaultServerCodecConfigurer(); } @Bean public WebFilter corsFilter() { return (ServerWebExchange ctx, WebFilterChain chain) -&gt; { ServerHttpRequest request = ctx.getRequest(); if (!CorsUtils.isCorsRequest(request)) { return chain.filter(ctx); } HttpHeaders requestHeaders = request.getHeaders(); ServerHttpResponse response = ctx.getResponse(); HttpMethod requestMethod = requestHeaders.getAccessControlRequestMethod(); HttpHeaders headers = response.getHeaders(); headers.add(HttpHeaders.ACCESS_CONTROL_ALLOW_ORIGIN, requestHeaders.getOrigin()); headers.addAll(HttpHeaders.ACCESS_CONTROL_ALLOW_HEADERS, requestHeaders.getAccessControlRequestHeaders()); if (requestMethod != null) { headers.add(HttpHeaders.ACCESS_CONTROL_ALLOW_METHODS, requestMethod.name()); } headers.add(HttpHeaders.ACCESS_CONTROL_ALLOW_CREDENTIALS, &quot;true&quot;); headers.add(HttpHeaders.ACCESS_CONTROL_EXPOSE_HEADERS, ALL); headers.add(HttpHeaders.ACCESS_CONTROL_MAX_AGE, MAX_AGE); if (request.getMethod() == HttpMethod.OPTIONS) { response.setStatusCode(HttpStatus.OK); return Mono.empty(); } return chain.filter(ctx); }; } // ----------------------------- 解决跨域 End ----------------------------- public static void main(String[] args) { SpringApplication.run(GatewayServiceApplication.class, args); } }]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>spring cloud alibaba</tag>
        <tag>gateway</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Alibaba整合xxl-job]]></title>
    <url>%2F2019%2F08%2F21%2FAlibaba%E5%AE%9A%E6%97%B6%E5%99%A8%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[基于spring cloud alibaba实战整合开发 ht-micro-record-service-job &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-service-job&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-service-job&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.xuxueli&lt;/groupId&gt; &lt;artifactId&gt;xxl-job-core&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt; &lt;artifactId&gt;jsr305&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt; &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rocketmq&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;!-- Projects Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-commons-service&lt;/artifactId&gt; &lt;version&gt;${project.parent.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Projects End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;ht.micro.record.service.job.JobServiceApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; bootstrap.properties spring.application.name=ht-micro-record-service-xxl-job-config spring.cloud.nacos.config.file-extension=yaml spring.cloud.nacos.config.server-addr=192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 application.yaml spring: datasource: druid: url: jdbc:mysql://192.168.2.7:185/ht_micro_record?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false username: root password: 123456 initial-size: 1 min-idle: 1 max-active: 20 test-on-borrow: true driver-class-name: com.mysql.jdbc.Driver cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 config: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 server: port: 9701 xxl: job: executor: logpath: logs/xxl-job/jobhandler appname: xxl-job-executor port: 1234 logretentiondays: -1 ip: 192.168.3.233 admin: addresses: http://192.168.2.7:183/xxl-job-admin accessToken:nacos配置 spring: application: name: ht-micro-record-service-xxl-job cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8719 dashboard: 192.168.2.7:190 server: port: 9701 management: endpoints: web: exposure: include: &quot;*&quot; com/ht/micro/record/service/job/JobServiceApplication.java @SpringBootApplication(scanBasePackages = &quot;com.ht.micro.record&quot;) @EnableDiscoveryClient @MapperScan(basePackages = &quot;com.ht.micro.record.commons.mapper&quot;) public class JobServiceApplication { public static void main(String[] args) { SpringApplication.run(JobServiceApplication.class, args); } } com/ht/micro/record/service/job/config/XxlJobConfig.java @Configuration @ComponentScan(basePackages = &quot;com.ht.micro.record.service.job.handler&quot;) public class XxlJobConfig { private Logger logger = LoggerFactory.getLogger(XxlJobConfig.class); @Value(&quot;${xxl.job.admin.addresses}&quot;) private String adminAddresses; @Value(&quot;${xxl.job.executor.appname}&quot;) private String appName; @Value(&quot;${xxl.job.executor.ip}&quot;) private String ip; @Value(&quot;${xxl.job.executor.port}&quot;) private int port; @Value(&quot;${xxl.job.accessToken}&quot;) private String accessToken; @Value(&quot;${xxl.job.executor.logpath}&quot;) private String logPath; @Value(&quot;${xxl.job.executor.logretentiondays}&quot;) private int logRetentionDays; @Bean(initMethod = &quot;start&quot;, destroyMethod = &quot;destroy&quot;) public XxlJobSpringExecutor xxlJobExecutor() { logger.info(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; xxl-job config init.&quot;); XxlJobSpringExecutor xxlJobSpringExecutor = new XxlJobSpringExecutor(); xxlJobSpringExecutor.setAdminAddresses(adminAddresses); xxlJobSpringExecutor.setAppName(appName); xxlJobSpringExecutor.setIp(ip); xxlJobSpringExecutor.setPort(port); xxlJobSpringExecutor.setAccessToken(accessToken); xxlJobSpringExecutor.setLogPath(logPath); xxlJobSpringExecutor.setLogRetentionDays(logRetentionDays); return xxlJobSpringExecutor; } } com/ht/micro/record/service/job/handler/TestJobHandler.java @JobHandler(value=&quot;testJobHandler&quot;) @Component public class TestJobHandler extends IJobHandler { @Override public ReturnT&lt;String&gt; execute(String param) throws Exception { XxlJobLogger.log(&quot;XXL-JOB, Hello World.&quot;); for (int i = 0; i &lt; 5; i++) { XxlJobLogger.log(&quot;beat at:&quot; + i); TimeUnit.SECONDS.sleep(2); } return SUCCESS; } } http://192.168.2.7:183/xxl-job-admin]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>spring cloud alibaba</tag>
        <tag>xxl-job</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Alibaba分布式session]]></title>
    <url>%2F2019%2F08%2F21%2FAlibaba%E5%88%86%E5%B8%83%E5%BC%8Fsession%2F</url>
    <content type="text"><![CDATA[基于spring cloud alibaba实战整合开发 com.ht.micro.record.commons.domain.User @Data public class User implements Serializable { private long userId; private String username; private String password; } ht-micro-record-service-smspom.xml &lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-freemarker&lt;/artifactId&gt; &lt;/dependency&gt;SmsServiceApplication.java @EnableRedisHttpSession(maxInactiveIntervalInSeconds= 1800) //开启redis session支持,并配置session过期时间UserController.java @Controller @RequestMapping(value = &quot;user&quot;) public class UserController extends AbstractBaseController&lt;TbUser&gt; { @RequestMapping public String index() { return &quot;index&quot;; } @RequestMapping(&quot;home&quot;) public String home() { return &quot;home&quot;; } @PostMapping(&quot;login&quot;) public String login(User user, HttpSession session) { // 随机生成用户id user.setUserId(Math.round(Math.floor(Math.random() * 10 * 1000))); // 将用户信息保存到id中 session.setAttribute(&quot;USER&quot;, user); return &quot;home&quot;; } @PostMapping(&quot;logout&quot;) public String logout(HttpSession session) { session.removeAttribute(&quot;USER&quot;); session.invalidate(); return &quot;home&quot;; } }templates/home.ftl &lt;!doctype html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;title&gt;主页面&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h5&gt;登录用户: ${Session[&quot;USER&quot;].username} &lt;/h5&gt; &lt;h5&gt;用户编号: ${Session[&quot;USER&quot;].userId} &lt;/h5&gt; &lt;/body&gt; &lt;/html&gt;templates/index.ftl &lt;!doctype html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;title&gt;登录页面&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;form action=&quot;/user/login&quot; method=&quot;post&quot;&gt; 用户：&lt;input type=&quot;text&quot; name=&quot;username&quot;&gt;&lt;br/&gt; 密码：&lt;input type=&quot;password&quot; name=&quot;password&quot;&gt;&lt;br/&gt; &lt;button type=&quot;submit&quot;&gt;登录&lt;/button&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt;ht-micro-record-service-user pom.xml &lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-freemarker&lt;/artifactId&gt; &lt;/dependency&gt; application.yml所有模块加入redis配置 redis: #host: 127.0.0.1 #port: 6379 jedis: pool: # 连接池最大连接数,使用负值表示无限制。 max-active: 8 # 连接池最大阻塞等待时间,使用负值表示无限制。 max-wait: -1s # 连接池最大空闲数,使用负值表示无限制。 max-idle: 8 # 连接池最小空闲连接，只有设置为正值时候才有效 min-idle: 1 timeout: 300ms session: # session 存储方式 支持redis、mongo、jdbc、hazelcast store-type: redis cluster: nodes: 192.168.2.5:8001,192.168.2.5:8002,192.168.2.5:8003,192.168.2.7:8004,192.168.2.7:8005,192.168.2.7:8006 # 如果是集群节点 采用如下配置指定节点 #spring.redis.cluster.nodes templates/index.ftl &lt;!doctype html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;title&gt;登录页面&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;form action=&quot;/user/login&quot; method=&quot;post&quot;&gt; 用户：&lt;input type=&quot;text&quot; name=&quot;username&quot;&gt;&lt;br/&gt; 密码：&lt;input type=&quot;password&quot; name=&quot;password&quot;&gt;&lt;br/&gt; &lt;button type=&quot;submit&quot;&gt;登录&lt;/button&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; templates/home.ftl &lt;!doctype html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;title&gt;主页面&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h5&gt;登录用户: ${Session[&quot;USER&quot;].username} &lt;/h5&gt; &lt;h5&gt;用户编号: ${Session[&quot;USER&quot;].userId} &lt;/h5&gt; &lt;/body&gt; &lt;/html&gt;UserServiceApplication.java @EnableRedisHttpSession(maxInactiveIntervalInSeconds= 1800) //开启redis session支持,并配置session过期时间 UserController.java @Controller @RequestMapping(value = &quot;user&quot;) public class UserController extends AbstractBaseController&lt;TbUser&gt; { @Autowired private TbUserService tbUserService; @Autowired private UserService userService; @RequestMapping public String index() { return &quot;index&quot;; } @RequestMapping(&quot;home&quot;) public String home() { return &quot;home&quot;; } @PostMapping(&quot;login&quot;) public String login(User user, HttpSession session) { // 随机生成用户id user.setUserId(Math.round(Math.floor(Math.random() * 10 * 1000))); // 将用户信息保存到id中 session.setAttribute(&quot;USER&quot;, user); return &quot;home&quot;; } @PostMapping(&quot;logout&quot;) public String logout(HttpSession session) { session.removeAttribute(&quot;USER&quot;); session.invalidate(); return &quot;home&quot;; } } http://localhost:9507/user 登陆后，进入http://localhost:9507/user/login 页面查看用户信息手动进入http://localhost:9506/user/home 查看相同用户信息，User实体位置在两个服务中保持一致。如果项目中有es配置，需要es优先配置Netty @PostConstruct public void init() { /*由于netty的冲突，需要在ElasticConfig中显示指定早于RedisConfig装配，并且指定初始化时再一次添加忽略es中netty的一些配置*/ System.setProperty(&quot;es.set.netty.runtime.available.processors&quot;, &quot;false&quot;); bulkRequest = elasticClientSingleton.getBulkRequest(esConfig); }]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>session</tag>
        <tag>redis</tag>
        <tag>freemarker</tag>
        <tag>spring cloud alibaba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Alibaba服务间调用的多种方式]]></title>
    <url>%2F2019%2F08%2F21%2FAlibaba%E6%9C%8D%E5%8A%A1%E9%97%B4%E8%B0%83%E7%94%A8%E7%9A%84%E5%A4%9A%E7%A7%8D%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[基于spring cloud alibaba实战整合开发 @[TOC] ht-micro-record-service-dubbopom.xml &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;modules&gt; &lt;module&gt;ht-micro-record-service-dubbo-provider&lt;/module&gt; &lt;module&gt;ht-micro-record-service-dubbo-consumer&lt;/module&gt; &lt;/modules&gt; &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo&lt;/artifactId&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;name&gt;ht-micro-record-service-dubbo&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; ht-micro-record-service-dubbo-apipom.xml &lt;parent&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo&lt;/artifactId&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo-api&lt;/artifactId&gt; PortApi.java public interface PortApi { String showPort(); } ht-micro-record-service-dubbo-providerpom.xml &lt;parent&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo&lt;/artifactId&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo-provider&lt;/artifactId&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo-api&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-dubbo&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--elasticsearch--&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.java.dev.jna&lt;/groupId&gt; &lt;artifactId&gt;jna&lt;/artifactId&gt; &lt;!-- &lt;version&gt;3.0.9&lt;/version&gt; --&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt; &lt;artifactId&gt;gson&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;com.ht.micro.record.service.dubbo.provider.DubboProviderApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; logback.xml &lt;configuration&gt; &lt;!-- 控制台输出 --&gt; &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;layout class=&quot;ch.qos.logback.classic.PatternLayout&quot;&gt; &lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符 --&gt; &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n&lt;/pattern&gt; &lt;/layout&gt; &lt;/appender&gt; &lt;!--定义日志文件的存储地址 勿在 LogBack 的配置中使用相对路径 --&gt; &lt;property name=&quot;LOG_HOME&quot; value=&quot;E:/log&quot; /&gt; &lt;!-- 按照每天生成日志文件 --&gt; &lt;appender name=&quot;FILE&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;!--日志文件输出的文件名 --&gt; &lt;FileNamePattern&gt;${LOG_HOME}/log.%d{yyyy-MM-dd}.log&lt;/FileNamePattern&gt; &lt;MaxHistory&gt;30&lt;/MaxHistory&gt; &lt;/rollingPolicy&gt; &lt;layout class=&quot;ch.qos.logback.classic.PatternLayout&quot;&gt; &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n&lt;/pattern&gt; &lt;/layout&gt; &lt;!--日志文件最大的大小 --&gt; &lt;triggeringPolicy class=&quot;ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy&quot;&gt; &lt;MaxFileSize&gt;10MB&lt;/MaxFileSize&gt; &lt;/triggeringPolicy&gt; &lt;/appender&gt; &lt;!-- 日志输出级别 --&gt; &lt;root level=&quot;INFO&quot;&gt; &lt;appender-ref ref=&quot;STDOUT&quot; /&gt; &lt;appender-ref ref=&quot;FILE&quot; /&gt; &lt;/root&gt; &lt;!--mybatis 日志配置--&gt; &lt;logger name=&quot;com.mapper&quot; level=&quot;DEBUG&quot; /&gt; &lt;/configuration&gt; bootstrap.yml spring: application: name: ht-micro-record-service-dubbo-provider cloud: nacos: config: file-extension: yaml server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 main: allow-bean-definition-overriding: true dubbo: scan: base-packages: com.ht.micro.record.service.dubbo.provider.dubbo protocol: name: dubbo port: -1 registry: address: spring-cloud://192.168.2.7:8848?backup=192.168.2.7:8849,192.168.2.7:8850 es: nodes: 192.168.2.5:1800,192.168.2.7:1800 host: 192.168.2.5,192.168.2.7 port: 1801,1801 types: doc clusterName: elasticsearch-cluster #笔录分析库 blDbName: t_record_analyze #接警信息库 jjDbName: p_answer_alarm #处警信息库 cjDbName: p_handle_alarm #警情分析结果信息库 jqfxDbName: t_alarm_analysis_result #标准化分析 cjjxqDbName: t_cjjxq #接处警整合库 jcjDbName: p_answer_handle_alarm #警情分析 daDbName: t_data_analysis #警情结果表(neo4j使用) neo4jData: t_neo4j_data #市局接处警表 cityDbName: city_answer_handle_alarm nacos配置ht-micro-record-service-dubbo-provider.yaml spring: application: name: ht-micro-record-service-dubbo-provider cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8719 dashboard: 192.168.2.7:190 server: port: 9502com.ht.micro.record.service.dubbo.provider.DubboProviderApplication @SpringBootApplication @EnableDiscoveryClient public class DubboProviderApplication { public static void main(String[] args) { SpringApplication.run(DubboProviderApplication.class, args); } } com.ht.micro.record.service.dubbo.provider.utils.ElasticClient @Service public class ElasticClient { @Autowired private EsConfig esConfig; private static final Logger logger = LoggerFactory.getLogger(ElasticClient.class); private static BulkRequestBuilder bulkRequest; @Autowired private ElasticClientSingleton elasticClientSingleton; @PostConstruct public void init() { bulkRequest = elasticClientSingleton.getBulkRequest(esConfig); } /** * @param url * @param query * @return * @Description 发送请求 * @Author 裴健(peij@htdatacloud.com) * @Date 2016年6月13日 * @History * @his1 */ public static String postRequest(String url, String query) { RestTemplate restTemplate = new RestTemplate(); MediaType type = MediaType.parseMediaType(&quot;application/json; charset=UTF-8&quot;); HttpHeaders headers = new HttpHeaders(); headers.setContentType(type); headers.add(&quot;Accept&quot;, MediaType.APPLICATION_JSON.toString()); HttpEntity&lt;String&gt; formEntity = new HttpEntity&lt;String&gt;(query, headers); String result = restTemplate.postForObject(url, formEntity, String.class); return result; } /** * action 提交操作 */ // public void action() { // int reqSize = bulkRequest.numberOfActions(); // //读不到数据了，默认已经全部读取 // if (reqSize == 0) { // bulkRequest.request().requests().clear(); // } // bulkRequest.setTimeout(new TimeValue(1000 * 60 * 5)); //超时30秒 // BulkResponse bulkResponse = bulkRequest.execute().actionGet(); // //持久化异常 // if (bulkResponse.hasFailures()) { // logger.error(bulkResponse.buildFailureMessage()); // bulkRequest.request().requests().clear(); // } // logger.info(&quot;import over....&quot; + bulkResponse.getItems().length); // } }com.ht.micro.record.service.dubbo.provider.utils.ElasticClientSingleton @Service public class ElasticClientSingleton { protected final Logger logger = LoggerFactory.getLogger(ElasticClientSingleton.class); private AtomicInteger atomicPass = new AtomicInteger(); // 0 未初始化, 1 已初始化 private TransportClient transportClient; private BulkRequestBuilder bulkRequest; public synchronized void init(EsConfig esConfig) { try { String ipArray = esConfig.getHost(); String portArray = esConfig.getPort(); String cluster = esConfig.getClusterName(); Settings settings = Settings.builder() .put(&quot;cluster.name&quot;, cluster) //连接的集群名 .put(&quot;client.transport.ignore_cluster_name&quot;, true) .put(&quot;client.transport.sniff&quot;, false)//如果集群名不对，也能连接 .build(); transportClient = new PreBuiltTransportClient(settings); String[] ips = ipArray.split(&quot;,&quot;); String[] ports = portArray.split(&quot;,&quot;); for (int i = 0; i &lt; ips.length; i++) { transportClient.addTransportAddress(new TransportAddress(InetAddress.getByName(ips[i]), Integer .parseInt(ports[i]))); } atomicPass.set(1); } catch (Exception e) { e.printStackTrace(); logger.error(e.getMessage()); atomicPass.set(0); destroy(); } } public void destroy() { if (transportClient != null) { transportClient.close(); transportClient = null; } } public BulkRequestBuilder getBulkRequest(EsConfig esConfig) { if (atomicPass.get() == 0) { // 初始化 init(esConfig); } bulkRequest = transportClient.prepareBulk(); return bulkRequest; } public TransportClient getTransportClient(EsConfig esConfig) { if (atomicPass.get() == 0) { // 初始化 init(esConfig); } return transportClient; } }com.ht.micro.record.service.dubbo.provider.utils.EsConfig @Service public class EsConfig { @Value(&quot;${es.nodes}&quot;) private String nodes; @Value(&quot;${es.host}&quot;) private String host; @Value(&quot;${es.port}&quot;) private String port; @Value(&quot;${es.blDbName}&quot;) private String blDbName; @Value(&quot;${es.jjDbName}&quot;) private String jjDbName; @Value(&quot;${es.cjDbName}&quot;) private String cjDbName; @Value(&quot;${es.jqfxDbName}&quot;) private String jqfxDbName; @Value(&quot;${es.clusterName}&quot;) private String clusterName; @Value(&quot;${es.jjDbName}&quot;) private String answerDbName; @Value(&quot;${es.cjDbName}&quot;) private String handleDbName; @Value(&quot;${es.cjjxqDbName}&quot;) private String cjjxqDbName; @Value(&quot;${es.jcjDbName}&quot;) private String jcjDbName; @Value(&quot;${es.daDbName}&quot;) private String daDbName; @Value(&quot;${es.daDbName}&quot;) private String fxDbName; @Value(&quot;${es.types}&quot;) private String types; @Value(&quot;${es.neo4jData}&quot;) private String neo4jData; @Value(&quot;${es.cityDbName}&quot;) private String cityDbName; public String getCityDbName() { return cityDbName; } public String getTypes() { return types; } public String getFxDbName() { return fxDbName; } public String getDaDbName() { return daDbName; } public String getJcjDbName() { return jcjDbName; } public String getCjjxqDbName() { return cjjxqDbName; } public String getNodes() { return nodes; } public String getHost() { return host; } public String getPort() { return port; } public String getClusterName() { return clusterName; } public String getBlDbName() { return blDbName; } public String getJjDbName() { return jjDbName; } public String getCjDbName() { return cjDbName; } public String getJqfxDbName() { return jqfxDbName; } public String getNeo4jData() { return neo4jData; } } com.ht.micro.record.service.dubbo.provider.dubbo.PortApiImpl @Service public class PortApiImpl implements PortApi { @Value(&quot;${server.port}&quot;) private Integer port; @Override public String showPort() { return &quot;port= &quot;+ port; } }com.ht.micro.record.service.dubbo.provider.controller.ProviderController @RestController @RequestMapping(&quot;/provider&quot;) public class ProviderController { @Autowired private ConfigurableApplicationContext applicationContext; @Value(&quot;${server.port}&quot;) private Integer port; @GetMapping(&quot;/port&quot;) public Object port() { return &quot;port= &quot;+ port + &quot;, name=&quot; + applicationContext.getEnvironment().getProperty(&quot;user.name&quot;); } }ht-micro-record-service-dubbo-consumerpom.xml &lt;parent&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo&lt;/artifactId&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo-consumer&lt;/artifactId&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo-api&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-dubbo&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt; &lt;artifactId&gt;jsr305&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt; &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;com.ht.micro.record.service.dubbo.consumer.DubboConsumerApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;bootstrap.yml spring: application: name: ht-micro-record-service-dubbo-consumer main: allow-bean-definition-overriding: true cloud: nacos: config: file-extension: yaml server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 # 与nacos兼容性不好，配置到项目中 dubbo: scan: base-packages: com.ht.micro.record.service.dubbo.consumer protocol: name: dubbo port: -1 registry: address: spring-cloud://192.168.2.7:8848?backup=192.168.2.7:8849,192.168.2.7:8850com.ht.micro.record.service.dubbo.consumer.DubboConsumerApplication @SpringBootApplication(scanBasePackages = &quot;com.ht.micro.record&quot;, exclude = {DataSourceAutoConfiguration.class}) @EnableFeignClients @EnableDiscoveryClient public class DubboConsumerApplication { public static void main(String[] args) { SpringApplication.run(DubboConsumerApplication.class, args); } } com.ht.micro.record.service.dubbo.consumer.service.PortService @FeignClient(value = &quot;ht-micro-record-service-dubbo-provider&quot;) public interface PortService { @GetMapping(value = &quot;/provider/port&quot;) String showPort(); }com.ht.micro.record.service.dubbo.consumer.controller.ConsumerController @RestController @RequestMapping(&quot;/consumer&quot;) public class ConsumerController { @Autowired private LoadBalancerClient loadBalancerClient; @Autowired private PortService portService; private RestTemplate restTemplate = new RestTemplate(); @Reference(check = false) private PortApi portApi; @GetMapping(&quot;/rest&quot;) public Object rest() { ServiceInstance serviceInstance = loadBalancerClient.choose(&quot;ht-micro-record-service-dubbo-provider&quot;); String url = String.format(&quot;http://%s:%s/provider/port&quot;, serviceInstance.getHost(), serviceInstance.getPort()); System.out.println(&quot;request url:&quot; + url); return restTemplate.getForObject(url, String.class); } @GetMapping(&quot;/rpc&quot;) public Object rpc() { return portApi.showPort(); } @GetMapping(&quot;/feign&quot;) public Object feign(){ return portService.showPort(); } }]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>spring cloud alibaba</tag>
        <tag>nacos</tag>
        <tag>dubbo</tag>
        <tag>feign</tag>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Alibaba服务搭建]]></title>
    <url>%2F2019%2F08%2F21%2FAlibaba%E6%9C%8D%E5%8A%A1%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[基于spring cloud alibaba实战整合开发 基础服务通用类com.ht.micro.record.commonsdto.AbstractBaseDomain @Data public abstract class AbstractBaseDomain implements Serializable { /** * 该注解需要保留，用于 tk.mybatis 回显 ID */ @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; /** * 格式化日期，由于是北京时间（我们是在东八区），所以时区 +8 */ @JsonFormat(pattern = &quot;yyyy-MM-dd HH:mm:ss&quot;, timezone = &quot;GMT+8&quot;) private Date created; @JsonFormat(pattern = &quot;yyyy-MM-dd HH:mm:ss&quot;, timezone = &quot;GMT+8&quot;) private Date updated; } dto.AbstractBaseResult @Data public abstract class AbstractBaseResult implements Serializable { /** * 此为内部类,JsonInclude.Include.NON_NULL去掉返回值中为null的属性 */ @Data @JsonInclude(JsonInclude.Include.NON_NULL) protected static class Links { private String self; private String next; private String last; } @Data @JsonInclude(JsonInclude.Include.NON_NULL) protected static class DataBean&lt;T extends AbstractBaseDomain&gt; { private String type; private Long id; private T attributes; private T relationships; private Links links; } } dto.BaseResultFactory public class BaseResultFactory&lt;T extends AbstractBaseDomain&gt; { /** * 设置日志级别，用于限制发生错误时，是否显示调试信息(detail) * * @see ErrorResult#detail */ public static final String LOGGER_LEVEL_DEBUG = &quot;DEBUG&quot;; private static BaseResultFactory baseResultFactory; private BaseResultFactory() { } // 设置通用的响应 private static HttpServletResponse response; public static BaseResultFactory getInstance(HttpServletResponse response) { if (baseResultFactory == null) { synchronized (BaseResultFactory.class) { if (baseResultFactory == null) { baseResultFactory = new BaseResultFactory(); } } } BaseResultFactory.response = response; // 设置通用响应 baseResultFactory.initResponse(); return baseResultFactory; } public static BaseResultFactory getInstance() { if (baseResultFactory == null) { synchronized (BaseResultFactory.class) { if (baseResultFactory == null) { baseResultFactory = new BaseResultFactory(); } } } return baseResultFactory; } /** * 构建单笔数据结果集 * * @param self 当前请求路径 * @return */ public AbstractBaseResult build(String self, T attributes) { return new SuccessResult(self, attributes); } /** * 构建多笔数据结果集 * * @param self 当前请求路径 * @param next 下一页的页码 * @param last 最后一页的页码 * @return */ public AbstractBaseResult build(String self, int next, int last, List&lt;T&gt; attributes) { return new SuccessResult(self, next, last, attributes); } /** * 构建请求错误的响应结构，调试显示detail，上线不显示，通过配置日志级别 * * @param code HTTP 状态码 * @param title 错误信息 * @param detail 调试信息 * @param level 日志级别，只有 DEBUG 时才显示详情 * @return */ public AbstractBaseResult build(int code, String title, String detail, String level) { // 设置请求失败的响应码 response.setStatus(code); if (LOGGER_LEVEL_DEBUG.equals(level)) { return new ErrorResult(code, title, detail); } else { return new ErrorResult(code, title, null); } } /** * 初始化 HttpServletResponse */ private void initResponse() { // 需要符合 JSON API 规范 response.setHeader(&quot;Content-Type&quot;, &quot;application/vnd.api+json&quot;); } } dto.ErrorResult @Data @AllArgsConstructor @EqualsAndHashCode(callSuper = false) // JSON 不显示为 null 的属性 @JsonInclude(JsonInclude.Include.NON_NULL) public class ErrorResult extends AbstractBaseResult { private int code; private String title; /** * 调试信息 */ private String detail; } dto.SuccessResult @Data @EqualsAndHashCode(callSuper = false) public class SuccessResult&lt;T extends AbstractBaseDomain&gt; extends AbstractBaseResult { private Links links; private List&lt;DataBean&gt; data; /** * 请求的结果（单笔） * @param self 当前请求路径 * @param attributes 领域模型 */ public SuccessResult(String self, T attributes) { links = new Links(); links.setSelf(self); createDataBean(null, attributes); } /** * 请求的结果（分页） * @param self 当前请求路径 * @param next 下一页的页码 * @param last 最后一页的页码 * @param attributes 领域模型集合 */ public SuccessResult(String self, int next, int last, List&lt;T&gt; attributes) { links = new Links(); links.setSelf(self); links.setNext(self + &quot;?page=&quot; + next); links.setLast(self + &quot;?page=&quot; + last); attributes.forEach(attribute -&gt; createDataBean(self, attribute)); } /** * 创建 DataBean * @param self 当前请求路径 * @param attributes 领域模型 */ private void createDataBean(String self, T attributes) { if (data == null) { data = new ArrayList&lt;&gt;(); } DataBean dataBean = new DataBean(); dataBean.setId(attributes.getId()); dataBean.setType(attributes.getClass().getSimpleName()); dataBean.setAttributes(attributes); if (StringUtils.isNotBlank(self)) { Links links = new Links(); links.setSelf(self + &quot;/&quot; + attributes.getId()); dataBean.setLinks(links); } data.add(dataBean); } } utils.MapperUtils public class MapperUtils { private final static ObjectMapper objectMapper = new ObjectMapper(); public static ObjectMapper getInstance() { return objectMapper; } /** * 转换为 JSON 字符串 * * @param obj * @return * @throws Exception */ public static String obj2json(Object obj) throws Exception { return objectMapper.writeValueAsString(obj); } /** * 转换为 JSON 字符串，忽略空值 * * @param obj * @return * @throws Exception */ public static String obj2jsonIgnoreNull(Object obj) throws Exception { ObjectMapper mapper = new ObjectMapper(); mapper.setSerializationInclusion(JsonInclude.Include.NON_NULL); return mapper.writeValueAsString(obj); } /** * 转换为 JavaBean * * @param jsonString * @param clazz * @return * @throws Exception */ public static &lt;T&gt; T json2pojo(String jsonString, Class&lt;T&gt; clazz) throws Exception { objectMapper.configure(DeserializationFeature.ACCEPT_SINGLE_VALUE_AS_ARRAY, true); return objectMapper.readValue(jsonString, clazz); } /** * 字符串转换为 Map&lt;String, Object&gt; * * @param jsonString * @return * @throws Exception */ public static &lt;T&gt; Map&lt;String, Object&gt; json2map(String jsonString) throws Exception { ObjectMapper mapper = new ObjectMapper(); mapper.setSerializationInclusion(JsonInclude.Include.NON_NULL); return mapper.readValue(jsonString, Map.class); } /** * 字符串转换为 Map&lt;String, T&gt; */ public static &lt;T&gt; Map&lt;String, T&gt; json2map(String jsonString, Class&lt;T&gt; clazz) throws Exception { Map&lt;String, Map&lt;String, Object&gt;&gt; map = objectMapper.readValue(jsonString, new TypeReference&lt;Map&lt;String, T&gt;&gt;() { }); Map&lt;String, T&gt; result = new HashMap&lt;String, T&gt;(); for (Map.Entry&lt;String, Map&lt;String, Object&gt;&gt; entry : map.entrySet()) { result.put(entry.getKey(), map2pojo(entry.getValue(), clazz)); } return result; } /** * 深度转换 JSON 成 Map * * @param json * @return */ public static Map&lt;String, Object&gt; json2mapDeeply(String json) throws Exception { return json2MapRecursion(json, objectMapper); } /** * 把 JSON 解析成 List，如果 List 内部的元素存在 jsonString，继续解析 * * @param json * @param mapper 解析工具 * @return * @throws Exception */ private static List&lt;Object&gt; json2ListRecursion(String json, ObjectMapper mapper) throws Exception { if (json == null) { return null; } List&lt;Object&gt; list = mapper.readValue(json, List.class); for (Object obj : list) { if (obj != null &amp;&amp; obj instanceof String) { String str = (String) obj; if (str.startsWith(&quot;[&quot;)) { obj = json2ListRecursion(str, mapper); } else if (obj.toString().startsWith(&quot;{&quot;)) { obj = json2MapRecursion(str, mapper); } } } return list; } /** * 把 JSON 解析成 Map，如果 Map 内部的 Value 存在 jsonString，继续解析 * * @param json * @param mapper * @return * @throws Exception */ private static Map&lt;String, Object&gt; json2MapRecursion(String json, ObjectMapper mapper) throws Exception { if (json == null) { return null; } Map&lt;String, Object&gt; map = mapper.readValue(json, Map.class); for (Map.Entry&lt;String, Object&gt; entry : map.entrySet()) { Object obj = entry.getValue(); if (obj != null &amp;&amp; obj instanceof String) { String str = ((String) obj); if (str.startsWith(&quot;[&quot;)) { List&lt;?&gt; list = json2ListRecursion(str, mapper); map.put(entry.getKey(), list); } else if (str.startsWith(&quot;{&quot;)) { Map&lt;String, Object&gt; mapRecursion = json2MapRecursion(str, mapper); map.put(entry.getKey(), mapRecursion); } } } return map; } /** * 将 JSON 数组转换为集合 * * @param jsonArrayStr * @param clazz * @return * @throws Exception */ public static &lt;T&gt; List&lt;T&gt; json2list(String jsonArrayStr, Class&lt;T&gt; clazz) throws Exception { JavaType javaType = getCollectionType(ArrayList.class, clazz); List&lt;T&gt; list = (List&lt;T&gt;) objectMapper.readValue(jsonArrayStr, javaType); return list; } /** * 获取泛型的 Collection Type * * @param collectionClass 泛型的Collection * @param elementClasses 元素类 * @return JavaType Java类型 * @since 1.0 */ public static JavaType getCollectionType(Class&lt;?&gt; collectionClass, Class&lt;?&gt;... elementClasses) { return objectMapper.getTypeFactory().constructParametricType(collectionClass, elementClasses); } /** * 将 Map 转换为 JavaBean * * @param map * @param clazz * @return */ public static &lt;T&gt; T map2pojo(Map map, Class&lt;T&gt; clazz) { return objectMapper.convertValue(map, clazz); } /** * 将 Map 转换为 JSON * * @param map * @return */ public static String mapToJson(Map map) { try { return objectMapper.writeValueAsString(map); } catch (Exception e) { e.printStackTrace(); } return &quot;&quot;; } /** * 将 JSON 对象转换为 JavaBean * * @param obj * @param clazz * @return */ public static &lt;T&gt; T obj2pojo(Object obj, Class&lt;T&gt; clazz) { return objectMapper.convertValue(obj, clazz); } } utils.RegexpUtils public class RegexpUtils { /** * 验证手机号 */ public static final String PHONE = &quot;^((13[0-9])|(15[^4,\\D])|(18[0,5-9]))\\d{8}$&quot;; /** * 验证邮箱地址 */ public static final String EMAIL = &quot;\\w+(\\.\\w)*@\\w+(\\.\\w{2,3}){1,3}&quot;; /** * 验证手机号 * @param phone * @return */ public static boolean checkPhone(String phone) { return phone.matches(PHONE); } /** * 验证邮箱 * @param email * @return */ public static boolean checkEmail(String email) { return email.matches(EMAIL); } } web.AbstractBaseController public abstract class AbstractBaseController&lt;T extends AbstractBaseDomain&gt; { // 用于动态获取配置文件的属性值 private static final String ENVIRONMENT_LOGGING_LEVEL_MY_SHOP = &quot;logging.level.com.ht.micro.record&quot;; @Resource protected HttpServletRequest request; @Resource protected HttpServletResponse response; @Autowired private ConfigurableApplicationContext applicationContext; @ModelAttribute public void initReqAndRes(HttpServletRequest request, HttpServletResponse response) { this.request = request; this.response = response; } /** * 请求成功 * @param self * @param attribute * @return */ protected AbstractBaseResult success(String self, T attribute) { return BaseResultFactory.getInstance(response).build(self, attribute); } /** * 请求成功 * @param self * @param next * @param last * @param attributes * @return */ protected AbstractBaseResult success(String self, int next, int last, List&lt;T&gt; attributes) { return BaseResultFactory.getInstance(response).build(self, next, last, attributes); } /** * 请求失败 * @param title * @param detail * @return */ protected AbstractBaseResult error(String title, String detail) { return error(HttpStatus.UNAUTHORIZED.value(), title, detail); } /** * 请求失败 * @param code * @param title * @param detail * @return */ protected AbstractBaseResult error(int code, String title, String detail) { return BaseResultFactory.getInstance(response).build(code, title, detail, applicationContext.getEnvironment().getProperty(ENVIRONMENT_LOGGING_LEVEL_MY_SHOP)); } } ht-micro-record-commons-domaindomain.TbUser @Table(name = &quot;tb_user&quot;) @JsonInclude(JsonInclude.Include.NON_NULL) public class TbUser extends AbstractBaseDomain { /** * 用户名 */ @NotNull(message = &quot;用户名不可为空&quot;) @Length(min = 5, max = 20, message = &quot;用户名长度必须介于 5 和 20 之间&quot;) private String username; /** * 密码，加密存储 */ @JsonIgnore private String password; /** * 注册手机号 */ private String phone; /** * 注册邮箱 */ @NotNull(message = &quot;邮箱不可为空&quot;) @Pattern(regexp = RegexpUtils.EMAIL, message = &quot;邮箱格式不正确&quot;) private String email; /** * 获取用户名 * * @return username - 用户名 */ public String getUsername() { return username; } /** * 设置用户名 * * @param username 用户名 */ public void setUsername(String username) { this.username = username; } /** * 获取密码，加密存储 * * @return password - 密码，加密存储 */ public String getPassword() { return password; } /** * 设置密码，加密存储 * * @param password 密码，加密存储 */ public void setPassword(String password) { this.password = password; } /** * 获取注册手机号 * * @return phone - 注册手机号 */ public String getPhone() { return phone; } /** * 设置注册手机号 * * @param phone 注册手机号 */ public void setPhone(String phone) { this.phone = phone; } /** * 获取注册邮箱 * * @return email - 注册邮箱 */ public String getEmail() { return email; } /** * 设置注册邮箱 * * @param email 注册邮箱 */ public void setEmail(String email) { this.email = email; } } validator.BeanValidator @Component public class BeanValidator { @Autowired private Validator validatorInstance; private static Validator validator; // 系统启动时将静态对象注入容器，不可用Autowire直接注入 @PostConstruct public void init() { BeanValidator.validator = validatorInstance; } /** * 调用 JSR303 的 validate 方法, 验证失败时抛出 ConstraintViolationException. */ private static void validateWithException(Validator validator, Object object, Class&lt;?&gt;... groups) throws ConstraintViolationException { Set constraintViolations = validator.validate(object, groups); if (!constraintViolations.isEmpty()) { throw new ConstraintViolationException(constraintViolations); } } /** * 辅助方法, 转换 ConstraintViolationException 中的 Set&lt;ConstraintViolations&gt; 中为 List&lt;message&gt;. */ private static List&lt;String&gt; extractMessage(ConstraintViolationException e) { return extractMessage(e.getConstraintViolations()); } /** * 辅助方法, 转换 Set&lt;ConstraintViolation&gt; 为 List&lt;message&gt; */ private static List&lt;String&gt; extractMessage(Set&lt;? extends ConstraintViolation&gt; constraintViolations) { List&lt;String&gt; errorMessages = new ArrayList&lt;&gt;(); for (ConstraintViolation violation : constraintViolations) { errorMessages.add(violation.getMessage()); } return errorMessages; } /** * 辅助方法, 转换 ConstraintViolationException 中的 Set&lt;ConstraintViolations&gt; 为 Map&lt;property, message&gt;. */ private static Map&lt;String, String&gt; extractPropertyAndMessage(ConstraintViolationException e) { return extractPropertyAndMessage(e.getConstraintViolations()); } /** * 辅助方法, 转换 Set&lt;ConstraintViolation&gt; 为 Map&lt;property, message&gt;. */ private static Map&lt;String, String&gt; extractPropertyAndMessage(Set&lt;? extends ConstraintViolation&gt; constraintViolations) { Map&lt;String, String&gt; errorMessages = new HashMap&lt;&gt;(); for (ConstraintViolation violation : constraintViolations) { errorMessages.put(violation.getPropertyPath().toString(), violation.getMessage()); } return errorMessages; } /** * 辅助方法, 转换 ConstraintViolationException 中的 Set&lt;ConstraintViolations&gt; 为 List&lt;propertyPath message&gt;. */ private static List&lt;String&gt; extractPropertyAndMessageAsList(ConstraintViolationException e) { return extractPropertyAndMessageAsList(e.getConstraintViolations(), &quot; &quot;); } /** * 辅助方法, 转换 Set&lt;ConstraintViolations&gt; 为 List&lt;propertyPath message&gt;. */ private static List&lt;String&gt; extractPropertyAndMessageAsList(Set&lt;? extends ConstraintViolation&gt; constraintViolations) { return extractPropertyAndMessageAsList(constraintViolations, &quot; &quot;); } /** * 辅助方法, 转换 ConstraintViolationException 中的 Set&lt;ConstraintViolations&gt; 为 List&lt;propertyPath + separator + message&gt;. */ private static List&lt;String&gt; extractPropertyAndMessageAsList(ConstraintViolationException e, String separator) { return extractPropertyAndMessageAsList(e.getConstraintViolations(), separator); } /** * 辅助方法, 转换 Set&lt;ConstraintViolation&gt; 为 List&lt;propertyPath + separator + message&gt;. */ private static List&lt;String&gt; extractPropertyAndMessageAsList(Set&lt;? extends ConstraintViolation&gt; constraintViolations, String separator) { List&lt;String&gt; errorMessages = new ArrayList&lt;&gt;(); for (ConstraintViolation violation : constraintViolations) { errorMessages.add(violation.getPropertyPath() + separator + violation.getMessage()); } return errorMessages; } /** * 服务端参数有效性验证 * * @param object 验证的实体对象 * @param groups 验证组 * @return 验证成功：返回 null；验证失败：返回错误信息 */ public static String validator(Object object, Class&lt;?&gt;... groups) { try { validateWithException(validator, object, groups); } catch (ConstraintViolationException ex) { List&lt;String&gt; list = extractMessage(ex); list.add(0, &quot;数据验证失败：&quot;); // 封装错误消息为字符串 StringBuilder sb = new StringBuilder(); for (int i = 0; i &lt; list.size(); i++) { String exMsg = list.get(i); if (i != 0) { sb.append(String.format(&quot;%s. %s&quot;, i, exMsg)).append(list.size() &gt; 1 ? &quot;&lt;br/&gt;&quot; : &quot;&quot;); } else { sb.append(exMsg).append(list.size() &gt; 1 ? &quot;&lt;br/&gt;&quot; : &quot;&quot;); } } return sb.toString(); } return null; } } ht-micro-record-commons-mapper├─src│ └─main│ ├─java│ │ ├─com│ │ │ └─ht│ │ │ └─micro│ │ │ └─record│ │ │ └─commons│ │ │ └─mapper│ │ │ ├─baseMapper│ │ │ └─dicMapper│ │ └─tk│ │ └─mybatis│ │ └─mapper│ └─resources│ ├─baseMapper│ └─dicMappermapper分开存储用来识别多数据源tk.mybatis.mapper.MyMapper public interface MyMapper&lt;T&gt; extends Mapper&lt;T&gt;, MySqlMapper&lt;T&gt; { } ht-micro-record-commons-serviceBaseCrudService public interface BaseCrudService&lt;T extends AbstractBaseDomain&gt; { /** * 查询属性值是否唯一 * * @param property * @param value * @return true/唯一，false/不唯一 */ default boolean unique(String property, String value) { return false; } /** * 保存 * * @param domain * @return */ default T save(T domain) { return null; } /** * 分页查询 * @param domain * @param pageNum * @param pageSize * @return */ default PageInfo&lt;T&gt; page(T domain, int pageNum, int pageSize) { return null; } } impl.BaseCrudServiceImpl public class BaseCrudServiceImpl&lt;T extends AbstractBaseDomain, M extends MyMapper&lt;T&gt;&gt; implements BaseCrudService&lt;T&gt; { @Autowired protected M mapper; private Class&lt;T&gt; entityClass = (Class&lt;T&gt;) ((ParameterizedType) getClass().getGenericSuperclass()).getActualTypeArguments()[0]; @Override public boolean unique(String property, String value) { Example example = new Example(entityClass); example.createCriteria().andEqualTo(property, value); int result = mapper.selectCountByExample(example); if (result &gt; 0) { return false; } return true; } @Override public T save(T domain) { int result = 0; Date currentDate = new Date(); // 创建 if (domain.getId() == null) { /** * 用于自动回显 ID，领域模型中需要 @ID 注解的支持 * {@link AbstractBaseDomain} */ result = mapper.insertUseGeneratedKeys(domain); } // 更新 else { result = mapper.updateByPrimaryKey(domain); } // 保存数据成功 if (result &gt; 0) { return domain; } // 保存数据失败 return null; } @Override public PageInfo&lt;T&gt; page(T domain, int pageNum, int pageSize) { Example example = new Example(entityClass); example.createCriteria().andEqualTo(domain); PageHelper.startPage(pageNum, pageSize); PageInfo&lt;T&gt; pageInfo = new PageInfo&lt;&gt;(mapper.selectByExample(example)); return pageInfo; } } TbUserService public interface TbUserService extends BaseCrudService&lt;TbUser&gt; { TbUser getById(long id); } impl.TbUserServiceImpl @Service public class TbUserServiceImpl extends BaseCrudServiceImpl&lt;TbUser, TbUserMapper&gt; implements TbUserService { @Autowired private TbUserMapper tbUserMapper; public TbUser getById(long id){ return tbUserMapper.selectByPrimaryKey(id); } } ht-micro-record-service-user &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath/&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-service-user&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-service-user&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt; &lt;artifactId&gt;jsr305&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt; &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt; &lt;artifactId&gt;jsr305&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt; &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rocketmq&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;!-- Projects Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-commons-service&lt;/artifactId&gt; &lt;version&gt;${project.parent.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Projects End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;!-- docker的maven插件，官网 https://github.com/spotify/docker-maven-plugin --&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt; &lt;version&gt;0.4.13&lt;/version&gt; &lt;configuration&gt; &lt;imageName&gt;192.168.2.7:5000/${project.artifactId}:${project.version}&lt;/imageName&gt; &lt;baseImage&gt;onejane-jdk1.8 &lt;/baseImage&gt; &lt;entryPoint&gt;[&quot;java&quot;, &quot;-jar&quot;,&quot;/${project.build.finalName}.jar&quot;]&lt;/entryPoint&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/&lt;/targetPath&gt; &lt;directory&gt;${project.build.directory}&lt;/directory&gt; &lt;include&gt;${project.build.finalName}.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;dockerHost&gt;http://192.168.2.7:2375&lt;/dockerHost&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;com.ht.micro.record.UserServiceApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; application.yml配置多数据源及RocketMQ spring: cloud: stream: rocketmq: binder: name-server: 192.168.2.7:9876 bindings: output: content-type: application/json destination: topic-email producer: group: group-email datasource: base: #监控统计拦截的filters filters: stat type: com.alibaba.druid.pool.DruidDataSource jdbc-url: jdbc:mysql://192.168.2.7:185/ht_micro_record?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false username: root password: 123456 driver-class-name: com.mysql.jdbc.Driver max-idle: 10 max-wait: 10000 min-idle: 5 initial-size: 5 validation-query: SELECT 1 test-on-borrow: false test-while-idle: true time-between-eviction-runs-millis: 18800 dic: #监控统计拦截的filters filters: stat type: com.alibaba.druid.pool.DruidDataSource jdbc-url: jdbc:mysql://192.168.1.48:3306/ht_nlp?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false username: root password: Sonar@1234 driver-class-name: com.mysql.jdbc.Driver max-idle: 10 max-wait: 10000 min-idle: 5 initial-size: 5 validation-query: SELECT 1 test-on-borrow: false test-while-idle: true time-between-eviction-runs-millis: 18800 bootstrap.properties spring.application.name=ht-micro-record-service-user-config spring.cloud.nacos.config.file-extension=yaml spring.cloud.nacos.config.server-addr=192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 logging.level.com.ht.micro.record=DEBUG nacos配置 spring: application: name: ht-micro-record-service-user cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8719 dashboard: 192.168.2.7:190 server: port: 9506 management: endpoints: web: exposure: include: &quot;*&quot; com.ht.micro.record.UserServiceApplication @SpringBootApplication(scanBasePackages = &quot;com.ht.micro.record&quot;,exclude = {DataSourceAutoConfiguration.class, DataSourceTransactionManagerAutoConfiguration.class, MybatisAutoConfiguration.class}) @EnableDiscoveryClient @EnableBinding({Source.class}) @MapperScan(basePackages = &quot;com.ht.micro.record.commons.mapper&quot;) @EnableAsync @EnableSwagger2 public class UserServiceApplication { public static void main(String[] args) { SpringApplication.run(UserServiceApplication.class, args); } } com.ht.micro.record.user.config.BaseMybatisConfig @Configuration @MapperScan(basePackages = {&quot;com.ht.micro.record.commons.mapper.baseMapper&quot;}, sqlSessionTemplateRef = &quot;baseSqlSessionTemplate&quot;) public class BaseMybatisConfig { @Value(&quot;${spring.datasource.base.filters}&quot;) String filters; @Value(&quot;${spring.datasource.base.driver-class-name}&quot;) String driverClassName; @Value(&quot;${spring.datasource.base.username}&quot;) String username; @Value(&quot;${spring.datasource.base.password}&quot;) String password; @Value(&quot;${spring.datasource.base.jdbc-url}&quot;) String url; @Bean(name=&quot;baseDataSource&quot;) @Primary//必须加此注解，不然报错，下一个类则不需要添加 spring.datasource @ConfigurationProperties(prefix=&quot;spring.datasource.base&quot;)//prefix值必须是application.properteis中对应属性的前缀 public DataSource baseDataSource() throws SQLException { DruidDataSource druid = new DruidDataSource(); // 监控统计拦截的filters druid.setFilters(filters); // 配置基本属性 druid.setDriverClassName(driverClassName); druid.setUsername(username); druid.setPassword(password); druid.setUrl(url); return druid; } @Bean(name=&quot;baseSqlSessionFactory&quot;) @Primary public SqlSessionFactory baseSqlSessionFactory(@Qualifier(&quot;baseDataSource&quot;)DataSource dataSource)throws Exception{ // 创建Mybatis的连接会话工厂实例 SqlSessionFactoryBean bean=new SqlSessionFactoryBean(); bean.setDataSource(dataSource);//// 设置数据源bean //添加XML目录 ResourcePatternResolver resolver=new PathMatchingResourcePatternResolver(); try{ bean.setMapperLocations(resolver.getResources(&quot;classpath:baseMapper/*Mapper.xml&quot;));//// 设置mapper文件路径 return bean.getObject(); }catch(Exception e){ e.printStackTrace(); throw new RuntimeException(e); } } @Bean(name=&quot;baseSqlSessionTemplate&quot;) @Primary public SqlSessionTemplate baseSqlSessionTemplate(@Qualifier(&quot;baseSqlSessionFactory&quot;)SqlSessionFactory sqlSessionFactory)throws Exception{ SqlSessionTemplate template=new SqlSessionTemplate(sqlSessionFactory);//使用上面配置的Factory return template; } } com.ht.micro.record.user.config.DicMybatisConfig @Configuration @MapperScan(basePackages = {&quot;com.ht.micro.record.commons.mapper.dicMapper&quot;}, sqlSessionTemplateRef = &quot;dicSqlSessionTemplate&quot;) public class DicMybatisConfig { @Value(&quot;${spring.datasource.dic.filters}&quot;) String filters; @Value(&quot;${spring.datasource.dic.driver-class-name}&quot;) String driverClassName; @Value(&quot;${spring.datasource.dic.username}&quot;) String username; @Value(&quot;${spring.datasource.dic.password}&quot;) String password; @Value(&quot;${spring.datasource.dic.jdbc-url}&quot;) String url; @Bean(name = &quot;dicDataSource&quot;) @ConfigurationProperties(prefix=&quot;spring.datasource.dic&quot;) public DataSource dicDataSource() throws SQLException { DruidDataSource druid = new DruidDataSource(); // 监控统计拦截的filters // druid.setFilters(filters); // 配置基本属性 druid.setDriverClassName(driverClassName); druid.setUsername(username); druid.setPassword(password); druid.setUrl(url); /* //初始化时建立物理连接的个数 druid.setInitialSize(initialSize); //最大连接池数量 druid.setMaxActive(maxActive); //最小连接池数量 druid.setMinIdle(minIdle); //获取连接时最大等待时间，单位毫秒。 druid.setMaxWait(maxWait); //间隔多久进行一次检测，检测需要关闭的空闲连接 druid.setTimeBetweenEvictionRunsMillis(timeBetweenEvictionRunsMillis); //一个连接在池中最小生存的时间 druid.setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis); //用来检测连接是否有效的sql druid.setValidationQuery(validationQuery); //建议配置为true，不影响性能，并且保证安全性。 druid.setTestWhileIdle(testWhileIdle); //申请连接时执行validationQuery检测连接是否有效 druid.setTestOnBorrow(testOnBorrow); druid.setTestOnReturn(testOnReturn); //是否缓存preparedStatement，也就是PSCache，oracle设为true，mysql设为false。分库分表较多推荐设置为false druid.setPoolPreparedStatements(poolPreparedStatements); // 打开PSCache时，指定每个连接上PSCache的大小 druid.setMaxPoolPreparedStatementPerConnectionSize(maxPoolPreparedStatementPerConnectionSize); */ return druid; } @Bean(name = &quot;dicSqlSessionFactory&quot;) public SqlSessionFactory dicSqlSessionFactory(@Qualifier(&quot;dicDataSource&quot;)DataSource dataSource)throws Exception{ SqlSessionFactoryBean bean=new SqlSessionFactoryBean(); bean.setDataSource(dataSource); //添加XML目录 ResourcePatternResolver resolver=new PathMatchingResourcePatternResolver(); try{ bean.setMapperLocations(resolver.getResources(&quot;classpath:dicMapper/*Mapper.xml&quot;)); return bean.getObject(); }catch(Exception e){ e.printStackTrace(); throw new RuntimeException(e); } } @Bean(name=&quot;dicSqlSessionTemplate&quot;) public SqlSessionTemplate dicSqlSessionTemplate(@Qualifier(&quot;dicSqlSessionFactory&quot;)SqlSessionFactory sqlSessionFactory)throws Exception{ SqlSessionTemplate template=new SqlSessionTemplate(sqlSessionFactory);//使用上面配置的Factory return template; } } com.ht.micro.record.user.service.UserService @Service public class UserService { @Autowired private MessageChannel output; // @EnableAsync在Application中开启异步 @Async public void sendEmail(TbUser tbUser) throws Exception { output.send(MessageBuilder.withPayload(MapperUtils.obj2json(tbUser)).build()); } } com.ht.micro.record.user.controller.UserController @RestController @RequestMapping(value = &quot;user&quot;) public class UserController extends AbstractBaseController&lt;TbUser&gt; { @Autowired private TbUserService tbUserService; @Autowired private UserService userService; // http://localhost:9506/user/2 // @ApiOperation(value = &quot;查询用户&quot;, notes = &quot;根据id获取用户名&quot;) @GetMapping(value = {&quot;{id}&quot;}) public String getName(@PathVariable long id){ return tbUserService.getById(id).getUsername(); } @ApiOperation(value = &quot;用户注册&quot;, notes = &quot;参数为实体类，注意用户名和邮箱不要重复&quot;) @PostMapping(value = &quot;reg&quot;) public AbstractBaseResult reg(@ApiParam(name = &quot;tbUser&quot;, value = &quot;用户模型&quot;) TbUser tbUser) { // 数据校验 String message = BeanValidator.validator(tbUser); if (StringUtils.isNotBlank(message)) { return error(message, null); } // 验证密码是否为空 if (StringUtils.isBlank(tbUser.getPassword())) { return error(&quot;密码不可为空&quot;, null); } // 验证用户名是否重复 if (!tbUserService.unique(&quot;username&quot;, tbUser.getUsername())) { return error(&quot;用户名已存在&quot;, null); } // 验证邮箱是否重复 if (!tbUserService.unique(&quot;email&quot;, tbUser.getEmail())) { return error(&quot;邮箱重复，请重试&quot;, null); } // 注册用户 try { tbUser.setPassword(DigestUtils.md5DigestAsHex(tbUser.getPassword().getBytes())); TbUser user = tbUserService.save(tbUser); if (user != null) { userService.sendEmail(user); response.setStatus(HttpStatus.CREATED.value()); return success(request.getRequestURI(), user); } } catch (Exception e) { // 这里补一句，将 RegService 中的异常抛到 Controller 中，这样可以打印出调试信息 return error(HttpStatus.INTERNAL_SERVER_ERROR.value(), &quot;注册邮件发送失败&quot;, e.getMessage()); } // 注册失败 return error(&quot;注册失败，请重试&quot;, null); } } ht-micro-record-service-sms &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-service-sms&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-service-sms&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-mail&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.sourceforge.nekohtml&lt;/groupId&gt; &lt;artifactId&gt;nekohtml&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt; &lt;artifactId&gt;jsr305&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt; &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rocketmq&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;!-- Projects Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-commons-domain&lt;/artifactId&gt; &lt;version&gt;${project.parent.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Projects End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;ht-micro-record-service-sms&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;com.ht.micro.record.service.email.SmsServiceApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- docker的maven插件，官网 https://github.com/spotify/docker-maven-plugin --&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt; &lt;version&gt;0.4.13&lt;/version&gt; &lt;configuration&gt; &lt;imageName&gt;192.168.2.7:5000/${project.artifactId}:${project.version}&lt;/imageName&gt; &lt;baseImage&gt;jdk1.8&lt;/baseImage&gt; &lt;entryPoint&gt;[&quot;java&quot;, &quot;-jar&quot;,&quot;/${project.build.finalName}.jar&quot;]&lt;/entryPoint&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/&lt;/targetPath&gt; &lt;directory&gt;${project.build.directory}&lt;/directory&gt; &lt;include&gt;${project.build.finalName}.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;dockerHost&gt;http://192.168.2.7:2375&lt;/dockerHost&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; application.yml spring: cloud: stream: rocketmq: binder: name-server: 192.168.2.7:9876 bindings: input: consumer: orderly: true bindings: input: destination: topic-email content-type: application/json group: group-email consumer: maxAttempts: 1 thymeleaf: cache: false mode: HTML encoding: UTF-8 servlet: content-type: text/html bootstrap.properties spring.application.name=ht-micro-record-service-sms-config spring.cloud.nacos.config.file-extension=yaml spring.cloud.nacos.config.server-addr=192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 nacos配置 spring: application: name: ht-micro-record-service-sms mail: host: smtp.163.com port: 25 # 你的邮箱授权码 password: codewj123456 properties: mail: smtp: auth: true starttls: enable: true required: true # 发送邮件的邮箱地址 username: m15806204096@163.com cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8719 dashboard: 192.168.2.7:190 server: port: 9507 management: endpoints: web: exposure: include: &quot;*&quot; com.ht.micro.record.service.email.SmsServiceApplication @SpringBootApplication(scanBasePackages = &quot;com.ht.micro.record&quot;) @EnableDiscoveryClient @EnableBinding({Sink.class}) @EnableAsync public class SmsServiceApplication { public static void main(String[] args) { SpringApplication.run(SmsServiceApplication.class, args); } } com.ht.micro.record.service.email.service.EmailService @Service public class EmailService { @Autowired private ConfigurableApplicationContext applicationContext; @Autowired private JavaMailSender javaMailSender; @Autowired private TemplateEngine templateEngine; @StreamListener(&quot;input&quot;) public void receive(String json) { try { // 发送普通邮件 TbUser tbUser = MapperUtils.json2pojo(json, TbUser.class); sendEmail(&quot;欢迎注册&quot;, &quot;欢迎 &quot; + tbUser.getUsername() + &quot; 加入华通晟云！&quot;, tbUser.getEmail()); // 发送 HTML 模板邮件 Context context = new Context(); context.setVariable(&quot;username&quot;, tbUser.getUsername()); String emailTemplate = templateEngine.process(&quot;reg&quot;, context); sendTemplateEmail(&quot;欢迎注册&quot;, emailTemplate, tbUser.getEmail()); } catch (Exception e) { e.printStackTrace(); } } /** * 发送普通邮件 * @param subject * @param body * @param to */ @Async public void sendEmail(String subject, String body, String to) { SimpleMailMessage message = new SimpleMailMessage(); message.setFrom(applicationContext.getEnvironment().getProperty(&quot;spring.mail.username&quot;)); message.setTo(to); message.setSubject(subject); message.setText(body); javaMailSender.send(message); } /** * 发送 HTML 模板邮件 * @param subject * @param body * @param to */ @Async public void sendTemplateEmail(String subject, String body, String to) { MimeMessage message = javaMailSender.createMimeMessage(); try { MimeMessageHelper helper = new MimeMessageHelper(message, true); helper.setFrom(applicationContext.getEnvironment().getProperty(&quot;spring.mail.username&quot;)); helper.setTo(to); helper.setSubject(subject); helper.setText(body, true); javaMailSender.send(message); } catch (Exception e) { } } } templates/reg.html &lt;!DOCTYPE html SYSTEM &quot;http://www.thymeleaf.org/dtd/xhtml1-strict-thymeleaf-spring4-4.dtd&quot;&gt; &lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot; xmlns:th=&quot;http://www.thymeleaf.org&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot;&gt; &lt;title&gt;注册通知&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div&gt; 欢迎 &lt;span th:text=&quot;${username}&quot;&gt;&lt;/span&gt; 加入 华通晟云！！ &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; post http://localhost:9506/user/reg?password=123456&amp;username=codewj&amp;email=1051103813@qq.com 实现注册 集成Swagger2ht-micro-record-commons/pom.xml &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;/dependency&gt;com.ht.micro.record.commons.config.Swagger2Configuration @Configuration public class Swagger2Configuration { @Bean public Docket createRestApi() { return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) .select() .apis(RequestHandlerSelectors.basePackage(&quot;com.ht.micro.record&quot;)) // 配置controller地址 .paths(PathSelectors.any()) .build(); } private ApiInfo apiInfo() { return new ApiInfoBuilder() .title(&quot;微服务 API 文档&quot;) .description(&quot;微服务 API 网关接口，http://www.htdatacloud.com/&quot;) .termsOfServiceUrl(&quot;http://www.htdatacloud.com&quot;) .version(&quot;1.0.0&quot;) .build(); } }com.ht.micro.record.UserServiceApplication @EnableSwagger2 com.ht.micro.record.user.controller.UserController @ApiOperation(value = &quot;用户注册&quot;, notes = &quot;参数为实体类，注意用户名和邮箱不要重复&quot;) @PostMapping 服务提供消费配置项目skywalking链路追踪并启动 -javaagent:E:\Project\ht-micro-record\ht-micro-record-external-skywalking\agent\skywalking-agent.jar -Dskywalking.agent.service_name=ht-micro-record-service-user -Dskywalking.collector.backend_service=192.168.2.7:11800 -javaagent:E:\Project\hello-spring-cloud-alibaba\hello-spring-cloud-external-skywalking\agent\skywalking-agent.jar -Dskywalking.agent.service_name=nacos-consumer-feign -Dskywalking.collector.backend_service=192.168.3.229:11800http://192.168.3.233:9501/user/10 调用服务http://192.168.2.7:193/#/monitor/dashboard 查看skywalking的链路追踪]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>spring cloud alibaba</tag>
        <tag>skywalking</tag>
        <tag>rocketmq</tag>
        <tag>validate</tag>
        <tag>multi druid</tag>
        <tag>swagger</tag>
        <tag>email</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Alibaba限流熔断降级]]></title>
    <url>%2F2019%2F08%2F21%2FAlibaba%E9%99%90%E6%B5%81%E7%86%94%E6%96%AD%E9%99%8D%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[基于spring cloud alibaba实战整合开发 限流sentinel存储 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; application.yml spring: application: name: ht-micro-record-service-user cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8719 dashboard: 192.168.2.7:19 访问ht-micro-record-service-user服务，http://localhost:9506/apply/page/1/2快速的调用两次http://localhost:9506/apply/page/1/2 接口之后，第三次调用被限流了 nacos存储 &lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-datasource-nacos&lt;/artifactId&gt; &lt;/dependency&gt; application.yml spring: cloud: sentinel: datasource: ds: nacos: dataId: ${spring.application.name}-sentinel groupId: DEFAULT_GROUP rule-type: flow server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850nacos中配置ht-micro-record-service-user-sentinel [ { &quot;resource&quot;: &quot;/apply/page/1/2&quot;, &quot;limitApp&quot;: &quot;default&quot;, &quot;grade&quot;: 1, &quot;count&quot;: 5, &quot;strategy&quot;: 0, &quot;controlBehavior&quot;: 0, &quot;clusterMode&quot;: false } ] 进入http://192.168.2.7:190/#/dashboard/identity/ht-micro-record-service-user 访问 Sentinel控制台中修改规则：仅存在于服务的内存中，不会修改Nacos中的配置值，重启后恢复原来的值。 Nacos控制台中修改规则：服务的内存中规则会更新，Nacos中持久化规则也会更新，重启后依然保持。限流捕获处理异常ht-micro-record-service-dubbo-provider &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-datasource-nacos&lt;/artifactId&gt; &lt;/dependency&gt; com.ht.micro.record.service.dubbo.provider.DubboProviderApplication // 注解支持的配置Bean @Bean public SentinelResourceAspect sentinelResourceAspect() { return new SentinelResourceAspect(); } bootstrap.yml spring: application: name: ht-micro-record-service-dubbo-provider cloud: nacos: config: file-extension: yaml server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8720 dashboard: 192.168.2.7:190 datasource: ds: nacos: dataId: ${spring.application.name}-sentinel groupId: DEFAULT_GROUP rule-type: flow data-type: json server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850nacos配置ht-micro-record-service-dubbo-provider-sentinel [ { &quot;resource&quot;: &quot;protected-resource&quot;, &quot;controlBehavior&quot;: 2, &quot;count&quot;: 1, &quot;grade&quot;: 1, &quot;limitApp&quot;: &quot;default&quot;, &quot;strategy&quot;: 0 } ] nacos配置ht-micro-record-service-dubbo-provider.yaml spring: application: name: ht-micro-record-service-dubbo-provider cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 server: port: 9502 com.ht.micro.record.service.dubbo.provider.controller.ProviderController @GetMapping(&quot;/port&quot;) @SentinelResource(value = &quot;protected-resource&quot;, blockHandler = &quot;handleBlock&quot;) public Object port() { return &quot;port= &quot;+ port + &quot;, name=&quot; + applicationContext.getEnvironment().getProperty(&quot;user.name&quot;); } public String handleBlock(BlockException ex) { return &quot;限流了&quot;; }启动服务后http://192.168.2.7:190/#/dashboard/flow/ht-micro-record-service-dubbo-provider快速访问http://localhost:9502/provider/port 每秒超过1个请求将会显示限流了 熔断 @GetMapping(&quot;name&quot;) @SentinelResource(value = &quot;getName&quot;, fallback = &quot;getNameFallback&quot;) public String userName(String name){ for (int i = 0; i &lt; 100000000L; i++) { } return &quot;getName &quot; + name; } // 该方法降级处理函数，参数要与原函数getName相同，并且返回值类型也要与原函数相同，此外，该方法必须与原函数在同一个类中 public String getNameFallback(String name){ return &quot;getNameFallback&quot;; } 访问http://localhost:9502/provider/name查看http://192.168.2.7:190/#/dashboard/identity/ht-micro-record-service-dubbo-provider根据响应时间，大于10毫秒，则熔断降级（要连续超过5个请求超过10毫秒才会熔断）快速访问http://localhost:9502/provider/name @GetMapping(&quot;name&quot;) @SentinelResource(value = &quot;getName&quot;, fallback = &quot;getNameFallback&quot;) public String userName(String name){ for (int i = 0; i &lt; 100000000L; i++) { throw new RuntimeException(); } return &quot;getName &quot; + name; } // 该方法降级处理函数，参数要与原函数getName相同，并且返回值类型也要与原函数相同，此外，该方法必须与原函数在同一个类中 public String getNameFallback(String name){ return &quot;getNameFallback&quot;; } 显示抛出是10个异常，然后返回结果是熔断处理方法返回结果 流控规则 降级规则timeWindow为熔断恢复时间熔断模式，当熔断触发后，需要等待timewindow时间，再关闭熔断器。 0 根据rt时间，当超过指定规则的时间连续超过5笔，则触发熔断。 1 根据异常比例熔断 DEGRADE_GRADE_EXCEPTION_RATIO 2 根据单位时间内异常总数做熔断热点规则系统规则授权规则 Sentinel整合Nacos动态发布git clone https://github.com/alibaba/Sentinel.git cd Sentinel/sentinel-dashboard vim pom.xml &lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-datasource-nacos&lt;/artifactId&gt; &lt;!--&lt;scope&gt;test&lt;/scope&gt;--&gt; &lt;/dependency&gt; 修改resources/app/scripts/directives/sidebar/sidebar.html &lt;li ui-sref-active=&quot;active&quot;&gt; &lt;a ui-sref=&quot;dashboard.flowV1({app: entry.app})&quot;&gt; &lt;i class=&quot;glyphicon glyphicon-filter&quot;&gt;&lt;/i&gt;&amp;nbsp;&amp;nbsp;流控规则 &lt;/a&gt; &lt;/li&gt; 为 &lt;li ui-sref-active=&quot;active&quot;&gt; &lt;a ui-sref=&quot;dashboard.flow({app: entry.app})&quot;&gt; &lt;i class=&quot;glyphicon glyphicon-filter&quot;&gt;&lt;/i&gt;&amp;nbsp;&amp;nbsp;流控规则 &lt;/a&gt; &lt;/li&gt; com.alibaba.csp.sentinel.dashboard.rule.nacos.NacosConfigConstant public final class NacosConfigConstant { public static final String GROUP_ID = &quot;DEFAULT_GROUP&quot;; public static final String FLOW_DATA_ID_POSTFIX = &quot;-flow-rules&quot;; public static final String PARAM_FLOW_DATA_ID_POSTFIX = &quot;-param-flow-rules&quot;; public static final String DEGRADE_DATA_ID_POSTFIX = &quot;-degrade-rules&quot;; public static final String SYSTEM_DATA_ID_POSTFIX = &quot;-system-rules&quot;; public static final String AUTHORITY_DATA_ID_POSTFIX = &quot;-authority-rules&quot;; } com.alibaba.csp.sentinel.dashboard.rule.nacos.NacosConfigProperties @Component public class NacosConfigProperties { @Value(&quot;${houyi.nacos.server.ip}&quot;) private String ip; @Value(&quot;${houyi.nacos.server.port}&quot;) private String port; @Value(&quot;${houyi.nacos.server.namespace}&quot;) private String namespace; @Value(&quot;${houyi.nacos.server.group-id}&quot;) private String groupId; public String getIp() { return ip; } public void setIp(String ip) { this.ip = ip; } public String getPort() { return port; } public void setPort(String port) { this.port = port; } public String getNamespace() { return namespace; } public void setNamespace(String namespace) { this.namespace = namespace; } public String getGroupId() { return groupId; } public void setGroupId(String groupId) { this.groupId = groupId; } public String getServerAddr() { return this.getIp()+&quot;:&quot;+this.getPort(); } @Override public String toString() { return &quot;NacosConfigProperties [ip=&quot; + ip + &quot;, port=&quot; + port + &quot;, namespace=&quot; + namespace + &quot;, groupId=&quot; + groupId + &quot;]&quot;; } } com.alibaba.csp.sentinel.dashboard.rule.nacos.NacosConfig @Configuration public class NacosConfig { @Autowired private NacosConfigProperties nacosConfigProperties; @Bean public ConfigService nacosConfigService() throws Exception { Properties properties = new Properties(); properties.put(PropertyKeyConst.SERVER_ADDR, nacosConfigProperties.getServerAddr()); if(nacosConfigProperties.getNamespace() != null &amp;&amp; !&quot;&quot;.equals(nacosConfigProperties.getNamespace())) properties.put(PropertyKeyConst.NAMESPACE, nacosConfigProperties.getNamespace()); return ConfigFactory.createConfigService(properties); } } application.properties houyi.nacos.server.ip=192.168.2.7 houyi.nacos.server.port=8848 houyi.nacos.server.namespace= houyi.nacos.server.group-id=DEFAULT_GROUP FlowRulecom.alibaba.csp.sentinel.dashboard.rule.nacos.FlowRuleNacosPublisher @Component(&quot;flowRuleNacosPublisher&quot;) public class FlowRuleNacosPublisher implements DynamicRulePublisher&lt;List&lt;FlowRuleEntity&gt;&gt; { @Autowired private ConfigService configService; @Autowired private NacosConfigProperties nacosConfigProperties; @Override public void publish(String app, List&lt;FlowRuleEntity&gt; rules) throws Exception { AssertUtil.notEmpty(app, &quot;app name cannot be empty&quot;); if (rules == null) { return; } configService.publishConfig(app + NacosConfigConstant.FLOW_DATA_ID_POSTFIX, nacosConfigProperties.getGroupId(), JSON.toJSONString(rules.stream().map(FlowRuleEntity::toRule).collect(Collectors.toList()))); } } com.alibaba.csp.sentinel.dashboard.rule.nacos.FlowRuleNacosProvider @Component(&quot;flowRuleNacosProvider&quot;) public class FlowRuleNacosProvider implements DynamicRuleProvider&lt;List&lt;FlowRuleEntity&gt;&gt; { private static Logger logger = LoggerFactory.getLogger(FlowRuleNacosProvider.class); @Autowired private ConfigService configService; @Autowired private NacosConfigProperties nacosConfigProperties; @Override public List&lt;FlowRuleEntity&gt; getRules(String appName) throws Exception { String rulesStr = configService.getConfig(appName + NacosConfigConstant.FLOW_DATA_ID_POSTFIX, nacosConfigProperties.getGroupId(), 3000); logger.info(&quot;nacosConfigProperties{}:&quot;, nacosConfigProperties); logger.info(&quot;从Nacos中获取到限流规则信息{}&quot;, rulesStr); if (StringUtil.isEmpty(rulesStr)) { return new ArrayList&lt;&gt;(); } List&lt;FlowRule&gt; rules = RuleUtils.parseFlowRule(rulesStr); if (rules != null) { return rules.stream().map(rule -&gt; FlowRuleEntity.fromFlowRule(appName, nacosConfigProperties.getIp(), Integer.valueOf(nacosConfigProperties.getPort()), rule)) .collect(Collectors.toList()); } else { return new ArrayList&lt;&gt;(); } } } com.alibaba.csp.sentinel.dashboard.controller.FlowControllerV1将原始HTTP调用改为对应的Provider及Publisher去除 @Autowired private SentinelApiClient sentinelApiClient; 新增Qualifier和Component值保持一致 @Autowired @Qualifier(&quot;flowRuleNacosProvider&quot;) private DynamicRuleProvider&lt;List&lt;FlowRuleEntity&gt;&gt; provider; @Autowired @Qualifier(&quot;flowRuleNacosPublisher&quot;) private DynamicRulePublisher&lt;List&lt;FlowRuleEntity&gt;&gt; publisher; @GetMapping(&quot;/rules&quot;) public Result&lt;List&lt;FlowRuleEntity&gt;&gt; apiQueryMachineRules(HttpServletRequest request, @RequestParam String app, @RequestParam String ip, @RequestParam Integer port) { AuthUser authUser = authService.getAuthUser(request); authUser.authTarget(app, PrivilegeType.READ_RULE); if (StringUtil.isEmpty(app)) { return Result.ofFail(-1, &quot;app can&#39;t be null or empty&quot;); } if (StringUtil.isEmpty(ip)) { return Result.ofFail(-1, &quot;ip can&#39;t be null or empty&quot;); } if (port == null) { return Result.ofFail(-1, &quot;port can&#39;t be null&quot;); } try { List&lt;FlowRuleEntity&gt; rules = provider.getRules(app); rules = repository.saveAll(rules); return Result.ofSuccess(rules); } catch (Throwable throwable) { logger.error(&quot;Error when querying flow rules&quot;, throwable); return Result.ofThrowable(-1, throwable); } } private boolean publishRules(String app, String ip, Integer port) { List&lt;FlowRuleEntity&gt; rules = repository.findAllByMachine(MachineInfo.of(app, ip, port)); try { publisher.publish(app, rules); logger.info(&quot;添加限流规则成功{}&quot;, JSON.toJSONString(rules.stream().map(FlowRuleEntity::toRule).collect(Collectors.toList()))); return true; } catch (Exception e) { logger.info(&quot;添加限流规则失败{}&quot;,JSON.toJSONString(rules.stream().map(FlowRuleEntity::toRule).collect(Collectors.toList()))); e.printStackTrace(); return false; } } DegradeRulecom.alibaba.csp.sentinel.dashboard.rule.nacos.DegradeRuleNacosPublisher @Component(&quot;degradeRuleNacosPublisher&quot;) public class DegradeRuleNacosPublisher implements DynamicRulePublisher&lt;List&lt;DegradeRuleEntity&gt;&gt; { @Autowired private ConfigService configService; @Autowired private NacosConfigProperties nacosConfigProperties; @Override public void publish(String app, List&lt;DegradeRuleEntity&gt; rules) throws Exception { AssertUtil.notEmpty(app, &quot;app name cannot be empty&quot;); if (rules == null) { return; } configService.publishConfig(app + NacosConfigConstant.DEGRADE_DATA_ID_POSTFIX, nacosConfigProperties.getGroupId(), JSON.toJSONString(rules.stream().map(DegradeRuleEntity::toRule).collect(Collectors.toList()))); } } com.alibaba.csp.sentinel.dashboard.rule.nacos.DegradeRuleNacosProvider @Component(&quot;degradeRuleNacosProvider&quot;) public class DegradeRuleNacosProvider implements DynamicRuleProvider&lt;List&lt;DegradeRuleEntity&gt;&gt; { private static Logger logger = LoggerFactory.getLogger(DegradeRuleNacosProvider.class); @Autowired private ConfigService configService; @Autowired private NacosConfigProperties nacosConfigProperties; @Override public List&lt;DegradeRuleEntity&gt; getRules(String appName) throws Exception { String rulesStr = configService.getConfig(appName + NacosConfigConstant.DEGRADE_DATA_ID_POSTFIX, nacosConfigProperties.getGroupId(), 3000); logger.info(&quot;nacosConfigProperties{}:&quot;, nacosConfigProperties); logger.info(&quot;从Nacos中获取到熔断降级规则信息{}&quot;, rulesStr); if (StringUtil.isEmpty(rulesStr)) { return new ArrayList&lt;&gt;(); } List&lt;DegradeRule&gt; rules = RuleUtils.parseDegradeRule(rulesStr); if (rules != null) { return rules.stream().map(rule -&gt; DegradeRuleEntity.fromDegradeRule(appName, nacosConfigProperties.getIp(), Integer.valueOf(nacosConfigProperties.getPort()), rule)) .collect(Collectors.toList()); } else { return new ArrayList&lt;&gt;(); } } } com.alibaba.csp.sentinel.dashboard.controller.DegradeController去除 @Autowired private SentinelApiClient sentinelApiClient;新增 @Autowired @Qualifier(&quot;degradeRuleNacosProvider&quot;) private DynamicRuleProvider&lt;List&lt;DegradeRuleEntity&gt;&gt; provider; @Autowired @Qualifier(&quot;degradeRuleNacosPublisher&quot;) private DynamicRulePublisher&lt;List&lt;DegradeRuleEntity&gt;&gt; publisher; @ResponseBody @RequestMapping(&quot;/rules.json&quot;) public Result&lt;List&lt;DegradeRuleEntity&gt;&gt; queryMachineRules(HttpServletRequest request, String app, String ip, Integer port) { AuthUser authUser = authService.getAuthUser(request); authUser.authTarget(app, PrivilegeType.READ_RULE); if (StringUtil.isEmpty(app)) { return Result.ofFail(-1, &quot;app can&#39;t be null or empty&quot;); } if (StringUtil.isEmpty(ip)) { return Result.ofFail(-1, &quot;ip can&#39;t be null or empty&quot;); } if (port == null) { return Result.ofFail(-1, &quot;port can&#39;t be null&quot;); } try { List&lt;DegradeRuleEntity&gt; rules = provider.getRules(app); rules = repository.saveAll(rules); return Result.ofSuccess(rules); } catch (Throwable throwable) { logger.error(&quot;queryApps error:&quot;, throwable); return Result.ofThrowable(-1, throwable); } } private boolean publishRules(String app, String ip, Integer port) { List&lt;DegradeRuleEntity&gt; rules = repository.findAllByMachine(MachineInfo.of(app, ip, port)); try { publisher.publish(app, rules); logger.info(&quot;添加熔断降级规则成功{}&quot;, JSON.toJSONString(rules.stream().map(DegradeRuleEntity::toRule).collect(Collectors.toList()))); return true; } catch (Exception e) { logger.info(&quot;添加熔断降级规则失败{}&quot;,JSON.toJSONString(rules.stream().map(DegradeRuleEntity::toRule).collect(Collectors.toList()))); e.printStackTrace(); return false; } }重新打包启动 mvn clean package -DskipTests cd sentinel-dashboard/target/ nohup java -Dserver.port=190 -Dcsp.sentinel.dashboard.server=localhost:190 -Dproject.name=sentinel-dashboard -jar sentinel-dashboard.jar &amp;项目中bootstrap.yml spring: application: name: ht-micro-record-service-dubbo-provider cloud: nacos: config: file-extension: yaml server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8720 dashboard: localhost:8080 datasource: ds: nacos: dataId: ${spring.application.name}-flow-rules groupId: DEFAULT_GROUP rule-type: flow # 流控 # rule-type: degrade # 熔断 data-type: json server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 ds1: nacos: dataId: ${spring.application.name}-degrade-rules groupId: DEFAULT_GROUP rule-type: degrade # 熔断 data-type: json server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 nacos配置ht-micro-record-service-dubbo-provider.yaml spring: application: name: ht-micro-record-service-dubbo-provider cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 server: port: 9502http://localhost:8080/#/dashboard 新增规则在http://192.168.2.7:8848/nacos 中自动同步暂时实现flow，degrade 基于Feign的熔断controller @GetMapping(&quot;name&quot;) @SentinelResource(value = &quot;getName&quot;, fallback = &quot;getNameFallback&quot;) public String userName(String name){ microServiceUserInf.getUserInfoById(1); return &quot;getName &quot; + name; } service@FeignClient(value = &quot;ht-micro-record-service-user&quot;,fallback = MicroServiceUserInfFallBack.class) public interface MicroServiceUserInf { /** * 通过用户id 获取用户信息 * @param id * @return */ @GetMapping(value = &quot;/user/getUserInfoById/{id}&quot;) TUser getUserInfoById(@PathVariable(value=&quot;id&quot; ) Integer id); } serviceFallBack@Component public class MicroServiceUserInfFallBack implements MicroServiceUserInf { @Autowired private TUserMapper userMapper; @Override public TUser getUserInfoById(Integer id) { log.info(&quot;开启熔断&quot;); return userMapper.selectByPrimaryKey(id); } } nacosspring: application: name: ht-micro-record-service-caserecord cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8723 dashboard: 192.168.2.7:190 datasource: ds: nacos: dataId: ${spring.application.name}-degrade-rules groupId: DEFAULT_GROUP rule-type: degrade data-type: json server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 feign: sentinel: enabled: true pom&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-datasource-nacos&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt; &lt;artifactId&gt;jsr305&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt; &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt;]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>spring cloud alibaba</tag>
        <tag>sentinel</tag>
        <tag>nacos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Alibaba环境搭建]]></title>
    <url>%2F2019%2F08%2F21%2FAlibaba%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[基于spring cloud alibaba实战整合开发 创建统一的依赖管理ht-micro-record-dependencies &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.2.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;name&gt;ht-micro-record-dependencies&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;properties&gt; &lt;!-- Environment Settings --&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;!-- Spring Cloud Settings --&gt; &lt;spring-cloud.version&gt;Greenwich.RELEASE&lt;/spring-cloud.version&gt; &lt;spring-cloud-alibaba.version&gt;0.9.0.RELEASE&lt;/spring-cloud-alibaba.version&gt; &lt;!-- Spring Boot Settings --&gt; &lt;spring-boot-alibaba-druid.version&gt;1.1.18&lt;/spring-boot-alibaba-druid.version&gt; &lt;spring-boot-tk-mybatis.version&gt;2.1.4&lt;/spring-boot-tk-mybatis.version&gt; &lt;spring-boot-pagehelper.version&gt;1.2.12&lt;/spring-boot-pagehelper.version&gt; &lt;!-- Commons Settings --&gt; &lt;mysql.version&gt;5.1.38&lt;/mysql.version&gt; &lt;swagger2.version&gt;2.9.2&lt;/swagger2.version&gt; &lt;/properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;${spring-cloud.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;${spring-cloud-alibaba.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;${spring-boot-alibaba-druid.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;${spring-boot-tk-mybatis.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;${spring-boot-pagehelper.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Commons Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;${mysql.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;${swagger2.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;${swagger2.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!--防止版本冲突，统一版本在父pom中--&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;24.0-jre&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Commons End --&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;build&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;!-- Compiler 插件, 设定 JDK 版本，所有子服务统一打包jdk版本 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.8.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- 打包 jar 文件时，配置 manifest 文件，加入 lib 包的 jar 依赖 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;addMavenDescriptor&gt;false&lt;/addMavenDescriptor&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;!-- Add directory entries --&gt; &lt;addDefaultImplementationEntries&gt;true&lt;/addDefaultImplementationEntries&gt; &lt;addDefaultSpecificationEntries&gt;true&lt;/addDefaultSpecificationEntries&gt; &lt;addClasspath&gt;true&lt;/addClasspath&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- resource --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!-- install --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-install-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!-- clean --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-clean-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!-- ant --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!-- dependency --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;!-- 资源文件配置 --&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;excludes&gt; &lt;exclude&gt;**/*.java&lt;/exclude&gt; &lt;/excludes&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/build&gt; &lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;name&gt;Nexus Release Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;name&gt;Nexus Snapshot Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;Nexus Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-public/&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;aliyun-repos&lt;/id&gt; &lt;name&gt;Aliyun Repository&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;sonatype-repos&lt;/id&gt; &lt;name&gt;Sonatype Repository&lt;/name&gt; &lt;url&gt;https://oss.sonatype.org/content/groups/public&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;sonatype-repos-s&lt;/id&gt; &lt;name&gt;Sonatype Repository&lt;/name&gt; &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;spring-snapshots&lt;/id&gt; &lt;name&gt;Spring Snapshots&lt;/name&gt; &lt;url&gt;https://repo.spring.io/snapshot&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;spring-milestones&lt;/id&gt; &lt;name&gt;Spring Milestones&lt;/name&gt; &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;Nexus Plugin Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-public/&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;/pluginRepository&gt; &lt;pluginRepository&gt; &lt;id&gt;aliyun-repos&lt;/id&gt; &lt;name&gt;Aliyun Repository&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt;创建通用的工具类库ht-micro-record-commons &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath/&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-commons&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-commons&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;!-- Spring Begin ，每个RequestMapping之上都执行@ModelAttribute注解的方法，请求时都会带入request和response--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; &lt;artifactId&gt;tomcat-embed-core&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring End HttpServletRequest等实体，provided 其他服务依赖commons时不会自动引入tomcat依赖--&gt; &lt;!-- Apache Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Apache End --&gt; &lt;!-- Commons Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.ben-manes.caffeine&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.persistence&lt;/groupId&gt; &lt;artifactId&gt;javax.persistence-api&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Commons End --&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;ht-micro-record-commons&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;classifier&gt;exec&lt;/classifier&gt; &lt;mainClass&gt;com.ht.micro.record.commons.CommonsApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;创建通用的领域模型ht-micro-record-commons-domain &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath/&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-commons-domain&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-commons-domain&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;!-- Spring Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring End --&gt; &lt;!-- Commons Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate.validator&lt;/groupId&gt; &lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Commons End --&gt; &lt;!-- Projects Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-commons&lt;/artifactId&gt; &lt;version&gt;${project.parent.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Projects End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;ht-micro-record-commons-domain&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;classifier&gt;exec&lt;/classifier&gt; &lt;mainClass&gt;com.ht.micro.record.commons.CommonsDomainApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;创建通用的数据访问ht-micro-record-commons-mapper &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-commons-mapper&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-commons-mapper&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Commons Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Commons End --&gt; &lt;!-- Projects Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-commons-domain&lt;/artifactId&gt; &lt;version&gt;${project.parent.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Projects End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;ht-micro-record-commons-domain&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;classifier&gt;exec&lt;/classifier&gt; &lt;mainClass&gt;com.ht.micro.record.commons.CommonsMapperApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;创建通用的业务逻辑ht-micro-record-commons-service &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-commons-service&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-commons-service&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-commons-mapper&lt;/artifactId&gt; &lt;version&gt;${project.parent.version}&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;ht-micro-record-commons-domain&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;classifier&gt;exec&lt;/classifier&gt; &lt;mainClass&gt;com.ht.micro.record.commons.CommonsMapperApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;创建通用的代码生成ht-micro-record-database &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-database&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-database&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2018-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper&lt;/artifactId&gt; &lt;version&gt;4.1.4&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.3.5&lt;/version&gt; &lt;configuration&gt; &lt;configurationFile&gt;${basedir}/src/main/resources/generator/generatorConfig.xml&lt;/configurationFile&gt; &lt;overwrite&gt;true&lt;/overwrite&gt; &lt;verbose&gt;true&lt;/verbose&gt; &lt;/configuration&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.38&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper&lt;/artifactId&gt; &lt;version&gt;4.1.4&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;tk.mybatis.mapper.MyMapper public interface MyMapper&lt;T&gt; extends Mapper&lt;T&gt;, MySqlMapper&lt;T&gt; { }generator/generatorConfig.xml &lt;generatorConfiguration&gt; &lt;!-- 引入数据库连接配置 --&gt; &lt;properties resource=&quot;jdbc.properties&quot;/&gt; &lt;context id=&quot;Mysql&quot; targetRuntime=&quot;MyBatis3Simple&quot; defaultModelType=&quot;flat&quot;&gt; &lt;property name=&quot;beginningDelimiter&quot; value=&quot;`&quot;/&gt; &lt;property name=&quot;endingDelimiter&quot; value=&quot;`&quot;/&gt; &lt;!-- 配置 tk.mybatis 插件 --&gt; &lt;plugin type=&quot;tk.mybatis.mapper.generator.MapperPlugin&quot;&gt; &lt;property name=&quot;mappers&quot; value=&quot;tk.mybatis.mapper.MyMapper&quot;/&gt; &lt;/plugin&gt; &lt;!-- 配置数据库连接 --&gt; &lt;jdbcConnection driverClass=&quot;${jdbc.driverClass}&quot; connectionURL=&quot;${jdbc.connectionURL}&quot; userId=&quot;${jdbc.username}&quot; password=&quot;${jdbc.password}&quot;&gt; &lt;/jdbcConnection&gt; &lt;!-- 配置实体类存放路径 --&gt; &lt;javaModelGenerator targetPackage=&quot;com.ht.micro.record.commons.domain&quot; targetProject=&quot;src/main/java&quot;/&gt; &lt;!-- 配置 XML 存放路径 --&gt; &lt;sqlMapGenerator targetPackage=&quot;mapper&quot; targetProject=&quot;src/main/resources/baseMapper&quot;/&gt; &lt;!-- 配置 DAO 存放路径 --&gt; &lt;javaClientGenerator targetPackage=&quot;com.ht.micro.record.commons.mapper.baseMapper&quot; targetProject=&quot;src/main/java&quot; type=&quot;XMLMAPPER&quot;/&gt; &lt;!-- 配置需要指定生成的数据库和表，% 代表所有表 生成@Table中删除ht-micro-record.. --&gt; &lt;table catalog=&quot;ht_micro_record&quot; tableName=&quot;%&quot;&gt; &lt;!-- mysql 配置 --&gt; &lt;generatedKey column=&quot;id&quot; sqlStatement=&quot;Mysql&quot; identity=&quot;true&quot;/&gt; &lt;/table&gt; &lt;/context&gt; &lt;/generatorConfiguration&gt;jdbc.properties jdbc.driverClass=com.mysql.jdbc.Driver jdbc.connectionURL=jdbc:mysql://192.168.2.5:185/ht_micro_record?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false jdbc.username=root jdbc.password=123456mvn mybatis-generator:generate 自动生成表实体和mapper接口 创建外部链路追踪ht-micro-record-external-skywalking &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-external-skywalking&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;name&gt;ht-micro-record-external-skywalking&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;!-- 配置执行器 --&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;!-- 绑定到 package 生命周期阶段上 --&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;!-- 只运行一次 --&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;finalName&gt;skywalking&lt;/finalName&gt; &lt;descriptors&gt; &lt;!-- 配置描述文件路径 --&gt; &lt;descriptor&gt;src/main/assembly/assembly.xml&lt;/descriptor&gt; &lt;/descriptors&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;ht-micro-record-external-skywalking/src/main/assembly/assembly.xml &lt;assembly&gt; &lt;id&gt;6.0.0-Beta&lt;/id&gt; &lt;formats&gt; &lt;!-- 打包的文件格式，支持 zip、tar.gz、tar.bz2、jar、dir、war --&gt; &lt;format&gt;tar.gz&lt;/format&gt; &lt;/formats&gt; &lt;!-- tar.gz 压缩包下是否生成和项目名相同的根目录，有需要请设置成 true --&gt; &lt;includeBaseDirectory&gt;false&lt;/includeBaseDirectory&gt; &lt;dependencySets&gt; &lt;dependencySet&gt; &lt;!-- 是否把本项目添加到依赖文件夹下，有需要请设置成 true --&gt; &lt;useProjectArtifact&gt;false&lt;/useProjectArtifact&gt; &lt;outputDirectory&gt;lib&lt;/outputDirectory&gt; &lt;!-- 将 scope 为 runtime 的依赖包打包 --&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependencySet&gt; &lt;/dependencySets&gt; &lt;fileSets&gt; &lt;fileSet&gt; &lt;!-- 设置需要打包的文件路径 --&gt; &lt;directory&gt;agent&lt;/directory&gt; &lt;!-- 打包后的输出路径 --&gt; &lt;outputDirectory&gt;&lt;/outputDirectory&gt; &lt;/fileSet&gt; &lt;/fileSets&gt; &lt;/assembly&gt;apache-skywalking-apm-incubating-6.0.0-beta.tar.gz解压获取apache-skywalking-apm-bin/agent到ht-micro-record-external-skywalking下mvn clean package 会在 target 目录下创建名为 skywalking-6.0.0-Beta.tar.gz 的压缩包mvn clean install 会在本地仓库目录下创建名为 hello-spring-cloud-external-skywalking-1.0.0-SNAPSHOT-6.0.0-Beta.tar.gz 的压缩包 mkdir /data2/skywalking vim docker-compose.yml version: &#39;3.3&#39; services: elasticsearch: image: wutang/elasticsearch-shanghai-zone:6.3.2 container_name: elasticsearch restart: always ports: - 191:9200 - 192:9300 environment: cluster.name: elasticsearch docker-compose up -d https://mirrors.huaweicloud.com/apache/incubator/skywalking/6.0.0-beta/apache-skywalking-apm-incubating-6.0.0-beta.tar.gz tar zxf apache-skywalking-apm-incubating-6.0.0-beta.tar.gz cd apache-skywalking-apm-incubating vim apache-skywalking-apm-bin/config/application.yml 注释h2,打开es并修改clusterNodes地址 # h2: # driver: ${SW_STORAGE_H2_DRIVER:org.h2.jdbcx.JdbcDataSource} # url: ${SW_STORAGE_H2_URL:jdbc:h2:mem:skywalking-oap-db} # user: ${SW_STORAGE_H2_USER:sa} elasticsearch: nameSpace: ${SW_NAMESPACE:&quot;&quot;} clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:192.168.2.7:191} indexShardsNumber: ${SW_STORAGE_ES_INDEX_SHARDS_NUMBER:2} indexReplicasNumber: ${SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:0} # Batch process setting, refer to https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.5/java-docs-bulk-processor.html bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:2000} # Execute the bulk every 2000 requests bulkSize: ${SW_STORAGE_ES_BULK_SIZE:20} # flush the bulk every 20mb flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10} # flush the bulk every 10 seconds whatever the number of requests concurrentRequests: ${SW_STORAGE_ES_CONCURRENT_REQUESTS:2} # the number of concurrent requests apache-skywalking-apm-incubating/webapp/webapp.yml port 193 ./apache-skywalking-apm-incubating/bin/startup.sh]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>spring cloud alibaba</tag>
        <tag>tk mybatis</tag>
        <tag>skywalking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux]]></title>
    <url>%2F2019%2F08%2F20%2Flinux%2F</url>
    <content type="text"><![CDATA[linux使用技巧及常规软件安装 网卡防火墙vi /etc/sysconfig/network-scripts/ifcfg-eth0 ONBOOT=yes IPADDR=192.168.56.101 BOOTPROTO=static IPADDR与当前网卡在同一个网段 service network restart service iptables stop firewall-cmd --permanent --add-port=8080-8085/tcp firewall-cmd --permanent --remove-port=8080-8085/tcp firewall-cmd --permanent --list-ports firewall-cmd --permanent --list-services 查看使用互联网的程序 firewall-cmd --reload chkconfig --del iptables setenforce 0 systemctl stop firewalld.service systemctl disable firewalld.service vim /etc/sysconfig/selinux SELINUX=disabled 基本命令du -h --max-depth=1 查看各文件夹大小 nohup sh inotify3.sh &gt;&gt;333.out &amp; 后台执行脚本并把输出都指定文件 jobs -l 查看运行的后台进程 fg 1 通过jobid将后台进程提取到前台运行 ctrl + z 将暂停当前正在运行到进程，fg放入后台运行 yum -c /etc/yum.conf --installroot=/usr/local --releasever=/ install lszrz 安装文件到其他目录ekillvim /usr/local/bin/ekill ps aux | grep -e $* | grep -v grep | awk &#39;{print $2}&#39; | xargs -i kill {}chmod a+x /usr/local/bin/ekill 通过ekill删除进程 免密登陆192.168.2.7： ssh-keygen -t rsa ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.2.5 ssh root@192.168.2.5 jdk+maventar zxf jdk-8u60-linux-x64.tar.gz -C /data2/ &amp;&amp; mv jdk1.8.0_60 jdk tar zxf apache-maven-3.6.1-bin.tar.gz -C /data2 &amp;&amp; mv apache-maven-3.6.1 maven vim /etc/profile JAVA_HOME=/data2/jdk JRE_HOME=$JAVA_HOME/jre MAVEN_HOME=/data2/maven PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin:$MAVEN_HOME/bin CLASSPATH=:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib/dt.jar export JAVA_HOME JRE_HOME PATH CLASSPATH MAVEN_HOME source /etc/profile vim setting.xml &lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt; &lt;!-- localRepository | The path to the local repository maven will use to store artifacts. | | Default: ${user.home}/.m2/repository &lt;localRepository&gt;/path/to/local/repo&lt;/localRepository&gt; --&gt; &lt;localRepository&gt;E:\apache-maven-3.6.1\respository&lt;/localRepository&gt; &lt;servers&gt; &lt;server&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;123456&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;123456&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;3rdParty&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;123456&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt; &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;ui&lt;/id&gt; &lt;name&gt;Mirror from UK&lt;/name&gt; &lt;url&gt;http://uk.maven.org/maven2/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;mirrorOf&gt;maven-releases&lt;/mirrorOf&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-releases/&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;mirrorOf&gt;maven-snapshots&lt;/mirrorOf&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-snapshots/&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-releases/&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-snapshots/&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;3rdParty&lt;/id&gt; &lt;url&gt;http://192.168.2.7:182/repository/3rdParty/&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/repository&gt; &lt;!-- 私有库地址--&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;url&gt;http://192.168.2.4:8081/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;ui&lt;/id&gt; &lt;name&gt;ui&lt;/name&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;url&gt;http://uk.maven.org/maven2/&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;!--插件库地址--&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;url&gt;http://192.168.2.4:8081/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-releases/&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-snapshots/&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;pluginRepository&gt; &lt;id&gt;aliyun&lt;/id&gt; &lt;name&gt;aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;activeProfiles&gt; &lt;activeProfile&gt;nexus&lt;/activeProfile&gt; &lt;/activeProfiles&gt; &lt;/settings&gt; Gitgitignore无效git rm -r --cached . git add . git commit -m &#39;.gitignore&#39; git push origin master git config –system core.longpaths true 提交长文件名或者idea取消run git hooks GitLab方案1docker run \ --publish 1443:443 --publish 180:80 --publish 122:22 \ --name gitlab \ --volume /usr/local/docker/gitlab/config:/etc/gitlab \ --volume /usr/local/docker/gitlab/logs:/var/log/gitlab \ --volume /usr/local/docker/gitlab/data:/var/opt/gitlab \ gitlab/gitlab-ce方案2vim /usr/local/docker/gitlab/docker-compose.yml version: &#39;3&#39; services: gitlab: image: &#39;twang2218/gitlab-ce-zh:10.5&#39; restart: always hostname: &#39;192.168.2.5&#39; container_name: gitlab environment: TZ: &#39;Asia/Shanghai&#39; GITLAB_OMNIBUS_CONFIG: | external_url &#39;http://192.168.2.5:180&#39; gitlab_rails[&#39;gitlab_shell_ssh_port&#39;] = 2222 unicorn[&#39;port&#39;] = 8888 nginx[&#39;listen_port&#39;] = 8080 ports: - &#39;180:8080&#39; - &#39;8443:443&#39; - &#39;2222:22&#39; volumes: - /usr/local/docker/gitlab/config:/etc/gitlab - /usr/local/docker/gitlab/data:/var/opt/gitlab - /usr/local/docker/gitlab/logs:/var/log/gitlab ERROR: error while removing network: network gitlab_default id e3f084651bcc6b6ca5d5b7fb122d0ef3aba108292989441abc82f14343fea827 has active endpoints docker network inspect gitlab_default docker network disconnect -f gitlab_default gitlab docker-compose down --remove-orphans docker-compose logs -ft gitlab 配置邮箱 vim /usr/local/docker/gitlab/config/gitlab.rb gitlab_rails[&#39;smtp_enable&#39;] = true gitlab_rails[&#39;smtp_address&#39;] = &quot;smtp.163.com&quot; gitlab_rails[&#39;smtp_port&#39;] = 25 gitlab_rails[&#39;smtp_user_name&#39;] = &quot;m15806204096@163.com&quot; gitlab_rails[&#39;smtp_password&#39;] = &quot;codewj123456&quot; gitlab_rails[&#39;smtp_domain&#39;] = &quot;163.com&quot; gitlab_rails[&#39;smtp_authentication&#39;] = &quot;login&quot; gitlab_rails[&#39;smtp_enable_starttls_auto&#39;] = true gitlab_rails[&#39;smtp_tls&#39;] = false gitlab_rails[&#39;gitlab_email_from&#39;] = &quot;m15806204096@163.com&quot; user[&quot;git_user_email&quot;] = &quot;m15806204096@163.com&quot;root/123456 管理区域-&gt;设置-&gt;开启注册 注册时发送确认邮件docker-compose restart新增ssh密钥 ssh-keygen -t rsa -C “15806204096@163.com“,将.ssh的公钥加入Gitlab的SSH密钥访问 http://192.168.2.5:180/ root 12345678 方案3安装gityum –y install git cd /usr/local mkdir git cd git git init --bare learngit.git useradd git passwd git chown -R git:git learngit.git vi /etc/passwd git:x:1000:1000::/home/git:/usr/bin/git-shell 复制客户端的ssh-keygen -t rsa -C &quot;你的邮箱&quot; 获得的id_rsa.pub公钥到/root/.ssh/authorized_keys和/root/.ssh/authorized_keys gitignore无效的解决方案 git rm -r --cached . git add . git commit -m &#39;.gitignore&#39; git push origin master *.cache *.cache.lock *.iml *.log **/target **/logs .idea **/.project **/.settingsgitlab安装git config --global http.sslVerify false yum -y install curl policycoreutils openssh-server openssh-clients postfix systemctl start sshd systemctl start postfix systemctl enable sshd systemctl enable postfix curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash yum -y install gitlab-ce 太慢的话使用清华的源，yum makecache再install vim /etc/yum.repos.d/gitlab_gitlab-ce.repo [gitlab-ce] name=gitlab-ce baseurl=http://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el6 repo_gpgcheck=0 gpgcheck=0 enabled=1 gpgkey=https://packages.gitlab.com/gpg.key mkdir -p /etc/gitlab/ssl openssl genrsa -out &quot;/etc/gitlab/ssl/gitlab.example.com.key&quot; 2048 openssl req -new -key &quot;/etc/gitlab/ssl/gitlab.example.com.key&quot; -out &quot;/etc/gitlab/ssl/gitlab.example.com.csr&quot; Country Name (2 letter code) [XX]:cn State or Province Name (full name) []:bj Locality Name (eg, city) (Default City]:bj Organization Name (eg, company) [Default Company Ltd]: Organizational Unit Name (eg, section) []: Common Name (eg, your name or your server&#39;s hostname) []:gitlab.example.com Email Address (]:admin@example.com Please enter the following &#39;extra&#39; attributes to be sent with your certificate request A challenge password [):123456 An optional company name []:GitLab基本配置openssl x509 -req -days 3650 -in &quot;/etc/gitlab/ssl/gitlab.example.com.csr&quot; -signkey &quot;/etc/gitlab/ssl/gitlab.example.com.key&quot; -out &quot;/etc/gitlab/ssl/gitlab.example.com.crt&quot; openssl dhparam -out /etc/gitlab/ssl/dhparams.pem 2048 chmod 600 /etc/gitlab/ssl/* vim /etc/gitlab/gitlab.rb 将external_url &#39;http://gitlab.example.com&#39;的http修改为https 将# nginx[&#39;redirect_http_to_https&#39;] = false的注释去掉，修改为nginx[&#39;redirect_http_to_https&#39;] = true 将# nginx[&#39;ssl_certificate&#39;] = &quot;/etc/gitlab/ssl/#{node[&#39;fqdn&#39;]}.crt&quot;修改为# nginx[&#39;ssl_certificate&#39;] = &quot;/etc/gitlab/ssl/gitlab.example.com.crt&quot; 将# nginx[&#39;ssl_certificate_key&#39;] = &quot;/etc/gitlab/ssl/#{node[&#39;fqdn&#39;]}.key&quot;修改为# nginx[&#39;ssl_certificate_key&#39;] = &quot;/etc/gitlab/ssl/gitlab.example.com.key&quot; 将# nginx[&#39;ssl_dhparam&#39;] = nil 修改为# nginx[&#39;ssl_dhparam&#39;] = &quot;/etc/gitlab/ssl/dhparams.pem&quot; gitlab-ctl reconfigure vim /var/opt/gitlab/nginx/conf/gitlab-http.conf 在server_name下添加如下配置内容：rewrite ^(.*)$ https://$host$1 permanent; 重启gitlab，使配置生效,gitlab-ctl restart，如遇到访问错误直接等待启动完成 修改本地hosts文件 将gitlab服务器的地址添加 gitlab.example.com 初始化时修改管理员密码，root 12345678 git -c http.sslVerify=false clone https://gitlab.example.com/root/test-repo.git git add . #git config --global user.email &quot;admin@example.com&quot; #git config --global user.name &quot;admin&quot; git commit -m &#39;init&#39; git -c http.sslVerify=false pull origin master git -c http.sslVerify=false push origin master https://gitlab.example.com/admin/system_info 查看系统资源状态值 https://gitlab.example.com/admin/logs 其中application.log记录了git用户的记录，production.log实时查看所有的访问链接 https://gitlab.example.com/admin/users/new 创建用户https://gitlab.example.com/root/test-repo/project_members 修改指定项目的成员，在项目的manage access中，修改登录密码 rm -rf test-repo/ git -c http.sslVerify=false clone https://gitlab.example.com/root/test-repo.git dev 12345678 cd test-repo/ git checkout -b release-1.0 git add . git commit -m &#39;release-1.0&#39; git -c http.sslVerify=false push origin release-1.0 dev登录后create merge request lead登录将受到release-1.0的merge申请，点击merge后可以填写comment并提交 Skywalkingmkdir /data2/skywalking vim docker-compose.yml version: &#39;3.3&#39; services: elasticsearch: image: wutang/elasticsearch-shanghai-zone:6.3.2 container_name: elasticsearch restart: always ports: - 191:9200 - 192:9300 environment: cluster.name: elasticsearch docker-compose up -d https://mirrors.huaweicloud.com/apache/incubator/skywalking/6.0.0-beta/apache-skywalking-apm-incubating-6.0.0-beta.tar.gz tar zxf apache-skywalking-apm-incubating-6.0.0-beta.tar.gz cd apache-skywalking-apm-incubating vim apache-skywalking-apm-bin/config/application.yml 注释h2,打开es并修改clusterNodes地址 # h2: # driver: ${SW_STORAGE_H2_DRIVER:org.h2.jdbcx.JdbcDataSource} # url: ${SW_STORAGE_H2_URL:jdbc:h2:mem:skywalking-oap-db} # user: ${SW_STORAGE_H2_USER:sa} elasticsearch: nameSpace: ${SW_NAMESPACE:&quot;&quot;} clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:192.168.2.7:191} indexShardsNumber: ${SW_STORAGE_ES_INDEX_SHARDS_NUMBER:2} indexReplicasNumber: ${SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:0} # Batch process setting, refer to https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.5/java-docs-bulk-processor.html bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:2000} # Execute the bulk every 2000 requests bulkSize: ${SW_STORAGE_ES_BULK_SIZE:20} # flush the bulk every 20mb flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10} # flush the bulk every 10 seconds whatever the number of requests concurrentRequests: ${SW_STORAGE_ES_CONCURRENT_REQUESTS:2} # the number of concurrent requests apache-skywalking-apm-incubating/webapp/webapp.yml port 193 ./apache-skywalking-apm-incubating/bin/startup.shSentinelcd /data2/ &amp;&amp; git clone https://github.com/alibaba/Sentinel.git cd Sentinel/ mvn clean package -DskipTests cd sentinel-dashboard/target/ nohup java -Dserver.port=190 -Dcsp.sentinel.dashboard.server=localhost:190 -Dproject.name=sentinel-dashboard -jar sentinel-dashboard.jar &amp;安装黑体字体yum -y install fontconfig fc-list :lang=zh cd /usr/share/fonts &amp;&amp; mkdir chinese chmod -R 755 /usr/share/fonts/chinese cd chinese/ &amp;&amp; rz simhei.ttf fs_GB2312.ttf fzxbsjt.ttf yum -y install ttmkfdir ttmkfdir -e /usr/share/X11/fonts/encodings/encodings.dir vim /etc/fonts/fonts.conf &lt;dir&gt;/usr/share/fonts&lt;/dir&gt; &lt;dir&gt;/usr/share/fonts/chinese&lt;/dir&gt; &lt;dir&gt;/usr/share/X11/fonts/Type1&lt;/dir&gt; &lt;dir&gt;/usr/share/X11/fonts/TTF&lt;/dir&gt; &lt;dir&gt;/usr/local/share/fonts&lt;/dir&gt; &lt;dir prefix=&quot;xdg&quot;&gt;fonts&lt;/dir&gt; &lt;!-- the following element will be removed in the future --&gt; &lt;dir&gt;~/.fonts&lt;/dir&gt; fc-cache fc-list :lang=zh RocketMQmkdir /data2/RocketMQ vim docker-compose.yml version: &#39;3.5&#39; services: rmqnamesrv: image: foxiswho/rocketmq:server container_name: rmqnamesrv ports: - 9876:9876 volumes: - ./data/logs:/opt/logs - ./data/store:/opt/store networks: rmq: aliases: - rmqnamesrv rmqbroker: image: foxiswho/rocketmq:broker container_name: rmqbroker ports: - 10909:10909 - 10911:10911 volumes: - ./data/logs:/opt/logs - ./data/store:/opt/store - ./data/brokerconf/broker.conf:/etc/rocketmq/broker.conf environment: NAMESRV_ADDR: &quot;rmqnamesrv:9876&quot; JAVA_OPTS: &quot; -Duser.home=/opt&quot; JAVA_OPT_EXT: &quot;-server -Xms128m -Xmx128m -Xmn128m&quot; command: mqbroker -c /etc/rocketmq/broker.conf depends_on: - rmqnamesrv networks: rmq: aliases: - rmqbroker rmqconsole: image: styletang/rocketmq-console-ng container_name: rmqconsole ports: - 8088:8080 environment: JAVA_OPTS: &quot;-Drocketmq.namesrv.addr=rmqnamesrv:9876 -Dcom.rocketmq.sendMessageWithVIPChannel=false&quot; depends_on: - rmqnamesrv networks: rmq: aliases: - rmqconsole networks: rmq: name: rmq driver: bridge vim /data2/RocketMQ/data/brokerconf/broker.conf brokerClusterName=DefaultCluster brokerName=broker-a brokerId=0 brokerIP1=192.168.2.7 defaultTopicQueueNums=4 autoCreateTopicEnable=true autoCreateSubscriptionGroup=true listenPort=10911 deleteWhen=04 fileReservedTime=120 mapedFileSizeCommitLog=1073741824 mapedFileSizeConsumeQueue=300000 diskMaxUsedSpaceRatio=88 maxMessageSize=65536 brokerRole=ASYNC_MASTER flushDiskType=ASYNC_FLUSH docker-compose up -d docker-compose down http://192.168.2.7:8088/RabbitMQdocker-compose.yml version: &#39;3&#39; services: rabbitmq: image: rabbitmq:management-alpine container_name: rabbitmq environment: - RABBITMQ_DEFAULT_USER=admin - RABBITMQ_DEFAULT_PASS=admin restart: always ports: - &quot;15672:15672&quot; - &quot;5672:5672&quot; logging: driver: &quot;json-file&quot; options: max-size: &quot;200k&quot; max-file: &quot;10&quot;docker-compose up -d,5672为rabbitmq的服务端口，15672为rabbitmq的web管理界面端口。 jenkinsrpm -qa|grep jenkins 搜索已安装 rpm -e --nodeps jenkins-2.83-1.1.noarch find / -name jenkins*|xargs rm -rf rpm -ivh jdk-8u171-linux-x64.rpm 安装java http://mirrors.jenkins.io/redhat/jenkins-2.180-1.1.noarch.rpm rpm -ivh jenkins-2.180-1.1.noarch.rpm 安装jenkins新版本 vi /etc/sysconfig/jenkins JENKINS_USER=&quot;root&quot; JENKINS_PORT=&quot;181&quot; vim /etc/rc.d/init.d/jenkins 修改candidates /data2/jdk/bin/java systemctl daemon-reload systemctl restart jenkins http://192.168.2.7:181 cat /var/lib/jenkins/secrets/initialAdminPassword 初始密码串并安装默认的插件 vim /var/lib/jenkins/hudson.model.UpdateCenter.xml 必须在填写完密码后修改 改https为http或者改为http://mirror.xmission.com/jenkins/updates/update-center.json systemctl restart jenkins 若页面空白/var/lib/jenkins/config.xml &lt;authorizationStrategy class=&quot;hudson.security.AuthorizationStrategy$Unsecured&quot;/&gt; &lt;securityRealm class=&quot;hudson.security.SecurityRealm$None&quot;/&gt; 系统管理-全局工具配置:/data2/maven /data2/jdk /data2/maven/conf/settings.xml 安装插件：Maven Integration，GitHub plugin，Git plugin 新建任务时，丢弃旧的构建，保持构建的天数3，保持构建的最大个数5 定时删除none的docker镜像手动执行：docker rmi $(docker images -f “dangling=true” -q)定时构建语法： 每天凌晨2:00跑一次 H 2 * * * 每隔5分钟构建一次 H/5 * * * * 每两小时构建一次 H H/2 * * * 每天中午12点定时构建一次 H 12 * * * 或0 12 * * *（0这种写法也被H替代了） 每天下午18点前定时构建一次 H 18 * * * 每15分钟构建一次 H/15 * * * * 或*/5 * * * *(这种方式已经被第一种替代了，jenkins也不推荐这种写法了) 周六到周日，18点-23点，三小时构建一次 H 18-23/3 * * 6-7 shell脚本 echo ---------------Stop-Rm-Containers...------------------ docker stop `docker ps -a| grep expire | awk &#39;{print $1}&#39;`|xargs docker rm echo ---------------Clear-Images...------------------ clearImagesList=$(docker images -f &quot;dangling=true&quot; -q) if [ ! -n &quot;$clearImagesList&quot; ]; then echo &quot;no images need clean up.&quot; else docker rmi $(docker images -f &quot;dangling=true&quot; -q) echo &quot;clear success.&quot; fi cron 每隔5秒执行一次：*/5 * * * * ? 每隔1分钟执行一次：0 */1 * * * ? 每天23点执行一次：0 0 23 * * ? 每天凌晨1点执行一次：0 0 1 * * ? 每月1号凌晨1点执行一次：0 0 1 1 * ? 每月最后一天23点执行一次：0 0 23 L * ? 每周星期天凌晨1点实行一次：0 0 1 ? * L 在26分、29分、33分执行一次：0 26,29,33 * * * ? 每天的0点、13点、18点、21点都执行一次：0 0 0,13,18,21 * * ? toptop -d 2 -c -p 123456 //每隔2秒显示pid是12345的进程的资源使用情况，并显式该进程启动的命令行参数 M —根据驻留内存大小进行排序 P —根据CPU使用百分比大小进行排序 T —根据时间/累计时间进行排序 c —切换显示命令名称和完整命令行 t —切换显示进程和CPU信息 m —切换显示内存信息 l —切换显示平均负载和启动时间信息 o —改变显示项目的顺序 f —从当前显示中添加或删除项目 S —切换到累计模式 s —改变两次刷新之间的延迟时间。系统将提示用户输入新的时间，单位为s。如果有小数，就换算成ms。 q —退出top程序 i —忽略闲置和僵尸进程。这是一个开关式的命令 k —终止一个进程更改显示内容通过 f 键可以选择显示的内容。按 f 键之后会显示列的列表，按 a-z 即可显示或隐藏对应的列，最后按回车键确定。 Exampletop - 01:06:48 up 1:22, 1 user, load average: 0.06, 0.60, 0.48 Tasks: 29 total, 1 running, 28 sleeping, 0 stopped, 0 zombie Cpu(s): 0.3% us, 1.0% sy, 0.0% ni, 98.7% id, 0.0% wa, 0.0% hi, 0.0% si Mem: 191272k total, 173656k used, 17616k free, 22052k buffers Swap: 192772k total, 0k used, 192772k free, 123988k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND root 16 0 7976 2456 1980 S 0.7 1.3 0:11.03 sshd root 16 0 2128 980 796 R 0.7 0.5 0:02.72 top root 16 0 1992 632 544 S 0.0 0.3 0:00.90 init root 34 19 0 0 0 S 0.0 0.0 0:00.00 ksoftirqd/0 root RT 0 0 0 0 S 0.0 0.0 0:00.00 watchdog/0 统计信息区前五行是系统整体的统计信息。第一行是任务队列信息，同 uptime 命令的执行结果。其内容如下： 01:06:48 当前时间 up 1:22 系统运行时间，格式为时:分 1 user 当前登录用户数 load average: 0.06, 0.60, 0.48 系统负载，即任务队列的平均长度。三个数值分别为 1分钟、5分钟、15分钟前到现在的平均值。 第二、三行为进程和CPU的信息。当有多个CPU时，这些内容可能会超过两行。内容如下： total 进程总数 running 正在运行的进程数 sleeping 睡眠的进程数 stopped 停止的进程数 zombie 僵尸进程数 Cpu(s): 0.3% us 用户空间占用CPU百分比 1.0% sy 内核空间占用CPU百分比 0.0% ni 用户进程空间内改变过优先级的进程占用CPU百分比 98.7% id 空闲CPU百分比 0.0% wa 等待输入输出的CPU时间百分比 0.0%hi：硬件CPU中断占用百分比 0.0%si：软中断占用百分比 0.0%st：虚拟机占用百分比最后两行为内存信息。内容如下： Mem: 191272k total 物理内存总量 173656k used 使用的物理内存总量 17616k free 空闲内存总量 22052k buffers 用作内核缓存的内存量 Swap: 192772k total 交换区总量 0k used 使用的交换区总量 192772k free 空闲交换区总量 123988k cached 缓冲的交换区总量,内存中的内容被换出到交换区，而后又被换入到内存，但使用过的交换区尚未被覆盖，该数值即为这些内容已存在于内存中的交换区的大小,相应的内存再次被换出时可不必再对交换区写入。进程信息区统计信息区域的下方显示了各个进程的详细信息。首先来认识一下各列的含义 序号 列名 含义 a PID 进程id b PPID 父进程id c RUSER Real user name d UID 进程所有者的用户id e USER 进程所有者的用户名 f GROUP 进程所有者的组名 g TTY 启动进程的终端名。不是从终端启动的进程则显示为 ? h PR 优先级 i NI nice值。负值表示高优先级，正值表示低优先级 j P 最后使用的CPU，仅在多CPU环境下有意义 k %CPU 上次更新到现在的CPU时间占用百分比 l TIME 进程使用的CPU时间总计，单位秒 m TIME+ 进程使用的CPU时间总计，单位1/100秒 n %MEM 进程使用的物理内存百分比 o VIRT 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES p SWAP 进程使用的虚拟内存中，被换出的大小，单位kb。 q RES 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA r CODE 可执行代码占用的物理内存大小，单位kb s DATA 可执行代码以外的部分(数据段+栈)占用的物理内存大小，单位kb t SHR 共享内存大小，单位kb u nFLT 页面错误次数 v nDRT 最后一次写入到现在，被修改过的页面数。 w S 进程状态(D=不可中断的睡眠状态,R=运行,S=睡眠,T=跟踪/停止,Z=僵尸进程) x COMMAND 命令名/命令行 y WCHAN 若该进程在睡眠，则显示睡眠中的系统函数名 z Flags 任务标志，参考 sched.h VIRT：virtual memory usage。Virtual这个词很神，一般解释是：virtual adj.虚的, 实质的,[物]有效的, 事实上的。到底是虚的还是实的？让Google给Define之后，将就明白一点，就是这东西还是非物质的，但是有效果的，不发生在真实世界的，发生在软件世界的等等。这个内存使用就是一个应用占有的地址空间，只是要应用程序要求的，就全算在这里，而不管它真的用了没有。写程序怕出错，又不在乎占用的时候，多开点内存也是很正常的。RES：resident memory usage。常驻内存。这个值就是该应用程序真的使用的内存，但还有两个小问题，一是有些东西可能放在交换盘上了（SWAP），二是有些内存可能是共享的。SHR：shared memory。共享内存。就是说这一块内存空间有可能也被其他应用程序使用着；而Virt － Shr似乎就是这个程序所要求的并且没有共享的内存空间。DATA：数据占用的内存。如果top没有显示，按f键可以显示出来。这一块是真正的该程序要求的数据空间，是真正在运行中要使用的。SHR是一个潜在的可能会被共享的数字，如果只开一个程序，也没有别人共同使用它；VIRT里面的可能性更多，比如它可能计算了被许多X的库所共享的内存；RES应该是比较准确的，但不含有交换出去的空间；但基本可以说RES是程序当前使用的内存量。 Q1:-bash: fork: Cannot allocate memory进程数满了,echo 1000000 &gt; /proc/sys/kernel/pid_max,echo “kernel.pid_max=1000000 “ &gt;&gt; /etc/sysctl.conf,sysctl -ptop:展示进程视图，监控服务器进程数值默认进入top时，各进程是按照CPU的占用量来排序的,-f查看实际内存占用量]]></content>
      <categories>
        <category>系统</category>
      </categories>
      <tags>
        <tag>gitlab</tag>
        <tag>jenkins</tag>
        <tag>sentinel</tag>
        <tag>linux</tag>
        <tag>rocketmq</tag>
        <tag>rabbitmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker]]></title>
    <url>%2F2019%2F08%2F20%2Fdocker%2F</url>
    <content type="text"><![CDATA[docker安装使用及镜像制作 安装docker基本配置yum install -y vim lrzsz git setenforce 0 vim /etc/selinux/config SELINUX=disabled systemctl stop firewalld systemctl disable firewalldcentosyum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-selinux \ docker-engine-selinux \ docker-engine -y 移除旧版本 yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum makecache fast yum list docker-ce --showduplicates | sort -r yum install -y docker-ce-18.09.5 systemctl restart docker yum update -y nss curl libcurl curl -L https://github.com/docker/compose/releases/download/1.20.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose docker-compose --versionubuntuapt-get update apt-get -y install apt-transport-https ca-certificates curl software-properties-common # step 2: 安装GPG证书 curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add - # Step 3: 写入软件源信息 sudo add-apt-repository &quot;deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable&quot; # Step 4: 更新并安装 Docker-CE sudo apt-get -y update sudo apt-get -y install docker-ce # 安装指定版本的Docker-CE: # Step 1: 查找Docker-CE的版本: # apt-cache madison docker-ce # docker-ce | 17.03.1~ce-0~ubuntu-xenial | http://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages # docker-ce | 17.03.0~ce-0~ubuntu-xenial | http://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages # Step 2: 安装指定版本的Docker-CE: (VERSION 例如上面的 17.03.1~ce-0~ubuntu-xenial) # sudo apt-get -y install docker-ce=[VERSION]Docker配置及基本使用vim ~/.bashrc alias dops=&#39;docker ps --format &quot;table {{.ID}}\t{{.Names}}\t{{.Image}}\t&quot;&#39; alias dopsa=&#39;docker ps -a --format &quot;table {{.ID}}\t{{.Names}}\t{{.Image}}\t&quot;&#39; alias dolo=&#39;docker logs -ft&#39; vim /etc/docker/daemon.json { &quot;registry-mirrors&quot;: [&quot;https://3gki6pei.mirror.aliyuncs.com&quot;], &quot;storage-driver&quot;:&quot;devicemapper&quot;, &quot;insecure-registries&quot;:[&quot;192.168.2.7:5000&quot;] &quot;log-driver&quot;: &quot;json-file&quot;, &quot;log-opt&quot;: { &quot;max-size&quot;: &quot;10m&quot;, &quot;max-file&quot;: &quot;10&quot; } } systemctl daemon-reload systemctl restart docker.service service docker stop 删除所有镜像 rm -rf /var/lib/docker systemctl start docker.service --restart=always 设置重启docker自动启动容器 docker update --restart=always es-node2 docker run -itd myimage:test /bin/bash -c &quot;命令1;命令2&quot; 启动容器自动执行命令 docker export registry &gt; /home/registry.tar 将容器打成tar包 scp /home/registry.tar root@192.168.2.7:/root cat ~/registry.tar | docker import - registry/2.5 将tar包打成镜像 docker save jdk1.8 &gt; /home/java.tar.gz 导出镜像 docker load &lt; /home/java.tar.gz 导入镜像 docker stop sentine1 | xargs docker rm docker rm -f redis-slave1|true docker logs -f -t --since=&quot;2018-02-08&quot; --tail=100 CONTAINER_ID 查看指定时间后的日志，只显示最后100行 docker logs --since 30m CONTAINER_ID 查看最近30分钟的日志 /var/lib/docker/containers/contain id 下rm -rf *.log 删除docker日志 进入https://homenew.console.aliyun.com/ 搜索容器镜像服务，进入侧边栏的镜像加速器获取自己的Docker加速镜像地址。 安装Registry私服方案1docker run -di --name=registry -p 5000:5000 docker.io/registry访问 http://192.168.2.5:5000/v2/_catalog 方案2vim /usr/local/docker/registry/docker-compose.yml version: &#39;3.1&#39; services: registry: image: registry restart: always container_name: registry ports: - 5000:5000 volumes: - ./data:/var/lib/registry frontend: image: konradkleine/docker-registry-frontend:v2 container_name: registry-frontend restart: always ports: - 184:80 volumes: - ./certs/frontend.crt:/etc/apache2/server.crt:ro - ./certs/frontend.key:/etc/apache2/server.key:ro environment: - ENV_DOCKER_REGISTRY_HOST=192.168.2.5 - ENV_DOCKER_REGISTRY_PORT=5000使用docker-compose up -d启动registry容器，http://192.168.2.5:184/ 访问私有镜像库 curl -I -H &quot;Accept: application/vnd.docker.distribution.manifest.v2+json&quot; 192.168.2.7:5000/v2/ht-micro-record-service-user/manifests/1.0.0-SNAPSHOT 查看镜像Etag curl -I -H &quot;Accept: application/vnd.docker.distribution.manifest.v2+json&quot; 192.168.2.5:5000/v2/ht-micro-record-commons/manifests/1.0.0-SNAPSHOT 查看镜像Etag curl -i -X DELETE 192.168.2.5:5000/v2/codewj-redis-cluster/manifests/sha256:d6d6fad1ac67310ee34adbaa72986c6b233bd713906013961c722ecb10a049e5 删除codewj-redis-cluster:latest镜像 curl -I -X DELETE http://192.168.2.7:5000/v2/ht-micro-record-service-user/manifests/sha256:a6d4e02fa593f0ae30476bda8d992dcb0fc2341e6fef85a9887444b5e4b75a04 删除user镜像 docker exec -it registry registry garbage-collect /etc/docker/registry/config.yml 垃圾回收blob docker exec registry rm -rf /var/lib/registry/docker/registry/v2/repositories/codewj-redis-cluster 强制删除 docker exec registry rm -rf /var/lib/registry/docker/registry/v2/repositories/ht-micro-record-service-user 强制删除JDK镜像制作环境配置vim /etc/docker/daemon.json { &quot;registry-mirrors&quot;: [&quot;https://dhq9bx4f.mirror.aliyuncs.com&quot;], &quot;storage-driver&quot;: &quot;devicemapper&quot;, &quot;insecure-registries&quot;:[&quot;192.168.2.5:5000&quot;] } vim /lib/systemd/system/docker.service 开放访问 ExecStart 新增 -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock \ systemctl daemon-reload systemctl restart docker docker start registry制作镜像mkdir -p /usr/local/dockerjdk8 &amp;&amp; cd /usr/local/dockerjdk8sz jdk-8u60-linux-x64.tar.gz 传到该目录下vim Dockerfile #依赖镜像名称和ID FROM docker.io/centos:7 #指定镜像创建者信息 MAINTAINER OneJane #切换工作目录 WORKDIR /usr RUN mkdir /usr/local/java #ADD 是相对路径jar,把java添加到容器中 ADD jdk-8u60-linux-x64.tar.gz /usr/local/java/ ENV LANG en_US.UTF-8 #配置java环境变量 ENV JAVA_HOME /usr/local/java/jdk1.8.0_60 ENV JRE_HOME $JAVA_HOME/jre ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib:$CLASSPATH ENV PATH $JAVA_HOME/bin:$PATHdocker build -t='onejane-jdk1.8' .安装中文环境 yum -y install fontconfig fc-list :lang=zh cd /usr/share/fonts &amp;&amp; mkdir chinese chmod -R 755 /usr/share/fonts/chinese cd chinese/ &amp;&amp; rz simhei.ttf fs_GB2312.ttf fzxbsjt.ttf yum -y install ttmkfdir ttmkfdir -e /usr/share/X11/fonts/encodings/encodings.dir vim /etc/fonts/fonts.conf &lt;dir&gt;/usr/share/fonts&lt;/dir&gt; &lt;dir&gt;/usr/share/fonts/chinese&lt;/dir&gt; &lt;dir&gt;/usr/share/X11/fonts/Type1&lt;/dir&gt; &lt;dir&gt;/usr/share/X11/fonts/TTF&lt;/dir&gt; &lt;dir&gt;/usr/local/share/fonts&lt;/dir&gt; &lt;dir prefix=&quot;xdg&quot;&gt;fonts&lt;/dir&gt; &lt;!-- the following element will be removed in the future --&gt; &lt;dir&gt;~/.fonts&lt;/dir&gt; fc-cache fc-list :lang=zh 上传到私服docker commit 6ea1085dfc2a pxc:v1.0 将镜像保存本地 docker commit -m &quot;容器说明&quot; -a &quot;OneJane&quot; [CONTAINER ID] [给新的镜像命名] 将容器打包成镜像 docker tag onejane-jdk1.8 192.168.2.7:5000/onejane-jdk1.8 docker push 192.168.2.7:5000/onejane-jdk1.8 将镜像推到仓库Redis镜像制作vim entrypoint.sh #!/bin/sh #只作用于当前进程,不作用于其创建的子进程 set -e #$0--Shell本身的文件名 $1--第一个参数 $@--所有参数列表 # allow the container to be started with `--user` if [ &quot;$1&quot; = &#39;redis-server&#39; -a &quot;$(id -u)&quot; = &#39;0&#39; ]; then sed -i &#39;s/REDIS_PORT/&#39;$REDIS_PORT&#39;/g&#39; /usr/local/etc/redis.conf chown -R redis . #改变当前文件所有者 exec gosu redis &quot;$0&quot; &quot;$@&quot; #gosu是sudo轻量级”替代品” fi exec &quot;$@&quot; vim redis.conf #端口 port REDIS_PORT #开启集群 cluster-enabled yes #配置文件 cluster-config-file nodes.conf cluster-node-timeout 5000 #更新操作后进行日志记录 appendonly yes #设置主服务的连接密码 # masterauth #设置从服务的连接密码 # requirepassvi Dockerfile #基础镜像 FROM redis #修复时区 RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime RUN echo &#39;Asia/Shanghai&#39; &gt;/etc/timezone #环境变量 ENV REDIS_PORT 8000 #ENV REDIS_PORT_NODE 18000 #暴露变量 EXPOSE $REDIS_PORT #EXPOSE $REDIS_PORT_NODE #复制 COPY entrypoint.sh /usr/local/bin/ COPY redis.conf /usr/local/etc/ #for config rewrite RUN chmod 777 /usr/local/etc/redis.conf RUN chmod +x /usr/local/bin/entrypoint.sh #入口 ENTRYPOINT [&quot;/usr/local/bin/entrypoint.sh&quot;] #命令 CMD [&quot;redis-server&quot;, &quot;/usr/local/etc/redis.conf&quot;]docker build -t codewj/redis-cluster:1.0 . 上传到私服docker tag codewj/redis-cluster:1.0 192.168.2.5:5000/codewj-redis-cluster docker push 192.168.2.5:5000/codewj-redis-cluster上传到docker hubdocker login 输入https://hub.docker.com/账户密码 docker tag onejane-jdk1.8 onejane/onejane-jdk1.8 docker push onejane/onejane-jdk1.8:latest docker logout Redisdocker run -d –privileged=true -p 6379:6379 -v $PWD/redis.conf:/etc/redis/redis.conf -v $PWD/data:/data –name redis redis redis-server /etc/redis/redis.conf –appendonly yeshttp://download.redis.io/redis-stable/redis.conf bind 0.0.0.0 port 6379 daemonize no appendonly yes protected-mode no 手动部署 mvn clean install deploy docker:build -DpushImage docker run -di --name=panchip -v /tmp/saas:/tmp/saas --net=host 192.168.2.7:5000/panchip:1.0.0-SNAPSHOT docker export panchip &gt; /home/panchip.tar 容器打成tar cat panchip.tar | docker import - panchip tar转镜像]]></content>
      <categories>
        <category>持续集成</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitLab+jenkins+docker]]></title>
    <url>%2F2019%2F08%2F20%2FGitLab-jenkins-docker%2F</url>
    <content type="text"><![CDATA[GitLab+jenkins+docker自动发布 http://192.168.2.5:181/view/all/newJob 构建一个maven项目ht-micro-record-service-note-provider添加jenkins主机公钥到gitlab，并生成全局凭据1.Username with password root/1234562.SSH Username with private key Enter Directly,添加gitlab服务器私钥 parent.relativePath修改为，发布单个服务时设定一个空值将始终从仓库中获取，不从本地路径获取 基础服务&lt;build&gt; &lt;finalName&gt;ht-micro-record-commons-domain&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;classifier&gt;exec&lt;/classifier&gt; &lt;!--被依赖的包加该配置--&gt; &lt;mainClass&gt;com.ht.micro.record.commons.CommonsMapperApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 微服务提供者 &lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;name&gt;Nexus Release Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;name&gt;Nexus Snapshot Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt; &lt;build&gt; &lt;finalName&gt;ht-micro-record-service-note-consumer&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;!--被其他服务引用必须增加该配置--&gt; &lt;classifier&gt;exec&lt;/classifier&gt; &lt;mainClass&gt;com.ht.micro.record.service.consumer.NoteConsumerServiceApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- docker的maven插件，官网 https://github.com/spotify/docker-maven-plugin --&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt; &lt;version&gt;0.4.13&lt;/version&gt; &lt;configuration&gt; &lt;imageName&gt;192.168.2.5:5000/${project.artifactId}:${project.version}&lt;/imageName&gt; &lt;baseImage&gt;jdk1.8&lt;/baseImage&gt; &lt;entryPoint&gt;[&quot;java&quot;, &quot;-jar&quot;,&quot;/${project.build.finalName}.jar&quot;]&lt;/entryPoint&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/&lt;/targetPath&gt; &lt;directory&gt;${project.build.directory}&lt;/directory&gt; &lt;include&gt;${project.build.finalName}.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;dockerHost&gt;http://192.168.2.5:2375&lt;/dockerHost&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; Jenkinsexport app_name=”ht-micro-record-service-note-consumer”export app_version=”1.0.0-SNAPSHOT”sh /usr/local/docker/ht-micro-record/deploy.sh #!/bin/bash # 判断项目是否在运行 DATE=`date +%s` last_app_name=`echo &quot;$app_name-expire-$DATE&quot;` if docker ps | grep $app_name;then docker stop $app_name docker rename $app_name $last_app_name docker run -di --name=$app_name -v /opt/template/:/opt/template/ -e JAVA_OPTS=&#39;-Xmx3g -Xms3g&#39; --net=host 192.168.2.7:5000/$app_name:$app_version # 判断项目是否存在 elif docker ps -a | grep $app_name;then docker start $app_name else docker run -di --name=$app_name -v /opt/template/:/opt/template/ -e JAVA_OPTS=&#39;-Xmx3g -Xms3g&#39; --net=host 192.168.2.7:5000/$app_name:$app_version fi vim /usr/local/bin/dokill docker images | grep -e $*|awk &#39;{print $3}&#39;|sed &#39;2p&#39;|xargs docker rmi chmod a+x /usr/local/bin/dokill dokill tensquare_recruit Build时必须clean，项目名必须小写 Q1:Failed to execute goal on project : Could not resolve dependencies for 对最父级项目clean install，再最子项目clean install Q2: repackage failed: Unable to find main class -&gt; [Help 1] 构建显示缺少主类 @SpringBootApplication public class CommonsApplication { public static void main(String[] args) { } }分布式构建192.168.2.7:181 访问jenkins，系统管理-节点管理-新建节点ln -s /data2/jdk/bin/java /usr/bin/java 分布式部署通过网关负载时，需要重新部署网关gateway服务。 免密登陆192.168.2.7 ssh-keygen -t rsa ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.2.5 ssh root@192.168.2.5 新建jenkins任务192.168.2.7ht-micro-record-service-user-2.5vim /data2/deploy2.5.sh #!/bin/bash echo &quot;------------------服务信息---------------&quot; echo $1 $2 echo &quot;------------------开始部署---------------&quot; ssh root@192.168.2.5 sh /data2/deploy.sh $1 $2 192.168.2.5vim /data2/deploy.sh #!/bin/bash # 判断项目是否在运行 if docker ps | grep $1;then docker stop $1|xargs docker rm docker rmi 192.168.2.7:5000/$1:$2 # 判断项目是否存在 elif docker ps -a | grep $1;then docker rm $1 docker rmi 192.168.2.7:5000/$1:$2 elif docker images|grep $1;then docker rmi 192.168.2.7:5000/$1:$2 fi docker run -di --name=$1 -v /opt/:/opt/ -e JAVA_OPTS=&#39;-Xmx3g -Xms3g&#39; --net=host 192.168.2.7:5000/$1:$2]]></content>
      <categories>
        <category>持续集成</category>
      </categories>
      <tags>
        <tag>gitlab</tag>
        <tag>jenkins</tag>
        <tag>docker</tag>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch]]></title>
    <url>%2F2019%2F08%2F20%2FElasticSearch%2F</url>
    <content type="text"><![CDATA[ElasticSearch基础及数据迁移 https://github.com/medcl/esm-abandoned/releases/download/v0.4.1/linux64-esm--2019.4.20.tar.gz 基本原理 写：发送一个请求到协调节点，对document进行hash路由到对应由primary shard的节点上，处理请求并同步到replica shard，协调节点检测同步后返回相应给客户端。读：发送一个请求到协调节点，根据doc id对document进行hash路由到对应node，通过随机轮询算法从primary和replica的shard中随机选择让读请求负载均衡，返回document给协调节点后给客户端。一个索引拆分为多个shard存储部分数据，每个shard由primary shard和replica shard组成，primary写入将同步到replica，类似kafka的partition副本。保证高可用。Es多个节点选举一个为master，管理和切换主副shard，若宕机则重新选举，并将宕机节点primary shard身份转移到其他机器的replica shard。重启将修改原primary为replica同步数据。 每个在文档上执行的写操作，包括删除，都会使其版本增加。 真正的删除时机： deleting a document doesn’t immediately remove the document from disk; it just marks it as deleted. Elasticsearch will clean up deleted documents in the background as you continue to index more data. 删除索引是会立即释放空间的，不存在所谓的“标记”逻辑。 删除文档的时候，是将新文档写入，同时将旧文档标记为已删除。 磁盘空间是否释放取决于新旧文档是否在同一个segment file里面，因此ES后台的segment merge在合并segment file的过程中有可能触发旧文档的物理删除。但因为一个shard可能会有上百个segment file，还是有很大几率新旧文档存在于不同的segment里而无法物理删除。想要手动释放空间，只能是定期做一下force merge，并且将max_num_segments设置为1。多机器集群搭建 vi /etc/sysctl.conf vm.max_map_count=262144 sysctl -pNode1 192.168.2.5最好挂载到大磁盘上 mkdir /usr/local/docker/ElasticSearch/data -p &amp;&amp; chmod 777 /usr/local/docker/ElasticSearch/data mkdir /usr/local/docker/ElasticSearch/config/ -p &amp;&amp; cd /usr/local/docker/ElasticSearch/config/ vi es.yml cluster.name: elasticsearch-cluster node.name: es-node1 network.bind_host: 0.0.0.0 network.publish_host: 192.168.2.5 http.port: 1800 transport.tcp.port: 1801 http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; node.master: true node.data: true discovery.zen.ping.unicast.hosts: [&quot;192.168.2.5:1801&quot;,&quot;192.168.2.6:1801&quot;] # 可配置多个 discovery.zen.minimum_master_nodes: 1 修改/usr/share/elasticsearch/config/jvm.options中-Xms10g -Xmx10g docker run -d -v /data3/elasticsearch/config/jvm.options:/usr/share/elasticsearch/config/jvm.options -v /data3/elasticsearch/config/es.yml:/usr/share/elasticsearch/config/elasticsearch.yml -v /data3/elasticsearch/data:/usr/share/elasticsearch/data --name es-node1 --net host --privileged elasticsearch:6.6.0 docker exec -it es-node1 bash elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.6.0/elasticsearch-analysis-ik-6.6.0.zip docker restart es-node1Node2 192.168.2.6mkdir /usr/local/docker/ElasticSearch/data -p &amp;&amp; chmod 777 /usr/local/docker/ElasticSearch/data mkdir /usr/local/docker/ElasticSearch/config/ -p &amp;&amp; cd /usr/local/docker/ElasticSearch/config/ vi es.yml cluster.name: elasticsearch-cluster node.name: es-node2 network.bind_host: 0.0.0.0 network.publish_host: 192.168.2.6 http.port: 1800 transport.tcp.port: 1801 http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; node.master: true node.data: true discovery.zen.ping.unicast.hosts: [&quot;192.168.2.5:1801&quot;,&quot;192.168.2.6:1801&quot;] # 可配置多个 discovery.zen.minimum_master_nodes: 1 # 集群最少需要有两个 node , 才能保证既可以不脑裂, 又可以高可用 修改/usr/share/elasticsearch/config/jvm.options中-Xms10g -Xmx10g docker run -d -v /data2/elasticsearch/config/jvm.options:/usr/share/elasticsearch/config/jvm.options -v /data2/elasticsearch/config/es.yml:/usr/share/elasticsearch/config/elasticsearch.yml -v /data3/elasticsearch/data:/usr/share/elasticsearch/data --name es-node2 --net host --privileged elasticsearch:6.6.0 docker exec -it es-node2 bash elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.6.0/elasticsearch-analysis-ik-6.6.0.zip docker restart es-node2http://192.168.2.5:1800/_cat/plugins 查看插件信息 esm数据迁移./esm -s http://192.168.2.6:9200 -d http://192.168.2.7:1800 -x p_answer_handle_alarm -w=5 -b=10 -c 10000; 同步数据 curl 192.168.2.7:1800/_cat/indices?v 查看所有索引信息 常见参数使用： ./esm -s http://192.168.2.6:9200 -d http://192.168.2.7:1800 -x t_record_analyze -y record_test --copy_settings --copy_mappings --shards=4 -w=5 -b=10 -c 10000;数据迁移完美方案通过两集群都建立同样索引，保证mappings和settings一致，再同步数据 curl -XGET 192.168.2.7:1800/p_answer_handle_alarm 查看索引信息 curl -XGET 192.168.2.7:1800/p_answer_handle_alarm/_search; curl -XGET http://192.168.2.5:1800/record_test/_settings?pretty 查看settings信息 PUT http://192.168.2.5:1800/record_set/_settings 修改副本数，片的信息分又重新做了调整，同curl -XPUT &#39;192.168.2.7:1800/record_set/_settings&#39; -d&#39;{&quot;index&quot;:{&quot;number_of_replicas&quot;:1}}&#39; { &quot;index&quot;:{ &quot;number_of_replicas&quot;:1 } } index.blocks.read_only //设置为 true 使索引和索引元数据为只读，false 为允许写入和元数据更改。 index.blocks.read // 设置为 true 可禁用对索引的读取操作 index.blocks.write //设置为 true 可禁用对索引的写入操作。 index.blocks.metadata // 设置为 true 可禁用索引元数据的读取和写入 index.mapping.total_fields.limit //1000 防止字段过多引起崩溃 curl -XGET http://192.168.2.5:1800/record_set/_mappings?pretty 查看mappings信息 http://192.168.2.5:1800/_cluster/settings?pretty 查看settings信息 PUT http:///192.168.2.5:1800/record_set/doc/_mapping 新增字段 { &quot;properties&quot;: { &quot;col1&quot;: { &quot;type&quot;: &quot;text&quot;, &quot;index&quot;: false } } } PUT 192.168.2.6:9200/test 新建索引 { &quot;mappings&quot;:{ &quot;doc&quot;:{ &quot;properties&quot;:{ &quot;jjbh&quot;:{&quot;type&quot;: &quot;keyword&quot;,&quot;index&quot;: &quot;true&quot;}, &quot;bccljg&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;index&quot;:&quot;true&quot;,&quot;analyzer&quot;: &quot;ik_max_word&quot;,&quot;search_analyzer&quot;: &quot;ik_max_word&quot;}, &quot;bjnr&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;index&quot;: &quot;true&quot;,&quot;analyzer&quot;:&quot;ik_max_word&quot;,&quot;search_analyzer&quot;:&quot;ik_max_word&quot;}, &quot;cjlb&quot;:{&quot;type&quot;: &quot;keyword&quot;,&quot;index&quot;: &quot;false&quot;} } } } } 192.168.2.7:1800/test/doc/{_id} 插入数据，_id不加默认随机字符串 { &quot;jjbh&quot;: &quot;4654132465&quot;, &quot;bccljg&quot;: &quot;我是一名合格的程序员&quot;, &quot;bjnr&quot;: &quot;今天天气真的好啊&quot;, &quot;cjlb&quot;: &quot;天上地下飞禽走兽&quot; } 192.168.2.7:1800/test/doc/{_id} 更新 { &quot;jjbh&quot;: &quot;11&quot;, &quot;bccljg&quot;: &quot;我是一名合格的程序员&quot;, &quot;bjnr&quot;: &quot;今天天气真的好啊&quot;, &quot;cjlb&quot;: &quot;天上地下飞禽走兽&quot; } PUT 192.168.2.5:1800/test 新建索引 { &quot;mappings&quot;:{ &quot;doc&quot;:{ &quot;properties&quot;:{ &quot;jjbh&quot;:{&quot;type&quot;: &quot;keyword&quot;,&quot;index&quot;: &quot;true&quot;}, &quot;bccljg&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;index&quot;:&quot;true&quot;,&quot;analyzer&quot;: &quot;ik_max_word&quot;,&quot;search_analyzer&quot;: &quot;ik_max_word&quot;}, &quot;bjnr&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;index&quot;: &quot;true&quot;,&quot;analyzer&quot;:&quot;ik_max_word&quot;,&quot;search_analyzer&quot;:&quot;ik_max_word&quot;}, &quot;cjlb&quot;:{&quot;type&quot;: &quot;keyword&quot;,&quot;index&quot;: &quot;false&quot;} } } } } ./esm -s http://192.168.2.6:9200 -d http://192.168.2.5:1800 -x test -y test -w=5 -b=10 -c 10000; 同步数据 POST 192.168.2.5:1800/test/doc 全局加ik分词器 { &quot;settings&quot;:{ &quot;analysis&quot;:{ &quot;analyzer&quot;:{ &quot;ik&quot;:{&quot;tokenizer&quot;: &quot;ik_max_keyword&quot;} } } } } POST 192.168.2.5:1800/test/doc/_search 查询 POST 192.168.2.5:1800/test/doc/_delete_by_query 删除 精确查询/删除 { &quot;query&quot;:{ &quot;term&quot;:{ &quot;bccljg.keyword&quot;:&quot;我是一名合格的程序员&quot; } } } 模糊查询/删除 { &quot;query&quot;: { &quot;match&quot;: { &quot;bccljg&quot;: &quot;合格&quot; } } } 正则模糊查询 { &quot;query&quot;: { &quot;regexp&quot;: { &quot;bccljg.keyword&quot;: &quot;.*我是.*&quot; } } } 文本开头查询 { &quot;query&quot;: { &quot;prefix&quot;: { &quot;bccljg.keyword&quot;: &quot;我是&quot; } } } Q1:若数据结果不一致，可能是磁盘不足。Q2:”caused_by”:{“type”:”search_context_missing_exception”,”reason”:”No search context found for id [69218326]”}} ./esm -s http://192.168.2.6:9200 -d http://192.168.2.7:1800 -x t_record_analyze -y t_record_analyze -w=5 -b=10 -c 10000; ./esm -s http://192.168.2.6:9200 -d http://192.168.2.7:1800 -x t_neo4j_data -y t_neo4j_data -w=5 -b=10 -c 10000; ./esm -s http://192.168.2.6:9200 -d http://192.168.2.7:1800 -x t_data_analysis -y t_data_analysis -w=5 -b=10 -c 10000; ./esm -s http://192.168.2.6:9200 -d http://192.168.2.7:1800 -x t_cjjxq -y t_cjjxq -w=5 -b=10 -c 10000; ./esm -s http://192.168.1.225:9200 -d http://192.168.2.7:1800 -x t_alarm_analysis_result -y t_alarm_analysis_result -w=5 -b=10 -c 10000; ./esm -s http://192.168.2.6:9200 -d http://192.168.2.7:1800 -x p_answer_handle_alarm -y p_answer_handle_alarm -t=10m -w=5 -b=10 -c 5000; ./esm -s http://192.168.2.6:9200 -d http://192.168.2.7:1800 -x city_answer_handle_alarm -y city_answer_handle_alarm -t=10m -w=5 -b=10 -c 5000;scroll time 超时，设置-t参数，默认是1m 同步后的数据结构 logstashES单节点安装docker run -di --name=tensquare_es -p 9200:9200 -p 9300:9300 --privileged docker.io/elasticsearch:6.6.0 docker cp tensquare_es:/usr/share/elasticsearch/config/elasticsearch.yml /usr/share/elasticsearch.yml docker stop tensquare_es docker rm tensquare_es vim /usr/share/elasticsearch.yml transport.host: 0.0.0.0 docker restart tensquare_es vim /etc/security/limits.conf * soft nofile 65536 * hard nofile 65536 nofile是单个进程允许打开的最大文件个数 soft nofile 是软限制 hard nofile是硬限制 vim /etc/sysctl.conf vm.max_map_count=655360 sysctl -p 修改内核参数立马生效 我们需要以文件挂载的 方式创建容器才行，这样我们就可以通过修改宿主机中的某个文件来实现对容器内配置 文件的修改 docker run -di --name=tensquare_es -p 9200:9200 -p 9300:9300 --privileged -v /usr/share/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml docker.io/elasticsearch:6.6.0 docker restart tensquare_es 才可以远程连接使用 同步mysqlhttps://www.elastic.co/cn/downloads/past-releases/logstash-6-6-0tar zxf logstash-6.6.0.tar.gz &amp;&amp; cd /root/logstash-6.6.0/binmkdir mysql &amp;&amp; cd mysql &amp;&amp; vim mysql.conf jdbc { # mysql jdbc connection string to our backup databse jdbc_connection_string =&gt; &quot;jdbc:mysql://192.168.2.5:185/ht_micro_record?characterEncoding=UTF8&quot; # the user we wish to excute our statement as jdbc_user =&gt; &quot;root&quot; jdbc_password =&gt; &quot;123456&quot; # the path to our downloaded jdbc driver jdbc_driver_library =&gt; &quot;/root/logstash-6.6.0/bin/mysql/mysql-connector-java-5.1.47.jar&quot; # the name of the driver class for mysql jdbc_driver_class =&gt; &quot;com.mysql.jdbc.Driver&quot; jdbc_paging_enabled =&gt; &quot;true&quot; jdbc_page_size =&gt; &quot;500000&quot; #以下对应着要执行的sql的绝对路径。 #statement_filepath =&gt; &quot;select id,title,content from tb_article&quot; statement =&gt; &quot;SELECT id,applyer_police_num,applyer_name FROM t_apply&quot; #定时字段 各字段含义（由左至右）分、时、天、月、年，全部为*默认含义为每分钟都更新（测试结果，不同的话请留言指出） schedule =&gt; &quot;* * * * *&quot; } } output { elasticsearch { #ESIP地址与端口 hosts =&gt; &quot;192.168.3.224:9200&quot; #ES索引名称（自己定义的） index =&gt; &quot;t_apply&quot; #自增ID编号 document_id =&gt; &quot;%{id}&quot; document_type =&gt; &quot;article&quot; } stdout { #以JSON格式输出 codec =&gt; json_lines } } nohup ./logstash -f mysql/mysql.conf &amp; 同步es/root/logstash-6.6.0/bin/es/es.conf input { elasticsearch { hosts =&gt; [&quot;192.168.2.5:1800&quot;,&quot;192.168.2.7:1800&quot;] index =&gt; &quot;t_neo4j_data&quot; size =&gt; 1000 scroll =&gt; &quot;1m&quot; codec =&gt; &quot;json&quot; docinfo =&gt; true } } filter { mutate { remove_field =&gt; [&quot;@timestamp&quot;, &quot;@version&quot;] } } output { elasticsearch { hosts =&gt; [&quot;192.168.3.224:9200&quot;] index =&gt; &quot;%{[@metadata][_index]}&quot; } stdout { codec =&gt; rubydebug { metadata =&gt; true } } } ./logstash -f es/es.conf –path.data ../logs/ elasticdumpwget https://nodejs.org/dist/v8.11.2/node-v8.11.2-linux-x64.tar.xz tar xf node-v8.11.2-linux-x64.tar.xz -C /usr/local/ ln -s /usr/local/node-v8.11.2-linux-x64/bin/npm /usr/local/bin/npm ln -s /usr/local/node-v8.11.2-linux-x64/bin/node /usr/local/bin/node npm install -g cnpm --registry=https://registry.npm.taobao.org ln -s /usr/local/node-v8.11.2-linux-x64/bin/cnpm /usr/local/bin/cnpm cnpm init -f cnpm install elasticdump cd node_modules/elasticdump/bin ./elasticdump \ --input=http://192.168.2.6:9200/t_record_analyze \ --output=http://192.168.3.224:9200/t_record_analyze \ --type=analyzer ./elasticdump \ --input=http://192.168.2.6:9200/t_record_analyze \ --output=http://192.168.3.224:9200/t_record_analyze \ --type=mapping ./elasticdump \ --input=http://192.168.2.6:9200/t_record_analyze \ --output=http://192.168.3.224:9200/t_record_analyze \ --type=data ./elasticdump \ --input=http://192.168.2.6:9200:9200/t_record_analyze \ --output=data.json \ --searchBody &#39;{&quot;query&quot;:{&quot;term&quot;:{&quot;username&quot;: &quot;admin&quot;}}} ./elasticdump \ --input=./data.json \ --output=http://192.168.2.7:1800整合springboot父pom &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.2.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt;子pom &lt;!--elasticsearch--&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.java.dev.jna&lt;/groupId&gt; &lt;artifactId&gt;jna&lt;/artifactId&gt; &lt;!-- &lt;version&gt;3.0.9&lt;/version&gt; --&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt; &lt;artifactId&gt;gson&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt;配置文件bootstrap.yml es: nodes: 192.168.2.5:1800,192.168.2.7:1800 host: 192.168.2.6,192.168.2.7 port: 1801,1801 types: doc clusterName: elasticsearch-cluster #笔录分析库 blDbName: t_record_analyze #接警信息库 jjDbName: p_answer_alarm #处警信息库 cjDbName: p_handle_alarm #警情分析结果信息库 jqfxDbName: t_alarm_analysis_result #标准化分析 cjjxqDbName: t_cjjxq #接处警整合库 jcjDbName: p_answer_handle_alarm #警情分析 daDbName: t_data_analysis #警情结果表(neo4j使用) neo4jData: t_neo4j_data #市局接处警表 cityDbName: city_answer_handle_alarm配置类com/ht/micro/record/service/dubbo/provider/utils/EsConfig.java @Service public class EsConfig { @Value(&quot;${es.nodes}&quot;) private String nodes; @Value(&quot;${es.host}&quot;) private String host; @Value(&quot;${es.port}&quot;) private String port; @Value(&quot;${es.blDbName}&quot;) private String blDbName; @Value(&quot;${es.jjDbName}&quot;) private String jjDbName; @Value(&quot;${es.cjDbName}&quot;) private String cjDbName; @Value(&quot;${es.jqfxDbName}&quot;) private String jqfxDbName; @Value(&quot;${es.clusterName}&quot;) private String clusterName; @Value(&quot;${es.jjDbName}&quot;) private String answerDbName; @Value(&quot;${es.cjDbName}&quot;) private String handleDbName; @Value(&quot;${es.cjjxqDbName}&quot;) private String cjjxqDbName; @Value(&quot;${es.jcjDbName}&quot;) private String jcjDbName; @Value(&quot;${es.daDbName}&quot;) private String daDbName; @Value(&quot;${es.daDbName}&quot;) private String fxDbName; @Value(&quot;${es.types}&quot;) private String types; @Value(&quot;${es.neo4jData}&quot;) private String neo4jData; @Value(&quot;${es.cityDbName}&quot;) private String cityDbName; } 配置类 com/ht/micro/record/service/dubbo/provider/utils/ElasticClientSingleton.java @Service public class ElasticClientSingleton { protected final Logger logger = LoggerFactory.getLogger(ElasticClientSingleton.class); private AtomicInteger atomicPass = new AtomicInteger(); // 0 未初始化, 1 已初始化 private TransportClient transportClient; private BulkRequestBuilder bulkRequest; public synchronized void init(EsConfig esConfig) { try { String ipArray = esConfig.getHost(); String portArray = esConfig.getPort(); String cluster = esConfig.getClusterName(); Settings settings = Settings.builder() .put(&quot;cluster.name&quot;, cluster) //连接的集群名 .put(&quot;client.transport.ignore_cluster_name&quot;, true) .put(&quot;client.transport.sniff&quot;, false)//如果集群名不对，也能连接 .build(); transportClient = new PreBuiltTransportClient(settings); String[] ips = ipArray.split(&quot;,&quot;); String[] ports = portArray.split(&quot;,&quot;); for (int i = 0; i &lt; ips.length; i++) { transportClient.addTransportAddress(new TransportAddress(InetAddress.getByName(ips[i]), Integer .parseInt(ports[i]))); } atomicPass.set(1); } catch (Exception e) { e.printStackTrace(); logger.error(e.getMessage()); atomicPass.set(0); destroy(); } } public void destroy() { if (transportClient != null) { transportClient.close(); transportClient = null; } } public BulkRequestBuilder getBulkRequest(EsConfig esConfig) { if (atomicPass.get() == 0) { // 初始化 init(esConfig); } bulkRequest = transportClient.prepareBulk(); return bulkRequest; } public TransportClient getTransportClient(EsConfig esConfig) { if (atomicPass.get() == 0) { // 初始化 init(esConfig); } return transportClient; } }配置类 com/ht/micro/record/service/dubbo/provider/utils/ElasticClient.java @Service public class ElasticClient { @Autowired private EsConfig esConfig; private static final Logger logger = LoggerFactory.getLogger(ElasticClient.class); private static BulkRequestBuilder bulkRequest; @Autowired private ElasticClientSingleton elasticClientSingleton; @PostConstruct public void init() { bulkRequest = elasticClientSingleton.getBulkRequest(esConfig); } public static String postRequest(String url, String query) { RestTemplate restTemplate = new RestTemplate(); MediaType type = MediaType.parseMediaType(&quot;application/json; charset=UTF-8&quot;); HttpHeaders headers = new HttpHeaders(); headers.setContentType(type); headers.add(&quot;Accept&quot;, MediaType.APPLICATION_JSON.toString()); HttpEntity&lt;String&gt; formEntity = new HttpEntity&lt;String&gt;(query, headers); String result = restTemplate.postForObject(url, formEntity, String.class); return result; } /** * action 提交操作 */ // public void action() { // int reqSize = bulkRequest.numberOfActions(); // //读不到数据了，默认已经全部读取 // if (reqSize == 0) { // bulkRequest.request().requests().clear(); // } // bulkRequest.setTimeout(new TimeValue(1000 * 60 * 5)); //超时30秒 // BulkResponse bulkResponse = bulkRequest.execute().actionGet(); // //持久化异常 // if (bulkResponse.hasFailures()) { // logger.error(bulkResponse.buildFailureMessage()); // bulkRequest.request().requests().clear(); // } // logger.info(&quot;import over....&quot; + bulkResponse.getItems().length); // } }测试com/ht/micro/record/service/dubbo/provider/controller/ProviderController.java @Autowired ElasticClientSingleton elasticClientSingleton; @Autowired EsConfig esConfig; @GetMapping(&quot;/test&quot;) public void test(){ SearchRequestBuilder srb = elasticClientSingleton.getTransportClient(esConfig).prepareSearch(esConfig.getCityDbName()).setTypes(esConfig.getTypes()); TermsQueryBuilder cjbsBuilder = QueryBuilders.termsQuery(&quot;cjbs&quot;, &quot;4&quot;); SearchResponse weekSearchResponse = srb.setQuery(cjbsBuilder).execute().actionGet(); System.out.println(weekSearchResponse.getHits().getTotalHits()); } ElasticSearch-SQL使用sql操作elasticsearch https://github.com/NLPchina/elasticsearch-sqlhttps://artifacts.elastic.co/maven/org/elasticsearch/client/x-pack-transport/6.6.0/x-pack-transport-6.6.0.jar 在github上查找相关案例，需要相关的jar并没有找到，后去elasticsearch-sql-6.6.0.0.zip插件包中找到elasticsearch-sql-6.6.0.0.jar，启动测试程序 Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/elasticsearch/xpack/client/PreBuiltXPackTransportClient at com.alibaba.druid.pool.ElasticSearchDruidDataSource.createPhysicalConnection(ElasticSearchDruidDataSource.java:686) at com.alibaba.druid.pool.ElasticSearchDruidDataSource.createPhysicalConnection(ElasticSearchDruidDataSource.java:632) at com.alibaba.druid.pool.ElasticSearchDruidDataSource.init(ElasticSearchDruidDataSource.java:579) at com.alibaba.druid.pool.ElasticSearchDruidDataSource.getConnection(ElasticSearchDruidDataSource.java:930) at com.alibaba.druid.pool.ElasticSearchDruidDataSource.getConnection(ElasticSearchDruidDataSource.java:926) at com.ht.micro.record.service.dubbo.provider.controller.Test.main(Test.java:20) Caused by: java.lang.ClassNotFoundException: org.elasticsearch.xpack.client.PreBuiltXPackTransportClient at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 6 more x-pack-transportx-pack-core后继续报错，陆续加入x-pack-core，unboundid-ldapsdk，bcpkix-jdk15on，bcprov-jdk15on等依赖才跑通流程。 安装elasticsearch-sqldocker exec -it es-node1 bash elasticsearch-plugin install https://github.com/NLPchina/elasticsearch-sql/releases/download/6.6.0.0/elasticsearch-sql-6.6.0.0.zip docker restart es-node1 本地安装jarmvn install:install-file -DgroupId=org.elasticsearch.plugin -DartifactId=x-pack-core -Dversion=6.6.0 -Dpackaging=jar -Dfile=x-pack-core-6.6.0.jarmvn install:install-file -DgroupId=org.elasticsearch.client -DartifactId=x-pack-transport -Dversion=6.6.0 -Dpackaging=jar -Dfile=x-pack-transport-6.6.0.jarmvn install:install-file -DgroupId=org.nlpcn -DartifactId=elasticsearch-sql -Dversion=6.6.0.0 -Dpackaging=jar -Dfile=elasticsearch-sql-6.6.0.0.jar 引入依赖&lt;dependency&gt; &lt;groupId&gt;org.nlpcn&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-sql&lt;/artifactId&gt; &lt;version&gt;6.6.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.15&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-client&lt;/artifactId&gt; &lt;version&gt;6.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;6.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;transport&lt;/artifactId&gt; &lt;version&gt;6.6.0&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.elasticsearch.plugin&lt;/groupId&gt; &lt;artifactId&gt;transport-netty4-client&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;artifactId&gt;elasticsearch-rest-client&lt;/artifactId&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.plugin&lt;/groupId&gt; &lt;artifactId&gt;transport-netty4-client&lt;/artifactId&gt; &lt;version&gt;6.6.0&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.38&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.unboundid&lt;/groupId&gt; &lt;artifactId&gt;unboundid-ldapsdk&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.bouncycastle&lt;/groupId&gt; &lt;artifactId&gt;bcprov-jdk15on&lt;/artifactId&gt; &lt;version&gt;1.58&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.bouncycastle&lt;/groupId&gt; &lt;artifactId&gt;bcpkix-jdk15on&lt;/artifactId&gt; &lt;version&gt;1.58&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;x-pack-transport&lt;/artifactId&gt; &lt;version&gt;6.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.plugin&lt;/groupId&gt; &lt;artifactId&gt;x-pack-core&lt;/artifactId&gt; &lt;version&gt;6.6.0&lt;/version&gt; &lt;/dependency&gt; 测试@RunWith(SpringRunner.class) @SpringBootTest(classes={DubboProviderApplication .class})// 指定启动类 public class ElasticSearchSql { @Test public void testselect() throws Exception { Properties properties = new Properties(); properties.put(&quot;url&quot;, &quot;jdbc:elasticsearch://192.168.2.7:1801,192.168.2.5:1801/&quot;); properties.put(PROP_CONNECTIONPROPERTIES, &quot;client.transport.ignore_cluster_name=true&quot;); DruidDataSource dds = (DruidDataSource) ElasticSearchDruidDataSourceFactory.createDataSource(properties); dds.setInitialSize(1); Connection connection = dds.getConnection(); String sql2 = &quot;select * FROM t_word_freq limit 10&quot;; PreparedStatement ps = connection.prepareStatement(sql2); ResultSet resultSet = ps.executeQuery(); while (resultSet.next()) { //sql对应输出 System.out.println(resultSet.getString(&quot;jjbh&quot;) ); } ps.close(); connection.close(); dds.close(); } } 由于springboot启动时自动加载druid autoconfigration,而低版本druid配合springboot2.x报错ClassNotFoundException Log4j2Filter，需要手动exclude druid包 &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-commons-service&lt;/artifactId&gt; &lt;version&gt;${project.parent.version}&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.16&lt;/version&gt; &lt;/dependency&gt; Q1:重启服务 停止所有index服务 执行curl -XPUT $url/_cluster/settings?pretty -d ‘{“transient” : {“cluster.routing.allocation.enable” : “none”}}’ 执行curl -XPOST $url/_flush/synced?pretty 重启ES集群 等待集群分片全部分配成功，执行curl -XPUT $url/_cluster/settings?pretty -d ‘{“transient” : {“cluster.routing.allocation.enable” : “all”}}’ 开启所有index服务 PUT http://192.168.2.7:1800/_cluster/settings 禁止分片分配。这一步阻止 Elasticsearch 再平衡缺失的分片，直到你告诉它可以进行了。 { &quot;transient&quot; : { &quot;cluster.routing.allocation.enable&quot; : &quot;none&quot; } }启动完毕后PUT http://192.168.2.7:1800/_cluster/settings 重启分片分配 { &quot;transient&quot; : { &quot;cluster.routing.allocation.enable&quot; : &quot;all&quot; } }Q2Elasticsearch默认安装后设置的内存是1GB，这是远远不够用于生产环境的。有两种方式修改Elasticsearch的堆内存： 设置环境变量：export ES_HEAP_SIZE=10g 在es启动时会读取该变量； 启动时作为参数传递给es： ./bin/elasticsearch -Xmx10g -Xms10g]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>springboot</tag>
        <tag>elastisearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-Cluster]]></title>
    <url>%2F2019%2F08%2F20%2FRedis-Cluster%2F</url>
    <content type="text"><![CDATA[redis-cluster集群搭建使用 支撑多个master node，每个master挂载多个slave，master写对应slave读，每个master都有slave节点，若挂掉则将某个slave转为master。与相比Sentinel（哨兵）实现的高可用，集群（cluster）更多的是强调数据的分片或者是节点的伸缩性，如果在集群的主节点上加入对应的从节点，集群还可以自动故障转移。主从复制： 通过把这个RDB文件或AOF文件传给slave服务器，slave服务器重新加载RDB文件，来实现复制的功能！当建立一个从服务器后，从服务器会想主服务器发送一个SYNC的命令，主服务器接收到SYNC命令之后会执行BGSAVE，然后保存到RDB文件，然后发送到从服务器！收到RDB文件然后就载入到内存！ Redis 集群中内置了 16384 个哈希槽，当需要在 Redis 集群中放置一个 key-value 时，redis 先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点 集群中所有master参与,如果半数以上master节点与master节点通信超过(cluster-node-timeout),认为当前master节点挂掉.如果集群任意master挂掉,且当前master没有slave.集群进入fail状态,也可以理解成集群的slot映射[0-16383]不完成时进入fail状态如果集群超过半数以上master挂掉，无论是否有slave集群进入fail状态. Sentinel和Cluster区别 Redis-Sentinel(哨兵模式)是Redis官方推荐的高可用性(HA)解决方案，当用Redis做Master-slave的高可用方案时，假如master宕机了，Redis本身(包括它的很多客户端)都没有实现自动进行主备切换，而Redis-sentinel本身也是一个独立运行的进程，它能监控多个master-slave集群，发现master宕机后能进行自动切换。 Redis-Cluster当遇到单机内存、并发、流量等瓶颈时，可以采用Cluster架构达到负载均衡的目的。分布式集群首要解决把整个数据集按照分区规则映射到多个节点的问题，即把数据集划分到多个节点上，每个节点负责整个数据的一个子集。 多机集群redis-cluster，也叫分布式redis集群，可以有多个master，数据分片分布在这些master上。 192.168.2.5vim /usr/local/docker/redis/docker-compose.yml version: &#39;3&#39; services: redis1: image: publicisworldwide/redis-cluster container_name: redis1 network_mode: host restart: always volumes: - ./8001/data:/data environment: - REDIS_PORT=8001 redis2: image: publicisworldwide/redis-cluster container_name: redis2 network_mode: host restart: always volumes: - ./8002/data:/data environment: - REDIS_PORT=8002 redis3: image: publicisworldwide/redis-cluster container_name: redis3 network_mode: host restart: always volumes: - ./8003/data:/data environment: - REDIS_PORT=8003docker-compose up -d 192.168.2.7vim /usr/local/docker/redis/docker-compose.yml version: &#39;3&#39; services: redis1: image: publicisworldwide/redis-cluster network_mode: host container_name: redis4 restart: always volumes: - ./8004/data:/data environment: - REDIS_PORT=8004 redis2: image: publicisworldwide/redis-cluster network_mode: host container_name: redis5 restart: always volumes: - ./8005/data:/data environment: - REDIS_PORT=8005 redis3: image: publicisworldwide/redis-cluster network_mode: host container_name: redis6 restart: always volumes: - ./8006/data:/data environment: - REDIS_PORT=8006docker-compose up -d 启动方式直接启动docker run --rm -it inem0o/redis-trib create --replicas 1 192.168.2.5:8001 192.168.2.5:8002 192.168.2.5:8003 192.168.2.7:8004 192.168.2.7:8005 192.168.2.7:8006 docker exec -it redis1 redis-cli -h 127.0.0.1 -p 8001 -c set a 100 cluster info cluster nodes 自动指定master slave 指定masterdocker run --rm -it inem0o/redis-trib create 192.168.2.5:8001 192.168.2.5:8002 192.168.2.5:8003 docker run --rm -it inem0o/redis-trib add-node --slave --master-id 84c3b7ecbc4933e1368a6927f26c79ecc76810b3 192.168.2.7:8004 192.168.2.5:8001 docker run --rm -it inem0o/redis-trib add-node --slave --master-id 716f11f2971e9494183937abd61f7a4baf0b3959 192.168.2.7:8005 192.168.2.5:8002 docker run --rm -it inem0o/redis-trib add-node --slave --master-id c93060613a8f1531c82b97d97eeac402048f0b25 192.168.2.7:8006 192.168.2.5:8003 docker run --rm -it inem0o/redis-trib info 192.168.2.5:8001 docker run --rm -it inem0o/redis-trib help 若同一台宿主机，不想使用host模式同一台，也可以把network_mode去掉，但就要加ports映射。redis-cluster的节点端口共分为2种，一种是节点提供服务的端口，如6379；一种是节点间通信的端口，固定格式为：10000+6379。 docker-compose.yml version: &#39;3&#39; services: redis1: image: publicisworldwide/redis-cluster restart: always volumes: - /app/app/redis/8001/data:/data environment: - REDIS_PORT=8001 ports: - &#39;8001:8001&#39; - &#39;18001:18001&#39; redis2: image: publicisworldwide/redis-cluster restart: always volumes: - /app/app/redis/8002/data:/data environment: - REDIS_PORT=8002 ports: - &#39;8002:8002&#39; - &#39;18002:18002&#39; redis3: image: publicisworldwide/redis-cluster restart: always volumes: - /app/app/redis/8003/data:/data environment: - REDIS_PORT=8003 ports: - &#39;8003:8003&#39; - &#39;18003:18003&#39; redis4: image: publicisworldwide/redis-cluster restart: always volumes: - /app/app/redis/8004/data:/data environment: - REDIS_PORT=8004 ports: - &#39;8004:8004&#39; - &#39;18004:18004&#39; redis5: image: publicisworldwide/redis-cluster restart: always volumes: - /app/app/redis/8005/data:/data environment: - REDIS_PORT=8005 ports: - &#39;8005:8005&#39; - &#39;18005:18005&#39; redis6: image: publicisworldwide/redis-cluster restart: always volumes: - /app/app/redis/8006/data:/data environment: - REDIS_PORT=8006 ports: - &#39;8006:8006&#39; - &#39;18006:18006&#39;自定义Redis集群制作redis镜像vim entrypoint.sh #!/bin/sh #只作用于当前进程,不作用于其创建的子进程 set -e #$0--Shell本身的文件名 $1--第一个参数 $@--所有参数列表 # allow the container to be started with `--user` if [ &quot;$1&quot; = &#39;redis-server&#39; -a &quot;$(id -u)&quot; = &#39;0&#39; ]; then sed -i &#39;s/REDIS_PORT/&#39;$REDIS_PORT&#39;/g&#39; /usr/local/etc/redis.conf chown -R redis . #改变当前文件所有者 exec gosu redis &quot;$0&quot; &quot;$@&quot; #gosu是sudo轻量级”替代品” fi exec &quot;$@&quot;vim redis.conf #端口 port REDIS_PORT #开启集群 cluster-enabled yes #配置文件 cluster-config-file nodes.conf cluster-node-timeout 5000 #更新操作后进行日志记录 appendonly yes #设置主服务的连接密码 # masterauth #设置从服务的连接密码 # requirepassvi Dockerfile #基础镜像 FROM redis #修复时区 RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime RUN echo &#39;Asia/Shanghai&#39; &gt;/etc/timezone #环境变量 ENV REDIS_PORT 8000 #ENV REDIS_PORT_NODE 18000 #暴露变量 EXPOSE $REDIS_PORT #EXPOSE $REDIS_PORT_NODE #复制 COPY entrypoint.sh /usr/local/bin/ COPY redis.conf /usr/local/etc/ #for config rewrite RUN chmod 777 /usr/local/etc/redis.conf RUN chmod +x /usr/local/bin/entrypoint.sh #入口 ENTRYPOINT [&quot;/usr/local/bin/entrypoint.sh&quot;] #命令 CMD [&quot;redis-server&quot;, &quot;/usr/local/etc/redis.conf&quot;] docker build -t codewj/redis-cluster:1.0 .docker tag codewj/redis-cluster:1.0 192.168.2.5:5000/codewj-redis-clusterdocker push 192.168.2.5:5000/codewj-redis-cluster 192.168.2.5version: &#39;3&#39; services: redis1: image: 192.168.2.7:5000/onejane-redis-cluster container_name: redis1 network_mode: host restart: always volumes: - ./8001/data:/data environment: - REDIS_PORT=8001 redis2: image: 192.168.2.7:5000/onejane-redis-cluster container_name: redis2 network_mode: host restart: always volumes: - ./8002/data:/data environment: - REDIS_PORT=8002 redis3: image: 192.168.2.7:5000/onejane-redis-cluster container_name: redis3 network_mode: host restart: always volumes: - ./8003/data:/data environment: - REDIS_PORT=8003192.168.2.7version: &#39;3&#39; services: redis1: image: 192.168.2.7:5000/onejane-redis-cluster network_mode: host container_name: redis4 restart: always volumes: - ./8004/data:/data environment: - REDIS_PORT=8004 redis2: image: 192.168.2.7:5000/onejane-redis-cluster network_mode: host container_name: redis5 restart: always volumes: - ./8005/data:/data environment: - REDIS_PORT=8005 redis3: image: 192.168.2.7:5000/onejane-redis-cluster network_mode: host container_name: redis6 restart: always volumes: - ./8006/data:/data environment: - REDIS_PORT=8006]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>redis-cluster</tag>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven]]></title>
    <url>%2F2019%2F08%2F20%2Fmaven%2F</url>
    <content type="text"><![CDATA[maven基本用法 一个GroupId（项目）下面可以有很多个ArtifactId（模块），每个ArtifactId（模块）会有很多个Version（版本），每个Version（版本）一般被Packaging（打包）为jar、war、pom中的一种。 父模块的packaging必须为pom,packaging的默认值为jar module“组织”子模块,基于父模块pom的子模块相对目录名，不是子模块的artifactId 多模块deploy的时候，根据子模块的相互依赖关系整理一个build顺序，然后依次build，独立模块之间根据配置顺序build 子pom 会直接继承 父pom 中声明的属性 父pom 中仅仅使用dependencies 中dependency 依赖的包，会被 子pom 直接继承（不需要显式依赖） 父pom 中仅仅使用plugins 中plugin依赖的插件，会被 子pom 直接继承（不需要显式依赖） 父pom中可以使用dependecyManagement和pluginManagement来统一管理jar包和插件pugin，不会被子pom 直接继承。子pom如果希望继承该包或插件，则需要显式依赖，同时像 等配置项可以不被显式写出，默认从父pom继承,同pluginManagement mvn 命令对应着maven项目生命周期，顺序为compile、test、package、install、depoly，执行其中任一周期，都会把前面周期 统一执行 具有依赖传递性，而不具有依赖传递性 安装jar包到本地 mvn install:install-file -DgroupId=org.csource.fastdfs -DartifactId=fastdfs -Dversion=1.2 -Dpackaging=jar -Dfile=D:\Project\pinyougou\fastDFSdemo\src\main\webapp\WEB-INF\lib\fastdfs_client_v1.20.jar properties&lt;properties&gt; &lt;spring.version&gt;2.5&lt;/spring.version&gt; &lt;/properties&gt; &lt;version&gt;${spring.version}&lt;/version&gt; pluginManagement父 &lt;pluginManagement&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.8.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; 子 &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; 由于 Maven 内置了 maven-compiler-plugin 与生命周期的绑定，因此子模块就不再需要任何 maven-compiler-plugin 的配置了。 资源打包配置 &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;${project.build.directory}/classes&lt;/targetPath&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;includes&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;include&gt;**/*.yml&lt;/include&gt; &lt;include&gt;**/*.txt&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt; &lt;/resources&gt; dependencyManagement&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.juvenxu.sample&lt;/groupId&gt; &lt;artifactid&gt;sample-dependency-infrastructure&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; 优势 Parent的dependencyManagement节点的作用是管理整个项目所需要用到的第三方依赖。只要约定了第三方依赖的坐标（GroupId:ArtifactId:version），后代模块即可通过GroupId:ArtifactId进行依赖的引入。其它元素如 version 和 scope 都能通过继承父 POM 的 dependencyManagement 得到这样能够避免依赖版本的冲突。当然，这里只是进行约定，并不会真正地引用依赖。 依赖统一管理(parent中定义，需要变动dependency版本，只要修改一处即可)； 代码简洁(子model只需要指定groupId、artifactId即可) dependencyManagement只会影响现有依赖的配置，但不会引入依赖，即子model不会继承parent中dependencyManagement所有预定义的depandency，只引入需要的依赖即可，简单说就是“按需引入依赖”或者“按需继承”；因此，在parent中严禁直接使用depandencys预定义依赖，坏处是子model会自动继承depandencys中所有预定义依赖；劣势单继承：maven的继承跟java一样，单继承，也就是说子model中只能出现一个parent标签；parent模块中，dependencyManagement中预定义太多的依赖，造成pom文件过长，而且很乱； 通过父pom的parent继承的方法，只能继承一个parent。实际开发中，用户很可能需要继承自己公司的标准parent配置，这个时候可以使用 scope=import 来实现多继承。 解决：scope=import只能用在dependencyManagement里面,且仅用于type=pom的dependency,要继承多个，可以在dependencyManagement通过非继承的方式来引入这段依赖管理配置，添加依赖scope=import，type=pom &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;1.3.3.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.test.sample&lt;/groupId&gt; &lt;artifactid&gt;base-parent1&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;自己的项目里面就不需要继承SpringBoot的module了，而可以继承自己项目的module了。 Scope provided 被依赖项目理论上可以参与编译、测试、运行等阶段，相当于compile，但是再打包阶段做了exclude的动作。,servlet-api和jsp-api都是由tomcat等servlet容器负责提供的包,这个 jar 包已由应用服务器提供,这里引入只用于开发,不进行打包 runtime被依赖项目无需参与项目的编译，但是会参与到项目的测试和运行,在编译的时候我们不需要 JDBC API 的 jar 包，而在运行的时候我们才需要 JDBC 驱动包。 compile（默认）被依赖项目需要参与到当前项目的编译，测试，打包，运行等阶段。打包的时候通常会包含被依赖项目。 test 被依赖项目仅仅参与测试相关的工作，包括测试代码的编译，执行,Junit 测试。 system 被依赖项不会从 maven 仓库中查找，而是从本地系统中获取，systemPath 元素用于制定本地系统中 jar 文件的路径发布 &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;!-- ID 名称必须要与 settings.xml 中 Servers 配置的 ID 名称保持一致。--&gt; &lt;name&gt;Nexus Release Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.5:182/repository/maven-releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;name&gt;Nexus Snapshot Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.5:182/repository/maven-snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt; mvn deploy发布到私服,在项目 pom.xml 中设置的版本号添加 SNAPSHOT 标识的都会发布为 SNAPSHOT 版本，没有 SNAPSHOT 标识的都会发布为 RELEASE 版本Nexus 3.0 不支持页面上传，可使用 maven 命令： mvn deploy:deploy-file -DgroupId=com.github.axet -DartifactId=kaptcha -Dversion=0.0.9 -Dpackaging=jar -Dfile=E:\kaptcha-0.0.9.jar -Durl=http://192.168.2.5:182/repository/maven-releases/ -DrepositoryId=nexus-releases 要求jar的pom中的repository.id和settings.xml中一致 上传第三方jarproxy：即你可以设置代理，设置了代理之后，在你的nexus中找不到的依赖就会去配置的代理的地址中找hosted：你可以上传你自己的项目到这里面group：它可以包含前面两个，是一个聚合体。一般用来给客户一个访问nexus的统一地址。 你可以上传私有的项目到hosted，以及配置proxy以获取第三方的依赖（比如可以配置中央仓库的地址）。前面两个都 弄好了之后，在通过group聚合给客户提供统一的访问地址。 settings.xml&lt;server&gt; &lt;id&gt;3rdParty&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;123456&lt;/password&gt; &lt;/server&gt; &lt;repository&gt; &lt;id&gt;3rdParty&lt;/id&gt; &lt;url&gt;http://192.168.2.7:182/repository/3rdParty/&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/repository&gt; mvn deploy:deploy-file -DgroupId=org.nlpcn -DartifactId=elasticsearch-sql -Dversion=6.6.0.0 -Dpackaging=jar -Dfile=elasticsearch-sql-6.6.0.0.jar -Durl=http://192.168.2.7:182/repository/3rdParty/ -DrepositoryId=3rdParty pom.xml &lt;repository&gt; &lt;id&gt;3rdParty&lt;/id&gt; &lt;name&gt;3rdParty Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.7:182/repository/3rdParty/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; parent 必须放在group Id 上]]></content>
      <categories>
        <category>持续集成</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xxl-job]]></title>
    <url>%2F2019%2F08%2F20%2Fxxl-job%2F</url>
    <content type="text"><![CDATA[springboot整合xxl-job定时器 搭建wget https://github.com/xuxueli/xxl-job/archive/2.1.0.tar.gz tar zxf xxl-job-2.1.0.tar.gz docker cp xxl-job-2.1.0/doc/db/tables_xxl_job.sql ht-mysql-slave:/root/ docker exec -it ht-mysql-slave mysql -u root -p use xxl_job; source /root/tables_xxl_job.sql docker run -d --rm \ -e PARAMS=&quot;--spring.datasource.url=jdbc:mysql://192.168.2.7:185/xxl_job?Unicode=true&amp;characterEncoding=UTF-8 --spring.datasource.username=root --spring.datasource.password=123456&quot; \ -p 183:8080 \ --name xxl-job-admin xuxueli/xxl-job-admin:2.1.0http://192.168.2.7:183/xxl-job-admin/ admin 123456 开发&lt;dependency&gt; &lt;groupId&gt;com.xuxueli&lt;/groupId&gt; &lt;artifactId&gt;xxl-job-core&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt;ht-micro-record-service-job/pom.xml &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-service-job&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-service-job&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.xuxueli&lt;/groupId&gt; &lt;artifactId&gt;xxl-job-core&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt; &lt;artifactId&gt;jsr305&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt; &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rocketmq&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.objenesis&lt;/groupId&gt; &lt;artifactId&gt;objenesis&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;!-- Projects Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-commons-service&lt;/artifactId&gt; &lt;version&gt;${project.parent.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Projects End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;ht.micro.record.service.job.JobServiceApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;application.yaml spring: application: name: ht-micro-record-service-job datasource: druid: url: jdbc:mysql://192.168.2.88:189/ht-micro-record?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false username: root password: 123456 initial-size: 1 min-idle: 1 max-active: 20 test-on-borrow: true driver-class-name: com.mysql.jdbc.Driver cloud: nacos: discovery: server-addr: 192.168.2.5:8848,192.168.2.5:8849,192.168.2.5:8850 config: server-addr: 192.168.2.5:8848,192.168.2.5:8849,192.168.2.5:8850 sentinel: transport: port: 8719 dashboard: 192.168.2.5:190 server: port: 9701 xxl: job: executor: logpath: logs/xxl-job/jobhandler appname: xxl-job-executor port: 9999 logretentiondays: -1 ip: 192.168.3.233 admin: addresses: http://192.168.2.7:183/xxl-job-admin accessTokencom.ht.micro.record.service.job.JobServiceApplication @SpringBootApplication(scanBasePackages = &quot;com.ht.micro.record&quot;) @EnableDiscoveryClient @MapperScan(basePackages = &quot;com.ht.micro.record.commons.mapper&quot;) public class JobServiceApplication { public static void main(String[] args) { SpringApplication.run(JobServiceApplication.class, args); } }com.ht.micro.record.service.job.config.XxlJobConfig @Configuration @ComponentScan(basePackages = &quot;com.ht.micro.record.service.job.handler&quot;) public class XxlJobConfig { private Logger logger = LoggerFactory.getLogger(XxlJobConfig.class); @Value(&quot;${xxl.job.admin.addresses}&quot;) private String adminAddresses; @Value(&quot;${xxl.job.executor.appname}&quot;) private String appName; @Value(&quot;${xxl.job.executor.ip}&quot;) private String ip; @Value(&quot;${xxl.job.executor.port}&quot;) private int port; @Value(&quot;${xxl.job.accessToken}&quot;) private String accessToken; @Value(&quot;${xxl.job.executor.logpath}&quot;) private String logPath; @Value(&quot;${xxl.job.executor.logretentiondays}&quot;) private int logRetentionDays; @Bean(initMethod = &quot;start&quot;, destroyMethod = &quot;destroy&quot;) public XxlJobSpringExecutor xxlJobExecutor() { logger.info(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; xxl-job config init.&quot;); XxlJobSpringExecutor xxlJobSpringExecutor = new XxlJobSpringExecutor(); xxlJobSpringExecutor.setAdminAddresses(adminAddresses); xxlJobSpringExecutor.setAppName(appName); xxlJobSpringExecutor.setIp(ip); xxlJobSpringExecutor.setPort(port); xxlJobSpringExecutor.setAccessToken(accessToken); xxlJobSpringExecutor.setLogPath(logPath); xxlJobSpringExecutor.setLogRetentionDays(logRetentionDays); return xxlJobSpringExecutor; } }com.ht.micro.record.service.job.handler.TestJobHandler @JobHandler(value=&quot;testJobHandler&quot;) @Component public class TestJobHandler extends IJobHandler { @Override public ReturnT&lt;String&gt; execute(String param) throws Exception { XxlJobLogger.log(&quot;XXL-JOB, Hello World.&quot;); for (int i = 0; i &lt; 5; i++) { XxlJobLogger.log(&quot;beat at:&quot; + i); TimeUnit.SECONDS.sleep(2); } return SUCCESS; } }]]></content>
      <categories>
        <category>定时器</category>
      </categories>
      <tags>
        <tag>xxl-job</tag>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot2-multi-druid]]></title>
    <url>%2F2019%2F08%2F20%2Fspringboot2-multi-druid%2F</url>
    <content type="text"><![CDATA[基于springboot2+druid实现多数据源 配置application.yml server: port: 10205 spring: datasource: base: #监控统计拦截的filters filters: stat type: com.alibaba.druid.pool.DruidDataSource jdbc-url: jdbc:mysql://192.168.1.250:3306/htbl_test?useUnicode=true&amp;characterEncoding=UTF-8 username: root password: 123456 driver-class-name: com.mysql.jdbc.Driver max-idle: 10 max-wait: 10000 min-idle: 5 initial-size: 5 validation-query: SELECT 1 test-on-borrow: false test-while-idle: true time-between-eviction-runs-millis: 18800 dic: #监控统计拦截的filters filters: stat type: com.alibaba.druid.pool.DruidDataSource jdbc-url: jdbc:mysql://192.168.1.48:3306/ht_nlp?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false username: root password: Sonar@1234 driver-class-name: com.mysql.jdbc.Driver max-idle: 10 max-wait: 10000 min-idle: 5 initial-size: 5 validation-query: SELECT 1 test-on-borrow: false test-while-idle: true time-between-eviction-runs-millis: 18800数据源1配置BaseMybatisConfig @Configuration @MapperScan(basePackages = {&quot;com.ht.micro.record.commons.mapper.baseMapper&quot;}, sqlSessionTemplateRef = &quot;baseSqlSessionTemplate&quot;) public class BaseMybatisConfig { @Value(&quot;${spring.datasource.base.filters}&quot;) String filters; @Value(&quot;${spring.datasource.base.driver-class-name}&quot;) String driverClassName; @Value(&quot;${spring.datasource.base.username}&quot;) String username; @Value(&quot;${spring.datasource.base.password}&quot;) String password; @Value(&quot;${spring.datasource.base.jdbc-url}&quot;) String url; @Bean(name=&quot;baseDataSource&quot;) @Primary//必须加此注解，不然报错，下一个类则不需要添加 spring.datasource @ConfigurationProperties(prefix=&quot;spring.datasource.base&quot;)//prefix值必须是application.properteis中对应属性的前缀 public DataSource baseDataSource() throws SQLException { DruidDataSource druid = new DruidDataSource(); // 监控统计拦截的filters druid.setFilters(filters); // 配置基本属性 druid.setDriverClassName(driverClassName); druid.setUsername(username); druid.setPassword(password); druid.setUrl(url); return druid; } @Bean(name=&quot;baseSqlSessionFactory&quot;) @Primary public SqlSessionFactory baseSqlSessionFactory(@Qualifier(&quot;baseDataSource&quot;)DataSource dataSource)throws Exception{ // 创建Mybatis的连接会话工厂实例 SqlSessionFactoryBean bean=new SqlSessionFactoryBean(); bean.setDataSource(dataSource);//// 设置数据源bean //添加XML目录 ResourcePatternResolver resolver=new PathMatchingResourcePatternResolver(); try{ bean.setMapperLocations(resolver.getResources(&quot;classpath:baseMapper/*Mapper.xml&quot;));//// 设置mapper文件路径 return bean.getObject(); }catch(Exception e){ e.printStackTrace(); throw new RuntimeException(e); } } @Bean(name=&quot;baseSqlSessionTemplate&quot;) @Primary public SqlSessionTemplate baseSqlSessionTemplate(@Qualifier(&quot;baseSqlSessionFactory&quot;)SqlSessionFactory sqlSessionFactory)throws Exception{ SqlSessionTemplate template=new SqlSessionTemplate(sqlSessionFactory);//使用上面配置的Factory return template; } } 数据源2配置@Configuration @MapperScan(basePackages = {&quot;com.ht.micro.record.commons.mapper.dicMapper&quot;}, sqlSessionTemplateRef = &quot;dicSqlSessionTemplate&quot;) public class DicMybatisConfig { @Value(&quot;${spring.datasource.dic.filters}&quot;) String filters; @Value(&quot;${spring.datasource.dic.driver-class-name}&quot;) String driverClassName; @Value(&quot;${spring.datasource.dic.username}&quot;) String username; @Value(&quot;${spring.datasource.dic.password}&quot;) String password; @Value(&quot;${spring.datasource.dic.jdbc-url}&quot;) String url; @Bean(name = &quot;dicDataSource&quot;) @ConfigurationProperties(prefix=&quot;spring.datasource.dic&quot;) public DataSource dicDataSource() throws SQLException { DruidDataSource druid = new DruidDataSource(); // 监控统计拦截的filters // druid.setFilters(filters); // 配置基本属性 druid.setDriverClassName(driverClassName); druid.setUsername(username); druid.setPassword(password); druid.setUrl(url); /* //初始化时建立物理连接的个数 druid.setInitialSize(initialSize); //最大连接池数量 druid.setMaxActive(maxActive); //最小连接池数量 druid.setMinIdle(minIdle); //获取连接时最大等待时间，单位毫秒。 druid.setMaxWait(maxWait); //间隔多久进行一次检测，检测需要关闭的空闲连接 druid.setTimeBetweenEvictionRunsMillis(timeBetweenEvictionRunsMillis); //一个连接在池中最小生存的时间 druid.setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis); //用来检测连接是否有效的sql druid.setValidationQuery(validationQuery); //建议配置为true，不影响性能，并且保证安全性。 druid.setTestWhileIdle(testWhileIdle); //申请连接时执行validationQuery检测连接是否有效 druid.setTestOnBorrow(testOnBorrow); druid.setTestOnReturn(testOnReturn); //是否缓存preparedStatement，也就是PSCache，oracle设为true，mysql设为false。分库分表较多推荐设置为false druid.setPoolPreparedStatements(poolPreparedStatements); // 打开PSCache时，指定每个连接上PSCache的大小 druid.setMaxPoolPreparedStatementPerConnectionSize(maxPoolPreparedStatementPerConnectionSize); */ return druid; } @Bean(name = &quot;dicSqlSessionFactory&quot;) public SqlSessionFactory dicSqlSessionFactory(@Qualifier(&quot;dicDataSource&quot;)DataSource dataSource)throws Exception{ SqlSessionFactoryBean bean=new SqlSessionFactoryBean(); bean.setDataSource(dataSource); //添加XML目录 ResourcePatternResolver resolver=new PathMatchingResourcePatternResolver(); try{ bean.setMapperLocations(resolver.getResources(&quot;classpath:dicMapper/*Mapper.xml&quot;)); return bean.getObject(); }catch(Exception e){ e.printStackTrace(); throw new RuntimeException(e); } } @Bean(name=&quot;dicSqlSessionTemplate&quot;) public SqlSessionTemplate dicSqlSessionTemplate(@Qualifier(&quot;dicSqlSessionFactory&quot;)SqlSessionFactory sqlSessionFactory)throws Exception{ SqlSessionTemplate template=new SqlSessionTemplate(sqlSessionFactory);//使用上面配置的Factory return template; } }启动类NoteProviderServiceApplication @SpringBootApplication(scanBasePackages = &quot;com.ht.micro.record&quot;) @EnableDiscoveryClient @MapperScan(basePackages = &quot;com.ht.micro.record.commons.mapper&quot;) @EnableSwagger2 public class NoteProviderServiceApplication { public static void main(String[] args) { SpringApplication.run(NoteProviderServiceApplication.class, args); } }测试服务层WebDicController @RestController @RequestMapping(value = &quot;webdic&quot;) public class WebDicController extends AbstractBaseController&lt;TbUser&gt; { @Autowired private WebDicService webDicService; @ApiOperation(value = &quot;获取所有字典&quot;) @RequestMapping public List&lt;WebDic&gt; getAll() { return webDicService.getAll(); } }TApplyController @RestController @RequestMapping(value = &quot;apply&quot;) public class TApplyController extends AbstractBaseController { @Autowired private TApplyService tApplyService; /** http://localhost:10105/apply/asked/任大龙 * @param name * @return */ @ApiImplicitParams({ @ApiImplicitParam(name = &quot;askedName&quot;, value = &quot;被询问人名&quot;, required = true, paramType = &quot;path&quot;) }) @GetMapping(value = &quot;asked/{name}&quot;) public List&lt;TApply&gt; getByAskedName(@PathVariable String name){ return tApplyService.getByAskedName(name); } }WebDicService @Service public class WebDicService { @Autowired WebDicMapper webDicMapper; public List&lt;WebDic&gt; getAll() { return webDicMapper.selectAll(); } }TApplyService public interface TApplyService extends BaseCrudService&lt;TApply&gt; { List&lt;TApply&gt; getByAskedName(String name); }TApplyServiceImpl @Service public class TApplyServiceImpl extends BaseCrudServiceImpl&lt;TApply, TApplyMapper&gt; implements TApplyService { @Autowired private TApplyMapper tApplyMapper; public List&lt;TApply&gt; getByAskedName(String name) { return tApplyMapper.selectByAskedName(name); } }mapper层com.ht.micro.record.commons.mapper.baseMapper.TApplyMapper public interface TApplyMapper extends MyMapper&lt;TApply&gt; { /** * 根据名称查找 * @param name * @return */ List&lt;TApply&gt; selectByAskedName(String name); }com.ht.micro.record.commons.mapper.dicMapper.WebDicMapper public interface WebDicMapper extends MyMapper&lt;WebDic&gt; { }tk.mybatis.mapper.MyMapper public interface MyMapper&lt;T&gt; extends Mapper&lt;T&gt;, MySqlMapper&lt;T&gt; { }baseMapper/TApplyMapper.xml &lt;mapper namespace=&quot;com.ht.micro.record.commons.mapper.baseMapper.TApplyMapper&quot;&gt; &lt;resultMap id=&quot;BaseResultMap&quot; type=&quot;com.ht.micro.record.commons.domain.TApply&quot;&gt; &lt;!-- WARNING - @mbg.generated --&gt; &lt;id column=&quot;id&quot; jdbcType=&quot;INTEGER&quot; property=&quot;id&quot; /&gt; &lt;result column=&quot;applyer_police_num&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;applyerPoliceNum&quot; /&gt; &lt;result column=&quot;applyer_name&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;applyerName&quot; /&gt; &lt;result column=&quot;asked_name&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;askedName&quot; /&gt; &lt;result column=&quot;applyer_id&quot; jdbcType=&quot;INTEGER&quot; property=&quot;applyerId&quot; /&gt; &lt;result column=&quot;unit_id&quot; jdbcType=&quot;INTEGER&quot; property=&quot;unitId&quot; /&gt; &lt;result column=&quot;case_info_ids&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;caseInfoIds&quot; /&gt; &lt;result column=&quot;case_count&quot; jdbcType=&quot;INTEGER&quot; property=&quot;caseCount&quot; /&gt; &lt;result column=&quot;apply_time&quot; jdbcType=&quot;TIMESTAMP&quot; property=&quot;applyTime&quot; /&gt; &lt;result column=&quot;apply_state&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;applyState&quot; /&gt; &lt;result column=&quot;apply_goal&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;applyGoal&quot; /&gt; &lt;result column=&quot;approve_time&quot; jdbcType=&quot;TIMESTAMP&quot; property=&quot;approveTime&quot; /&gt; &lt;result column=&quot;approve_state&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;approveState&quot; /&gt; &lt;result column=&quot;approver_id&quot; jdbcType=&quot;INTEGER&quot; property=&quot;approverId&quot; /&gt; &lt;result column=&quot;approve_prop&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;approveProp&quot; /&gt; &lt;/resultMap&gt; &lt;select id=&quot;selectByAskedName&quot; parameterType=&quot;string&quot; resultMap=&quot;BaseResultMap&quot;&gt; select * from t_apply where asked_name = #{name,jdbcType=VARCHAR} &lt;/select&gt; &lt;/mapper&gt;dicMapper/WebDicMapper.xml &lt;mapper namespace=&quot;com.ht.micro.record.commons.mapper.dicMapper&quot;&gt; &lt;resultMap id=&quot;BaseResultMap&quot; type=&quot;com.ht.micro.record.commons.domain.WebDic&quot;&gt; &lt;!-- WARNING - @mbg.generated --&gt; &lt;id column=&quot;id&quot; jdbcType=&quot;INTEGER&quot; property=&quot;id&quot; /&gt; &lt;result column=&quot;name&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;name&quot; /&gt; &lt;result column=&quot;type&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;type&quot; /&gt; &lt;/resultMap&gt; &lt;/mapper&gt;]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>springboot</tag>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[etcd+overlay+pxc]]></title>
    <url>%2F2019%2F08%2F20%2Fetcd-overlay-pxc%2F</url>
    <content type="text"><![CDATA[etcd+overlay+pxc实现基于swarm自定义网络数据库高可用 curl -L https://github.com/coreos/etcd/releases/download/v2.2.1/etcd-v2.2.1-linux-amd64.tar.gz -o etcd-v2.2.1-linux-amd64.tar.gz etcd集群tar xzvf etcd-v2.2.1-linux-amd64.tar.gz &amp;&amp; cd etcd-v2.2.1-linux-amd64 {NODE_NAME}:etcd节点名称，需要和命令中的-initial-cluster的对应的{NODE1_NAME}或{NODE2_NAME}对应 {NODE_IP}/{NODE1_IP}/{NODE2_NAME}：节点的IP ./etcd -name {NODE_NAME} -initial-advertise-peer-urls [http://{NODE_IP}:2380](http://NODE_IP:2380) \ -listen-peer-urls &lt;http://0.0.0.0:2380&gt; \ -listen-client-urls [http://0.0.0.0:2379,http://127.0.0.1:4001](http://0.0.0.0:2379,http:/127.0.0.1:4001) \ -advertise-client-urls &lt;http://0.0.0.0:2379&gt; \ -initial-cluster-token etcd-cluster \ -initial-cluster {NODE1_NAME}=http://{NODE1_IP}:2380,{NODE2_NAME}=http://{NODE2_IP}:2380 \ -initial-cluster-state new 单机tar xzvf etcd-v2.2.1-linux-amd64.tar.gz &amp;&amp; cd etcd-v2.2.1-linux-amd64 nohup ./etcd --advertise-client-urls &#39;http://192.168.3.226:2379&#39; --listen-client-urls &#39;http://0.0.0.0:2379&#39; &amp; ./etcdctl member list 查看启动情况 ./etcdctl mk name OneJane ./etcdctl get name 其他主机 ./etcdctl -endpoint http://192.168.3.226:2379 get name ./etcdctl -endpoint http://192.168.3.226:2379 mk age 22 swarmdocker配置192.168.3.224 vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --cluster-store=etcd://192.168.3.224:2379 --cluster-advertise=192.168.3.224:2375 systemctl daemon-reload service docker start 192.168.3.227 vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --cluster-store=etcd://192.168.3.224:2379 --cluster-advertise=192.168.3.227:2375 systemctl daemon-reload service docker start overlay192.168.3.224 docker swarm init --advertise-addr 192.168.3.224 192.168.3.227 docker swarm join --token SWMTKN-1-4qdkodh0g0c73iw5oehhn4rmsxxxca1cdfushtujspjsn1i827-3x1zyoo4tm4d4qm9x8f4o658q 192.168.3.227:2377 192.168.3.224 docker network create --driver overlay --attachable overnet docker run -itd --name=worker-1 --net=overnet ubuntu docker exec worker-1 apt-get update docker exec worker-1 apt-get install net-tools docker exec worker-1 ifconfig 10.0.0.5 192.168.3.227 docker run -itd --name=worker-2 --net=overnet ubuntu docker exec worker-2 apt-get update docker exec worker-2 apt-get install net-tools docker exec worker-2 apt-get install -y inetutils-ping docker exec worker-2 ifconfig docker exec worker-2 ping 10.0.0.5 PXC192.168.3.224 docker volume create v01 docker run -d \ -p 3306:3306 \ -e MYSQL_ROOT_PASSWORD=abc123456 \ -e CLUSTER_NAME=PXC \ -e XTRABACKUP_PASSWORD=abc123456 \ -v v01:/var/lib/mysql \ --privileged \ --name=node1 \ --net=overnet \ percona/percona-xtradb-cluster:5.7 192.168.3.227 docker volume create v02 docker run -d \ -p 3306:3306 \ -e MYSQL_ROOT_PASSWORD=abc123456 \ -e CLUSTER_NAME=PXC \ -e XTRABACKUP_PASSWORD=abc123456 \ -e CLUSTER_JOIN=node1 \ -v v02:/var/lib/mysql \ --name=node2 \ --net=overnet \ percona/percona-xtradb-cluster:5.7]]></content>
      <categories>
        <category>高可用</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>pxc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux-partition]]></title>
    <url>%2F2019%2F08%2F20%2Flinux-partition%2F</url>
    <content type="text"><![CDATA[parted实现linux大磁盘分区 已挂载磁盘分区分区需先把磁盘unmount /dev/mapper/centos_ht05-homeQ1:Error: /dev/dm-2: unrecognised disk labelmklabel gpt 转成gpt格式再分区 开始分区du -h –max-depth=1 查看各文件夹大小 parted /dev/dm-2 查看/dev/mapper/centos_ht05-home是l文件属性，链接到dm-2,分区dm-2磁盘 (parted) mkpart Partition name? []? data1 File system type? [ext2]? ext4 Start? 0 End? 3584GB (parted) mkpart Partition name? []? data2 File system type? [ext2]? ext4 Start? 3584GB End? 7168GB (parted) mkpart Partition name? []? data3 File system type? [ext2]? ext4 Start? 7168GB End? -1 (parted) p Model: Linux device-mapper (linear) (dm) Disk /dev/dm-2: 11.9TB Sector size (logical/physical): 512B/512B Partition Table: gpt Disk Flags: Number Start End Size File system Name Flags 1 17.4kB 3584GB 3584GB data1 2 3584GB 7168GB 3584GB data2 3 7168GB 11.9TB 4745GB data3 若分区错误：rm 1 挂载目录mkdir /data1 /data2 /data3 ll -t /dev/dm-* 发现 dm-2 dm-17 dm-28 dm-27最新 mkfs -t ext4 /dev/dm-2 mkfs -t ext4 /dev/dm-17 mkfs -t ext4 /dev/dm-27 mkfs -t ext4 /dev/dm-28 fdisk -l 查看/dev/mapper/centos_ht05-home3 /dev/mapper/centos_ht05-home2 /dev/mapper/centos_ht05-home1生成 mount /dev/mapper/centos_ht05-home1 /data1 mount /dev/mapper/centos_ht05-home2 /data2 mount /dev/mapper/centos_ht05-home3 /data3 开机自动挂载vi /etc/fstab /dev/mapper/centos_ht05-home1 /data1 ext4 defaults 0 0 /dev/mapper/centos_ht05-home2 /data2 ext4 defaults 0 0 /dev/mapper/centos_ht05-home3 /data3 ext4 defaults 0 0 df -hl 查看磁盘分区挂载情况]]></content>
      <categories>
        <category>系统</category>
      </categories>
      <tags>
        <tag>partition</tag>
        <tag>sentinel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis-sentinel]]></title>
    <url>%2F2019%2F08%2F20%2Fredis-sentinel%2F</url>
    <content type="text"><![CDATA[基于redis-sentinel实现redis哨兵集群 redis-sentinel，只有一个master，各实例数据保持一致； 单个redis-sentinel进程来监控redis集群是不可靠的，由于redis-sentinel本身也有single-point-of-failure-problem(单点问题)，当出现问题时整个redis集群系统将无法按照预期的方式切换主从。官方推荐：一个健康的集群部署，至少需要3个Sentinel实例。另外，redis-sentinel只需要配置监控redis master，而集群之间可以通过master相互通信。 首先Sentinel是集群部署的，Client可以链接任何一个Sentinel服务所获的结果都是一致的。其次，所有的Sentinel服务都会对Redis的主从服务进行监控，当监控到Master服务无响应的时候，Sentinel内部进行仲裁，从所有的 Slave选举出一个做为新的Master。并且把其他的slave作为新的Master的Slave。 name ip port redis-master 192.168.2.5 6300 redis-slave1 192.168.2.7 6301 redis-slave2 192.168.2.7 6302 sentinel1 192.168.2.5 26000 sentinel2 192.168.2.7 26001 sentinel3 192.168.2.7 26002 Redis部署在192.168.2.5上运行 docker run -it --name redis-master --network host -d redis --appendonly yes --port 6300 在192.168.2.7上运行 docker run -it --name redis-slave1 --network host -d redis --appendonly yes --port 6301 --slaveof 192.168.2.5 6300 docker run -it --name redis-slave2 --network host -d redis --appendonly yes --port 6302 --slaveof 192.168.2.5 6300 Sentinel部署wget http://download.redis.io/redis-stable/sentinel.conf mkdir /app/{sentine1,sentine2,sentine3}/{data,conf} -p 并复制sentinel1.conf，sentinel2.conf，sentinel3.conf /app├── sentine1│ ├── conf│ └── data├── sentine2│ ├── conf│ └── data└── sentine3 ├── conf └── data 192.168.2.5主节点chmod a+w -R /app/ port 26000 pidfile /var/run/redis-sentinel.pid logfile &quot;&quot; daemonize no dir /tmp sentinel monitor mymaster 192.168.2.5 6300 2 sentinel down-after-milliseconds mymaster 30000 sentinel parallel-syncs mymaster 1 sentinel failover-timeout mymaster 180000 sentinel deny-scripts-reconfig yes创建主节点 docker run -d --network host --name sentine1 \ -v /app/sentine1/data:/var/redis/data \ -v /app/sentine1/conf/sentinel.conf:/usr/local/etc/redis/sentinel.conf \ redis /usr/local/etc/redis/sentinel.conf --sentinel 192.168.2.7 从节点sentinel2.conf port 26001 daemonize no dir /tmp sentinel monitor mymaster 192.168.2.5 6300 2 sentinel down-after-milliseconds mymaster 30000 sentinel parallel-syncs mymaster 1 sentinel failover-timeout mymaster 180000 sentinel deny-scripts-reconfig yes创建从节点1 docker run -d --network host --name sentine2 \ -v /app/sentine2/data:/var/redis/data \ -v /app/sentine2/conf/sentinel.conf:/usr/local/etc/redis/sentinel.conf \ redis /usr/local/etc/redis/sentinel.conf --sentinel 192.168.2.7 从节点sentinel3.conf port 26002 daemonize no dir /tmp sentinel monitor mymaster 192.168.2.5 6300 2 sentinel down-after-milliseconds mymaster 30000 sentinel parallel-syncs mymaster 1 sentinel failover-timeout mymaster 180000 sentinel deny-scripts-reconfig yes创建从节点2 docker run -d --network host --name sentine3 \ -v /app/sentine3/data:/var/redis/data \ -v /app/sentine3/conf/sentinel.conf:/usr/local/etc/redis/sentinel.conf \ redis /usr/local/etc/redis/sentinel.conf --sentinel 测试[root@centoss2 app]# redis-cli -p 26000 127.0.0.1:26000&gt; sentinel master mymaster]]></content>
      <categories>
        <category>高可用</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>sentinel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Replication+Haproxy+Keepalived]]></title>
    <url>%2F2019%2F08%2F20%2FReplication-Haproxy-Keepalived%2F</url>
    <content type="text"><![CDATA[基于Replication+Haproxy+Keepalived实现数据库高可用 Haproxy 负载均衡haproxy提供负载均衡，并自动切换故障容器vim /usr/local/docker/mysql/haproxy/haproxy.cfg 编写配置文件 global #工作目录 chroot /usr/local/etc/haproxy #日志文件，使用rsyslog服务中local5日志设备（/var/log/local5），等级info log 127.0.0.1 local5 info #守护进程运行 daemon defaults log global mode http #日志格式 option httplog #日志中不记录负载均衡的心跳检测记录 option dontlognull #连接超时（毫秒） timeout connect 5000 #客户端超时（毫秒） timeout client 50000 #服务器超时（毫秒） timeout server 50000 #监控界面 listen admin_stats #监控界面的访问的IP和端口 bind 0.0.0.0:8888 #访问协议 mode http #URI相对地址 stats uri /dbs #统计报告格式 stats realm Global\ statistics #登陆帐户信息 stats auth admin:abc123456 #数据库负载均衡 listen proxy-mysql #访问的IP和端口 bind 0.0.0.0:185 #网络协议 mode tcp #负载均衡算法（轮询算法） #轮询算法：roundrobin #权重算法：static-rr #最少连接算法：leastconn #请求源IP算法：source balance roundrobin #日志格式 option tcplog #在MySQL中创建一个没有权限的haproxy用户，密码为空。Haproxy使用这个账户对MySQL数据库心跳检测 option mysql-check user haproxy server MySQL_1 192.168.3.226:3317 check weight 1 maxconn 2000 server MySQL_2 192.168.3.225:3318 check weight 1 maxconn 2000 #使用keepalive检测死链 option tcpka 在两台Replication组建的mysql集群同时创建mysql_cluster的haproxy容器，形成集群。 docker run -itd -v /usr/local/docker/mysql/haproxy:/usr/local/etc/haproxy --name mysql_cluster --privileged --net host haproxy docker exec -it ht-mysql-master mysql -u root -p drop user &#39;haproxy&#39;@&#39;%&#39;; create user &#39;haproxy&#39;@&#39;%&#39; IDENTIFIED BY &#39;&#39;;http://192.168.3.226:4001/dbs admin abc123456 实时查看haproxy监控页面 admin:abc123456192.168.3.226 185 root 123456 访问数据库，与Replication数据同步一致。 Keepalived双机热备高可用应用程序向宿主机65的发起请求，宿主机的Keepalived路由到docker内部的虚拟IP15。Haproxy容器内Keepalived抢占虚拟IP，接收到所有数据库请求将被转发到抢占虚拟IP的Haproxy，keepalived互相心跳检测，一旦主服务器挂了，备用服务器将有权抢到虚拟ip，再通过负载均衡分发到某一个PXC节点，并通过主从复制实现数据同步。 192.168.3.226docker exec -it mysql_cluster bash 进入h1 mv /etc/apt/sources.list sources.list.bak echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list apt-get update apt-get install gnupg -y --allow-unauthenticated apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 3B4FE6ACC0B21F32 apt-get update apt-get install keepalived vim -y vim /etc/keepalived/keepalived.conf vrrp_instance VI_1 { state MASTER interface ens34 # 宿主机192.168.3.226网卡 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 123456 } virtual_ipaddress { # 虚拟ip 192.168.3.222 } } virtual_server 192.168.3.222 189 { # 虚拟ip开放3306端口 delay_loop 3 lb_algo rr lb_kind NAT persistence_timeout 50 protocol TCP real_server 192.168.3.226 185 { # 分发请求到h1 weight 1 } } service keepalived start ping 192.168.3.222 192.168.3.225docker exec -it mysql_cluster bash 进入h2 mv /etc/apt/sources.list sources.list.bak echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list apt-get update apt-get install gnupg -y --allow-unauthenticated apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 3B4FE6ACC0B21F32 apt-get update apt-get install keepalived vim -y vim /etc/keepalived/keepalived.conf vrrp_instance VI_1 { state BACKUP interface ens34 # 宿主机192.168.3.225网卡 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 123456 } virtual_ipaddress { # 虚拟ip 192.168.3.222 } } virtual_server 192.168.3.222 189 { # 虚拟ip开放189端口 delay_loop 3 lb_algo rr lb_kind NAT persistence_timeout 50 protocol TCP real_server 192.168.3.225 185 { # 分发请求到h2 weight 1 } } service keepalived start ping 192.168.3.222访问192.168.3.222 189 root 123456]]></content>
      <categories>
        <category>高可用</category>
      </categories>
      <tags>
        <tag>haproxy</tag>
        <tag>replication</tag>
        <tag>keepalived</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Replication主从复制]]></title>
    <url>%2F2019%2F08%2F20%2FReplication%2F</url>
    <content type="text"><![CDATA[基于Replication实现mysql主从复制 简单介绍本文具体讲述mysql基于多机器的数据库高可用的一些解决方案。 主从复制：常见方案有PXC以及Replication。 Replication的主从在主库中操作，速度较快，弱一致性，单向异步，一旦stop slave将无法同步；PXC集群速度慢，强一致性，高价值数据，双向同步。 负载均衡：Nginx更适用于HTTP协议的应用负载，刚刚支持TCP；Haproxy提供负载，故障自动切换。 双机热备：Keepalived通过虚拟IP将请求分发，让抢占到虚拟IP的Haproxy通过负载分发给某一数据库节点。 Replication单机Masterdocker run -di --name=mysql_master -p 3300:3306 -e MYSQL_ROOT_PASSWORD=123456 mysql docker cp mysql_master:/etc/mysql/my.cnf /usr/share/mysql/my_master.cnf docker cp mysql_master:/etc/mysql/my.cnf /usr/share/mysql/my_slaver.cnf docker stop mysql_master docker rm mysql_master docker run -di --name=mysql_master -p 3300:3306 -e MYSQL_ROOT_PASSWORD=123456 -v /usr/share/mysql/my_master.cnf:/etc/mysql/my.cnf mysql:5.7.25 mkdir -p /usr/local/mysql_master chown -R 777 /usr/local/mysql_master/ 以上主要取出配置文件模板类型 vim /usr/share/mysql/my_master.cnf basedir = /usr/local/mysql_master port = 3306 server_id = 98 log_bin=zlinux01 docker restart mysql_master docker exec -it mysql_master /bin/bash mysql -u root -p ALTER USER &#39;root&#39;@&#39;%&#39; IDENTIFIED WITH mysql_native_password BY &#39;123456&#39;; GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39; WITH GRANT OPTION; FLUSH PRIVILEGES; grant replication slave on *.* to &#39;root&#39;@&#39;192.168.12.98&#39; identified by &#39;123456&#39;; flush tables with read lock; show master status; +-----------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +-----------------+----------+--------------+------------------+-------------------+ | zlinux01.000004 | 154 | | | | +-----------------+----------+--------------+------------------+-------------------+ Slavedocker run -di --name=mysql_slaver -p 3301:3306 -e MYSQL_ROOT_PASSWORD=123456 -v /usr/share/mysql/my_slaver.cnf:/etc/mysql/my.cnf mysql:5.7.25 mkdir -p /usr/local/mysql_slaver chown -R mysql.mysql /usr/local/mysql_slaver/ vim /usr/share/mysql/my_slaver.cnf basedir = /usr/local/mysql_slaver port = 3306 server_id = 89 log_bin=zlinux02 docker restart mysql_slaver docker exec -it mysql_slaver /bin/bash mysql -u root -p ALTER USER &#39;root&#39;@&#39;%&#39; IDENTIFIED WITH mysql_native_password BY &#39;123456&#39;; stop slave; change master to master_host=&#39;192.168.12.98&#39;,master_user=&#39;root&#39;, master_password=&#39;123456&#39;,master_port=3300, master_log_file=&#39;zlinux01.000004&#39;,master_log_pos=154; start slave; show slave status\G Slave_IO_Running: Yes Slave_SQL_Running: Yes则成功单机集群在实际应用中毫无意义，仅供参考。 多机一主多从Master 192.168.3.226mkdir -p /usr/local/mysql_master chown -R 777 /usr/local/mysql_master docker run -di --name=mysql_master -p 3300:3306 -e MYSQL_ROOT_PASSWORD=123456 mysql docker cp mysql_master:/etc/mysql/my.cnf /usr/share/mysql/my_master.cnf 并修改 basedir = /usr/local/mysql_master port = 3306 server_id = 98 log_bin=zlinux01 docker stop mysql_master docker rm mysql_master docker run -di --name=mysql_master -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -v /usr/share/mysql/my_master.cnf:/etc/mysql/my.cnf mysql:5.7.25 docker exec -it mysql_master /bin/bash mysql -u root -p ALTER USER &#39;root&#39;@&#39;%&#39; IDENTIFIED WITH mysql_native_password BY &#39;123456&#39;; GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39; WITH GRANT OPTION; FLUSH PRIVILEGES; grant replication slave on *.* to &#39;root&#39;@&#39;192.168.3.225&#39; identified by &#39;123456&#39;; flush tables with read lock; show master status; +-----------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +-----------------+----------+--------------+------------------+-------------------+ | zlinux01.000004 | 1135 | | | | +-----------------+----------+--------------+------------------+-------------------+Slaver 192.168.3.225mkdir -p /usr/local/mysql_slaver chown -R 777 /usr/local/mysql_slaver docker cp mysql_master:/etc/mysql/my.cnf /usr/share/mysql/my_slaver.cnf 并修改 basedir = /usr/local/mysql_slaver port = 3306 server_id = 89 log_bin=zlinux02 docker run -di --name=mysql_slaver -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -v /usr/share/mysql/my_slaver.cnf:/etc/mysql/my.cnf mysql:5.7.25 docker exec -it mysql_slaver /bin/bash mysql -u root -p ALTER USER &#39;root&#39;@&#39;%&#39; IDENTIFIED WITH mysql_native_password BY &#39;123456&#39;; stop slave; change master to master_host=&#39;192.168.3.226&#39;,master_user=&#39;root&#39;, master_password=&#39;123456&#39;,master_port=3306, master_log_file=&#39;zlinux01.000004&#39;,ster_logmaster_log_pos=1135; start slave; show slave status\G 证明主从复制实现 Slave_IO_Running: Yes Slave_SQL_Running: Yes以上即Replication的主从复制，单向复制，只可作为热备使用。 最终方案Master_1 192.168.3.226/root └── test └── mysql_test1 ├── haproxy │ └── haproxy.cfg ├── log ├── mone │ ├── conf │ │ └── my.cnf │ └── data └── mtwo ├── conf │ └── my.cnf └── data mkdir test/mysql_test1/{mone,mtwo}/{data,conf} -p vim test/mysql_test1/mone/conf/my.cnf [mysqld] server_id = 1 log-bin= mysql-bin replicate-ignore-db=mysql replicate-ignore-db=sys replicate-ignore-db=information_schema replicate-ignore-db=performance_schema read-only=0 relay_log=mysql-relay-bin log-slave-updates=on auto-increment-offset=1 auto-increment-increment=2 !includedir /etc/mysql/conf.d/ !includedir /etc/mysql/mysql.conf.d/ vim test/mysql_test1/mysql/mtwo/conf/my.cnf [mysqld] server_id = 2 log-bin= mysql-bin replicate-ignore-db=mysql replicate-ignore-db=sys replicate-ignore-db=information_schema replicate-ignore-db=performance_schema read-only=0 relay_log=mysql-relay-bin log-slave-updates=on auto-increment-offset=2 auto-increment-increment=2 !includedir /etc/mysql/conf.d/ !includedir /etc/mysql/mysql.conf.d/ scp -r test root@192.168.3.225:/root/ docker run --name monemysql -d -p 3317:3306 -e MYSQL_ROOT_PASSWORD=root -v ~/test/mysql_test1/mone/data:/var/lib/mysql -v ~/test/mysql_test1/mone/conf/my.cnf:/etc/mysql/my.cnf mysql:5.7 docker exec -it monemysql mysql -u root -p 输入root stop slave; GRANT REPLICATION SLAVE ON *.* to &#39;slave&#39;@&#39;%&#39; identified by &#39;123456&#39;; 创建一个slave同步账号slave，允许访问的IP地址为%，%表示通配符用来同步数据 show master status; +------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+-------------------+ | mysql-bin.000003 | 443 | | | | +------------------+----------+--------------+------------------+-------------------+ docker inspect monemysql | grep IPA 查看容器ip &quot;SecondaryIPAddresses&quot;: null, &quot;IPAddress&quot;: &quot;172.17.0.2&quot;, &quot;IPAMConfig&quot;: null, &quot;IPAddress&quot;: &quot;172.17.0.2&quot;,Master_2 192.168.3.225docker run --name mtwomysql -d -p 3318:3306 -e MYSQL_ROOT_PASSWORD=root -v ~/test/mysql_test1/mtwo/data:/var/lib/mysql -v ~/test/mysql_test1/mtwo/conf/my.cnf:/etc/mysql/my.cnf mysql:5.7 docker exec -it mtwomysql mysql -u root -p 输入root stop slave; change master to master_host=&#39;192.168.3.226&#39;,master_user=&#39;slave&#39;, master_password=&#39;123456&#39;,master_log_file=&#39;mysql-bin.000003&#39;, master_log_pos=443,master_port=3317; GRANT REPLICATION SLAVE ON *.* to &#39;slave&#39;@&#39;%&#39; identified by &#39;123456&#39;; 创建一个用户来同步数据 start slave ; 启动同步 show master status; 查看状态 +------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+-------------------+ | mysql-bin.000003 | 443 | | | | +------------------+----------+--------------+------------------+-------------------+双向同步Master_1 192.168.3.226 stop slave; change master to master_host=&#39;192.168.3.225&#39;,master_user=&#39;slave&#39;, master_password=&#39;123456&#39;,master_log_file=&#39;mysql-bin.000003&#39;, master_log_pos=443,master_port=3318; start slave ; 在两个容器中查看 show slave status\G; Slave_IO_Running: Yes Slave_SQL_Running: Yes 双向验证，数据同步]]></content>
      <categories>
        <category>高可用</category>
      </categories>
      <tags>
        <tag>replication</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[个人博客]]></title>
    <url>%2F2019%2F08%2F20%2Fblog%2F</url>
    <content type="text"><![CDATA[基于Nexmoe的搭建的个人博客 Hexohttps://nodejs.org/download/release/v10.15.3/ 安装node npm install -g cnpm --registry=https://registry.npm.taobao.org 安装cnpm npm config set registry https://registry.npm.taobao.org 使用npm淘宝源 npm install -g hexo-cli hexo init blog cd blog npm installNexmoecd themes git clone https://github.com/nexmoe/hexo-theme-nexmoe.git nexmoe cd nexmoe git checkout master npm i --save hexo-wordcount npm i hexo-deployer-git --save npm i -g gulp --save cp -i _config.example.yml _config.ymlvim package.json &quot;scripts&quot;: { &quot;build&quot;: &quot;hexo clean &amp;&amp; hexo g &amp;&amp; gulp &amp;&amp; hexo d &amp; git add * &amp; git commit -m &#39;加油&#39; &amp; git push origin master&quot;, &quot;test&quot;: &quot;hexo clean &amp;&amp; hexo g &amp;&amp; gulp &amp;&amp; hexo s&quot;, &quot;dev&quot;: &quot;hexo clean &amp;&amp; hexo g &amp;&amp; hexo s&quot; },默认自动开启github pages 使用npm install gulp npm install --save-dev gulp npm install gulp-htmlclean gulp-htmlmin gulp-minify-css gulp-uglify gulp-imagemin --savegulpfile.js var gulp = require(&#39;gulp&#39;); var minifycss = require(&#39;gulp-minify-css&#39;); var uglify = require(&#39;gulp-uglify&#39;); var htmlmin = require(&#39;gulp-htmlmin&#39;); var htmlclean = require(&#39;gulp-htmlclean&#39;); var imagemin = require(&#39;gulp-imagemin&#39;); // 压缩html gulp.task(&#39;minify-html&#39;, function() { return gulp.src(&#39;./public/**/*.html&#39;) .pipe(htmlclean()) .pipe(htmlmin({ removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, })) .pipe(gulp.dest(&#39;./public&#39;)) }); // 压缩css gulp.task(&#39;minify-css&#39;, function() { return gulp.src(&#39;./public/**/*.css&#39;) .pipe(minifycss({ compatibility: &#39;ie8&#39; })) .pipe(gulp.dest(&#39;./public&#39;)); }); // 压缩js gulp.task(&#39;minify-js&#39;, function() { return gulp.src(&#39;./public/js/**/*.js&#39;) .pipe(uglify()) .pipe(gulp.dest(&#39;./public&#39;)); }); // 压缩图片 gulp.task(&#39;minify-images&#39;, function() { return gulp.src(&#39;./public/images/**/*.*&#39;) .pipe(imagemin( [imagemin.gifsicle({&#39;optimizationLevel&#39;: 3}), imagemin.jpegtran({&#39;progressive&#39;: true}), imagemin.optipng({&#39;optimizationLevel&#39;: 7}), imagemin.svgo()], {&#39;verbose&#39;: true})) .pipe(gulp.dest(&#39;./public/images&#39;)) }); // 默认任务 gulp.task(&#39;default&#39;, [ &#39;minify-html&#39;,&#39;minify-css&#39;,&#39;minify-js&#39;,&#39;minify-images&#39; ]);hexo g &amp;&amp; gulp _config.ymltitle: OneJane subtitle: 码农养成记 description: keywords: Spring Cloud,Docker,Dubbo author: OneJane language: zh-CN timezone: Hongkong url: https://onejane.github.io/ theme: nexmoe deploy: type: git repo: github: git@github.com:OneJane/OneJane.github.io.git branch: master message: github highlight: enable: false line_number: true auto_detect: false tab_replace: themes/nexmoe/_config.ymlavatar: https://i.loli.net/2019/08/20/UIhTqdQiPasxLtr.jpg # 网站 Logo background: https://i.loli.net/2019/01/13/5c3aec85a4343.jpg # 既是博客的背景，又是文章默认头图 favicon: href: /img/a.ico # 网站图标 type: image/png # 图标类型，可能的值有(image/png, image/vnd.microsoft.icon, image/x-icon, image/gif) social: zhihu: - https://www.zhihu.com/people/codewj/activities - icon-zhihu - rgb(231, 106, 141) - rgba(231, 106, 141, .15) GitHub: - https://github.com/OneJane - icon-github - rgb(25, 23, 23) - rgba(25, 23, 23, .15) analytics: la_site_id: 20279757 comment: gitment gitment: owner: onejane # 持有该 repo 的 GitHub username repo: onejane.github.io # 存放评论的 issue 所在的 repo clientID: e677e59382e1c7a468fd # GitHub Client ID clientSecret: 717d041bc4ab749f069314862232cfb6ec8adc15 # GitHub Client Secret 打赏themes/nexmoe/layout/_partial/donate.ejs &lt;! -- 添加捐赠图标 --&gt; &lt;div class =&quot;post-donate&quot;&gt; &lt;div id=&quot;donate_board&quot; class=&quot;donate_bar center&quot;&gt; &lt;a id=&quot;btn_donate&quot; class=&quot;btn_donate&quot; href=&quot;javascript:;&quot; title=&quot;打赏&quot;&gt;&lt;/a&gt; &lt;span class=&quot;donate_txt&quot;&gt; ↑&lt;br&gt; &lt;%=theme.donate_message%&gt; &lt;/span&gt; &lt;br&gt; &lt;/div&gt; &lt;div id=&quot;donate_guide&quot; class=&quot;donate_bar center hidden&quot; &gt; ![](/images/alipay.jpg) ![](/images/alipay.jpg) &lt;!-- 支付宝打赏图案 --&gt; &lt;img src=&quot;&lt;%- theme.root_url %&gt;/images/alipay.jpg&quot; alt=&quot;支付宝打赏&quot;&gt; 666 &lt;!-- 微信打赏图案 --&gt; &lt;img src=&quot;&lt;%- theme.root_url %&gt;/images/wechatpay.png&quot; alt=&quot;微信打赏&quot;&gt; &lt;/div&gt; &lt;script type=&quot;text/javascript&quot;&gt; document.getElementById(&#39;btn_donate&#39;).onclick = function(){ $(&#39;#donate_board&#39;).addClass(&#39;hidden&#39;); $(&#39;#donate_guide&#39;).removeClass(&#39;hidden&#39;); } &lt;/script&gt; &lt;/div&gt; &lt;! -- 添加捐赠图标 --&gt; themes/nexmoe/source/css/_partial/donate.styl .donate_bar { text-align: center; margin-top: 5% } .donate_bar a.btn_donate { display: inline-block; width: 82px; height: 82px; margin-left: auto; margin-right: auto; background: url(http://img.t.sinajs.cn/t5/style/images/apps_PRF/e_media/btn_reward.gif)no-repeat; -webkit-transition: background 0s; -moz-transition: background 0s; -o-transition: background 0s; -ms-transition: background 0s; transition: background 0s } .donate_bar a.btn_donate:hover { background-position: 0 -82px } .donate_bar .donate_txt { display: block; color: #9d9d9d; font: 14px/2 &quot;Microsoft Yahei&quot; } .donate_bar.hidden{ display: none } .post-donate{ margin-top: 80px; } #donate_guide{ height: 210px; width: 420px; margin: 0 auto; } #donate_guide img{ height: 200px; height: 200px; } 在source\css\style.styl中添加@import ‘_partial/donate’themes/nexmoe/layout/post.ejs &lt;% if (theme.donate){ %&gt; &lt;%- partial(&#39;donate&#39;) %&gt; &lt;% } %&gt; themes/nexmoe/_config.yml #是否开启打赏功能 donate: true #打赏文案 donate_message: 欣赏此文？求鼓励，求支持！404插入音乐hexo new page 404 生成source/404.md --- title: 404 permalink: /404 cover: https://i.loli.net/2019/08/20/DMuWHOGTq4AR1iQ.png --- &lt;div class=&quot;aplayer&quot; data-id=&quot;439625244&quot; data-server=&quot;netease&quot; data-type=&quot;song&quot; data-autoplay=&quot;true&quot; data-mode=&quot;single&quot;&gt;&lt;/div&gt;themes/nexmoe/layout/layout.ejs &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.css&quot;&gt; &lt;script src=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.js&quot;&gt;&lt;/script&gt; ... &lt;script src=&quot;https://cdn.jsdelivr.net/npm/meting@1.1.0/dist/Meting.min.js&quot;&gt;&lt;/script&gt; windows右键复制路径Windows Registry Editor Version 5.00 [HKEY_CLASSES_ROOT\Directory\shell\copypath] @=&quot;copy path of dir&quot; [HKEY_CLASSES_ROOT\Directory\shell\copypath\command] @=&quot;mshta vbscript:clipboarddata.setdata(\&quot;text\&quot;,\&quot;%1\&quot;)(close)&quot; [HKEY_CLASSES_ROOT\*\shell\copypath] @=&quot;copy path of file&quot; [HKEY_CLASSES_ROOT\*\shell\copypath\command] @=&quot;mshta vbscript:clipboarddata.setdata(\&quot;text\&quot;,\&quot;%1\&quot;)(close)&quot;windows定时任务此电脑–&gt;管理–&gt;任务计划管理–&gt;任务计划程序 –&gt;任务计划程序库–&gt;Microsoft –&gt;windows vim C:\Users\codewj\Desktop\build-blog.bat E: &amp;&amp; cd E:\Project\blog &amp;&amp; npm run build robots.txtUser-agent: * Disallow: Disallow: /bin/ Sitemap: https://onejane.github.io/sitemap.txt 加入blog\themes\nexmoe\source google收录 https://search.google.com/search-console 添加资源 将google验证文件放入blog\themes\nexmoe\source，发布校验 站点地图 将生成的站点地图放入blog\themes\nexmoe\source bing收录https://www.bing.com/toolbox/webmaster/ 登陆后进入https://www.bing.com/webmaster/home/mysites#https://www.bing.com/webmaster/home/dashboard?url=https%3A%2F%2Fonejane.github.io%2F 使用小书匠小书匠 新建github reository为blog，并初始化 新建token:https://github.com/settings/tokens/new 并Generate token得到129483a01745abe39ed1ac109ec09f1d71b9e8c3数据存储&amp;图床服务 QuestionAssertionError [ERR_ASSERTION]: Task function must be specified?&quot;devDependencies&quot;: { &quot;gulp&quot;: &quot;^3.9.1&quot; } npm install GulpUglifyError:unable to minify JavaScriptnpm install gulp-util –save-dev var gutil = require(&#39;gulp-util&#39;); // 压缩public目录下的所有js gulp.task(&#39;minify-js&#39;, function() { return gulp.src(&#39;./public/**/*.js&#39;) .pipe(uglify()) .on(&#39;error&#39;, function (err) { gutil.log(gutil.colors.red(&#39;[Error]&#39;), err.toString()); }) //增加这一行 .pipe(gulp.dest(&#39;./public&#39;)); }); gittalk Error：validation failedgitalk.ejs id: window.location.pathname改为id: decodeURI(window.location.pathname)]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>nexmoe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单机Nacos集群]]></title>
    <url>%2F2019%2F08%2F20%2Fnacos%2F</url>
    <content type="text"><![CDATA[基于docker的单机nacos集群安装配置 环境搭建yum update -y nss curl libcurl curl -L https://github.com/docker/compose/releases/download/1.20.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose docker-compose --version git clone https://github.com/nacos-group/nacos-docker.git cd nacos-docker 映射cluster-hostname启动脚本 - ../docker-startup.sh:/home/nacos/bin/docker-startup.sh 修改docker-startup.sh中环境变量 JAVA_OPT=&quot;${JAVA_OPT} -server -Xms512m -Xmx512m -Xmn256m -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m&quot; vim ~/.bashrc alias dofo=&#39;docker ps --format &quot;table {{.Names}}\t{{.Ports}}\t{{.Image}}\t&quot;&#39; alias dolo=&#39;docker logs -ft&#39; source ~/.bashrc #docker-compose up -d #docker-compose logs -ft #docker-compose down docker-compose -f example/cluster-hostname.yaml up -d docker stop nacos1 nacos2 nacos3 docker start nacos1 docker start nacos2 docker start nacos3 docker-compose -f example/cluster-hostname.yaml downhttp://192.168.2.5:8848/nacos http://192.168.2.5:8849/nacos http://192.168.2.5:8850/nacos nacos/nacos Nginx负载均衡mkdir /usr/local/docker/nginx/nacos -p vim n1.conf upstream nacos { # 配置负载均衡 server 192.168.2.5:8848; server 192.168.2.5:8849; server 192.168.2.5:8850; } server { listen 8841; server_name 192.168.2.5; location / { proxy_pass http://nacos; index index.html index.htm; } vim n2.conf upstream nacos { # 配置负载均衡 server 192.168.2.5:8848; server 192.168.2.5:8849; server 192.168.2.5:8850; } server { listen 8842; server_name 192.168.2.5; location / { proxy_pass http://nacos; index index.html index.htm; } docker run -it -d –name nginx2 -v /usr/local/docker/nginx/nacos/n2.conf:/etc/nginx/nginx.conf –net=host –privileged nginxdocker run -it -d –name nginx1 -v /usr/local/docker/nginx/nacos/n1.conf:/etc/nginx/nginx.conf –net=host –privileged nginx http://192.168.2.5:8841/nacos/#/login http://192.168.2.5:8842/nacos/#/login docker pause nacos1 测试页面维持访问 Keepalive双机热备docker exec -it nginx1 bash mv /etc/apt/sources.list sources.list.bak echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list apt-get update apt-get install gnupg -y --allow-unauthenticated apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 3B4FE6ACC0B21F32 apt-get update apt-get install keepalived vim -y vim /etc/keepalived/keepalived.conf vrrp_instance VI_1 { state MASTER interface docker0 # 填写docker网卡名 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 123456 } virtual_ipaddress { 172.17.0.100 # 定义该网卡下的虚拟ip地址段地址 } } service keepalived start docker exec -it nginx2 bash mv /etc/apt/sources.list sources.list.bak echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list apt-get update apt-get install gnupg -y --allow-unauthenticated apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 3B4FE6ACC0B21F32 apt-get update apt-get install keepalived vim -y vim /etc/keepalived/keepalived.conf vrrp_instance VI_1 { state BACKUP interface docker0 # 填写docker网卡名 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 123456 } virtual_ipaddress { 172.17.0.100 # 定义虚拟ip地址段地址 } } service keepalived start 由于docker内的虚拟ip不能被外界访问借助宿主机keepalived映射外网可以访问的虚拟ip， exit yum install keepalived -y mv /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak vim /etc/keepalived/keepalived.conf vrrp_instance VI_1 { state MASTER interface enp1s0f0 # 宿主机网卡 virtual_router_id 51 # 保持一致 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.2.155 # 宿主机虚拟ip } } virtual_server 192.168.2.155 183 { # 宿主机虚拟ip及开放端口 delay_loop 3 lb_algo rr lb_kind NAT persistence_timeout 50 protocol TCP real_server 172.17.0.100 8841 { # nginx1容器虚拟ip及开放nacos端口 weight 1 } } virtual_server 192.168.2.155 183 { delay_loop 3 lb_algo rr lb_kind NAT persistence_timeout 50 protocol TCP real_server 172.17.0.100 8842 { # nginx2容器虚拟ip及开放nacos端口 weight 1 } } #apt-get --purge remove keepalived -y #/sbin/ip addr del 192.168.12.100/32 dev enp1s0f0 删除虚拟ip 如docker0没有则重启docker，再次检查创建systemctl daemon-reloadsystemctl restart docker 访问 http://192.168.2.155:183/nacos/#/login nacos nacos 关闭nacos心跳日志logging: level: com.alibaba.nacos.client.naming: error]]></content>
      <categories>
        <category>注册中心</category>
      </categories>
      <tags>
        <tag>spring cloud alibaba</tag>
        <tag>docker</tag>
      </tags>
  </entry>
</search>
