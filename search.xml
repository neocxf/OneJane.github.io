<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[吴恩达深度学习笔记(1-3课时)]]></title>
    <url>%2F2019%2F12%2F04%2Fnew_%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(1-3%E8%AF%BE%E6%97%B6)%2F</url>
    <content type="text"><![CDATA[吴恩达深度学习笔记 神经网络的编程基础(Basics of Neural Network programming)2.1 二分类(Binary Classification)这周我们将学习神经网络的基础知识，其中需要注意的是，当实现一个神经网络的时候，我们需要知道一些非常重要的技术和技巧。例如有一个包含$m$个样本的训练集，你很可能习惯于用一个for循环来遍历训练集中的每个样本，但是当实现一个神经网络的时候，我们通常不直接使用for循环来遍历整个训练集，所以在这周的课程中你将学会如何处理训练集。 另外在神经网络的计算中，通常先有一个叫做前向暂停(forward pause)或叫做前向传播(foward propagation)的步骤，接着有一个叫做反向暂停(backward pause) 或叫做反向传播(backward propagation)的步骤。所以这周我也会向你介绍为什么神经网络的训练过程可以分为前向传播和反向传播两个独立的部分。 在课程中我将使用逻辑回归(logistic regression)来传达这些想法，以使大家能够更加容易地理解这些概念。即使你之前了解过逻辑回归，我认为这里还是有些新的、有趣的东西等着你去发现和了解，所以现在开始进入正题。 逻辑回归是一个用于二分类(binary classification)的算法。首先我们从一个问题开始说起，这里有一个二分类问题的例子，假如你有一张图片作为输入，比如这只猫，如果识别这张图片为猫，则输出标签1作为结果；如果识别出不是猫，那么输出标签0作为结果。现在我们可以用字母 $y$来 表示输出的结果标签，如下图所示： 我们来看看一张图片在计算机中是如何表示的，为了保存一张图片，需要保存三个矩阵，它们分别对应图片中的红、绿、蓝三种颜色通道，如果你的图片大小为64x64像素，那么你就有三个规模为64x64的矩阵，分别对应图片中红、绿、蓝三种像素的强度值。为了便于表示，这里我画了三个很小的矩阵，注意它们的规模为5x4 而不是64x64，如下图所示： 为了把这些像素值放到一个特征向量中，我们需要把这些像素值提取出来，然后放入一个特征向量$x$。为了把这些像素值转换为特征向量 $x$，我们需要像下面这样定义一个特征向量 $x$ 来表示这张图片，我们把所有的像素都取出来，例如255、231等等，直到取完所有的红色像素，接着最后是255、134、…、255、134等等，直到得到一个特征向量，把图片中所有的红、绿、蓝像素值都列出来。如果图片的大小为64x64像素，那么向量 $x$ 的总维度，将是64乘以64乘以3，这是三个像素矩阵中像素的总量。在这个例子中结果为12,288。现在我们用$n_x=12,288$，来表示输入特征向量的维度，有时候为了简洁，我会直接用小写的$n$来表示输入特征向量$x$的维度。所以在二分类问题中，我们的目标就是习得一个分类器，它以图片的特征向量作为输入，然后预测输出结果$y$为1还是0，也就是预测图片中是否有猫： 接下来我们说明一些在余下课程中，需要用到的一些符号。 符号定义 ： $x$：表示一个$n_x$维数据，为输入数据，维度为$(n_x,1)$； $y$：表示输出结果，取值为$(0,1)$； $(x^{(i)},y^{(i)})$：表示第$i$组数据，可能是训练数据，也可能是测试数据，此处默认为训练数据； $X=[x^{(1)},x^{(2)},...,x^{(m)}]$：表示所有的训练数据集的输入值，放在一个 $n_x×m$的矩阵中，其中$m$表示样本数目; $Y=[y^{(1)},y^{(2)},...,y^{(m)}]$：对应表示所有训练数据集的输出值，维度为$1×m$。 用一对$(x,y)$来表示一个单独的样本，$x$代表$n_x$维的特征向量，$y$ 表示标签(输出结果)只能为0或1。而训练集将由$m$个训练样本组成，其中$(x^{(1)},y^{(1)})$表示第一个样本的输入和输出，$(x^{(2)},y^{(2)})$表示第二个样本的输入和输出，直到最后一个样本$(x^{(m)},y^{(m)})$，然后所有的这些一起表示整个训练集。有时候为了强调这是训练样本的个数，会写作$M_{train}$，当涉及到测试集的时候，我们会使用$M_{test}$来表示测试集的样本数，所以这是测试集的样本数： 最后为了能把训练集表示得更紧凑一点，我们会定义一个矩阵用大写$X$的表示，它由输入向量$x^{(1)}$、$x^{(2)}$等组成，如下图放在矩阵的列中，所以现在我们把$x^{(1)}$作为第一列放在矩阵中，$x^{(2)}$作为第二列，$x^{(m)}$放到第$m$列，然后我们就得到了训练集矩阵$X$。所以这个矩阵有$m$列，$m$是训练集的样本数量，然后这个矩阵的高度记为$n_x$，注意有时候可能因为其他某些原因，矩阵$X$会由训练样本按照行堆叠起来而不是列，如下图所示：$x^{(1)}$的转置直到$x^{(m)}$的转置，但是在实现神经网络的时候，使用左边的这种形式，会让整个实现的过程变得更加简单： 现在来简单温习一下:$X$是一个规模为$n_x$乘以$m$的矩阵，当你用Python实现的时候，你会看到X.shape，这是一条Python命令，用于显示矩阵的规模，即X.shape等于$(n_x,m)$，$X$是一个规模为$n_x$乘以$m$的矩阵。所以综上所述，这就是如何将训练样本（输入向量$X$的集合）表示为一个矩阵。 那么输出标签$y$呢？同样的道理，为了能更加容易地实现一个神经网络，将标签$y$放在列中将会使得后续计算非常方便，所以我们定义大写的$Y$等于${ {y}^{\left( 1 \right)} },{ {y}^{\left( m \right)} },...,{ {y}^{\left( m \right)} }$，所以在这里是一个规模为1乘以$m$的矩阵，同样地使用Python将表示为Y.shape等于$(1,m)$，表示这是一个规模为1乘以$m$的矩阵。 当你在后面的课程中实现神经网络的时候，你会发现，一个好的符号约定能够将不同训练样本的数据很好地组织起来。而我所说的数据不仅包括 $x$ 或者 $y$ 还包括之后你会看到的其他的量。将不同的训练样本的数据提取出来，然后就像刚刚我们对 $x$ 或者 $y$ 所做的那样，将他们堆叠在矩阵的列中，形成我们之后会在逻辑回归和神经网络上要用到的符号表示。如果有时候你忘了这些符号的意思，比如什么是 $m$，或者什么是 $n$，或者忘了其他一些东西，我们也会在课程的网站上放上符号说明，然后你可以快速地查阅每个具体的符号代表什么意思，好了，我们接着到下一个视频，在下个视频中，我们将以逻辑回归作为开始。备注：附录里也写了符号说明。 2.2 逻辑回归(Logistic Regression)在这个视频中，我们会重温逻辑回归学习算法，该算法适用于二分类问题，本节将主要介绍逻辑回归的Hypothesis Function（假设函数）。 对于二元分类问题来讲，给定一个输入特征向量$X$，它可能对应一张图片，你想识别这张图片识别看它是否是一只猫或者不是一只猫的图片，你想要一个算法能够输出预测，你只能称之为$\hat{y}$，也就是你对实际值 $y$ 的估计。更正式地来说，你想让 $\hat{y}$ 表示 $y$ 等于1的一种可能性或者是机会，前提条件是给定了输入特征$X$。换句话来说，如果$X$是我们在上个视频看到的图片，你想让 $\hat{y}$ 来告诉你这是一只猫的图片的机率有多大。在之前的视频中所说的，$X$是一个$n_x$维的向量（相当于有$n_x$个特征的特征向量）。我们用$w$来表示逻辑回归的参数，这也是一个$n_x$维向量（因为$w$实际上是特征权重，维度与特征向量相同），参数里面还有$b$，这是一个实数（表示偏差）。所以给出输入$x$以及参数$w$和$b$之后，我们怎样产生输出预测值$\hat{y}$，一件你可以尝试却不可行的事是让$\hat{y}={ {w}^{T} }x+b$。 这时候我们得到的是一个关于输入$x$的线性函数，实际上这是你在做线性回归时所用到的，但是这对于二元分类问题来讲不是一个非常好的算法，因为你想让$\hat{y}$表示实际值$y$等于1的机率的话，$\hat{y}$ 应该在0到1之间。这是一个需要解决的问题，因为${ {w}^{T} }x+b$可能比1要大得多，或者甚至为一个负值。对于你想要的在0和1之间的概率来说它是没有意义的，因此在逻辑回归中，我们的输出应该是$\hat{y}$等于由上面得到的线性函数式子作为自变量的sigmoid函数中，公式如上图最下面所示，将线性函数转换为非线性函数。 下图是sigmoid函数的图像，如果我把水平轴作为$z$轴，那么关于$z$的sigmoid函数是这样的，它是平滑地从0走向1，让我在这里标记纵轴，这是0，曲线与纵轴相交的截距是0.5，这就是关于$z$的sigmoid函数的图像。我们通常都使用$z$来表示${ {w}^{T} }x+b$的值。 关于sigmoid函数的公式是这样的，$\sigma \left( z \right)=\frac{1}{1+{ {e}^{-z} }}$,在这里$z$是一个实数，这里要说明一些要注意的事情，如果$z$非常大那么${ {e}^{-z} }$将会接近于0，关于$z$的sigmoid函数将会近似等于1除以1加上某个非常接近于0的项，因为$e$ 的指数如果是个绝对值很大的负数的话，这项将会接近于0，所以如果$z$很大的话那么关于$z$的sigmoid函数会非常接近1。相反地，如果$z$非常小或者说是一个绝对值很大的负数，那么关于${ {e}^{-z} }$这项会变成一个很大的数，你可以认为这是1除以1加上一个非常非常大的数，所以这个就接近于0。实际上你看到当$z$变成一个绝对值很大的负数，关于$z$的sigmoid函数就会非常接近于0，因此当你实现逻辑回归时，你的工作就是去让机器学习参数$w$以及$b$这样才使得$\hat{y}$成为对$y=1$这一情况的概率的一个很好的估计。 在继续进行下一步之前，介绍一种符号惯例，可以让参数$w$和参数$b$分开。在符号上要注意的一点是当我们对神经网络进行编程时经常会让参数$w$和参数$b$分开，在这里参数$b$对应的是一种偏置。在之前的机器学习课程里，你可能已经见过处理这个问题时的其他符号表示。比如在某些例子里，你定义一个额外的特征称之为${ {x}_{0} }$，并且使它等于1，那么现在$X$就是一个$n_x$加1维的变量，然后你定义$\hat{y}=\sigma \left( { {\theta }^{T} }x \right)$的sigmoid函数。在这个备选的符号惯例里，你有一个参数向量${ {\theta }_{0} },{ {\theta }_{1} },{ {\theta }_{2} },...,{ {\theta }_{ {{n}_{x} }} }$，这样${ {\theta }_{0} }$就充当了$b$，这是一个实数，而剩下的${ {\theta }_{1} }$ 直到${ {\theta }_{ {{n}_{x} }} }$充当了$w$，结果就是当你实现你的神经网络时，有一个比较简单的方法是保持$b$和$w$分开。但是在这节课里我们不会使用任何这类符号惯例，所以不用去担心。现在你已经知道逻辑回归模型是什么样子了，下一步要做的是训练参数$w$和参数$b$，你需要定义一个代价函数，让我们在下节课里对其进行解释。 2.3 逻辑回归的代价函数（Logistic Regression Cost Function）在上个视频中，我们讲了逻辑回归模型，这个视频里，我们讲逻辑回归的代价函数（也翻译作成本函数）。 为什么需要代价函数： 为了训练逻辑回归模型的参数参数$w$和参数$b$我们，需要一个代价函数，通过训练代价函数来得到参数$w$和参数$b$。先看一下逻辑回归的输出函数： 为了让模型通过学习调整参数，你需要给予一个$m$样本的训练集，这会让你在训练集上找到参数$w$和参数$b$,，来得到你的输出。 对训练集的预测值，我们将它写成$\hat{y}$，我们更希望它会接近于训练集中的$y$值，为了对上面的公式更详细的介绍，我们需要说明上面的定义是对一个训练样本来说的，这种形式也使用于每个训练样本，我们使用这些带有圆括号的上标来区分索引和样本，训练样本$i$所对应的预测值是${ {y}^{(i)} }$,是用训练样本的${ {w}^{T} }{ {x}^{(i)} }+b$然后通过sigmoid函数来得到，也可以把$z$定义为${ {z}^{(i)} }={ {w}^{T} }{ {x}^{(i)} }+b$,我们将使用这个符号$(i)$注解，上标$(i)$来指明数据表示$x$或者$y$或者$z$或者其他数据的第$i$个训练样本，这就是上标$(i)$的含义。 损失函数： 损失函数又叫做误差函数，用来衡量算法的运行情况，Loss function:$L\left( \hat{y},y \right)$. 我们通过这个$L$称为的损失函数，来衡量预测输出值和实际值有多接近。一般我们用预测值和实际值的平方差或者它们平方差的一半，但是通常在逻辑回归中我们不这么做，因为当我们在学习逻辑回归参数的时候，会发现我们的优化目标不是凸优化，只能找到多个局部最优值，梯度下降法很可能找不到全局最优值，虽然平方差是一个不错的损失函数，但是我们在逻辑回归模型中会定义另外一个损失函数。 我们在逻辑回归中用到的损失函数是：$L\left( \hat{y},y \right)=-y\log(\hat{y})-(1-y)\log (1-\hat{y})$ 为什么要用这个函数作为逻辑损失函数？当我们使用平方误差作为损失函数的时候，你会想要让这个误差尽可能地小，对于这个逻辑回归损失函数，我们也想让它尽可能地小，为了更好地理解这个损失函数怎么起作用，我们举两个例子： 当$y=1$时损失函数$L=-\log (\hat{y})$，如果想要损失函数$L$尽可能得小，那么$\hat{y}$就要尽可能大，因为sigmoid函数取值$[0,1]$，所以$\hat{y}$会无限接近于1。 当$y=0$时损失函数$L=-\log (1-\hat{y})$，如果想要损失函数$L$尽可能得小，那么$\hat{y}$就要尽可能小，因为sigmoid函数取值$[0,1]$，所以$\hat{y}$会无限接近于0。 在这门课中有很多的函数效果和现在这个类似，就是如果$y$等于1，我们就尽可能让$\hat{y}$变大，如果$y$等于0，我们就尽可能让 $\hat{y}$ 变小。损失函数是在单个训练样本中定义的，它衡量的是算法在单个训练样本中表现如何，为了衡量算法在全部训练样本上的表现如何，我们需要定义一个算法的代价函数，算法的代价函数是对$m$个样本的损失函数求和然后除以$m$: $J\left( w,b \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{L\left( { {{\hat{y} }}^{(i)} },{ {y}^{(i)} } \right)}=\frac{1}{m}\sum\limits_{i=1}^{m}{\left( -{ {y}^{(i)} }\log { {{\hat{y} }}^{(i)} }-(1-{ {y}^{(i)} })\log (1-{ {{\hat{y} }}^{(i)} }) \right)}$ 损失函数只适用于像这样的单个训练样本，而代价函数是参数的总代价，所以在训练逻辑回归模型时候，我们需要找到合适的$w$和$b$，来让代价函数 $J$ 的总代价降到最低。根据我们对逻辑回归算法的推导及对单个样本的损失函数的推导和针对算法所选用参数的总代价函数的推导，结果表明逻辑回归可以看做是一个非常小的神经网络，在下一个视频中，我们会看到神经网络会做什么。 2.4 梯度下降法（Gradient Descent）梯度下降法可以做什么？ 在你测试集上，通过最小化代价函数（成本函数）$J(w,b)$来训练的参数$w$和$b$， 如图，在第二行给出和之前一样的逻辑回归算法的代价函数（成本函数） 梯度下降法的形象化说明 在这个图中，横轴表示你的空间参数$w$和$b$，在实践中，$w$可以是更高的维度，但是为了更好地绘图，我们定义$w$和$b$，都是单一实数，代价函数（成本函数）$J(w,b)$是在水平轴$w$和$b$上的曲面，因此曲面的高度就是$J(w,b)$在某一点的函数值。我们所做的就是找到使得代价函数（成本函数）$J(w,b)$函数值是最小值，对应的参数$w$和$b$。 如图，代价函数（成本函数）$J(w,b)$是一个凸函数(convex function)，像一个大碗一样。 如图，这就与刚才的图有些相反，因为它是非凸的并且有很多不同的局部最小值。由于逻辑回归的代价函数（成本函数）$J(w,b)$特性，我们必须定义代价函数（成本函数）$J(w,b)$为凸函数。初始化$w$和$b$， 可以用如图那个小红点来初始化参数$w$和$b$，也可以采用随机初始化的方法，对于逻辑回归几乎所有的初始化方法都有效，因为函数是凸函数，无论在哪里初始化，应该达到同一点或大致相同的点。 我们以如图的小红点的坐标来初始化参数$w$和$b$。 2. 朝最陡的下坡方向走一步，不断地迭代 我们朝最陡的下坡方向走一步，如图，走到了如图中第二个小红点处。 我们可能停在这里也有可能继续朝最陡的下坡方向再走一步，如图，经过两次迭代走到第三个小红点处。 3.直到走到全局最优解或者接近全局最优解的地方 通过以上的三个步骤我们可以找到全局最优解，也就是代价函数（成本函数）$J(w,b)$这个凸函数的最小值点。 梯度下降法的细节化说明（仅有一个参数） 假定代价函数（成本函数）$J(w)$ 只有一个参数$w$，即用一维曲线代替多维曲线，这样可以更好画出图像。 迭代就是不断重复做如图的公式: $:=$表示更新参数, $a $ 表示学习率（**learning rate**），用来控制步长（**step**），即向下走一步的长度$\frac{dJ(w)}{dw}$ 就是函数$J(w)$对$w$ 求导（**derivative**），在代码中我们会使用$dw$表示这个结果 对于导数更加形象化的理解就是斜率（slope），如图该点的导数就是这个点相切于 $J(w)$的小三角形的高除宽。假设我们以如图点为初始化点，该点处的斜率的符号是正的，即$\frac{dJ(w)}{dw}>0$，所以接下来会向左走一步。 整个梯度下降法的迭代过程就是不断地向左走，直至逼近最小值点。 假设我们以如图点为初始化点，该点处的斜率的符号是负的，即$\frac{dJ(w)}{dw}&lt;0$，所以接下来会向右走一步。 整个梯度下降法的迭代过程就是不断地向右走，即朝着最小值点方向走。 梯度下降法的细节化说明（两个参数） 逻辑回归的代价函数（成本函数）$J(w,b)$是含有两个参数的。 $\partial $ 表示求偏导符号，可以读作**round**， $\frac{\partial J(w,b)}{\partial w}$ 就是函数$J(w,b)$ 对$w$ 求偏导，在代码中我们会使用$dw$ 表示这个结果， $\frac{\partial J(w,b)}{\partial b}$ 就是函数$J(w,b)$对$b$ 求偏导，在代码中我们会使用$db$ 表示这个结果， 小写字母$d$ 用在求导数（derivative），即函数只有一个参数，偏导数符号$\partial $ 用在求偏导（partial derivative），即函数含有两个以上的参数。 2.5 导数（Derivatives）这个视频我主要是想帮你获得对微积分和导数直观的理解。或许你认为自从大学毕以后你再也没有接触微积分。这取决于你什么时候毕业，也许有一段时间了，如果你顾虑这点，请不要担心。为了高效应用神经网络和深度学习，你并不需要非常深入理解微积分。因此如果你观看这个视频或者以后的视频时心想：“哇哦，这些知识、这些运算对我来说很复杂。”我给你的建议是：坚持学习视频，最好下课后做作业，成功的完成编程作业，然后你就可以使用深度学习了。在第四周之后的学习中，你会看到定义的很多种类的函数，通过微积分他们能够帮助你把所有的知识结合起来，其中有的叫做前向函数和反向函数，因此你不需要了解所有你使用的那些微积分中的函数。所以你不用担心他们，除此之外在对深度学习的尝试中，这周我们要进一步深入了解微积分的细节。所有你只需要直观地认识微积分，用来构建和成功的应用这些算法。最后，如果你是精通微积分的那一小部分人群，你对微积分非常熟悉，你可以跳过这部分视频。其他同学让我们开始深入学习导数。 一个函数$f(a)=3a$，它是一条直线。下面我们来简单理解下导数。让我们看看函数中几个点，假定$a=2$，那么$f(a)$是$a$的3倍等于6，也就是说如果$a=2$，那么函数$f(a)=6$。假定稍微改变一点点$a$的值，只增加一点，变为2.001，这时$a$将向右做微小的移动。0.001的差别实在是太小了，不能在图中显示出来，我们把它右移一点，现在$f(a)$等于$a$的3倍是6.003，画在图里，比例不太符合。请看绿色高亮部分的这个小三角形，如果向右移动0.001，那么$f(a)$增加0.003，$f(a)$的值增加3倍于右移的$a$，因此我们说函数$f(a)$在$a=2$，.是这个导数的斜率，或者说，当$a=2$时，斜率是3。导数这个概念意味着斜率，导数听起来是一个很可怕、很令人惊恐的词，但是斜率以一种很友好的方式来描述导数这个概念。所以提到导数，我们把它当作函数的斜率就好了。更正式的斜率定义为在上图这个绿色的小三角形中，高除以宽。即斜率等于0.003除以0.001，等于3。或者说导数等于3，这表示当你将$a$右移0.001，$f(a)$的值增加3倍水平方向的量。 现在让我们从不同的角度理解这个函数。假设$a=5$ ，此时$f(a)=3a=15$。把$a$右移一个很小的幅度，增加到5.001，$f(a)=15.003$。即在$a=5$ 时，斜率是3，这就是表示，当微小改变变量$a$的值，$\frac{df(a)}{da}=3$ 。一个等价的导数表达式可以这样写$\frac{d}{da}f(a)$ ，不管你是否将$f(a)$放在上面或者放在右边都没有关系。在这个视频中，我讲解导数讨论的情况是我们将$a$偏移0.001，如果你想知道导数的数学定义，导数是你右移很小的$a$值（不是0.001，而是一个非常非常小的值）。通常导数的定义是你右移$a$(可度量的值)一个无限小的值，$f(a)$增加3倍（增加了一个非常非常小的值）。也就是这个三角形右边的高度。 那就是导数的正式定义。但是为了直观的认识，我们将探讨右移$a=0.001$ 这个值，即使0.001并不是无穷小的可测数据。导数的一个特性是：这个函数任何地方的斜率总是等于3，不管$a=2$或 $a=5$，这个函数的斜率总等于3，也就是说不管$a$的值如何变化，如果你增加0.001，$f(a)$的值就增加3倍。这个函数在所有地方的斜率都相等。一种证明方式是无论你将小三角形画在哪里，它的高除以宽总是3。 我希望带给你一种感觉：什么是斜率？什么是导函数？对于一条直线，在例子中函数的斜率，在任何地方都是3。在下一个视频让我们看一个更复杂的例子，这个例子中函数在不同点的斜率是可变的。 2.6 更多的导数例子（More Derivative Examples）在这个视频中我将给出一个更加复杂的例子，在这个例子中，函数在不同点处的斜率是不一样的，先来举个例子: 我在这里画一个函数，$f(a)={ {\text{a} }^{\text{2} }}$，如果$a=\text{2}$ 的话，那么$f(a)=4$。让我们稍稍往右推进一点点，现在$a=\text{2}.\text{001}$ ，则$f(a)\approx 4.004$ (如果你用计算器算的话，这个准确的值应该为4.004。0.001 我只是为了简便起见，省略了后面的部分)，如果你在这儿画，一个小三角形，你就会发现，如果把$a$往右移动0.001，那么$f(a)$将增大四倍，即增大0.004。在微积分中我们把这个三角形斜边的斜率，称为$f(a)$在点$a=\text{2}$ 处的导数(即为4)，或者写成微积分的形式，当$a=\text{2}$ 的时候， $\frac{d}{da}f(a)=4$ 由此可知，函数$f(a)={ {a}^{ {2} }}$，在$a$取不同值的时候，它的斜率是不同的，这和上个视频中的例子是不同的。 这里有种直观的方法可以解释，为什么一个点的斜率，在不同位置会不同如果你在曲线上，的不同位置画一些小小的三角形你就会发现，三角形高和宽的比值，在曲线上不同的地方，它们是不同的。所以当$a=2$ 时，斜率为4；而当$a=5$时，斜率为10 。如果你翻看微积分的课本，课本会告诉你，函数$f(a)={ {a}^{ {2} }}$的斜率（即导数）为$2a$。这意味着任意给定一点$a$，如果你稍微将$a$，增大0.001，那么你会看到$f(a)$将增大$2a$，即增大的值为点在$a$处斜率或导数，乘以你向右移动的距离。 现在有个小细节需要注意，导数增大的值，不是刚好等于导数公式算出来的值，而只是根据导数算出来的一个估计值。 为了总结这堂课所学的知识，我们再来看看几个例子： 假设$f(a)={ {a}^{\text{3} }}$ 如果你翻看导数公式表，你会发现这个函数的导数，等于$3{ {a}^{2} }$。所以这是什么意思呢，同样地举一个例子：我们再次令$a=2$，所以${ {a}^{3} }=8$ ，如果我们又将$a$增大一点点，你会发现$f(a)\approx 8.012$， 你可以自己检查一遍，如果我们取8.012，你会发现${ {2.001}^{3} }$ ，和8.012很接近，事实上当$a=2$时，导数值为$3×{ {2}^{2} }$，即$3×4=12$。所以导数公式，表明如果你将$a$向右移动0.001时，$f(a)$ 将会向右移动12倍，即0.012。 来看最后一个例子，假设$f(a)={ {\log }_{e} }a$，有些可能会写作$\ln a$，函数$\log a$ 的斜率应该为$\frac{1}{a}$，所以我们可以解释如下：如果$a$取任何值，比如又取$a=2$，然后又把$a$向右边移动0.001 那么$f(a)$将增大$\frac{\text{1} }{a}\times \text{0}\text{.001}$，如果你借助计算器的话，你会发现当$a=2$时$f(a)\approx \text{0}\text{.69315}$ ；而$a=2.001$时，$f(a)\approx \text{0}\text{.69365}$。所以$f(a)$增大了0.0005，如果你查看导数公式，当$a=2$的时候，导数值$\frac{d}{da}f(a)=\frac{\text{1} }{\text{2} }$。这表明如果你把 增大0.001，$f(a)$将只会增大0.001的二分之一，即0.0005。如果你画个小三角形你就会发现，如果$x$ 轴增加了0.001，那么$y$ 轴上的函数$\log a$，将增大0.001的一半 即0.0005。所以 $\frac{1}{a}$ ，当$a=2$时这里是 ，就是当$a=2$时这条线的斜率。这些就是有关，导数的一些知识。 在这个视频中，你只需要记住两点： 第一点，导数就是斜率，而函数的斜率，在不同的点是不同的。在第一个例子中$f(a)=\text{3}a$ ，这是一条直线，在任何点它的斜率都是相同的，均为3。但是对于函数$f(a)={ {\text{a} }^{\text{2} }}$ ，或者$f(a)=\log a$，它们的斜率是变化的，所以它们的导数或者斜率，在曲线上不同的点处是不同的。 第二点，如果你想知道一个函数的导数，你可参考你的微积分课本或者维基百科，然后你应该就能找到这些函数的导数公式。 最后我希望，你能通过我生动的讲解，掌握这些有关导数和斜率的知识，下一课我们将讲解计算图，以及如何用它来求更加复杂的函数的导数。 2.7 计算图（Computation Graph）可以说，一个神经网络的计算，都是按照前向或反向传播过程组织的。首先我们计算出一个新的网络的输出（前向过程），紧接着进行一个反向传输操作。后者我们用来计算出对应的梯度或导数。计算图解释了为什么我们用这种方式组织这些计算过程。在这个视频中，我们将举一个例子说明计算图是什么。让我们举一个比逻辑回归更加简单的，或者说不那么正式的神经网络的例子。 我们尝试计算函数$J$，$J$是由三个变量$a,b,c$组成的函数，这个函数是$\text{3(a}+\text{bc)}$ 。计算这个函数实际上有三个不同的步骤，首先是计算 $b$ 乘以 $c$，我们把它储存在变量$u$中，因此${u}={bc}$；然后计算$v=a+u$；最后输出$J=3v$，这就是要计算的函数$J$。我们可以把这三步画成如下的计算图，我先在这画三个变量$a,b,c$，第一步就是计算$u=bc$，我在这周围放个矩形框，它的输入是$b,c$，接着第二步$v=a+u$，最后一步$J=3v$。举个例子: $a=5,b=3,c=2$ ，$u=bc$就是6，$v=a+u$ ，就是5+6=11。$J$是3倍的 ，因此。即$3×(5+3×2)$。如果你把它算出来，实际上得到33就是$J$的值。当有不同的或者一些特殊的输出变量时，例如本例中的$J$和逻辑回归中你想优化的代价函数$J$，因此计算图用来处理这些计算会很方便。从这个小例子中我们可以看出，通过一个从左向右的过程，你可以计算出$J$的值。为了计算导数，从右到左（红色箭头，和蓝色箭头的过程相反）的过程是用于计算导数最自然的方式。概括一下：计算图组织计算的形式是用蓝色箭头从左到右的计算，让我们看看下一个视频中如何进行反向红色箭头(也就是从右到左)的导数计算，让我们继续下一个视频的学习。 2.8 使用计算图求导数（Derivatives with a Computation Graph）在上一个视频中，我们看了一个例子使用流程计算图来计算函数J。现在我们清理一下流程图的描述，看看你如何利用它计算出函数$J$的导数。 下面用到的公式： $\frac{dJ}{du}=\frac{dJ}{dv}\frac{dv}{du}$ ， $\frac{dJ}{db}=\frac{dJ}{du}\frac{du}{db}$ ， $\frac{dJ}{da}=\frac{dJ}{du}\frac{du}{da}$ 这是一个流程图： 假设你要计算$\frac{ {dJ} }{ {dv} }$，那要怎么算呢？好，比如说，我们要把这个$v$值拿过来，改变一下，那么$J$的值会怎么变呢？ 所以定义上$J = 3v$，现在$v=11$，所以如果你让$v$增加一点点，比如到11.001，那么$J =3v =33.003$，所以我这里$v$增加了0.001，然后最终结果是$J$上升到原来的3倍，所以$\frac{ {dJ} }{ {dv} }=3$，因为对于任何 $v$ 的增量$J$都会有3倍增量，而且这类似于我们在上一个视频中的例子，我们有$f(a)=3a$，然后我们推导出$\frac{ {df}(a)}{ {da} }= 3$，所以这里我们有$J=3v$，所以$\frac{ {dJ} }{ {dv} } =3$，这里$J$扮演了$f$的角色，在之前的视频里的例子。 在反向传播算法中的术语，我们看到，如果你想计算最后输出变量的导数，使用你最关心的变量对$v$的导数，那么我们就做完了一步反向传播，在这个流程图中是一个反向步骤。 我们来看另一个例子，$\frac{ {dJ} }{da}$是多少呢？换句话说，如果我们提高$a$的数值，对$J$的数值有什么影响？ 好，我们看看这个例子。变量$a=5$，我们让它增加到5.001，那么对v的影响就是$a+u$，之前$v=11$，现在变成11.001，我们从上面看到现在$J$ 就变成33.003了，所以我们看到的是，如果你让$a$增加0.001，$J$增加0.003。那么增加$a$，我是说如果你把这个5换成某个新值，那么$a$的改变量就会传播到流程图的最右，所以$J$最后是33.003。所以J的增量是3乘以$a$的增量，意味着这个导数是3。 要解释这个计算过程，其中一种方式是：如果你改变了$a$，那么也会改变$v$，通过改变$v$，也会改变$J$，所以$J$值的净变化量，当你提升这个值（0.001），当你把$a$值提高一点点，这就是$J$的变化量（0.003）。 首先a增加了，$v$也会增加，$v$增加多少呢？这取决于$\frac{ {dv} }{da}$，然后$v$的变化导致$J$也在增加，所以这在微积分里实际上叫链式法则，如果$a$影响到$v$，$v$影响到$J$，那么当你让$a$变大时，$J$的变化量就是当你改变$a$时，$v$的变化量乘以改变$v$时$J$的变化量，在微积分里这叫链式法则。 我们从这个计算中看到，如果你让$a$增加0.001，$v$也会变化相同的大小，所以$\frac{ {dv} }{da}= 1$。事实上，如果你代入进去，我们之前算过$\frac{ {dJ} }{ {dv} } =3$，$\frac{ {dv} }{da} =1$，所以这个乘积3×1，实际上就给出了正确答案，$\frac{ {dJ} }{da} = 3$。 这张小图表示了如何计算，$\frac{ {dJ} }{ {dv} }$就是$J$对变量$v$的导数，它可以帮助你计算$\frac{ {dJ} }{da}$，所以这是另一步反向传播计算。 现在我想介绍一个新的符号约定，当你编程实现反向传播时，通常会有一个最终输出值是你要关心的，最终的输出变量，你真正想要关心或者说优化的。在这种情况下最终的输出变量是J，就是流程图里最后一个符号，所以有很多计算尝试计算输出变量的导数，所以输出变量对某个变量的导数，我们就用$dvar$命名，所以在很多计算中你需要计算最终输出结果的导数，在这个例子里是$J$，还有各种中间变量，比如$a、b、c、u、v$，当你在软件里实现的时候，变量名叫什么？你可以做的一件事是，在python中，你可以写一个很长的变量名，比如${dFinalOutputvar}\_{dvar}$，但这个变量名有点长，我们就用$dJ\_dvar$，但因为你一直对$dJ$求导，对这个最终输出变量求导。我这里要介绍一个新符号，在程序里，当你编程的时候，在代码里，我们就使用变量名$dvar$，来表示那个量。 好，所以在程序里是$dvar$表示导数，你关心的最终变量$J$的导数，有时最后是$L$，对代码中各种中间量的导数，所以代码里这个东西，你用$dv$表示这个值，所以$dv=3$，你的代码表示就是$da=3$。 好，所以我们通过这个流程图完成部分的后向传播算法。我们在下一张幻灯片看看这个例子剩下的部分。 我们清理出一张新的流程图，我们回顾一下，到目前为止，我们一直在往回传播，并计算$dv=3$，再次，$dv$是代码里的变量名，其真正的定义是$\frac{ {dJ} }{ {dv} }$。我发现$da=3$，再次，$da$是代码里的变量名，其实代表$\frac{ {dJ} }{da}$的值。 大概手算了一下，两条直线怎么计算反向传播。 好，我们继续计算导数，我们看看这个值$u$，那么$\frac{dJ}{du}$是多少呢？通过和之前类似的计算，现在我们从$u=6$出发，如果你令$u$增加到6.001，那么$v$之前是11，现在变成11.001了，$J$ 就从33变成33.003，所以$J$ 增量是3倍，所以$\frac{ {dJ} }{du}= 3$。对$u$的分析很类似对a的分析，实际上这计算起来就是$\frac{ {dJ} }{dv}\cdot \frac{ {dv} }{du}$，有了这个，我们可以算出$\frac{ {dJ} }{dv} =3$，$\frac{ {dv} }{du} = 1$，最终算出结果是$3×1=3$。 所以我们还有一步反向传播，我们最终计算出$du=3$，这里的$du$当然了，就是$\frac{ {dJ} }{du}$。 现在，我们仔细看看最后一个例子，那么$\frac{ {dJ} }{db}$呢？想象一下，如果你改变了$b$的值，你想要然后变化一点，让$J$ 值到达最大或最小，那么导数是什么呢？这个$J$函数的斜率，当你稍微改变$b$值之后。事实上，使用微积分链式法则，这可以写成两者的乘积，就是$\frac{ {dJ} }{du}\cdot \frac{ {du} }{db}$，理由是，如果你改变$b$一点点，所以$b$变化比如说3.001，它影响J的方式是，首先会影响$u$，它对$u$的影响有多大？好，$u$的定义是$b\cdot c$，所以$b=3$时这是6，现在就变成6.002了，对吧，因为在我们的例子中$c=2$，所以这告诉我们$\frac{ {du} }{db}= 2$当你让$b$增加0.001时，$u$就增加两倍。所以$\frac{ {du} }{db} =2$，现在我想$u$的增加量已经是$b$的两倍，那么$\frac{ {dJ} }{du}$是多少呢？我们已经弄清楚了，这等于3，所以让这两部分相乘，我们发现$\frac{ {dJ} }{db}= 6$。 好，这就是第二部分的推导，其中我们想知道 $u$ 增加0.002，会对$J$ 有什么影响。实际上$\frac{ {dJ} }{du}=3$，这告诉我们u增加0.002之后，$J$上升了3倍，那么$J$ 应该上升0.006，对吧。这可以从$\frac{ {dJ} }{du}= 3$推导出来。 如果你仔细看看这些数学内容，你会发现，如果$b$变成3.001，那么$u$就变成6.002，$v$变成11.002，然后$J=3v=33.006$，对吧？这就是如何得到$\frac{ {dJ} }{db}= 6$。 为了填进去，如果我们反向走的话，$db=6$，而$db$其实是Python代码中的变量名，表示$\frac{ {dJ} }{db}$。 我不会很详细地介绍最后一个例子，但事实上，如果你计算$\frac{ {dJ} }{dc} =\frac{ {dJ} }{du}\cdot \frac{ {du} }{dc} = 3 \times 3$，这个结果是9。 我不会详细说明这个例子，在最后一步，我们可以推出$dc=9$。 所以这个视频的要点是，对于那个例子，当计算所有这些导数时，最有效率的办法是从右到左计算，跟着这个红色箭头走。特别是当我们第一次计算对$v$的导数时，之后在计算对$a$导数就可以用到。然后对$u$的导数，比如说这个项和这里这个项： 可以帮助计算对$b$的导数，然后对$c$的导数。 所以这是一个计算流程图，就是正向或者说从左到右的计算来计算成本函数J，你可能需要优化的函数，然后反向从右到左计算导数。如果你不熟悉微积分或链式法则，我知道这里有些细节讲的很快，但如果你没有跟上所有细节，也不用怕。在下一个视频中，我会再过一遍。在逻辑回归的背景下过一遍，并给你介绍需要做什么才能编写代码，实现逻辑回归模型中的导数计算。 2.9 逻辑回归中的梯度下降（Logistic Regression Gradient Descent）本节我们讨论怎样通过计算偏导数来实现逻辑回归的梯度下降算法。它的关键点是几个重要公式，其作用是用来实现逻辑回归中梯度下降算法。但是在本节视频中，我将使用计算图对梯度下降算法进行计算。我必须要承认的是，使用计算图来计算逻辑回归的梯度下降算法有点大材小用了。但是，我认为以这个例子作为开始来讲解，可以使你更好的理解背后的思想。从而在讨论神经网络时，你可以更深刻而全面地理解神经网络。接下来让我们开始学习逻辑回归的梯度下降算法。 假设样本只有两个特征${ {x}_{1} }$和${ {x}_{2} }$，为了计算$z$，我们需要输入参数${ {w}_{1} }$、${ {w}_{2} }$ 和$b$，除此之外还有特征值${ {x}_{1} }$和${ {x}_{2} }$。因此$z$的计算公式为： $z={ {w}_{1} }{ {x}_{1} }+{ {w}_{2} }{ {x}_{2} }+b$ 回想一下逻辑回归的公式定义如下： $\hat{y}=a=\sigma (z)$ 其中$z={ {w}^{T} }x+b$ $\sigma \left( z \right)=\frac{1}{1+{ {e}^{-z} }}$ 损失函数： $L( { {{\hat{y} }}^{(i)} },{ {y}^{(i)} })=-{ {y}^{(i)} }\log { {\hat{y} }^{(i)} }-(1-{ {y}^{(i)} })\log (1-{ {\hat{y} }^{(i)} })$ 代价函数： $J\left( w,b \right)=\frac{1}{m}\sum\nolimits_{i}^{m}{L( { {{\hat{y} }}^{(i)} },{ {y}^{(i)} })}$ 假设现在只考虑单个样本的情况，单个样本的代价函数定义如下： $L(a,y)=-(y\log (a)+(1-y)\log (1-a))$ 其中$a$是逻辑回归的输出，$y$是样本的标签值。现在让我们画出表示这个计算的计算图。这里先复习下梯度下降法，$w$和$b$的修正量可以表达如下： $w:=w-a \frac{\partial J(w,b)}{\partial w}$，$b:=b-a\frac{\partial J(w,b)}{\partial b}$ 如图：在这个公式的外侧画上长方形。然后计算： $\hat{y}=a=\sigma(z)$ 也就是计算图的下一步。最后计算损失函数$L(a,y)$。有了计算图，我就不需要再写出公式了。因此，为了使得逻辑回归中最小化代价函数$L(a,y)$，我们需要做的仅仅是修改参数$w$和$b$的值。前面我们已经讲解了如何在单个训练样本上计算代价函数的前向步骤。现在让我们来讨论通过反向计算出导数。因为我们想要计算出的代价函数$L(a,y)$的导数，首先我们需要反向计算出代价函数$L(a,y)$关于$a$的导数，在编写代码时，你只需要用$da$ 来表示$\frac{dL(a,y)}{da}$ 。通过微积分得到： $\frac{dL(a,y)}{da}=-y/a+(1-y)/(1-a)$ 如果你不熟悉微积分，也不必太担心，我们会列出本课程涉及的所有求导公式。那么如果你非常熟悉微积分，我们鼓励你主动推导前面介绍的代价函数的求导公式，使用微积分直接求出$L(a,y)$关于变量$a$的导数。如果你不太了解微积分，也不用太担心。现在我们已经计算出$da$，也就是最终输出结果的导数。现在可以再反向一步，在编写Python代码时，你只需要用$dz$来表示代价函数$L$关于$z$ 的导数$\frac{dL}{dz}$，也可以写成$\frac{dL(a,y)}{dz}$，这两种写法都是正确的。 $\frac{dL}{dz}=a-y$ 。 因为$\frac{dL(a,y)}{dz}=\frac{dL}{dz}=(\frac{dL}{da})\cdot (\frac{da}{dz})$，并且$\frac{da}{dz}=a\cdot (1-a)$，而 $\frac{dL}{da}=(-\frac{y}{a}+\frac{(1-y)}{(1-a)})$，因此将这两项相乘，得到： ${dz} = \frac{ {dL}(a,y)}{ {dz} } = \frac{ {dL} }{ {dz} } = \left( \frac{ {dL} }{ {da} } \right) \cdot \left(\frac{ {da} }{ {dz} } \right) = ( - \frac{y}{a} + \frac{(1 - y)}{(1 - a)})\cdot a(1 - a) = a - y$ 视频中为了简化推导过程，假设${ {n}_{x} }$ 这个推导的过程就是我之前提到过的链式法则。如果你对微积分熟悉，放心地去推导整个求导过程，如果不熟悉微积分，你只需要知道$dz=(a-y)$已经计算好了。 现在进行最后一步反向推导，也就是计算$w$和$b$变化对代价函数$L$的影响，特别地，可以用: $d{ {w}_{1} }=\frac{1}{m}\sum\limits_{i}^{m}{x_{1}^{(i)} }({ {a}^{(i)} }-{ {y}^{(i)} })$ $d{ {w}_{2} }=\frac{1}{m}\sum\limits_{i}^{m}{x_{2}^{(i)} }({ {a}^{(i)} }-{ {y}^{(i)} })$ $db=\frac{1}{m}\sum\limits_{i}^{m}{({ {a}^{(i)} }-{ {y}^{(i)} })}$ 视频中， $d{ {w}_{1} }$ 表示$\frac{\partial L}{\partial { {w}_{1} }}={ {x}_{1} }\cdot dz$， $d{ {w}_{\text{2} }}$ 表示$\frac{\partial L}{\partial { {w}_{2} }}={ {x}_{2} }\cdot dz$， $db=dz$。 因此，关于单个样本的梯度下降算法，你所需要做的就是如下的事情：使用公式$dz=(a-y)$计算$dz$，使用$d{ {w}_{1} }={ {x}_{1} }\cdot dz$ 计算$d{ {w}_{1} }$， $d{ {w}_{2} }={ {x}_{2} }\cdot dz$计算$d{ {w}_{2} }$， $db=dz$ 来计算$db$， 然后:更新${ {w}_{1} }={ {w}_{1} }-a d{ {w}_{1} }$，更新${ {w}_{2} }={ {w}_{2} }-a d{ {w}_{2} }$，更新$b=b-\alpha db$。这就是关于单个样本实例的梯度下降算法中参数更新一次的步骤。现在你已经知道了怎样计算导数，并且实现针对单个训练样本的逻辑回归的梯度下降算法。但是，训练逻辑回归模型不仅仅只有一个训练样本，而是有$m$个训练样本的整个训练集。因此在下一节视频中，我们将这些思想应用到整个训练样本集中，而不仅仅只是单个样本上。 2.10 m 个样本的梯度下降(Gradient Descent on m Examples)在之前的视频中,你已经看到如何计算导数，以及应用梯度下降在逻辑回归的一个训练样本上。现在我们想要把它应用在$m$个训练样本上。 首先，让我们时刻记住有关于损失函数$J(w,b)$的定义。 $J(w,b)=\frac{1}{m}\sum\limits_{i=1}^{m}{L({ {a}^{(i)} },{ {y}^{(i)} })}$ 当你的算法输出关于样本$y$的${ {a}^{(i)} }$，${ {a}^{(i)} }$是训练样本的预测值，即：$\sigma ( { {z}^{(i)} })=\sigma( { {w}^{T} }{ {x}^{\left( i \right)} }+b)$。所以我们在前面的幻灯中展示的是对于任意单个训练样本，如何计算微分当你只有一个训练样本。因此$d{ {w}_{1} }$，$d{ {w}_{\text{2} }}$和$db$ 添上上标$i$表示你求得的相应的值。如果你面对的是我们在之前的幻灯中演示的那种情况，但只使用了一个训练样本$({ {x}^{(i)} },{ {y}^{(i)} })$。现在你知道带有求和的全局代价函数，实际上是1到$m$项各个损失的平均。 所以它表明全局代价函数对${ {w}_{1} }$的微分，对${ {w}_{1} }$的微分也同样是各项损失对${ {w}_{1} }$微分的平均。 但之前我们已经演示了如何计算这项，即之前幻灯中演示的如何对单个训练样本进行计算。所以你真正需要做的是计算这些微分，如我们在之前的训练样本上做的。并且求平均，这会给你全局梯度值，你能够把它直接应用到梯度下降算法中。 所以这里有很多细节，但让我们把这些装进一个具体的算法。同时你需要一起应用的就是逻辑回归和梯度下降。 我们初始化$J=0,d{ {w}_{1} }=0,d{ {w}_{2} }=0,db=0$ 代码流程： J=0;dw1=0;dw2=0;db=0; for i = 1 to m z(i) = wx(i)+b; a(i) = sigmoid(z(i)); J += -[y(i)log(a(i))+(1-y(i)）log(1-a(i)); dz(i) = a(i)-y(i); dw1 += x1(i)dz(i); dw2 += x2(i)dz(i); db += dz(i); J/= m; dw1/= m; dw2/= m; db/= m; w=w-alpha*dw b=b-alpha*db幻灯片上只应用了一步梯度下降。因此你需要重复以上内容很多次，以应用多次梯度下降。看起来这些细节似乎很复杂，但目前不要担心太多。希望你明白，当你继续尝试并应用这些在编程作业里，所有这些会变的更加清楚。 但这种计算中有两个缺点，也就是说应用此方法在逻辑回归上你需要编写两个for循环。第一个for循环是一个小循环遍历$m$个训练样本，第二个for循环是一个遍历所有特征的for循环。这个例子中我们只有2个特征，所以$n$等于2并且${ {n}_{x} }$ 等于2。 但如果你有更多特征，你开始编写你的因此$d{ {w}_{1} }$，$d{ {w}_{2} }$，你有相似的计算从$d{ {w}_{3} }$一直下去到$d{ {w}_{n} }$。所以看来你需要一个for循环遍历所有$n$个特征。 当你应用深度学习算法，你会发现在代码中显式地使用for循环使你的算法很低效，同时在深度学习领域会有越来越大的数据集。所以能够应用你的算法且没有显式的for循环会是重要的，并且会帮助你适用于更大的数据集。所以这里有一些叫做向量化技术,它可以允许你的代码摆脱这些显式的for循环。 我想在先于深度学习的时代，也就是深度学习兴起之前，向量化是很棒的。可以使你有时候加速你的运算，但有时候也未必能够。但是在深度学习时代向量化，摆脱for循环已经变得相当重要。因为我们越来越多地训练非常大的数据集，因此你真的需要你的代码变得非常高效。所以在接下来的几个视频中，我们会谈到向量化，以及如何应用向量化而连一个for循环都不使用。所以学习了这些，我希望你有关于如何应用逻辑回归，或是用于逻辑回归的梯度下降，事情会变得更加清晰。当你进行编程练习，但在真正做编程练习之前让我们先谈谈向量化。然后你可以应用全部这些东西，应用一个梯度下降的迭代而不使用任何for循环。 2.11 向量化(Vectorization)参考视频: 2.11 向量化 向量化是非常基础的去除代码中for循环的艺术，在深度学习安全领域、深度学习实践中，你会经常发现自己训练大数据集，因为深度学习算法处理大数据集效果很棒，所以你的代码运行速度非常重要，否则如果在大数据集上，你的代码可能花费很长时间去运行，你将要等待非常长的时间去得到结果。所以在深度学习领域，运行向量化是一个关键的技巧，让我们举个栗子说明什么是向量化。 在逻辑回归中你需要去计算$z={ {w}^{T} }x+b$，$w$、$x$都是列向量。如果你有很多的特征那么就会有一个非常大的向量，所以$w\in { {\mathbb{R} }^{ {{n}_{x} }} }$ , $x\in{ {\mathbb{R} }^{ {{n}_{x} }} }$，所以如果你想使用非向量化方法去计算${ {w}^{T} }x$，你需要用如下方式（python） z=0 for i in range(n_x): z += w[i]*x[i] z += b这是一个非向量化的实现，你会发现这真的很慢，作为一个对比，向量化实现将会非常直接计算${ {w}^{T} }x$，代码如下： z=np.dot(w,x)+b 这是向量化计算${ {w}^{T} }x$的方法，你将会发现这个非常快 让我们用一个小例子说明一下，在我的我将会写一些代码（以下为教授在他的Jupyter notebook上写的Python代码，） import numpy as np #导入numpy库 a = np.array([1,2,3,4]) #创建一个数据a print(a) # [1 2 3 4] import time #导入时间库 a = np.random.rand(1000000) b = np.random.rand(1000000) #通过round随机得到两个一百万维度的数组 tic = time.time() #现在测量一下当前时间 #向量化的版本 c = np.dot(a,b) toc = time.time() print(&quot;Vectorized version:&quot; + str(1000*(toc-tic)) +&quot;ms&quot;) #打印一下向量化的版本的时间 #继续增加非向量化的版本 c = 0 tic = time.time() for i in range(1000000): c += a[i]*b[i] toc = time.time() print(c) print(&quot;For loop:&quot; + str(1000*(toc-tic)) + &quot;ms&quot;)#打印for循环的版本的时间返回值见图。 在两个方法中，向量化和非向量化计算了相同的值，如你所见，向量化版本花费了1.5毫秒，非向量化版本的for循环花费了大约几乎500毫秒，非向量化版本多花费了300倍时间。所以在这个例子中，仅仅是向量化你的代码，就会运行300倍快。这意味着如果向量化方法需要花费一分钟去运行的数据，for循环将会花费5个小时去运行。 一句话总结，以上都是再说和for循环相比，向量化可以快速得到结果。 你可能听过很多类似如下的话，“大规模的深度学习使用了GPU或者图像处理单元实现”，但是我做的所有的案例都是在jupyter notebook上面实现，这里只有CPU，CPU和GPU都有并行化的指令，他们有时候会叫做SIMD指令，这个代表了一个单独指令多维数据，这个的基础意义是，如果你使用了built-in函数,像np.function或者并不要求你实现循环的函数，它可以让python的充分利用并行化计算，这是事实在GPU和CPU上面计算，GPU更加擅长SIMD计算，但是CPU事实上也不是太差，可能没有GPU那么擅长吧。接下来的视频中，你将看到向量化怎么能够加速你的代码，经验法则是，无论什么时候，避免使用明确的for循环。 以下代码及运行结果截图： 2.12 向量化的更多例子（More Examples of Vectorization）从上节视频中，你知道了怎样通过numpy内置函数和避开显式的循环(loop)的方式进行向量化，从而有效提高代码速度。 经验提醒我，当我们在写神经网络程序时，或者在写逻辑(logistic)回归，或者其他神经网络模型时，应该避免写循环(loop)语句。虽然有时写循环(loop)是不可避免的，但是我们可以使用比如numpy的内置函数或者其他办法去计算。当你这样使用后，程序效率总是快于循环(loop)。 让我们看另外一个例子。如果你想计算向量$u=Av$，这时矩阵乘法定义为，矩阵乘法的定义就是：$u_{i} =\sum_{j}^{}{A_{\text{ij} }v_{i} }$，这取决于你怎么定义$u_{i}$值。同样使用非向量化实现，$u=np.zeros(n,1)$， 并且通过两层循环$for(i):for(j):$，得到$u[i]=u[i]+A[i][j]*v[j]$ 。现在就有了$i$ 和 $j$ 的两层循环，这就是非向量化。向量化方式就可以用$u=np.dot(A,v)$，右边这种向量化实现方式，消除了两层循环使得代码运行速度更快。 下面通过另一个例子继续了解向量化。如果你已经有一个向量$v$，并且想要对向量$v$的每个元素做指数操作，得到向量$u$等于$e$的$v_1$，$e$的$v_2$，一直到$e$的$v_n$次方。这里是非向量化的实现方式，首先你初始化了向量$u=np.zeros(n,1)$，并且通过循环依次计算每个元素。但事实证明可以通过python的numpy内置函数，帮助你计算这样的单个函数。所以我会引入import numpy as np，执行 $u=np.exp(v)$ 命令。注意到，在之前有循环的代码中，这里仅用了一行代码，向量$v$作为输入，$u$作为输出。你已经知道为什么需要循环，并且通过右边代码实现，效率会明显的快于循环方式。 事实上，numpy库有很多向量函数。比如 u=np.log是计算对数函数($log$)、 np.abs() 计算数据的绝对值、np.maximum(v, 0) 按元素计算$v$中每个元素和和0相比的最大值，v**2 代表获得元素 $v$ 每个值的平方、 1/v 获取 $v$ 中每个元素的倒数等等。所以当你想写循环时候，检查numpy是否存在类似的内置函数，从而避免使用循环(loop)方式。 那么，将刚才所学到的内容，运用在逻辑回归的梯度下降上，看看我们是否能简化两个计算过程中的某一步。这是我们逻辑回归的求导代码，有两层循环。在这例子我们有$n$个特征值。如果你有超过两个特征时，需要循环 $dw_1$ 、$dw_2$ 、$dw_3$ 等等。所以 $j$ 的实际值是1、2 和 $n_x$，就是你想要更新的值。所以我们想要消除第二循环，在这一行，这样我们就不用初始化 $dw_1$ ， $dw_2$ 都等于0。去掉这些，而是定义 $dw$ 为一个向量，设置 $u=np.zeros(n(x),1)$。定义了一个$x$行的一维向量，从而替代循环。我们仅仅使用了一个向量操作 $dw=dw+x^{(i)}dz^{(i)}$ 。最后，我们得到 $dw=dw/m$ 。现在我们通过将两层循环转成一层循环，我们仍然还有这个循环训练样本。 希望这个视频给了你一点向量化感觉，减少一层循环使你代码更快，但事实证明我们能做得更好。所以在下个视频，我们将进一步的讲解逻辑回归，你将会看到更好的监督学习结果。在训练中不需要使用任何 for 循环，你也可以写出代码去运行整个训练集。到此为止一切都好，让我们看下一个视频。 2.13 向量化逻辑回归(Vectorizing Logistic Regression)我们已经讨论过向量化是如何显著加速你的代码，在本次视频中我们将讨论如何实现逻辑回归的向量化计算。这样就能处理整个数据集，甚至不会用一个明确的for循环就能实现对于整个数据集梯度下降算法的优化。我对这项技术感到非常激动，并且当我们后面谈到神经网络时同样也不会用到一个明确的 for 循环。 让我们开始吧，首先我们回顾一下逻辑回归的前向传播步骤。所以，如果你有 $m$ 个训练样本，然后对第一个样本进行预测，你需要这样计算。计算 $z$，我正在使用这个熟悉的公式 $z^{(1)}=w^{T}x^{(1)}+b$ 。然后计算激活函数 $a^{(1)}=\sigma (z^{(1)})$ ，计算第一个样本的预测值 $y$ 。 然后对第二个样本进行预测，你需要计算 $z^{(2)}=w^{T}x^{(2)}+b$ ， $a^{(2)}=\sigma (z^{(2)})$ 。然后对第三个样本进行预测，你需要计算 $z^{(3)}=w^{T}x^{(3)}+b$ ， $a^{(3)}=\sigma (z^{(3)})$ ，依次类推。如果你有 $m$ 个训练样本，你可能需要这样做 $m$ 次，可以看出，为了完成前向传播步骤，即对我们的 $m$ 个样本都计算出预测值。有一个办法可以并且不需要任何一个明确的for循环。让我们来看一下你该怎样做。 首先，回忆一下我们曾经定义了一个矩阵 $X$ 作为你的训练输入，(如下图中蓝色 $X$ )像这样在不同的列中堆积在一起。这是一个 $n_x$ 行 $m$ 列的矩阵。我现在将它写为Python numpy的形式 $$(n_{x},m)$$ ，这只是表示 $X$ 是一个 $n_x$ 乘以 $m$ 的矩阵 $$R^{n_x \times m}$$。 现在我首先想做的是告诉你该如何在一个步骤中计算 $z_1$、 $z_2$ 、$z_3$ 等等。实际上，只用了一行代码。所以，我打算先构建一个 $1\times m$ 的矩阵，实际上它是一个行向量，同时我准备计算 $z^{(1)}$， $z^{(2)}$ ……一直到 $z^{(m)}$ ，所有值都是在同一时间内完成。结果发现它可以表达为 $w$ 的转置乘以大写矩阵 $x$ 然后加上向量 $[b b...b]$ ， $([z^{(1)} z^{(2)}...z^{(m)}]=w^{T}+[bb...b])$ 。$[b b...b]$ 是一个 $1\times m$ 的向量或者 $1\times m$ 的矩阵或者是一个 $m$ 维的行向量。所以希望你熟悉矩阵乘法，你会发现的 $w$ 转置乘以 $x^{(1)}$ ， $x^{(2)}$ 一直到 $x^{(m)}$ 。所以 $w$ 转置可以是一个行向量。所以第一项 $w^{T}X$ 将计算 $w$ 的转置乘以 $x^{(1)}$， $w$ 转置乘以$x^{(2)}$ 等等。然后我们加上第二项 $[b b...b]$ ，你最终将 $b$ 加到了每个元素上。所以你最终得到了另一个 $1\times m$ 的向量， $[z^{(1)} z^{(2)}...z^{(m)}]=w^{T}X+[b b...b]=[w^{T}x^{(1)}+b,w^{T}x^{(2)}+b...w^{T}x^{(m)}+b]$ 。 $w^{T}x^{(1)}+b$ 这是第一个元素，$w^{T}x^{(2)}+b$ 这是第二个元素， $w^{T}x^{(m)}+b$ 这是第 $m$ 个元素。 如果你参照上面的定义，第一个元素恰好是 $z^{(1)}$ 的定义，第二个元素恰好是 $z^{(2)}$ 的定义，等等。所以，因为$X$是一次获得的，当你得到你的训练样本，一个一个横向堆积起来，这里我将 $[z^{(1)} z^{(2)} ... z^{(m)}]$ 定义为大写的 $Z$ ，你用小写 $z$ 表示并将它们横向排在一起。所以当你将不同训练样本对应的小写 $x$ 横向堆积在一起时得到大写变量 $X$ 并且将小写变量也用相同方法处理，将它们横向堆积起来，你就得到大写变量 $Z$ 。结果发现，为了计算 $W^{T}X+[b b ... b]$ ，numpy命令是$Z=np.dot(w.T,X)+b$。这里在Python中有一个巧妙的地方，这里 $b$ 是一个实数，或者你可以说是一个 $1\times 1$ 矩阵，只是一个普通的实数。但是当你将这个向量加上这个实数时，Python自动把这个实数 $b$ 扩展成一个 $1\times m$ 的行向量。所以这种情况下的操作似乎有点不可思议，它在Python中被称作广播(brosdcasting)，目前你不用对此感到顾虑，我们将在下一个视频中进行进一步的讲解。话说回来它只用一行代码，用这一行代码，你可以计算大写的 $Z$，而大写 $Z$ 是一个包含所有小写$z^{(1)}$ 到 $ z^{(m)}$ 的 $1\times m$ 的矩阵。这就是 $Z$ 的内容，关于变量 $a$ 又是如何呢？ 我们接下来要做的就是找到一个同时计算 $[a^{(1)} a^{(2)} ... a^{(m)}]$ 的方法。就像把小写 $x$ 堆积起来得到大写 $X$ 和横向堆积小写 $z$ 得到大写 $Z$ 一样，堆积小写变量 $a$ 将形成一个新的变量，我们将它定义为大写 $A$。在编程作业中，你将看到怎样用一个向量在sigmoid函数中进行计算。所以sigmoid函数中输入大写 $Z$ 作为变量并且非常高效地输出大写 $A$。你将在编程作业中看到它的细节。 总结一下，在这张幻灯片中我们已经看到，不需要for循环，利用 $m$ 个训练样本一次性计算出小写 $z$ 和小写 $a$，用一行代码即可完成。 Z = np.dot(w.T,X) + b 这一行代码：$A=[a^{(1)} a^{(2)} ... a^{(m)}]=\sigma (Z)$ ，通过恰当地运用$\sigma$一次性计算所有 $a$。这就是在同一时间内你如何完成一个所有 $m$ 个训练样本的前向传播向量化计算。 概括一下，你刚刚看到如何利用向量化在同一时间内高效地计算所有的激活函数的所有 $a$值。接下来，可以证明，你也可以利用向量化高效地计算反向传播并以此来计算梯度。让我们在下一个视频中看该如何实现。 2.14 向量化 logistic 回归的梯度输出（Vectorizing Logistic Regression’s Gradient）注：本节中大写字母代表向量，小写字母代表元素 如何向量化计算的同时，对整个训练集预测结果$a$，这是我们之前已经讨论过的内容。在本次视频中我们将学习如何向量化地计算$m$个训练数据的梯度，本次视频的重点是如何同时计算 $m$ 个数据的梯度，并且实现一个非常高效的逻辑回归算法(Logistic Regression)。 之前我们在讲梯度计算的时候，列举过几个例子， $dz^{(1)}=a^{(1)}-y^{(1)}$，$dz^{(2)}=a^{(2)}-y^{(2)}$ ……等等一系列类似公式。现在，对 $m$个训练数据做同样的运算，我们可以定义一个新的变量 $dZ=[dz^{(1)} ,dz^{(2)} ... dz^{(m)}]$，所有的 $dz$ 变量横向排列，因此，$dZ$ 是一个 $1\times m$ 的矩阵，或者说，一个 $m$ 维行向量。在之前的幻灯片中，我们已经知道如何计算$A$，即 $[a^{(1)},a^{(2)} ... a^{(m)}]$,我们需要找到这样的一个行向量 $Y=[y^{(1)} y^{(2)} ... y^{(m)}]$ ，由此，我们可以这样计算 $dZ=A-Y=[a^{(1)}-y^{(1)} a^{(2)}-y^{(2)} ... a^{(m)}-y^{(m)}]$，不难发现第一个元素就是 $dz^{(1)}$，第二个元素就是 $dz^{(2)}$ ……所以我们现在仅需一行代码，就可以同时完成这所有的计算。 在之前的实现中，我们已经去掉了一个for循环，但我们仍有一个遍历训练集的循环，如下所示： $dw=0$ $dw + = x^{(1)}*{dz}^{(1)}$ $dw + = x^{(2)}\ *dz^{(2)}$ …………. $dw + = x^{(m)}*{dz}^{(m)}$ $dw = \frac{ {dw} }{m}$ $db = 0$ $db + = {dz}^{(1)}$ $db + = {dz}^{(2)}$ …………. $db + = dz^{(m)}$ $db = \frac{ {db} }{m}$ 上述（伪）代码就是我们在之前实现中做的，我们已经去掉了一个for循环，但用上述方法计算 $dw$ 仍然需要一个循环遍历训练集，我们现在要做的就是将其向量化！ 首先我们来看 $db$，不难发现 $$db=\frac{1}{m}\sum_{i=1}^{m}dz^{(i)}$$ ，之前的讲解中，我们知道所有的$dz^{i)}$已经组成一个行向量 $dZ$了，所以在Python中，我们很容易地想到$$db=\frac{1}{m}np.sum(dZ)$$；接下来看$dw$，我们先写出它的公式 $$dw=\frac{1}{m}Xdz^{T}$$其中，$X$ 是一个行向量。因此展开后 $$dw=\frac{1}{m}(x^{(1)}dz^{(1)}+x^{(2)}dz^{(2)}+…+x^{m}dz^{m})$$ 。因此我们可以仅用两行代码进行计算：$$db=\frac{1}{m}np.sum(dZ)$$， $$dw=\frac{1}{m}X*dz^{T}$$。这样，我们就避免了在训练集上使用for循环。 现在，让我们回顾一下，看看我们之前怎么实现的逻辑回归，可以发现，没有向量化是非常低效的，如下图所示代码： 我们的目标是不使用for循环，而是向量，我们可以这么做： $Z = w^{T}X + b = np.dot( w.T,X)+b$ $A = \sigma( Z )$ $dZ = A - Y$ ${ {dw} = \frac{1}{m}Xdz^{T}\ }$ $db= \frac{1}{m}*np.sum( dZ)$ $w: = w - a*dw$ $b: = b - a*db$ 现在我们利用前五个公式完成了前向和后向传播，也实现了对所有训练样本进行预测和求导，再利用后两个公式，梯度下降更新参数。我们的目的是不使用for循环，所以我们就通过一次迭代实现一次梯度下降，但如果你希望多次迭代进行梯度下降，那么仍然需要for循环，放在最外层。不过我们还是觉得一次迭代就进行一次梯度下降，避免使用任何循环比较舒服一些。 最后，我们得到了一个高度向量化的、非常高效的逻辑回归的梯度下降算法，我们将在下次视频中讨论Python中的Broadcasting技术。 2.15 Python 中的广播（Broadcasting in Python）这是一个不同食物(每100g)中不同营养成分的卡路里含量表格，表格为3行4列，列表示不同的食物种类，从左至右依次为苹果，牛肉，鸡蛋，土豆。行表示不同的营养成分，从上到下依次为碳水化合物，蛋白质，脂肪。 那么，我们现在想要计算不同食物中不同营养成分中的卡路里百分比。 现在计算苹果中的碳水化合物卡路里百分比含量，首先计算苹果（100g）中三种营养成分卡路里总和56+1.2+1.8= 59，然后用56/59 = 94.9%算出结果。 可以看出苹果中的卡路里大部分来自于碳水化合物，而牛肉则不同。 对于其他食物，计算方法类似。首先，按列求和，计算每种食物中（100g）三种营养成分总和，然后分别用不用营养成分的卡路里数量除以总和，计算百分比。 那么，能否不使用for循环完成这样的一个计算过程呢？ 假设上图的表格是一个4行3列的矩阵$A$，记为 $A_{3\times 4}$，接下来我们要使用Python的numpy库完成这样的计算。我们打算使用两行代码完成，第一行代码对每一列进行求和，第二行代码分别计算每种食物每种营养成分的百分比。 在jupyter notebook中输入如下代码，按shift+Enter运行，输出如下。 下面使用如下代码计算每列的和，可以看到输出是每种食物(100g)的卡路里总和。 其中sum的参数axis=0表示求和运算按列执行，之后会详细解释。 接下来计算百分比，这条指令将 $3\times 4$的矩阵$A$除以一个$1 \times 4$的矩阵，得到了一个 $3 \times 4$的结果矩阵，这个结果矩阵就是我们要求的百分比含量。 下面再来解释一下A.sum(axis = 0)中的参数axis。axis用来指明将要进行的运算是沿着哪个轴执行，在numpy中，0轴是垂直的，也就是列，而1轴是水平的，也就是行。 而第二个A/cal.reshape(1,4)指令则调用了numpy中的广播机制。这里使用 $3 \times 4$的矩阵$A$除以 $1 \times 4$的矩阵$cal$。技术上来讲，其实并不需要再将矩阵$cal$ reshape(重塑)成 $1 \times 4$，因为矩阵$cal$本身已经是 $1 \times 4$了。但是当我们写代码时不确定矩阵维度的时候，通常会对矩阵进行重塑来确保得到我们想要的列向量或行向量。重塑操作reshape是一个常量时间的操作，时间复杂度是$O(1)$，它的调用代价极低。 那么一个 $3 \times 4$ 的矩阵是怎么和 $1 \times 4$的矩阵做除法的呢？让我们来看一些更多的广播的例子。 在numpy中，当一个 $4 \times 1$的列向量与一个常数做加法时，实际上会将常数扩展为一个 $4 \times 1$的列向量，然后两者做逐元素加法。结果就是右边的这个向量。这种广播机制对于行向量和列向量均可以使用。 再看下一个例子。 用一个 $2 \times 3$的矩阵和一个 $1 \times 3$ 的矩阵相加，其泛化形式是 $m \times n$ 的矩阵和 $1 \times n$的矩阵相加。在执行加法操作时，其实是将 $1 \times n$ 的矩阵复制成为 $m \times n$ 的矩阵，然后两者做逐元素加法得到结果。针对这个具体例子，相当于在矩阵的第一列加100，第二列加200，第三列加300。这就是在前一张幻灯片中计算卡路里百分比的广播机制，只不过这里是除法操作（广播机制与执行的运算种类无关）。 下面是最后一个例子 这里相当于是一个 $m \times n$ 的矩阵加上一个 $m \times 1$ 的矩阵。在进行运算时，会先将 $m \times 1$ 矩阵水平复制 $n$ 次，变成一个 $m \times n$ 的矩阵，然后再执行逐元素加法。 广播机制的一般原则如下： 这里我先说一下我本人对numpy广播机制的理解，再解释上面这张PPT。 首先是numpy广播机制 如果两个数组的后缘维度的轴长度相符或其中一方的轴长度为1，则认为它们是广播兼容的。广播会在缺失维度和轴长度为1的维度上进行。 后缘维度的轴长度：A.shape[-1] 即矩阵维度元组中的最后一个位置的值 对于视频中卡路里计算的例子，矩阵 $A_{3,4}$ 后缘维度的轴长度是4，而矩阵 $cal_{1,4}$ 的后缘维度也是4，则他们满足后缘维度轴长度相符，可以进行广播。广播会在轴长度为1的维度进行，轴长度为1的维度对应axis=0，即垂直方向，矩阵 $$\text{cal}_{1,4}$$ 沿`axis=0`(垂直方向)复制成为 $$\text{cal_temp}_{3,4}$$ ，之后两者进行逐元素除法运算。 现在解释上图中的例子 矩阵 $A_{m,n}$ 和矩阵 $B_{1,n}$ 进行四则运算，后缘维度轴长度相符，可以广播，广播沿着轴长度为1的轴进行，即 $B_{1,n}$ 广播成为 ${B_{m,n} }'$ ，之后做逐元素四则运算。 矩阵 $A_{m,n}$ 和矩阵 $B_{m,1}$ 进行四则运算，后缘维度轴长度不相符，但其中一方轴长度为1，可以广播，广播沿着轴长度为1的轴进行，即 $B_{m,1}$ 广播成为 ${B_{m,n} }'$ ，之后做逐元素四则运算。 矩阵 $A_{m,1}$ 和常数$ R$ 进行四则运算，后缘维度轴长度不相符，但其中一方轴长度为1，可以广播，广播沿着缺失维度和轴长度为1的轴进行，缺失维度就是axis=0,轴长度为1的轴是axis=1，即$R$广播成为 ${B_{m,1} }'$ ，之后做逐元素四则运算。 最后，对于Matlab/Octave 有类似功能的函数bsxfun。 总结一下broadcasting，可以看看下面的图： 2.16 关于 python _ numpy 向量的说明（A note on python or numpy vectors）参考视频：本节主要讲Python中的numpy一维数组的特性，以及与行向量或列向量的区别。并介绍了老师在实际应用中的一些小技巧，去避免在coding中由于这些特性而导致的bug。 Python的特性允许你使用广播（broadcasting）功能，这是Python的numpy程序语言库中最灵活的地方。而我认为这是程序语言的优点，也是缺点。优点的原因在于它们创造出语言的表达性，Python语言巨大的灵活性使得你仅仅通过一行代码就能做很多事情。但是这也是缺点，由于广播巨大的灵活性，有时候你对于广播的特点以及广播的工作原理这些细节不熟悉的话，你可能会产生很细微或者看起来很奇怪的bug。例如，如果你将一个列向量添加到一个行向量中，你会以为它报出维度不匹配或类型错误之类的错误，但是实际上你会得到一个行向量和列向量的求和。 在Python的这些奇怪的影响之中，其实是有一个内在的逻辑关系的。但是如果对Python不熟悉的话，我就曾经见过的一些学生非常生硬、非常艰难地去寻找bug。所以我在这里想做的就是分享给你们一些技巧，这些技巧对我非常有用，它们能消除或者简化我的代码中所有看起来很奇怪的bug。同时我也希望通过这些技巧，你也能更容易地写没有bug的Python和numpy代码。 为了演示Python-numpy的一个容易被忽略的效果，特别是怎样在Python-numpy中构造向量，让我来做一个快速示范。首先设置$a=np.random.randn(5)$，这样会生成存储在数组 $a$ 中的5个高斯随机数变量。之后输出 $a$，从屏幕上可以得知，此时 $a$ 的shape（形状）是一个$(5,)$的结构。这在Python中被称作一个一维数组。它既不是一个行向量也不是一个列向量，这也导致它有一些不是很直观的效果。举个例子，如果我输出一个转置阵，最终结果它会和$a$看起来一样，所以$a$和$a$的转置阵最终结果看起来一样。而如果我输出$a$和$a$的转置阵的内积，你可能会想：$a$乘以$a$的转置返回给你的可能会是一个矩阵。但是如果我这样做，你只会得到一个数。 所以建议你编写神经网络时，不要使用shape为 (5,)_、(n,)_ 或者其他一维数组的数据结构。相反，如果你设置 $a$ 为$(5,1)$，那么这就将置于5行1列向量中。在先前的操作里 $a$ 和 $a$ 的转置看起来一样，而现在这样的 $a$ 变成一个新的 $a$ 的转置，并且它是一个行向量。请注意一个细微的差别，在这种数据结构中，当我们输出 $a$ 的转置时有两对方括号，而之前只有一对方括号，所以这就是1行5列的矩阵和一维数组的差别。 如果你输出 $a$ 和 $a$ 的转置的乘积，然后会返回给你一个向量的外积，是吧？所以这两个向量的外积返回给你的是一个矩阵。 就我们刚才看到的，再进一步说明。首先我们刚刚运行的命令是这个 $(a=np.random.randn(5))$，它生成了一个数据结构$a$，其中$a.shape$是$(5,)$。这被称作 $a$ 的一维数组，同时这也是一个非常有趣的数据结构。它不像行向量和列向量那样表现的很一致，这使得它带来一些不直观的影响。所以我建议，当你在编程练习或者在执行逻辑回归和神经网络时，你不需要使用这些一维数组。 相反，如果你每次创建一个数组，你都得让它成为一个列向量，产生一个$(5,1)$向量或者你让它成为一个行向量，那么你的向量的行为可能会更容易被理解。所以在这种情况下，$a.shape$等同于$(5,1)$。这种表现很像 $a$，但是实际上却是一个列向量。同时这也是为什么当它是一个列向量的时候，你能认为这是矩阵$(5,1)$；同时这里 $a.shape$ 将要变成$(1,5)$，这就像行向量一样。所以当你需要一个向量时，我会说用这个或那个(column vector or row vector)，但绝不会是一维数组。 我写代码时还有一件经常做的事，那就是如果我不完全确定一个向量的维度(dimension)，我经常会扔进一个断言语句(assertion statement)。像这样，去确保在这种情况下是一个$(5,1)$向量，或者说是一个列向量。这些断言语句实际上是要去执行的，并且它们也会有助于为你的代码提供信息。所以不论你要做什么，不要犹豫直接插入断言语句。如果你不小心以一维数组来执行，你也能够重新改变数组维数 $a=reshape$，表明一个$(5,1)$数组或者一个$(1,5)$数组，以致于它表现更像列向量或行向量。 我有时候看见学生因为一维数组不直观的影响，难以定位bug而告终。通过在原先的代码里清除一维数组，我的代码变得更加简洁。而且实际上就我在代码中表现的事情而言，我从来不使用一维数组。因此，要去简化你的代码，而且不要使用一维数组。总是使用 $n \times 1$ 维矩阵（基本上是列向量），或者 $1 \times n$ 维矩阵（基本上是行向量），这样你可以减少很多assert语句来节省核矩阵和数组的维数的时间。另外，为了确保你的矩阵或向量所需要的维数时，不要羞于 reshape 操作。 总之，我希望这些建议能帮助你解决一个Python中的bug，从而使你更容易地完成练习。 2.17 Jupyter/iPython Notebooks快速入门（Quick tour of Jupyter/iPython Notebooks）学到现在，你即将要开始处理你的第一个编程作业。但在那之前，让我快速地给你介绍一下在Coursera上的iPython Notebooks工具。 这就是Jupyter iPython Notebooks的界面，你可以通过它连接到Coursera。让我快速地讲解下它的一些特性。关于它的说明已经被写入这个Notebook中。 这里有一些空白区域的代码块，你可以在这里编写代码。有时，你也会看到一些函数块。而关于这些的说明都已经在iPython Notebook的文本中。在iPython Notebook中，在这些较长的灰色的区域就是代码块。 有时，你会看到代码块中有像这样的开始代码和结束代码。在进行编程练习时，请确保你的代码写在开始代码和结束代码之间。 比如，编写打印输出Hello World的代码，然后执行这一代码块（你可以按shift +enter来执行这一代码块）。最终，它就会输出我们想要的Hello World。 在运行一个单元格cell时，你也可以选择运行其中的一块代码区域。通过点击Cell菜单的Run Cells执行这部分代码。 也许，在你的计算机上，运行cell的键盘快捷方式可能并非是shift enter。但是，Mac应该和我的个人电脑一样，可以使用shift + enter来运行cell。 当你正在阅读指南时，如果不小心双击了它，点中的区域就会变成markdown语言形式。如果你不小心使其变成了这样的文本框，只要运行下单元格cell，就可以回到原来的形式。所以，点击cell菜单的Run Cells或者使用shift + enter，就可以使得它变回原样。 这里还有一些其他的小技巧。比如当你执行上面所使用的代码时，它实际上会使用一个内核在服务器上运行这段代码。如果你正在运行超负荷的进程，或者电脑运行了很长一段时间，或者在运行中出了错，又或者网络连接失败，这里依然有机会让Kernel重新工作。你只要点击Kernel，选择Restart，它会重新运行Kernel使程序继续工作。 所以，如果你只是运行相对较小的工作并且才刚刚启动你的ipad或笔记本电脑，这种情况应该是不会发生的。但是，如果你看见错误信息，比如Kernel已经中断或者其他信息,你可以试着重启Kernel。 当我使用iPython Notebook时会有多个代码区域块。尽管我并没有在前面的代码块中添加自己的代码，但还是要确保先执行这块代码。因为在这个例子，它导入了numpy包并另命名为np等，并声明了一些你可能需要的变量。为了能顺利地执行下面的代码，就必须确保先执行上面的代码，即使不要求你去写其他的代码。 最后，当你完成作业后，可以通过点击右上方蓝色的Submit Assignment按钮提交你的作业。 我发现这种交互式的shell命令，在iPython Notebooks是非常有用的，能使你快速地实现代码并且查看输出结果，便于学习。所以我希望这些练习和Jupyter iPython Notebooks会帮助你更快地学习和实践，并且帮助你了解如何去实现这些学习算法。后面一个视频是一个选学视频，它主要是讲解逻辑回归中的代价函数。你可以选择是否观看。不管怎样，都祝愿你能通过这两次编程作业。我会在新一周的课程里等待着你。 2.18 （选修）logistic 损失函数的解释（Explanation of logistic regression cost function）在前面的视频中，我们已经分析了逻辑回归的损失函数表达式，在这节选修视频中，我将给出一个简洁的证明来说明逻辑回归的损失函数为什么是这种形式。 回想一下，在逻辑回归中，需要预测的结果$\hat{y}$,可以表示为$\hat{y}=\sigma(w^{T}x+b)$，$\sigma$是我们熟悉的$S$型函数 $\sigma(z)=\sigma(w^{T}x+b)=\frac{1}{1+e^{-z} }$ 。我们约定 $\hat{y}=p(y=1|x)$ ，即算法的输出$\hat{y}$ 是给定训练样本 $x$ 条件下 $y$ 等于1的概率。换句话说，如果$y=1$，在给定训练样本 $x$ 条件下$y=\hat{y}$；反过来说，如果$y=0$，在给定训练样本$x$条件下 $y$ 等于1减去$\hat{y}(y=1-\hat{y})$，因此，如果 $\hat{y}$ 代表 $y=1$ 的概率，那么$1-\hat{y}$就是 $y=0$的概率。接下来，我们就来分析这两个条件概率公式。 这两个条件概率公式定义形式为 $p(y|x)$并且代表了 $y=0$ 或者 $y=1$ 这两种情况，我们可以将这两个公式合并成一个公式。需要指出的是我们讨论的是二分类问题的损失函数，因此，$y$的取值只能是0或者1。上述的两个条件概率公式可以合并成如下公式： $p(y|x)={\hat{y} }^{y}{(1-\hat{y})}^{(1-y)}$ 接下来我会解释为什么可以合并成这种形式的表达式：$(1-\hat{y})$的$(1-y)$次方这行表达式包含了上面的两个条件概率公式，我来解释一下为什么。 第一种情况，假设 $y=1$，由于$y=1$，那么${(\hat{y})}^{y}=\hat{y}$，因为 $\hat{y}$的1次方等于$\hat{y}$，$1-{(1-\hat{y})}^{(1-y)}$的指数项$(1-y)$等于0，由于任何数的0次方都是1，$\hat{y}$乘以1等于$\hat{y}$。因此当$y=1$时 $p(y|x)=\hat{y}$（图中绿色部分）。 第二种情况，当 $y=0$ 时 $p(y|x)$ 等于多少呢?假设$y=0$，$\hat{y}$的$y$次方就是 $$\hat{y}$$ 的0次方，任何数的0次方都等于1，因此 $p(y|x)=1×{(1-\hat{y})}^{1-y}$ ，前面假设 $y=0$ 因此$(1-y)$就等于1，因此 $p(y|x)=1×(1-\hat{y})$。因此在这里当$y=0$时，$p(y|x)=1-\hat{y}$。这就是这个公式(第二个公式，图中紫色字体部分)的结果。 因此，刚才的推导表明 $p(y|x)={\hat{y} }^{(y)}{(1-\hat{y})}^{(1-y)}$，就是 $p(y|x)$ 的完整定义。由于 log 函数是严格单调递增的函数，最大化 $log(p(y|x))$ 等价于最大化 $p(y|x)$ 并且地计算 $p(y|x)$ 的 log对数，就是计算 $log({\hat{y} }^{(y)}{(1-\hat{y})}^{(1-y)})$ (其实就是将 $p(y|x)$ 代入)，通过对数函数化简为： $ylog\hat{y}+(1-y)log(1-\hat{y})$ 而这就是我们前面提到的损失函数的负数 $(-L(\hat{y},y))$ ，前面有一个负号的原因是当你训练学习算法时需要算法输出值的概率是最大的（以最大的概率预测这个值），然而在逻辑回归中我们需要最小化损失函数，因此最小化损失函数与最大化条件概率的对数 $log(p(y|x))$ 关联起来了，因此这就是单个训练样本的损失函数表达式。 在 $m$个训练样本的整个训练集中又该如何表示呢，让我们一起来探讨一下。 让我们一起来探讨一下，整个训练集中标签的概率，更正式地来写一下。假设所有的训练样本服从同一分布且相互独立，也即独立同分布的，所有这些样本的联合概率就是每个样本概率的乘积: $P\left(\text{labels in training set} \right) = \prod_{i =1}^{m}{P(y^{(i)}|x^{(i)})}$。 如果你想做最大似然估计，需要寻找一组参数，使得给定样本的观测值概率最大，但令这个概率最大化等价于令其对数最大化，在等式两边取对数： $logp\left( \text{labels in training set} \right) = log\prod_{i =1}^{m}{P(y^{(i)}|x^{(i)})} = \sum_{i = 1}^{m}{logP(y^{(i)}|x^{(i)})} = \sum_{i =1}^{m}{- L(\hat y^{(i)},y^{(i)})}$ 在统计学里面，有一个方法叫做最大似然估计，即求出一组参数，使这个式子取最大值，也就是说，使得这个式子取最大值，$\sum_{i= 1}^{m}{- L(\hat y^{(i)},y^{(i)})}$，可以将负号移到求和符号的外面，$- \sum_{i =1}^{m}{L(\hat y^{(i)},y^{(i)})}$，这样我们就推导出了前面给出的logistic回归的成本函数$J(w,b)= \sum_{i = 1}^{m}{L(\hat y^{(i)},y^{\hat( i)})}$。 由于训练模型时，目标是让成本函数最小化，所以我们不是直接用最大似然概率，要去掉这里的负号，最后为了方便，可以对成本函数进行适当的缩放，我们就在前面加一个额外的常数因子$\frac{1}{m}$，即:$J(w,b)= \frac{1}{m}\sum_{i = 1}^{m}{L(\hat y^{(i)},y^{(i)})}$。 总结一下，为了最小化成本函数$J(w,b)$，我们从logistic回归模型的最大似然估计的角度出发，假设训练集中的样本都是独立同分布的条件下。尽管这节课是选修性质的，但还是感谢观看本节视频。我希望通过本节课您能更好地明白逻辑回归的损失函数，为什么是那种形式，明白了损失函数的原理，希望您能继续完成课后的练习，前面课程的练习以及本周的测验，在课后的小测验和编程练习中，祝您好运。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达机器学习笔记(6-10周)]]></title>
    <url>%2F2019%2F12%2F04%2Fnew_%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(6-10%E5%91%A8)%2F</url>
    <content type="text"><![CDATA[吴恩达机器学习笔记 应用机器学习的建议(Advice for Applying Machine Learning)决定下一步做什么参考视频: 10 - 1 - Deciding What to Try Next (6 min).mkv 到目前为止，我们已经介绍了许多不同的学习算法，如果你一直跟着这些视频的进度学习，你会发现自己已经不知不觉地成为一个了解许多先进机器学习技术的专家了。 然而，在懂机器学习的人当中依然存在着很大的差距，一部分人确实掌握了怎样高效有力地运用这些学习算法。而另一些人他们可能对我马上要讲的东西，就不是那么熟悉了。他们可能没有完全理解怎样运用这些算法。因此总是把时间浪费在毫无意义的尝试上。我想做的是确保你在设计机器学习的系统时，你能够明白怎样选择一条最合适、最正确的道路。因此，在这节视频和之后的几段视频中，我将向你介绍一些实用的建议和指导，帮助你明白怎样进行选择。具体来讲，我将重点关注的问题是假如你在开发一个机器学习系统，或者想试着改进一个机器学习系统的性能，你应如何决定接下来应该选择哪条道路？为了解释这一问题，我想仍然使用预测房价的学习例子，假如你已经完成了正则化线性回归，也就是最小化代价函数$J$的值，假如，在你得到你的学习参数以后，如果你要将你的假设函数放到一组新的房屋样本上进行测试，假如说你发现在预测房价时产生了巨大的误差，现在你的问题是要想改进这个算法，接下来应该怎么办？ 实际上你可以想出很多种方法来改进这个算法的性能，其中一种办法是使用更多的训练样本。具体来讲，也许你能想到通过电话调查或上门调查来获取更多的不同的房屋出售数据。遗憾的是，我看到好多人花费了好多时间想收集更多的训练样本。他们总认为，要是我有两倍甚至十倍数量的训练数据，那就一定会解决问题的是吧？但有时候获得更多的训练数据实际上并没有作用。在接下来的几段视频中，我们将解释原因。 我们也将知道怎样避免把过多的时间浪费在收集更多的训练数据上，这实际上是于事无补的。另一个方法，你也许能想到的是尝试选用更少的特征集。因此如果你有一系列特征比如$x_1,x_2,x_3$等等。也许有很多特征，也许你可以花一点时间从这些特征中仔细挑选一小部分来防止过拟合。或者也许你需要用更多的特征，也许目前的特征集，对你来讲并不是很有帮助。你希望从获取更多特征的角度来收集更多的数据，同样地，你可以把这个问题扩展为一个很大的项目，比如使用电话调查来得到更多的房屋案例，或者再进行土地测量来获得更多有关，这块土地的信息等等，因此这是一个复杂的问题。同样的道理，我们非常希望在花费大量时间完成这些工作之前，我们就能知道其效果如何。我们也可以尝试增加多项式特征的方法，比如$x_1$的平方，$x_2$的平方，$x_1,x_2$的乘积，我们可以花很多时间来考虑这一方法，我们也可以考虑其他方法减小或增大正则化参数$\lambda$的值。我们列出的这个单子，上面的很多方法都可以扩展开来扩展成一个六个月或更长时间的项目。遗憾的是，大多数人用来选择这些方法的标准是凭感觉的，也就是说，大多数人的选择方法是随便从这些方法中选择一种，比如他们会说“噢，我们来多找点数据吧”，然后花上六个月的时间收集了一大堆数据，然后也许另一个人说：“好吧，让我们来从这些房子的数据中多找点特征吧”。我很遗憾不止一次地看到很多人花了至少六个月时间来完成他们随便选择的一种方法，而在六个月或者更长时间后，他们很遗憾地发现自己选择的是一条不归路。幸运的是，有一系列简单的方法能让你事半功倍，排除掉单子上的至少一半的方法，留下那些确实有前途的方法，同时也有一种很简单的方法，只要你使用，就能很轻松地排除掉很多选择，从而为你节省大量不必要花费的时间。最终达到改进机器学习系统性能的目的假设我们需要用一个线性回归模型来预测房价，当我们运用训练好了的模型来预测未知数据的时候发现有较大的误差，我们下一步可以做什么？ 获得更多的训练样本——通常是有效的，但代价较大，下面的方法也可能有效，可考虑先采用下面的几种方法。 尝试减少特征的数量 尝试获得更多的特征 尝试增加多项式特征 尝试减少正则化程度$\lambda$ 尝试增加正则化程度$\lambda$ 我们不应该随机选择上面的某种方法来改进我们的算法，而是运用一些机器学习诊断法来帮助我们知道上面哪些方法对我们的算法是有效的。 在接下来的两段视频中，我首先介绍怎样评估机器学习算法的性能，然后在之后的几段视频中，我将开始讨论这些方法，它们也被称为”机器学习诊断法”。“诊断法”的意思是：这是一种测试法，你通过执行这种测试，能够深入了解某种算法到底是否有用。这通常也能够告诉你，要想改进一种算法的效果，什么样的尝试，才是有意义的。在这一系列的视频中我们将介绍具体的诊断法，但我要提前说明一点的是，这些诊断法的执行和实现，是需要花些时间的，有时候确实需要花很多时间来理解和实现，但这样做的确是把时间用在了刀刃上，因为这些方法让你在开发学习算法时，节省了几个月的时间，因此，在接下来几节课中，我将先来介绍如何评价你的学习算法。在此之后，我将介绍一些诊断法，希望能让你更清楚。在接下来的尝试中，如何选择更有意义的方法。 评估一个假设参考视频: 10 - 2 - Evaluating a Hypothesis (8 min).mkv 在本节视频中我想介绍一下怎样用你学过的算法来评估假设函数。在之后的课程中，我们将以此为基础来讨论如何避免过拟合和欠拟合的问题。 当我们确定学习算法的参数的时候，我们考虑的是选择参量来使训练误差最小化，有人认为得到一个非常小的训练误差一定是一件好事，但我们已经知道，仅仅是因为这个假设具有很小的训练误差，并不能说明它就一定是一个好的假设函数。而且我们也学习了过拟合假设函数的例子，所以这推广到新的训练集上是不适用的。 那么，你该如何判断一个假设函数是过拟合的呢？对于这个简单的例子，我们可以对假设函数$h(x)$进行画图，然后观察图形趋势，但对于特征变量不止一个的这种一般情况，还有像有很多特征变量的问题，想要通过画出假设函数来进行观察，就会变得很难甚至是不可能实现。 因此，我们需要另一种方法来评估我们的假设函数过拟合检验。 为了检验算法是否过拟合，我们将数据分成训练集和测试集，通常用70%的数据作为训练集，用剩下30%的数据作为测试集。很重要的一点是训练集和测试集均要含有各种类型的数据，通常我们要对数据进行“洗牌”，然后再分成训练集和测试集。 测试集评估在通过训练集让我们的模型学习得出其参数后，对测试集运用该模型，我们有两种方式计算误差： 对于线性回归模型，我们利用测试集数据计算代价函数$J$ 对于逻辑回归模型，我们除了可以利用测试数据集来计算代价函数外： $$ J_{test}{(\theta)} = -\frac{1}{ {m}_{test} }\sum_\limits{i=1}^{m_{test} }\log{h_{\theta}(x^{(i)}_{test})}+(1-{y^{(i)}_{test} })\log{h_{\theta}(x^{(i)}_{test})}$$ 误分类的比率，对于每一个测试集样本，计算： 然后对计算结果求平均。 模型选择和交叉验证集参考视频: 10 - 3 - Model Selection and Train_Validation_Test Sets (12 min).mkv 假设我们要在10个不同次数的二项式模型之间进行选择： 显然越高次数的多项式模型越能够适应我们的训练数据集，但是适应训练数据集并不代表着能推广至一般情况，我们应该选择一个更能适应一般情况的模型。我们需要使用交叉验证集来帮助选择模型。 即：使用60%的数据作为训练集，使用 20%的数据作为交叉验证集，使用20%的数据作为测试集 模型选择的方法为： 使用训练集训练出10个模型 用10个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值） 选取代价函数值最小的模型 用步骤3中选出的模型对测试集计算得出推广误差（代价函数的值） Train/validation/test error Training error: $J_{train}(\theta) = \frac{1}{2m}\sum_\limits{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2$ Cross Validation error: $J_{cv}(\theta) = \frac{1}{2m_{cv} }\sum_\limits{i=1}^{m}(h_{\theta}(x^{(i)}_{cv})-y^{(i)}_{cv})^2​$ Test error: $J_{test}(\theta)=\frac{1}{2m_{test} }\sum_\limits{i=1}^{m_{test} }(h_{\theta}(x^{(i)}_{cv})-y^{(i)}_{cv})^2$ 诊断偏差和方差参考视频: 10 - 4 - Diagnosing Bias vs. Variance (8 min).mkv 当你运行一个学习算法时，如果这个算法的表现不理想，那么多半是出现两种情况：要么是偏差比较大，要么是方差比较大。换句话说，出现的情况要么是欠拟合，要么是过拟合问题。那么这两种情况，哪个和偏差有关，哪个和方差有关，或者是不是和两个都有关？搞清楚这一点非常重要，因为能判断出现的情况是这两种情况中的哪一种。其实是一个很有效的指示器，指引着可以改进算法的最有效的方法和途径。在这段视频中，我想更深入地探讨一下有关偏差和方差的问题，希望你能对它们有一个更深入的理解，并且也能弄清楚怎样评价一个学习算法，能够判断一个算法是偏差还是方差有问题，因为这个问题对于弄清如何改进学习算法的效果非常重要，高偏差和高方差的问题基本上来说是欠拟合和过拟合的问题。 我们通常会通过将训练集和交叉验证集的代价函数误差与多项式的次数绘制在同一张图表上来帮助分析： Bias/variance Training error: $J_{train}(\theta) = \frac{1}{2m}\sum_\limits{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2$ Cross Validation error: $J_{cv}(\theta) = \frac{1}{2m_{cv} }\sum_\limits{i=1}^{m}(h_{\theta}(x^{(i)}_{cv})-y^{(i)}_{cv})^2$ 对于训练集，当 $d$ 较小时，模型拟合程度更低，误差较大；随着 $d$ 的增长，拟合程度提高，误差减小。 对于交叉验证集，当 $d$ 较小时，模型拟合程度低，误差较大；但是随着 $d$ 的增长，误差呈现先减小后增大的趋势，转折点是我们的模型开始过拟合训练数据集的时候。 如果我们的交叉验证集误差较大，我们如何判断是方差还是偏差呢？根据上面的图表，我们知道: 训练集误差和交叉验证集误差近似时：偏差/欠拟合 交叉验证集误差远大于训练集误差时：方差/过拟合 正则化和偏差/方差参考视频: 10 - 5 - Regularization and Bias_Variance (11 min).mkv 在我们在训练模型的过程中，一般会使用一些正则化方法来防止过拟合。但是我们可能会正则化的程度太高或太小了，即我们在选择λ的值时也需要思考与刚才选择多项式模型次数类似的问题。 我们选择一系列的想要测试的 λ 值，通常是 0-10之间的呈现2倍关系的值（如：$0,0.01,0.02,0.04,0.08,0.15,0.32,0.64,1.28,2.56,5.12,10$共12个）。 我们同样把数据分为训练集、交叉验证集和测试集。 选择$\lambda$的方法为： 使用训练集训练出12个不同程度正则化的模型 用12个模型分别对交叉验证集计算的出交叉验证误差 选择得出交叉验证误差最小的模型 运用步骤3中选出模型对测试集计算得出推广误差，我们也可以同时将训练集和交叉验证集模型的代价函数误差与λ的值绘制在一张图表上： • 当 λ 较小时，训练集误差较小（过拟合）而交叉验证集误差较大 • 随着 λ的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加 学习曲线参考视频: 10 - 6 - Learning Curves (12 min).mkv 学习曲线就是一种很好的工具，我经常使用学习曲线来判断某一个学习算法是否处于偏差、方差问题。学习曲线是学习算法的一个很好的合理检验（sanity check）。学习曲线是将训练集误差和交叉验证集误差作为训练集样本数量（$m$）的函数绘制的图表。 即，如果我们有100行数据，我们从1行数据开始，逐渐学习更多行的数据。思想是：当训练较少行数据的时候，训练的模型将能够非常完美地适应较少的训练数据，但是训练出来的模型却不能很好地适应交叉验证集数据或测试集数据。 如何利用学习曲线识别高偏差/欠拟合：作为例子，我们尝试用一条直线来适应下面的数据，可以看出，无论训练集有多么大误差都不会有太大改观： 也就是说在高偏差/欠拟合的情况下，增加数据到训练集不一定能有帮助。 如何利用学习曲线识别高方差/过拟合：假设我们使用一个非常高次的多项式模型，并且正则化非常小，可以看出，当交叉验证集误差远大于训练集误差时，往训练集增加更多数据可以提高模型的效果。 也就是说在高方差/过拟合的情况下，增加更多数据到训练集可能可以提高算法效果。 决定下一步做什么参考视频: 10 - 7 - Deciding What to Do Next Revisited (7 min).mkv 我们已经介绍了怎样评价一个学习算法，我们讨论了模型选择问题，偏差和方差的问题。那么这些诊断法则怎样帮助我们判断，哪些方法可能有助于改进学习算法的效果，而哪些可能是徒劳的呢？ 让我们再次回到最开始的例子，在那里寻找答案，这就是我们之前的例子。回顾 1.1 中提出的六种可选的下一步，让我们来看一看我们在什么情况下应该怎样选择： 获得更多的训练样本——解决高方差 尝试减少特征的数量——解决高方差 尝试获得更多的特征——解决高偏差 尝试增加多项式特征——解决高偏差 尝试减少正则化程度λ——解决高偏差 尝试增加正则化程度λ——解决高方差 神经网络的方差和偏差： 使用较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合，但计算代价较小使用较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，虽然计算代价比较大，但是可以通过正则化手段来调整而更加适应数据。 通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好。 对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数，为了更好地作选择，可以把数据分为训练集、交叉验证集和测试集，针对不同隐藏层层数的神经网络训练神经网络， 然后选择交叉验证集代价最小的神经网络。 好的，以上就是我们介绍的偏差和方差问题，以及诊断该问题的学习曲线方法。在改进学习算法的表现时，你可以充分运用以上这些内容来判断哪些途径可能是有帮助的。而哪些方法可能是无意义的。如果你理解了以上几节视频中介绍的内容，并且懂得如何运用。那么你已经可以使用机器学习方法有效的解决实际问题了。你也能像硅谷的大部分机器学习从业者一样，他们每天的工作就是使用这些学习算法来解决众多实际问题。我希望这几节中提到的一些技巧，关于方差、偏差，以及学习曲线为代表的诊断法能够真正帮助你更有效率地应用机器学习，让它们高效地工作。 机器学习系统的设计(Machine Learning System Design)首先要做什么参考视频: 11 - 1 - Prioritizing What to Work On (10 min).mkv 在接下来的视频中，我将谈到机器学习系统的设计。这些视频将谈及在设计复杂的机器学习系统时，你将遇到的主要问题。同时我们会试着给出一些关于如何巧妙构建一个复杂的机器学习系统的建议。下面的课程的的数学性可能不是那么强，但是我认为我们将要讲到的这些东西是非常有用的，可能在构建大型的机器学习系统时，节省大量的时间。 本周以一个垃圾邮件分类器算法为例进行讨论。 为了解决这样一个问题，我们首先要做的决定是如何选择并表达特征向量$x$。我们可以选择一个由100个最常出现在垃圾邮件中的词所构成的列表，根据这些词是否有在邮件中出现，来获得我们的特征向量（出现为1，不出现为0），尺寸为100×1。 为了构建这个分类器算法，我们可以做很多事，例如： 收集更多的数据，让我们有更多的垃圾邮件和非垃圾邮件的样本 基于邮件的路由信息开发一系列复杂的特征 基于邮件的正文信息开发一系列复杂的特征，包括考虑截词的处理 为探测刻意的拼写错误（把watch 写成w4tch）开发复杂的算法 在上面这些选项中，非常难决定应该在哪一项上花费时间和精力，作出明智的选择，比随着感觉走要更好。当我们使用机器学习时，总是可以“头脑风暴”一下，想出一堆方法来试试。实际上，当你需要通过头脑风暴来想出不同方法来尝试去提高精度的时候，你可能已经超越了很多人了。大部分人并不尝试着列出可能的方法，他们做的只是某天早上醒来，因为某些原因有了一个突发奇想：”让我们来试试用Honey Pot项目收集大量的数据吧。” 我们将在随后的课程中讲误差分析，我会告诉你怎样用一个更加系统性的方法，从一堆不同的方法中，选取合适的那一个。因此，你更有可能选择一个真正的好方法，能让你花上几天几周，甚至是几个月去进行深入的研究。 误差分析参考视频: 11 - 2 - Error Analysis (13 min).mkv 在本次课程中，我们将会讲到误差分析（Error Analysis）的概念。这会帮助你更系统地做出决定。如果你准备研究机器学习的东西，或者构造机器学习应用程序，最好的实践方法不是建立一个非常复杂的系统，拥有多么复杂的变量；而是构建一个简单的算法，这样你可以很快地实现它。 每当我研究机器学习的问题时，我最多只会花一天的时间，就是字面意义上的24小时，来试图很快的把结果搞出来，即便效果不好。坦白的说，就是根本没有用复杂的系统，但是只是很快的得到的结果。即便运行得不完美，但是也把它运行一遍，最后通过交叉验证来检验数据。一旦做完，你可以画出学习曲线，通过画出学习曲线，以及检验误差，来找出你的算法是否有高偏差和高方差的问题，或者别的问题。在这样分析之后，再来决定用更多的数据训练，或者加入更多的特征变量是否有用。这么做的原因是：这在你刚接触机器学习问题时是一个很好的方法，你并不能提前知道你是否需要复杂的特征变量，或者你是否需要更多的数据，还是别的什么。提前知道你应该做什么，是非常难的，因为你缺少证据，缺少学习曲线。因此，你很难知道你应该把时间花在什么地方来提高算法的表现。但是当你实践一个非常简单即便不完美的方法时，你可以通过画出学习曲线来做出进一步的选择。你可以用这种方式来避免一种电脑编程里的过早优化问题，这种理念是：我们必须用证据来领导我们的决策，怎样分配自己的时间来优化算法，而不是仅仅凭直觉，凭直觉得出的东西一般总是错误的。除了画出学习曲线之外，一件非常有用的事是误差分析，我的意思是说：当我们在构造垃圾邮件分类器时，我会看一看我的交叉验证数据集，然后亲自看一看哪些邮件被算法错误地分类。因此，通过这些被算法错误分类的垃圾邮件与非垃圾邮件，你可以发现某些系统性的规律：什么类型的邮件总是被错误分类。经常地这样做之后，这个过程能启发你构造新的特征变量，或者告诉你：现在这个系统的短处，然后启发你如何去提高它。 构建一个学习算法的推荐方法为： 从一个简单的能快速实现的算法开始，实现该算法并用交叉验证集数据测试这个算法 2.绘制学习曲线，决定是增加更多数据，或者添加更多特征，还是其他选择 3.进行误差分析：人工检查交叉验证集中我们算法中产生预测误差的样本，看看这些样本是否有某种系统化的趋势 以我们的垃圾邮件过滤器为例，误差分析要做的既是检验交叉验证集中我们的算法产生错误预测的所有邮件，看：是否能将这些邮件按照类分组。例如医药品垃圾邮件，仿冒品垃圾邮件或者密码窃取邮件等。然后看分类器对哪一组邮件的预测误差最大，并着手优化。 思考怎样能改进分类器。例如，发现是否缺少某些特征，记下这些特征出现的次数。 例如记录下错误拼写出现了多少次，异常的邮件路由情况出现了多少次等等，然后从出现次数最多的情况开始着手优化。 误差分析并不总能帮助我们判断应该采取怎样的行动。有时我们需要尝试不同的模型，然后进行比较，在模型比较时，用数值来判断哪一个模型更好更有效，通常我们是看交叉验证集的误差。 在我们的垃圾邮件分类器例子中，对于“我们是否应该将discount/discounts/discounted/discounting处理成同一个词？”如果这样做可以改善我们算法，我们会采用一些截词软件。误差分析不能帮助我们做出这类判断，我们只能尝试采用和不采用截词软件这两种不同方案，然后根据数值检验的结果来判断哪一种更好。 因此，当你在构造学习算法的时候，你总是会去尝试很多新的想法，实现出很多版本的学习算法，如果每一次你实践新想法的时候，你都要手动地检测这些例子，去看看是表现差还是表现好，那么这很难让你做出决定。到底是否使用词干提取，是否区分大小写。但是通过一个量化的数值评估，你可以看看这个数字，误差是变大还是变小了。你可以通过它更快地实践你的新想法，它基本上非常直观地告诉你：你的想法是提高了算法表现，还是让它变得更坏，这会大大提高你实践算法时的速度。所以我强烈推荐在交叉验证集上来实施误差分析，而不是在测试集上。但是，还是有一些人会在测试集上来做误差分析。即使这从数学上讲是不合适的。所以我还是推荐你在交叉验证向量上来做误差分析。 总结一下，当你在研究一个新的机器学习问题时，我总是推荐你实现一个较为简单快速、即便不是那么完美的算法。我几乎从未见过人们这样做。大家经常干的事情是：花费大量的时间在构造算法上，构造他们以为的简单的方法。因此，不要担心你的算法太简单，或者太不完美，而是尽可能快地实现你的算法。当你有了初始的实现之后，它会变成一个非常有力的工具，来帮助你决定下一步的做法。因为我们可以先看看算法造成的错误，通过误差分析，来看看他犯了什么错，然后来决定优化的方式。另一件事是：假设你有了一个快速而不完美的算法实现，又有一个数值的评估数据，这会帮助你尝试新的想法，快速地发现你尝试的这些想法是否能够提高算法的表现，从而你会更快地做出决定，在算法中放弃什么，吸收什么误差分析可以帮助我们系统化地选择该做什么。 类偏斜的误差度量参考视频: 11 - 3 - Error Metrics for Skewed Classes (12 min).mkv 在前面的课程中，我提到了误差分析，以及设定误差度量值的重要性。那就是，设定某个实数来评估你的学习算法，并衡量它的表现，有了算法的评估和误差度量值。有一件重要的事情要注意，就是使用一个合适的误差度量值，这有时会对于你的学习算法造成非常微妙的影响，这件重要的事情就是偏斜类（skewed classes）的问题。类偏斜情况表现为我们的训练集中有非常多的同一种类的样本，只有很少或没有其他类的样本。 例如我们希望用算法来预测癌症是否是恶性的，在我们的训练集中，只有0.5%的实例是恶性肿瘤。假设我们编写一个非学习而来的算法，在所有情况下都预测肿瘤是良性的，那么误差只有0.5%。然而我们通过训练而得到的神经网络算法却有1%的误差。这时，误差的大小是不能视为评判算法效果的依据的。 查准率（Precision）和查全率（Recall） 我们将算法预测的结果分成四种情况： 正确肯定（True Positive,TP）：预测为真，实际为真 2.正确否定（True Negative,TN）：预测为假，实际为假 3.错误肯定（False Positive,FP）：预测为真，实际为假 4.错误否定（False Negative,FN）：预测为假，实际为真 则：查准率=TP/(TP+FP)。例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。 查全率=TP/(TP+FN)。例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。 这样，对于我们刚才那个总是预测病人肿瘤为良性的算法，其查全率是0。 预测值 Positive Negtive 实际值 Positive TP FN Negtive FP TN 查准率和查全率之间的权衡参考视频: 11 - 4 - Trading Off Precision and Recall (14 min).mkv 在之前的课程中，我们谈到查准率和召回率，作为遇到偏斜类问题的评估度量值。在很多应用中，我们希望能够保证查准率和召回率的相对平衡。 在这节课中，我将告诉你应该怎么做，同时也向你展示一些查准率和召回率作为算法评估度量值的更有效的方式。继续沿用刚才预测肿瘤性质的例子。假使，我们的算法输出的结果在0-1 之间，我们使用阀值0.5 来预测真和假。 查准率(Precision)=TP/(TP+FP) 例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。 查全率(Recall)=TP/(TP+FN)例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。 如果我们希望只在非常确信的情况下预测为真（肿瘤为恶性），即我们希望更高的查准率，我们可以使用比0.5更大的阀值，如0.7，0.9。这样做我们会减少错误预测病人为恶性肿瘤的情况，同时却会增加未能成功预测肿瘤为恶性的情况。 如果我们希望提高查全率，尽可能地让所有有可能是恶性肿瘤的病人都得到进一步地检查、诊断，我们可以使用比0.5更小的阀值，如0.3。 我们可以将不同阀值情况下，查全率与查准率的关系绘制成图表，曲线的形状根据数据的不同而不同： 我们希望有一个帮助我们选择这个阀值的方法。一种方法是计算F1 值（F1 Score），其计算公式为： ${ {F}_{1} }Score:2\frac{PR}{P+R}$ 我们选择使得F1值最高的阀值。 机器学习的数据参考视频: 11 - 5 - Data For Machine Learning (11 min).mkv 在之前的视频中，我们讨论了评价指标。在这个视频中，我要稍微转换一下，讨论一下机器学习系统设计中另一个重要的方面，这往往涉及到用来训练的数据有多少。在之前的一些视频中，我曾告诫大家不要盲目地开始，而是花大量的时间来收集大量的数据，因为数据有时是唯一能实际起到作用的。但事实证明，在一定条件下，我会在这个视频里讲到这些条件是什么。得到大量的数据并在某种类型的学习算法中进行训练，可以是一种有效的方法来获得一个具有良好性能的学习算法。而这种情况往往出现在这些条件对于你的问题都成立。 并且你能够得到大量数据的情况下。这可以是一个很好的方式来获得非常高性能的学习算法。因此，在这段视频中，让我们一起讨论一下这个问题。 很多很多年前，我认识的两位研究人员Michele Banko 和Eric Brill进行了一项有趣的研究，他们尝试通过机器学习算法来区分常见的易混淆的单词，他们尝试了许多种不同的算法，并发现数据量非常大时，这些不同类型的算法效果都很好。 比如，在这样的句子中：早餐我吃了__个鸡蛋(to,two,too)，在这个例子中，“早餐我吃了2个鸡蛋”，这是一个易混淆的单词的例子。于是他们把诸如这样的机器学习问题，当做一类监督学习问题，并尝试将其分类，什么样的词，在一个英文句子特定的位置，才是合适的。他们用了几种不同的学习算法，这些算法都是在他们2001年进行研究的时候，都已经被公认是比较领先的。因此他们使用了一个方差，用于逻辑回归上的一个方差，被称作”感知器”(perceptron)。他们也采取了一些过去常用，但是现在比较少用的算法，比如 Winnow算法，很类似于回归问题，但在一些方面又有所不同，过去用得比较多，但现在用得不太多。还有一种基于内存的学习算法，现在也用得比较少了，但是我稍后会讨论一点，而且他们用了一个朴素算法。这些具体算法的细节不那么重要，我们下面希望探讨，什么时候我们会希望获得更多数据，而非修改算法。他们所做的就是改变了训练数据集的大小，并尝试将这些学习算法用于不同大小的训练数据集中，这就是他们得到的结果。 这些趋势非常明显，首先大部分算法，都具有相似的性能，其次，随着训练数据集的增大，在横轴上代表以百万为单位的训练集大小，从0.1个百万到1000百万，也就是到了10亿规模的训练集的样本，这些算法的性能也都对应地增强了。 事实上，如果你选择任意一个算法，可能是选择了一个”劣等的”算法，如果你给这个劣等算法更多的数据，那么从这些例子中看起来的话，它看上去很有可能会其他算法更好，甚至会比”优等算法”更好。由于这项原始的研究非常具有影响力，因此已经有一系列许多不同的研究显示了类似的结果。这些结果表明，许多不同的学习算法有时倾向于表现出非常相似的表现，这还取决于一些细节，但是真正能提高性能的，是你能够给一个算法大量的训练数据。像这样的结果，引起了一种在机器学习中的普遍共识：”取得成功的人不是拥有最好算法的人，而是拥有最多数据的人”。 那么这种说法在什么时候是真，什么时候是假呢？因为如果我们有一个学习算法，并且如果这种说法是真的，那么得到大量的数据通常是保证我们具有一个高性能算法的最佳方式，而不是去争辩应该用什么样的算法。 假如有这样一些假设，在这些假设下有大量我们认为有用的训练集，我们假设在我们的机器学习问题中，特征值$x$包含了足够的信息，这些信息可以帮助我们用来准确地预测$y$，例如，如果我们采用了一些容易混淆的词，如：two、to、too，假如说它能够描述$x$，捕捉到需要填写的空白处周围的词语，那么特征捕捉到之后，我们就希望有对于“早饭我吃了__鸡蛋”，那么这就有大量的信息来告诉我中间我需要填的词是“两个”(two)，而不是单词 to 或too，因此特征捕捉，哪怕是周围词语中的一个词，就能够给我足够的信息来确定出标签 $y$是什么。换句话说，从这三组易混淆的词中，我应该选什么词来填空。 那么让我们来看一看，大量的数据是有帮助的情况。假设特征值有足够的信息来预测$y$值，假设我们使用一种需要大量参数的学习算法，比如有很多特征的逻辑回归或线性回归，或者用带有许多隐藏单元的神经网络，那又是另外一种带有很多参数的学习算法，这些都是非常强大的学习算法，它们有很多参数，这些参数可以拟合非常复杂的函数，因此我要调用这些，我将把这些算法想象成低偏差算法，因为我们能够拟合非常复杂的函数，而且因为我们有非常强大的学习算法，这些学习算法能够拟合非常复杂的函数。很有可能，如果我们用这些数据运行这些算法，这种算法能很好地拟合训练集，因此，训练误差就会很低了。 现在假设我们使用了非常非常大的训练集，在这种情况下，尽管我们希望有很多参数，但是如果训练集比参数的数量还大，甚至是更多，那么这些算法就不太可能会过度拟合。也就是说训练误差有希望接近测试误差。 另一种考虑这个问题的角度是为了有一个高性能的学习算法，我们希望它不要有高的偏差和方差。 因此偏差问题，我么将通过确保有一个具有很多参数的学习算法来解决，以便我们能够得到一个较低偏差的算法，并且通过用非常大的训练集来保证。 我们在此没有方差问题，我们的算法将没有方差，并且通过将这两个值放在一起，我们最终可以得到一个低误差和低方差的学习算法。这使得我们能够很好地测试测试数据集。从根本上来说，这是一个关键的假设：特征值有足够的信息量，且我们有一类很好的函数，这是为什么能保证低误差的关键所在。它有大量的训练数据集，这能保证得到更多的方差值，因此这给我们提出了一些可能的条件，如果你有大量的数据，而且你训练了一种带有很多参数的学习算法，那么这将会是一个很好的方式，来提供一个高性能的学习算法。 我觉得关键的测试：首先，一个人类专家看到了特征值 $x$，能很有信心的预测出$y$值吗？因为这可以证明 $ y$ 可以根据特征值$x$被准确地预测出来。其次，我们实际上能得到一组庞大的训练集，并且在这个训练集中训练一个有很多参数的学习算法吗？如果你不能做到这两者，那么更多时候，你会得到一个性能很好的学习算法。 支持向量机(Support Vector Machines)优化目标参考视频: 12 - 1 - Optimization Objective (15 min).mkv 到目前为止,你已经见过一系列不同的学习算法。在监督学习中，许多学习算法的性能都非常类似，因此，重要的不是你该选择使用学习算法A还是学习算法B，而更重要的是，应用这些算法时，所创建的大量数据在应用这些算法时，表现情况通常依赖于你的水平。比如：你为学习算法所设计的特征量的选择，以及如何选择正则化参数，诸如此类的事。还有一个更加强大的算法广泛的应用于工业界和学术界，它被称为支持向量机(Support Vector Machine)。与逻辑回归和神经网络相比，支持向量机，或者简称SVM，在学习复杂的非线性方程时提供了一种更为清晰，更加强大的方式。因此，在接下来的视频中，我会探讨这一算法。在稍后的课程中，我也会对监督学习算法进行简要的总结。当然，仅仅是作简要描述。但对于支持向量机，鉴于该算法的强大和受欢迎度，在本课中，我会花许多时间来讲解它。它也是我们所介绍的最后一个监督学习算法。 正如我们之前开发的学习算法，我们从优化目标开始。那么，我们开始学习这个算法。为了描述支持向量机，事实上，我将会从逻辑回归开始展示我们如何一点一点修改来得到本质上的支持向量机。 那么，在逻辑回归中我们已经熟悉了这里的假设函数形式，和右边的S型激励函数。然而，为了解释一些数学知识.我将用$z$ 表示$\theta^Tx$。 现在考虑下我们想要逻辑回归做什么：如果有一个 $y=1$的样本，我的意思是不管是在训练集中或是在测试集中，又或者在交叉验证集中，总之是 $y=1$，现在我们希望${ {h}_{\theta } }\left( x \right)$ 趋近1。因为我们想要正确地将此样本分类，这就意味着当 ${ {h}_{\theta } }\left( x \right)$趋近于1时，$\theta^Tx$ 应当远大于0，这里的$>>$意思是远远大于0。这是因为由于 $z$ 表示 $\theta^Tx$，当 $z$远大于0时，即到了该图的右边，你不难发现此时逻辑回归的输出将趋近于1。相反地，如果我们有另一个样本，即$y=0$。我们希望假设函数的输出值将趋近于0，这对应于$\theta^Tx$，或者就是 $z$ 会远小于0，因为对应的假设函数的输出值趋近0。 如果你进一步观察逻辑回归的代价函数，你会发现每个样本 $(x,y)$都会为总代价函数，增加这里的一项，因此，对于总代价函数通常会有对所有的训练样本求和，并且这里还有一个$1/m$项，但是，在逻辑回归中，这里的这一项就是表示一个训练样本所对应的表达式。现在，如果我将完整定义的假设函数代入这里。那么，我们就会得到每一个训练样本都影响这一项。 现在，先忽略 $1/m$ 这一项，但是这一项是影响整个总代价函数中的这一项的。 现在，一起来考虑两种情况： 一种是$y$等于1的情况；另一种是 $y$ 等于0的情况。 在第一种情况中，假设 $y=1$ ，此时在目标函数中只需有第一项起作用，因为$y=1$时，$(1-y)$项将等于0。因此，当在 $y=1$ 的样本中时，即在 $(x, y) $中 ，我们得到 $y=1$ $-\log(1-\frac{1}{1+e^{-z} })$这样一项，这里同上一张幻灯片一致。 我用 $z$ 表示$\theta^Tx$，即： $z= \theta^Tx$。当然，在代价函数中，$y$ 前面有负号。我们只是这样表示，如果 $y=1$ 代价函数中，这一项也等于1。这样做是为了简化此处的表达式。如果画出关于$z$ 的函数，你会看到左下角的这条曲线，我们同样可以看到，当$z$ 增大时，也就是相当于$\theta^Tx$增大时，$z$ 对应的值会变的非常小。对整个代价函数而言，影响也非常小。这也就解释了，为什么逻辑回归在观察到正样本$y=1$时，试图将$\theta^Tx$设置得非常大。因为，在代价函数中的这一项会变的非常小。 现在开始建立支持向量机，我们从这里开始： 我们会从这个代价函数开始，也就是$-\log(1-\frac{1}{1+e^{-z} })$一点一点修改，让我取这里的$z=1$ 点，我先画出将要用的代价函数。 新的代价函数将会水平的从这里到右边(图外)，然后我再画一条同逻辑回归非常相似的直线，但是，在这里是一条直线，也就是我用紫红色画的曲线，就是这条紫红色的曲线。那么，到了这里已经非常接近逻辑回归中使用的代价函数了。只是这里是由两条线段组成，即位于右边的水平部分和位于左边的直线部分，先别过多的考虑左边直线部分的斜率，这并不是很重要。但是，这里我们将使用的新的代价函数，是在$y=1$的前提下的。你也许能想到，这应该能做同逻辑回归中类似的事情，但事实上，在之后的优化问题中，这会变得更坚定，并且为支持向量机，带来计算上的优势。例如，更容易计算股票交易的问题等等。 目前，我们只是讨论了$y=1$的情况，另外一种情况是当$y=0$时，此时如果你仔细观察代价函数只留下了第二项，因为第一项被消除了。如果当$y=0$时，那么这一项也就是0了。所以上述表达式只留下了第二项。因此，这个样本的代价或是代价函数的贡献。将会由这一项表示。并且，如果你将这一项作为$z$的函数，那么，这里就会得到横轴$z$。现在，你完成了支持向量机中的部分内容，同样地，我们要替代这一条蓝色的线，用相似的方法。 如果我们用一个新的代价函数来代替，即这条从0点开始的水平直线，然后是一条斜线，像上图。那么，现在让我给这两个方程命名，左边的函数，我称之为${\cos}t_1{(z)}$，同时，右边函数我称它为${\cos}t_0{(z)}$。这里的下标是指在代价函数中，对应的 $y=1$ 和 $y=0$ 的情况，拥有了这些定义后，现在，我们就开始构建支持向量机。 这是我们在逻辑回归中使用代价函数$J(\theta)$。也许这个方程看起来不是非常熟悉。这是因为之前有个负号在方程外面，但是，这里我所做的是，将负号移到了表达式的里面，这样做使得方程看起来有些不同。对于支持向量机而言，实质上我们要将这替换为${\cos}t_1{(z)}$，也就是${\cos}t_1{(\theta^Tx)}$，同样地，我也将这一项替换为${\cos}t_0{(z)}$，也就是代价${\cos}t_0{(\theta^Tx)}$。这里的代价函数${\cos}t_1$，就是之前所提到的那条线。此外，代价函数${\cos}t_0$，也是上面所介绍过的那条线。因此，对于支持向量机，我们得到了这里的最小化问题，即: 然后，再加上正则化参数。现在，按照支持向量机的惯例，事实上，我们的书写会稍微有些不同，代价函数的参数表示也会稍微有些不同。 首先，我们要除去$1/m$这一项，当然，这仅仅是由于人们使用支持向量机时，对比于逻辑回归而言，不同的习惯所致，但这里我所说的意思是：你知道，我将要做的是仅仅除去$1/m$这一项，但是，这也会得出同样的 ${ {\theta } }$ 最优值，好的，因为$1/m$ 仅是个常量，因此，你知道在这个最小化问题中，无论前面是否有$1/m$ 这一项，最终我所得到的最优值${ {\theta } }$都是一样的。这里我的意思是，先给你举一个样本，假定有一最小化问题：即要求当$(u-5)^2+1$取得最小值时的$u$值，这时最小值为：当$u=5$时取得最小值。 现在，如果我们想要将这个目标函数乘上常数10，这里我的最小化问题就变成了：求使得$10×(u-5)^2+10$最小的值$u$，然而，使得这里最小的$u$值仍为5。因此将一些常数乘以你的最小化项，这并不会改变最小化该方程时得到$u$值。因此，这里我所做的是删去常量$m$。也相同的，我将目标函数乘上一个常量$m$，并不会改变取得最小值时的${ {\theta } }$值。 第二点概念上的变化，我们只是指在使用支持向量机时，一些如下的标准惯例，而不是逻辑回归。因此，对于逻辑回归，在目标函数中，我们有两项：第一个是训练样本的代价，第二个是我们的正则化项，我们不得不去用这一项来平衡。这就相当于我们想要最小化$A$加上正则化参数$\lambda$，然后乘以其他项$B$对吧？这里的$A$表示这里的第一项，同时我用B表示第二项，但不包括$\lambda$，我们不是优化这里的$A+\lambda\times B$。我们所做的是通过设置不同正则参数$\lambda$达到优化目的。这样，我们就能够权衡对应的项，是使得训练样本拟合的更好。即最小化$A$。还是保证正则参数足够小，也即是对于B项而言，但对于支持向量机，按照惯例，我们将使用一个不同的参数替换这里使用的$\lambda$来权衡这两项。你知道，就是第一项和第二项我们依照惯例使用一个不同的参数称为$C$，同时改为优化目标，$C×A+B$。因此，在逻辑回归中，如果给定$\lambda$，一个非常大的值，意味着给予$B$更大的权重。而这里，就对应于将$C$ 设定为非常小的值，那么，相应的将会给$B$比给$A$更大的权重。因此，这只是一种不同的方式来控制这种权衡或者一种不同的方法，即用参数来决定是更关心第一项的优化，还是更关心第二项的优化。当然你也可以把这里的参数$C$ 考虑成$1/\lambda$，同 $1/\lambda$所扮演的角色相同，并且这两个方程或这两个表达式并不相同，因为$C=1/\lambda$，但是也并不全是这样，如果当$C=1/\lambda$时，这两个优化目标应当得到相同的值，相同的最优值 ${ {\theta } }$。因此，就用它们来代替。那么，我现在删掉这里的$\lambda$，并且用常数$C$来代替。因此，这就得到了在支持向量机中我们的整个优化目标函数。然后最小化这个目标函数，得到SVM 学习到的参数$C$。 最后有别于逻辑回归输出的概率。在这里，我们的代价函数，当最小化代价函数，获得参数${ {\theta } }$时，支持向量机所做的是它来直接预测$y$的值等于1，还是等于0。因此，这个假设函数会预测1。当$\theta^Tx$大于或者等于0时，或者等于0时，所以学习参数${ {\theta } }$就是支持向量机假设函数的形式。那么，这就是支持向量机数学上的定义。 在接下来的视频中，让我们再回去从直观的角度看看优化目标，实际上是在做什么，以及SVM的假设函数将会学习什么，同时也会谈谈如何做些许修改，学习更加复杂、非线性的函数。 大边界的直观理解参考视频: 12 - 2 - Large Margin Intuition (11 min).mkv 人们有时将支持向量机看作是大间距分类器。在这一部分，我将介绍其中的含义，这有助于我们直观理解SVM模型的假设是什么样的。 这是我的支持向量机模型的代价函数，在左边这里我画出了关于$z$的代价函数${\cos}t_1{(z)}$，此函数用于正样本，而在右边这里我画出了关于$z$的代价函数${\cos}t_0{(z)}$，横轴表示$z$，现在让我们考虑一下，最小化这些代价函数的必要条件是什么。如果你有一个正样本，$y=1$，则只有在$z>=1$时，代价函数${\cos}t_1{(z)}$才等于0。 换句话说，如果你有一个正样本，我们会希望$\theta^Tx>=1$，反之，如果$y=0$，我们观察一下，函数${\cos}t_0{(z)}$，它只有在$z&lt;=-1$的区间里函数值为0。这是支持向量机的一个有趣性质。事实上，如果你有一个正样本$y=1$，则其实我们仅仅要求$\theta^Tx$大于等于0，就能将该样本恰当分出，这是因为如果$\theta^Tx$&gt;0大的话，我们的模型代价函数值为0，类似地，如果你有一个负样本，则仅需要$\theta^Tx$&amp;lt;=0就会将负例正确分离，但是，支持向量机的要求更高，不仅仅要能正确分开输入的样本，即不仅仅要求$\theta^Tx$&gt;0，我们需要的是比0值大很多，比如大于等于1，我也想这个比0小很多，比如我希望它小于等于-1，这就相当于在支持向量机中嵌入了一个额外的安全因子，或者说安全的间距因子。 当然，逻辑回归做了类似的事情。但是让我们看一下，在支持向量机中，这个因子会导致什么结果。具体而言，我接下来会考虑一个特例。我们将这个常数$C$设置成一个非常大的值。比如我们假设$C$的值为100000或者其它非常大的数，然后来观察支持向量机会给出什么结果？ 如果 $C$非常大，则最小化代价函数的时候，我们将会很希望找到一个使第一项为0的最优解。因此，让我们尝试在代价项的第一项为0的情形下理解该优化问题。比如我们可以把$C$设置成了非常大的常数，这将给我们一些关于支持向量机模型的直观感受。 $\min_\limits{\theta}C\sum_\limits{i=1}^{m}\left[y^{(i)}{\cos}t_{1}\left(\theta^{T}x^{(i)}\right)+\left(1-y^{(i)}\right){\cos}t\left(\theta^{T}x^{(i)}\right)\right]+\frac{1}{2}\sum_\limits{i=1}^{n}\theta^{2}_{j}$ 我们已经看到输入一个训练样本标签为$y=1$，你想令第一项为0，你需要做的是找到一个${ {\theta } }$，使得$\theta^Tx>=1$，类似地，对于一个训练样本，标签为$y=0$，为了使${\cos}t_0{(z)}$ 函数的值为0，我们需要$\theta^Tx&lt;=-1$。因此，现在考虑我们的优化问题。选择参数，使得第一项等于0，就会导致下面的优化问题，因为我们将选择参数使第一项为0，因此这个函数的第一项为0，因此是$C$乘以0加上二分之一乘以第二项。这里第一项是$C$乘以0，因此可以将其删去，因为我知道它是0。 这将遵从以下的约束：$\theta^Tx^{(i)}>=1$，如果 $y^{(i)}$是等于1 的，$\theta^Tx^{(i)}&lt;=-1$，如果样本$i$是一个负样本，这样当你求解这个优化问题的时候，当你最小化这个关于变量${ {\theta } }$的函数的时候，你会得到一个非常有趣的决策边界。 具体而言，如果你考察这样一个数据集，其中有正样本，也有负样本，可以看到这个数据集是线性可分的。我的意思是，存在一条直线把正负样本分开。当然有多条不同的直线，可以把正样本和负样本完全分开。 比如，这就是一个决策边界可以把正样本和负样本分开。但是多多少少这个看起来并不是非常自然是么? 或者我们可以画一条更差的决策界，这是另一条决策边界，可以将正样本和负样本分开，但仅仅是勉强分开，这些决策边界看起来都不是特别好的选择，支持向量机将会选择这个黑色的决策边界，相较于之前我用粉色或者绿色画的决策界。这条黑色的看起来好得多，黑线看起来是更稳健的决策界。在分离正样本和负样本上它显得的更好。数学上来讲，这是什么意思呢？这条黑线有更大的距离，这个距离叫做间距(margin)。 当画出这两条额外的蓝线，我们看到黑色的决策界和训练样本之间有更大的最短距离。然而粉线和蓝线离训练样本就非常近，在分离样本的时候就会比黑线表现差。因此，这个距离叫做支持向量机的间距，而这是支持向量机具有鲁棒性的原因，因为它努力用一个最大间距来分离样本。因此支持向量机有时被称为大间距分类器，而这其实是求解上一页幻灯片上优化问题的结果。 我知道你也许想知道求解上一页幻灯片中的优化问题为什么会产生这个结果？它是如何产生这个大间距分类器的呢？我知道我还没有解释这一点。 我将会从直观上略述为什么这个优化问题会产生大间距分类器。总之这个图示有助于你理解支持向量机模型的做法，即努力将正样本和负样本用最大的间距分开。 在本节课中关于大间距分类器，我想讲最后一点：我们将这个大间距分类器中的正则化因子常数$C$设置的非常大，我记得我将其设置为了100000，因此对这样的一个数据集，也许我们将选择这样的决策界，从而最大间距地分离开正样本和负样本。那么在让代价函数最小化的过程中，我们希望找出在$y=1$和$y=0$两种情况下都使得代价函数中左边的这一项尽量为零的参数。如果我们找到了这样的参数，则我们的最小化问题便转变成： 事实上，支持向量机现在要比这个大间距分类器所体现得更成熟，尤其是当你使用大间距分类器的时候，你的学习算法会受异常点(outlier) 的影响。比如我们加入一个额外的正样本。 在这里，如果你加了这个样本，为了将样本用最大间距分开，也许我最终会得到一条类似这样的决策界，对么？就是这条粉色的线，仅仅基于一个异常值，仅仅基于一个样本，就将我的决策界从这条黑线变到这条粉线，这实在是不明智的。而如果正则化参数$C$，设置的非常大，这事实上正是支持向量机将会做的。它将决策界，从黑线变到了粉线，但是如果$C$ 设置的小一点，如果你将C设置的不要太大，则你最终会得到这条黑线， 当然数据如果不是线性可分的，如果你在这里有一些正样本或者你在这里有一些负样本，则支持向量机也会将它们恰当分开。因此，大间距分类器的描述，仅仅是从直观上给出了正则化参数$C$非常大的情形，同时，要提醒你$C$的作用类似于$1/\lambda$，$\lambda$是我们之前使用过的正则化参数。这只是$C$非常大的情形，或者等价地 $\lambda$ 非常小的情形。你最终会得到类似粉线这样的决策界，但是实际上应用支持向量机的时候，当$C$不是非常非常大的时候，它可以忽略掉一些异常点的影响，得到更好的决策界。 甚至当你的数据不是线性可分的时候，支持向量机也可以给出好的结果。 回顾 $C=1/\lambda$，因此： $C$ 较大时，相当于 $\lambda$ 较小，可能会导致过拟合，高方差。 $C$ 较小时，相当于$\lambda$较大，可能会导致低拟合，高偏差。 我们稍后会介绍支持向量机的偏差和方差，希望在那时候关于如何处理参数的这种平衡会变得更加清晰。我希望，这节课给出了一些关于为什么支持向量机被看做大间距分类器的直观理解。它用最大间距将样本区分开，尽管从技术上讲，这只有当参数$C$是非常大的时候是真的，但是它对于理解支持向量机是有益的。 本节课中我们略去了一步，那就是我们在幻灯片中给出的优化问题。为什么会是这样的？它是如何得出大间距分类器的？我在本节中没有讲解，在下一节课中，我将略述这些问题背后的数学原理，来解释这个优化问题是如何得到一个大间距分类器的。 大边界分类背后的数学（选修）参考视频: 12 - 3 - Mathematics Behind Large Margin Classification (Optional) (20 min).mkv 在本节课中，我将介绍一些大间隔分类背后的数学原理。本节为选修部分，你完全可以跳过它，但是听听这节课可能让你对支持向量机中的优化问题，以及如何得到大间距分类器，产生更好的直观理解。 首先，让我来给大家复习一下关于向量内积的知识。假设我有两个向量，$u$和$v$，我将它们写在这里。两个都是二维向量，我们看一下，$u^T v$的结果。$u^T v$也叫做向量$u$和$v$之间的内积。由于是二维向量，我可以将它们画在这个图上。我们说，这就是向量$u$即在横轴上，取值为某个${ {u}_{1} }$，而在纵轴上，高度是某个${ {u}_{2} }$作为$u$的第二个分量。现在，很容易计算的一个量就是向量$u$的范数。$\left\| u \right\|$表示$u$的范数，即$u$的长度，即向量$u$的欧几里得长度。根据毕达哥拉斯定理，$\left\| u \right\|=\sqrt{u_{1}^{2}+u_{2}^{2} }$，这是向量$u$的长度，它是一个实数。现在你知道了这个的长度是多少了。我刚刚画的这个向量的长度就知道了。 现在让我们回头来看向量$v$ ，因为我们想计算内积。$v$是另一个向量，它的两个分量${ {v}_{1} }$和${ {v}_{2} }$是已知的。向量$v$可以画在这里，现在让我们来看看如何计算$u$和$v$之间的内积。这就是具体做法，我们将向量$v$投影到向量$u$上，我们做一个直角投影，或者说一个90度投影将其投影到$u$上，接下来我度量这条红线的长度。我称这条红线的长度为$p$，因此$p$就是长度，或者说是向量$v$投影到向量$u$上的量，我将它写下来，$p$是$v$投影到向量$u$上的长度，因此可以将${ {u}^{T} }v=p\centerdot \left\| u \right\|$，或者说$u$的长度。这是计算内积的一种方法。如果你从几何上画出$p$的值，同时画出$u$的范数，你也会同样地计算出内积，答案是一样的。另一个计算公式是：$u^T v$就是$\left[ { {u}_{1} }\text{ }{ {u}_{2} } \right]$ 这个一行两列的矩阵乘以$v$。因此可以得到${ {u}_{1} }\times { {v}_{1} }+{ {u}_{2} }\times { {v}_{2} }$。根据线性代数的知识，这两个公式会给出同样的结果。顺便说一句，$u^Tv=v^Tu$。因此如果你将$u$和$v$交换位置，将$u$投影到$v$上，而不是将$v$投影到$u$上，然后做同样地计算，只是把$u$和$v$的位置交换一下，你事实上可以得到同样的结果。申明一点，在这个等式中$u$的范数是一个实数，$p$也是一个实数，因此$u^T v$就是两个实数正常相乘。 最后一点，需要注意的就是$p$值，$p$事实上是有符号的，即它可能是正值，也可能是负值。我的意思是说，如果$u$是一个类似这样的向量，$v$是一个类似这样的向量，$u$和$v$之间的夹角大于90度，则如果将$v$投影到$u$上，会得到这样的一个投影，这是$p$的长度，在这个情形下我们仍然有${ {u}^{T} }v$是等于$p$乘以$u$的范数。唯一一点不同的是$p$在这里是负的。在内积计算中，如果$u$和$v$之间的夹角小于90度，那么那条红线的长度$p$是正值。然而如果这个夹角大于90度，则$p$将会是负的。就是这个小线段的长度是负的。如果它们之间的夹角大于90度，两个向量之间的内积也是负的。这就是关于向量内积的知识。我们接下来将会使用这些关于向量内积的性质试图来理解支持向量机中的目标函数。 这就是我们先前给出的支持向量机模型中的目标函数。为了讲解方便，我做一点简化，仅仅是为了让目标函数更容易被分析。 我接下来忽略掉截距，令${ {\theta }_{0} }=0$，这样更容易画示意图。我将特征数$n$置为2，因此我们仅有两个特征${ {x}_{1} },{ {x}_{2} }$，现在我们来看一下目标函数，支持向量机的优化目标函数。当我们仅有两个特征，即$n=2$时，这个式子可以写作：$\frac{1}{2}\left({\theta_1^2+\theta_2^2}\right)=\frac{1}{2}\left(\sqrt{\theta_1^2+\theta_2^2}\right)^2$，我们只有两个参数${ {\theta }_{1} },{ {\theta }_{2} }$。你可能注意到括号里面的这一项是向量${ {\theta } }$的范数，或者说是向量${ {\theta } }$的长度。我的意思是如果我们将向量${ {\theta } }$写出来，那么我刚刚画红线的这一项就是向量${ {\theta } }$的长度或范数。这里我们用的是之前学过的向量范数的定义，事实上这就等于向量${ {\theta } }$的长度。 当然你可以将其写作${ {\theta }_{0} }\text{,}{ {\theta }_{1} },{ {\theta }_{2} }$，如果${ {\theta }_{0} }=0$，那就是${ {\theta }_{1} },{ {\theta }_{2} }$的长度。在这里我将忽略${ {\theta }_{0} }$，这样来写$\theta$的范数，它仅仅和${ {\theta }_{1} },{ {\theta }_{2} }$有关。但是，数学上不管你是否包含，其实并没有差别，因此在我们接下来的推导中去掉${ {\theta }_{0} }$不会有影响这意味着我们的目标函数是等于$\frac{1}{2}\left\| \theta \right\|^2$。因此支持向量机做的全部事情，就是极小化参数向量${ {\theta } }$范数的平方，或者说长度的平方。 现在我将要看看这些项：$\theta^{T}x$更深入地理解它们的含义。给定参数向量$\theta $给定一个样本$x$，这等于什么呢?在前一页幻灯片上，我们画出了在不同情形下，$u^Tv$的示意图，我们将会使用这些概念，$\theta $和$x^{(i)}$就类似于$u$和$v$ 。 让我们看一下示意图：我们考察一个单一的训练样本，我有一个正样本在这里，用一个叉来表示这个样本$x^{(i)}$，意思是在水平轴上取值为$x_1^{(i)}$，在竖直轴上取值为$x_2^{(i)}$。这就是我画出的训练样本。尽管我没有将其真的看做向量。它事实上就是一个始于原点，终点位置在这个训练样本点的向量。现在，我们有一个参数向量我会将它也画成向量。我将$θ_1$画在横轴这里，将$θ_2$ 画在纵轴这里，那么内积$θ^T x^{(i)}$ 将会是什么呢？ 使用我们之前的方法，我们计算的方式就是我将训练样本投影到参数向量${ {\theta } }$，然后我来看一看这个线段的长度，我将它画成红色。我将它称为$p^{(i)}$用来表示这是第 $i$个训练样本在参数向量${ {\theta } }$上的投影。根据我们之前幻灯片的内容，我们知道的是$θ^Tx^{(i)}$将会等于$p$ 乘以向量 $θ$ 的长度或范数。这就等于$\theta_1\cdot{x_1^{(i)} }+\theta_2\cdot{x_2^{(i)} }$。这两种方式是等价的，都可以用来计算$θ$和$x^{(i)}$之间的内积。 这告诉了我们什么呢？这里表达的意思是：这个$θ^Tx^{(i)}>=1$ 或者$θ^Tx^{(i)}&lt;-1$的,约束是可以被$p^{(i)}\cdot{x}>=1$这个约束所代替的。因为$θ^Tx^{(i)}=p^{(i)}\cdot{\left\| \theta \right\|}$ ，将其写入我们的优化目标。我们将会得到没有了约束，$θ^Tx^{(i)}$而变成了$p^{(i)}\cdot{\left\| \theta \right\|}$。 需要提醒一点，我们之前曾讲过这个优化目标函数可以被写成等于$\frac{1}{2}\left\| \theta \right\|^2$。 现在让我们考虑下面这里的训练样本。现在，继续使用之前的简化，即${ {\theta }_{0} }=0$，我们来看一下支持向量机会选择什么样的决策界。这是一种选择，我们假设支持向量机会选择这个决策边界。这不是一个非常好的选择，因为它的间距很小。这个决策界离训练样本的距离很近。我们来看一下为什么支持向量机不会选择它。 对于这样选择的参数${ {\theta } }$，可以看到参数向量${ {\theta } }$事实上是和决策界是90度正交的，因此这个绿色的决策界对应着一个参数向量${ {\theta } }$这个方向,顺便提一句${ {\theta }_{0} }=0$的简化仅仅意味着决策界必须通过原点$(0,0)$。现在让我们看一下这对于优化目标函数意味着什么。 比如这个样本，我们假设它是我的第一个样本$x^{(1)}$，如果我考察这个样本到参数${ {\theta } }$的投影，投影是这个短的红线段，就等于$p^{(1)}$，它非常短。类似地，这个样本如果它恰好是$x^{(2)}$，我的第二个训练样本，则它到${ {\theta } }$的投影在这里。我将它画成粉色，这个短的粉色线段是$p^{(2)}$，即第二个样本到我的参数向量${ {\theta } }$的投影。因此，这个投影非常短。$p^{(2)}$事实上是一个负值，$p^{(2)}$是在相反的方向，这个向量和参数向量${ {\theta } }$的夹角大于90度，$p^{(2)}$的值小于0。 我们会发现这些$p^{(i)}$将会是非常小的数，因此当我们考察优化目标函数的时候，对于正样本而言，我们需要$p^{(i)}\cdot{\left\| \theta \right\|}>=1$,但是如果 $p^{(i)}$在这里非常小,那就意味着我们需要${ {\theta } }$的范数非常大.因为如果 $p^{(1)}$ 很小,而我们希望$p^{(1)}\cdot{\left\| \theta \right\|}>=1$,令其实现的唯一的办法就是这两个数较大。如果 $p^{(1)}$ 小，我们就希望${ {\theta } }$的范数大。类似地，对于负样本而言我们需要$p^{(2)}\cdot{\left\|\theta \right\|}&lt;=-1$。我们已经在这个样本中看到$p^{(2)}$会是一个非常小的数，因此唯一的办法就是${ {\theta } }$的范数变大。但是我们的目标函数是希望找到一个参数${ {\theta } }$，它的范数是小的。因此，这看起来不像是一个好的参数向量${ {\theta } }$的选择。 相反的，来看一个不同的决策边界。比如说，支持向量机选择了这个决策界，现在状况会有很大不同。如果这是决策界，这就是相对应的参数${ {\theta } }$的方向，因此，在这个决策界之下，垂直线是决策界。使用线性代数的知识，可以说明，这个绿色的决策界有一个垂直于它的向量${ {\theta } }$。现在如果你考察你的数据在横轴$x$上的投影，比如这个我之前提到的样本，我的样本$x^{(1)}$，当我将它投影到横轴$x$上，或说投影到${ {\theta } }$上，就会得到这样$p^{(1)}$。它的长度是$p^{(1)}$，另一个样本，那个样本是$x^{(2)}$。我做同样的投影，我会发现，$p^{(2)}$的长度是负值。你会注意到现在$p^{(1)}$ 和$p^{(2)}$这些投影长度是长多了。如果我们仍然要满足这些约束，$P^{(i)}\cdot{\left\| \theta \right\|}$&gt;1，则因为$p^{(1)}$变大了，${ {\theta } }$的范数就可以变小了。因此这意味着通过选择右边的决策界，而不是左边的那个，支持向量机可以使参数${ {\theta } }$的范数变小很多。因此，如果我们想令${ {\theta } }$的范数变小，从而令${ {\theta } }$范数的平方变小，就能让支持向量机选择右边的决策界。这就是支持向量机如何能有效地产生大间距分类的原因。 看这条绿线，这个绿色的决策界。我们希望正样本和负样本投影到$\theta$的值大。要做到这一点的唯一方式就是选择这条绿线做决策界。这是大间距决策界来区分开正样本和负样本这个间距的值。这个间距的值就是$p^{(1)},p^{(2)},p^{(3)}$等等的值。通过让间距变大，即通过这些$p^{(1)},p^{(2)},p^{(3)}$等等的值，支持向量机最终可以找到一个较小的${ {\theta } }$范数。这正是支持向量机中最小化目标函数的目的。 以上就是为什么支持向量机最终会找到大间距分类器的原因。因为它试图极大化这些$p^{(i)}$的范数，它们是训练样本到决策边界的距离。最后一点，我们的推导自始至终使用了这个简化假设，就是参数$θ_0=0$。 就像我之前提到的。这个的作用是：$θ_0=0$的意思是我们让决策界通过原点。如果你令$θ_0$不是0的话，含义就是你希望决策界不通过原点。我将不会做全部的推导。实际上，支持向量机产生大间距分类器的结论，会被证明同样成立，证明方式是非常类似的，是我们刚刚做的证明的推广。 之前视频中说过，即便$θ_0$不等于0，支持向量机要做的事情都是优化这个目标函数对应着$C$值非常大的情况，但是可以说明的是，即便$θ_0$不等于0，支持向量机仍然会找到正样本和负样本之间的大间距分隔。 总之，我们解释了为什么支持向量机是一个大间距分类器。在下一节我们，将开始讨论如何利用支持向量机的原理，应用它们建立一个复杂的非线性分类器。 核函数1参考视频: 12 - 4 - Kernels I (16 min).mkv 回顾我们之前讨论过可以使用高级数的多项式模型来解决无法用直线进行分隔的分类问题： 为了获得上图所示的判定边界，我们的模型可能是${ {\theta }_{0} }+{ {\theta }_{1} }{ {x}_{1} }+{ {\theta }_{2} }{ {x}_{2} }+{ {\theta }_{3} }{ {x}_{1} }{ {x}_{2} }+{ {\theta }_{4} }x_{1}^{2}+{ {\theta }_{5} }x_{2}^{2}+\cdots $的形式。 我们可以用一系列的新的特征$f$来替换模型中的每一项。例如令： ${ {f}_{1} }={ {x}_{1} },{ {f}_{2} }={ {x}_{2} },{ {f}_{3} }={ {x}_{1} }{ {x}_{2} },{ {f}_{4} }=x_{1}^{2},{ {f}_{5} }=x_{2}^{2}$ …得到$h_θ(x)={ {\theta }_{1} }f_1+{ {\theta }_{2} }f_2+...+{ {\theta }_{n} }f_n$。然而，除了对原有的特征进行组合以外，有没有更好的方法来构造$f_1,f_2,f_3$？我们可以利用核函数来计算出新的特征。 给定一个训练样本$x$，我们利用$x$的各个特征与我们预先选定的地标(landmarks)$l^{(1)},l^{(2)},l^{(3)}$的近似程度来选取新的特征$f_1,f_2,f_3$。 例如：${ {f}_{1} }=similarity(x,{ {l}^{(1)} })=e(-\frac{ {{\left\| x-{ {l}^{(1)} } \right\|}^{2} }}{2{ {\sigma }^{2} }})$ 其中：${ {\left\| x-{ {l}^{(1)} } \right\|}^{2} }=\sum{_{j=1}^{n} }{ {({ {x}_{j} }-l_{j}^{(1)})}^{2} }$，为实例$x$中所有特征与地标$l^{(1)}$之间的距离的和。上例中的$similarity(x,{ {l}^{(1)} })$就是核函数，具体而言，这里是一个高斯核函数(Gaussian Kernel)。 注：这个函数与正态分布没什么实际上的关系，只是看上去像而已。 这些地标的作用是什么？如果一个训练样本$x$与地标$l$之间的距离近似于0，则新特征 $f$近似于$e^{-0}=1$，如果训练样本$x$与地标$l$之间距离较远，则$f$近似于$e^{-(一个较大的数)}=0$。 假设我们的训练样本含有两个特征[$x_{1}$ $x{_2}$]，给定地标$l^{(1)}$与不同的$\sigma$值，见下图： 图中水平面的坐标为 $x_{1}$，$x_{2}$而垂直坐标轴代表$f$。可以看出，只有当$x$与$l^{(1)}$重合时$f$才具有最大值。随着$x$的改变$f$值改变的速率受到$\sigma^2$的控制。 在下图中，当样本处于洋红色的点位置处，因为其离$l^{(1)}$更近，但是离$l^{(2)}$和$l^{(3)}$较远，因此$f_1$接近1，而$f_2$,$f_3$接近0。因此$h_θ(x)=θ_0+θ_1f_1+θ_2f_2+θ_1f_3>0$，因此预测$y=1$。同理可以求出，对于离$l^{(2)}$较近的绿色点，也预测$y=1$，但是对于蓝绿色的点，因为其离三个地标都较远，预测$y=0$。 这样，图中红色的封闭曲线所表示的范围，便是我们依据一个单一的训练样本和我们选取的地标所得出的判定边界，在预测时，我们采用的特征不是训练样本本身的特征，而是通过核函数计算出的新特征$f_1,f_2,f_3$。 核函数2参考视频: 12 - 5 - Kernels II (16 min).mkv 在上一节视频里，我们讨论了核函数这个想法，以及怎样利用它去实现支持向量机的一些新特性。在这一节视频中，我将补充一些缺失的细节，并简单的介绍一下怎么在实际中使用应用这些想法。 如何选择地标？ 我们通常是根据训练集的数量选择地标的数量，即如果训练集中有$m$个样本，则我们选取$m$个地标，并且令:$l^{(1)}=x^{(1)},l^{(2)}=x^{(2)},.....,l^{(m)}=x^{(m)}$。这样做的好处在于：现在我们得到的新特征是建立在原有特征与训练集中所有其他特征之间距离的基础之上的，即： 下面我们将核函数运用到支持向量机中，修改我们的支持向量机假设为： • 给定$x$，计算新特征$f$，当$θ^Tf>=0$ 时，预测 $y=1$，否则反之。 相应地修改代价函数为：$\sum{_{j=1}^{n=m} }\theta _{j}^{2}={ {\theta}^{T} }\theta $， $min C\sum\limits_{i=1}^{m}{[{ {y}^{(i)} }cos { {t}_{1} }}( { {\theta }^{T} }{ {f}^{(i)} })+(1-{ {y}^{(i)} })cos { {t}_{0} }( { {\theta }^{T} }{ {f}^{(i)} })]+\frac{1}{2}\sum\limits_{j=1}^{n=m}{\theta _{j}^{2} }$ 在具体实施过程中，我们还需要对最后的正则化项进行些微调整，在计算$\sum{_{j=1}^{n=m} }\theta _{j}^{2}={ {\theta}^{T} }\theta $时，我们用$θ^TMθ$代替$θ^Tθ$，其中$M$是根据我们选择的核函数而不同的一个矩阵。这样做的原因是为了简化计算。 理论上讲，我们也可以在逻辑回归中使用核函数，但是上面使用 $M$来简化计算的方法不适用与逻辑回归，因此计算将非常耗费时间。 在此，我们不介绍最小化支持向量机的代价函数的方法，你可以使用现有的软件包（如liblinear,libsvm等）。在使用这些软件包最小化我们的代价函数之前，我们通常需要编写核函数，并且如果我们使用高斯核函数，那么在使用之前进行特征缩放是非常必要的。 另外，支持向量机也可以不使用核函数，不使用核函数又称为线性核函数(linear kernel)，当我们不采用非常复杂的函数，或者我们的训练集特征非常多而样本非常少的时候，可以采用这种不带核函数的支持向量机。 下面是支持向量机的两个参数$C$和$\sigma$的影响： $C=1/\lambda$ $C$ 较大时，相当于$\lambda$较小，可能会导致过拟合，高方差； $C$ 较小时，相当于$\lambda$较大，可能会导致低拟合，高偏差； $\sigma$较大时，可能会导致低方差，高偏差； $\sigma$较小时，可能会导致低偏差，高方差。 如果你看了本周的编程作业，你就能亲自实现这些想法，并亲眼看到这些效果。这就是利用核函数的支持向量机算法，希望这些关于偏差和方差的讨论，能给你一些对于算法结果预期的直观印象。 使用支持向量机参考视频: 12 - 6 - Using An SVM (21 min).mkv 目前为止，我们已经讨论了SVM比较抽象的层面，在这个视频中我将要讨论到为了运行或者运用SVM。你实际上所需要的一些东西：支持向量机算法，提出了一个特别优化的问题。但是就如在之前的视频中我简单提到的，我真的不建议你自己写软件来求解参数${ {\theta } }$，因此由于今天我们中的很少人，或者其实没有人考虑过自己写代码来转换矩阵，或求一个数的平方根等我们只是知道如何去调用库函数来实现这些功能。同样的，用以解决SVM最优化问题的软件很复杂，且已经有研究者做了很多年数值优化了。因此你提出好的软件库和好的软件包来做这样一些事儿。然后强烈建议使用高优化软件库中的一个，而不是尝试自己落实一些数据。有许多好的软件库，我正好用得最多的两个是liblinear和libsvm，但是真的有很多软件库可以用来做这件事儿。你可以连接许多你可能会用来编写学习算法的主要编程语言。 在高斯核函数之外我们还有其他一些选择，如： 多项式核函数（Polynomial Kernel） 字符串核函数（String kernel） 卡方核函数（ chi-square kernel） 直方图交集核函数（histogram intersection kernel） 等等… 这些核函数的目标也都是根据训练集和地标之间的距离来构建新特征，这些核函数需要满足Mercer’s定理，才能被支持向量机的优化软件正确处理。 多类分类问题 假设我们利用之前介绍的一对多方法来解决一个多类分类问题。如果一共有$k$个类，则我们需要$k$个模型，以及$k$个参数向量${ {\theta } }$。我们同样也可以训练$k$个支持向量机来解决多类分类问题。但是大多数支持向量机软件包都有内置的多类分类功能，我们只要直接使用即可。 尽管你不去写你自己的SVM的优化软件，但是你也需要做几件事： 1、是提出参数$C$的选择。我们在之前的视频中讨论过误差/方差在这方面的性质。 2、你也需要选择内核参数或你想要使用的相似函数，其中一个选择是：我们选择不需要任何内核参数，没有内核参数的理念，也叫线性核函数。因此，如果有人说他使用了线性核的SVM（支持向量机），这就意味这他使用了不带有核函数的SVM（支持向量机）。 从逻辑回归模型，我们得到了支持向量机模型，在两者之间，我们应该如何选择呢？ 下面是一些普遍使用的准则： $n$为特征数，$m$为训练样本数。 (1)如果相较于$m$而言，$n$要大许多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。 (2)如果$n$较小，而且$m$大小中等，例如$n$在 1-1000 之间，而$m$在10-10000之间，使用高斯核函数的支持向量机。 (3)如果$n$较小，而$m$较大，例如$n$在1-1000之间，而$m$大于50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。 值得一提的是，神经网络在以上三种情况下都可能会有较好的表现，但是训练神经网络可能非常慢，选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值。 今天的SVM包会工作得很好，但是它们仍然会有一些慢。当你有非常非常大的训练集，且用高斯核函数是在这种情况下，我经常会做的是尝试手动地创建，拥有更多的特征变量，然后用逻辑回归或者不带核函数的支持向量机。如果你看到这个幻灯片，看到了逻辑回归，或者不带核函数的支持向量机。在这个两个地方，我把它们放在一起是有原因的。原因是：逻辑回归和不带核函数的支持向量机它们都是非常相似的算法，不管是逻辑回归还是不带核函数的SVM，通常都会做相似的事情，并给出相似的结果。但是根据你实现的情况，其中一个可能会比另一个更加有效。但是在其中一个算法应用的地方，逻辑回归或不带核函数的SVM另一个也很有可能很有效。但是随着SVM的复杂度增加，当你使用不同的内核函数来学习复杂的非线性函数时，这个体系，你知道的，当你有多达1万（10,000）的样本时，也可能是5万（50,000），你的特征变量的数量这是相当大的。那是一个非常常见的体系，也许在这个体系里，不带核函数的支持向量机就会表现得相当突出。你可以做比这困难得多需要逻辑回归的事情。 最后，神经网络使用于什么时候呢？ 对于所有的这些问题，对于所有的这些不同体系一个设计得很好的神经网络也很有可能会非常有效。有一个缺点是，或者说是有时可能不会使用神经网络的原因是：对于许多这样的问题，神经网络训练起来可能会特别慢，但是如果你有一个非常好的SVM实现包，它可能会运行得比较快比神经网络快很多，尽管我们在此之前没有展示，但是事实证明，SVM具有的优化问题，是一种凸优化问题。因此，好的SVM优化软件包总是会找到全局最小值，或者接近它的值。对于SVM你不需要担心局部最优。在实际应用中，局部最优不是神经网络所需要解决的一个重大问题，所以这是你在使用SVM的时候不需要太去担心的一个问题。根据你的问题，神经网络可能会比SVM慢，尤其是在这样一个体系中，至于这里给出的参考，看上去有些模糊，如果你在考虑一些问题，这些参考会有一些模糊，但是我仍然不能完全确定，我是该用这个算法还是改用那个算法，这个没有太大关系，当我遇到机器学习问题的时候，有时它确实不清楚这是否是最好的算法，但是就如在之前的视频中看到的算法确实很重要。但是通常更加重要的是：你有多少数据，你有多熟练是否擅长做误差分析和排除学习算法，指出如何设定新的特征变量和找出其他能决定你学习算法的变量等方面，通常这些方面会比你使用逻辑回归还是SVM这方面更加重要。但是，已经说过了，SVM仍然被广泛认为是一种最强大的学习算法，这是一个体系，包含了什么时候一个有效的方法去学习复杂的非线性函数。因此，实际上与逻辑回归、神经网络、SVM一起使用这些方法来提高学习算法，我认为你会很好地建立很有技术的状态。（编者注：当时GPU计算比较慢，神经网络还不流行。） 机器学习系统对于一个宽泛的应用领域来说，这是另一个在你军械库里非常强大的工具，你可以把它应用到很多地方，如硅谷、在工业、学术等领域建立许多高性能的机器学习系统。 聚类(Clustering)无监督学习：简介参考视频: 13 - 1 - Unsupervised Learning_ Introduction (3 min).mkv 在这个视频中，我将开始介绍聚类算法。这将是一个激动人心的时刻，因为这是我们学习的第一个非监督学习算法。我们将要让计算机学习无标签数据，而不是此前的标签数据。 那么，什么是非监督学习呢？在课程的一开始，我曾简单的介绍过非监督学习，然而，我们还是有必要将其与监督学习做一下比较。 在一个典型的监督学习中，我们有一个有标签的训练集，我们的目标是找到能够区分正样本和负样本的决策边界，在这里的监督学习中，我们有一系列标签，我们需要据此拟合一个假设函数。与此不同的是，在非监督学习中，我们的数据没有附带任何标签，我们拿到的数据就是这样的： 在这里我们有一系列点，却没有标签。因此，我们的训练集可以写成只有$x^{(1)}$,$x^{(2)}$…..一直到$x^{(m)}$。我们没有任何标签$y$。因此，图上画的这些点没有标签信息。也就是说，在非监督学习中，我们需要将一系列无标签的训练数据，输入到一个算法中，然后我们告诉这个算法，快去为我们找找这个数据的内在结构给定数据。我们可能需要某种算法帮助我们寻找一种结构。图上的数据看起来可以分成两个分开的点集（称为簇），一个能够找到我圈出的这些点集的算法，就被称为聚类算法。 这将是我们介绍的第一个非监督学习算法。当然，此后我们还将提到其他类型的非监督学习算法，它们可以为我们找到其他类型的结构或者其他的一些模式，而不只是簇。 我们将先介绍聚类算法。此后，我们将陆续介绍其他算法。那么聚类算法一般用来做什么呢？ 在这门课程的早些时候，我曾经列举过一些应用：比如市场分割。也许你在数据库中存储了许多客户的信息，而你希望将他们分成不同的客户群，这样你可以对不同类型的客户分别销售产品或者分别提供更适合的服务。社交网络分析：事实上有许多研究人员正在研究这样一些内容，他们关注一群人，关注社交网络，例如Facebook，Google+，或者是其他的一些信息，比如说：你经常跟哪些人联系，而这些人又经常给哪些人发邮件，由此找到关系密切的人群。因此，这可能需要另一个聚类算法，你希望用它发现社交网络中关系密切的朋友。我有一个朋友正在研究这个问题，他希望使用聚类算法来更好的组织计算机集群，或者更好的管理数据中心。因为如果你知道数据中心中，那些计算机经常协作工作。那么，你可以重新分配资源，重新布局网络。由此优化数据中心，优化数据通信。 最后，我实际上还在研究如何利用聚类算法了解星系的形成。然后用这个知识，了解一些天文学上的细节问题。好的，这就是聚类算法。这将是我们介绍的第一个非监督学习算法。在下一个视频中，我们将开始介绍一个具体的聚类算法。 K-均值算法参考视频: 13 - 2 - K-Means Algorithm (13 min).mkv K-均值是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类成不同的组。 K-均值是一个迭代算法，假设我们想要将数据聚类成n个组，其方法为: 首先选择$K$个随机的点，称为聚类中心（cluster centroids）； 对于数据集中的每一个数据，按照距离$K$个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。 计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置。 重复步骤2-4直至中心点不再变化。 下面是一个聚类示例： 迭代 1 次 迭代 3 次 迭代 10 次 用$μ^1$,$μ^2$,…,$μ^k$ 来表示聚类中心，用$c^{(1)}$,$c^{(2)}$,…,$c^{(m)}$来存储与第$i$个实例数据最近的聚类中心的索引，K-均值算法的伪代码如下： Repeat { for i = 1 to m c(i) := index (form 1 to K) of cluster centroid closest to x(i) for k = 1 to K μk := average (mean) of points assigned to cluster k }算法分为两个步骤，第一个for循环是赋值步骤，即：对于每一个样例$i$，计算其应该属于的类。第二个for循环是聚类中心的移动，即：对于每一个类$K$，重新计算该类的质心。 K-均值算法也可以很便利地用于将数据分为许多不同组，即使在没有非常明显区分的组群的情况下也可以。下图所示的数据集包含身高和体重两项特征构成的，利用K-均值算法将数据分为三类，用于帮助确定将要生产的T-恤衫的三种尺寸。 优化目标参考视频: 13 - 3 - Optimization Objective (7 min).mkv K-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和，因此K-均值的代价函数（又称畸变函数 Distortion function）为： $$J(c^{(1)},...,c^{(m)},μ_1,...,μ_K)=\dfrac {1}{m}\sum^{m}_{i=1}\left\| X^{\left( i\right) }-\mu_{c^{(i)} }\right\| ^{2}$$ 其中${ {\mu }_{ {{c}^{(i)} }} }$代表与${ {x}^{(i)} }$最近的聚类中心点。我们的的优化目标便是找出使得代价函数最小的 $c^{(1)}$,$c^{(2)}$,…,$c^{(m)}$和$μ^1$,$μ^2$,…,$μ^k$： 回顾刚才给出的:K-均值迭代算法，我们知道，第一个循环是用于减小$c^{(i)}$引起的代价，而第二个循环则是用于减小${ {\mu }_{i} }$引起的代价。迭代的过程一定会是每一次迭代都在减小代价函数，不然便是出现了错误。 随机初始化参考视频: 13 - 4 - Random Initialization (8 min).mkv 在运行K-均值算法的之前，我们首先要随机初始化所有的聚类中心点，下面介绍怎样做： 我们应该选择$K&lt;m$，即聚类中心点的个数要小于所有训练集实例的数量 随机选择$K$个训练实例，然后令$K$个聚类中心分别与这$K$个训练实例相等 K-均值的一个问题在于，它有可能会停留在一个局部最小值处，而这取决于初始化的情况。 为了解决这个问题，我们通常需要多次运行K-均值算法，每一次都重新进行随机初始化，最后再比较多次运行K-均值的结果，选择代价函数最小的结果。这种方法在$K$较小的时候（2–10）还是可行的，但是如果$K$较大，这么做也可能不会有明显地改善。 选择聚类数参考视频: 13 - 5 - Choosing the Number of Clusters (8 min).mkv 没有所谓最好的选择聚类数的方法，通常是需要根据不同的问题，人工进行选择的。选择的时候思考我们运用K-均值算法聚类的动机是什么，然后选择能最好服务于该目的标聚类数。 当人们在讨论，选择聚类数目的方法时，有一个可能会谈及的方法叫作“肘部法则”。关于“肘部法则”，我们所需要做的是改变$K$值，也就是聚类类别数目的总数。我们用一个聚类来运行K均值聚类方法。这就意味着，所有的数据都会分到一个聚类里，然后计算成本函数或者计算畸变函数$J$。$K$代表聚类数字。 我们可能会得到一条类似于这样的曲线。像一个人的肘部。这就是“肘部法则”所做的，让我们来看这样一个图，看起来就好像有一个很清楚的肘在那儿。好像人的手臂，如果你伸出你的胳膊，那么这就是你的肩关节、肘关节、手。这就是“肘部法则”。你会发现这种模式，它的畸变值会迅速下降，从1到2，从2到3之后，你会在3的时候达到一个肘点。在此之后，畸变值就下降的非常慢，看起来就像使用3个聚类来进行聚类是正确的，这是因为那个点是曲线的肘点，畸变值下降得很快，$K=3$之后就下降得很慢，那么我们就选$K=3$。当你应用“肘部法则”的时候，如果你得到了一个像上面这样的图，那么这将是一种用来选择聚类个数的合理方法。 例如，我们的 T-恤制造例子中，我们要将用户按照身材聚类，我们可以分成3个尺寸:$S,M,L$，也可以分成5个尺寸$XS,S,M,L,XL$，这样的选择是建立在回答“聚类后我们制造的T-恤是否能较好地适合我们的客户”这个问题的基础上作出的。 聚类参考资料： 1.相似度/距离计算方法总结 (1). 闵可夫斯基距离Minkowski/（其中欧式距离：$p=2$) $dist(X,Y)={ {\left( { {\sum\limits_{i=1}^{n}{\left| { {x}_{i} }-{ {y}_{i} } \right|} }^{p} } \right)}^{\frac{1}{p} }}$ (2). 杰卡德相似系数(Jaccard)： $J(A,B)=\frac{\left| A\cap B \right|}{\left|A\cup B \right|}$ (3). 余弦相似度(cosine similarity)： $n$维向量$x$和$y$的夹角记做$\theta$，根据余弦定理，其余弦值为： $cos (\theta )=\frac{ {{x}^{T} }y}{\left|x \right|\cdot \left| y \right|}=\frac{\sum\limits_{i=1}^{n}{ {{x}_{i} }{ {y}_{i} }} }{\sqrt{\sum\limits_{i=1}^{n}{ {{x}_{i} }^{2} }}\sqrt{\sum\limits_{i=1}^{n}{ {{y}_{i} }^{2} }} }$ (4). Pearson皮尔逊相关系数： ${ {\rho }_{XY} }=\frac{\operatorname{cov}(X,Y)}{ {{\sigma }_{X} }{ {\sigma }_{Y} }}=\frac{E[(X-{ {\mu }_{X} })(Y-{ {\mu }_{Y} })]}{ {{\sigma }_{X} }{ {\sigma }_{Y} }}=\frac{\sum\limits_{i=1}^{n}{(x-{ {\mu }_{X} })(y-{ {\mu }_{Y} })} }{\sqrt{\sum\limits_{i=1}^{n}{ {{(x-{ {\mu }_{X} })}^{2} }} }\sqrt{\sum\limits_{i=1}^{n}{ {{(y-{ {\mu }_{Y} })}^{2} }} }}$ Pearson相关系数即将$x$、$y$坐标向量各自平移到原点后的夹角余弦。 2.聚类的衡量指标 (1). 均一性：$p$ 类似于精确率，一个簇中只包含一个类别的样本，则满足均一性。其实也可以认为就是正确率(每个 聚簇中正确分类的样本数占该聚簇总样本数的比例和) (2). 完整性：$r$ 类似于召回率，同类别样本被归类到相同簇中，则满足完整性;每个聚簇中正确分类的样本数占该类型的总样本数比例的和 (3). V-measure: 均一性和完整性的加权平均 $V = \frac{(1+\beta^2)pr}{\beta^2p+r}$ (4). 轮廓系数 样本$i$的轮廓系数：$s(i)$ 簇内不相似度:计算样本$i$到同簇其它样本的平均距离为$a(i)$，应尽可能小。 簇间不相似度:计算样本$i$到其它簇$C_j$的所有样本的平均距离$b_{ij}$，应尽可能大。 轮廓系数：$s(i)$值越接近1表示样本$i$聚类越合理，越接近-1，表示样本$i$应该分类到 另外的簇中，近似为0，表示样本$i$应该在边界上;所有样本的$s(i)$的均值被成为聚类结果的轮廓系数。 $s(i) = \frac{b(i)-a(i)}{max\{a(i),b(i)\} }$ (5). ARI 数据集$S$共有$N$个元素， 两个聚类结果分别是： $X=\{ {{X}_{1} },{ {X}_{2} },...,{ {X}_{r} }\},Y=\{ {{Y}_{1} },{ {Y}_{2} },...,{ {Y}_{s} }\}$ $X$和$Y$的元素个数为： $a=\{ {{a}_{1} },{ {a}_{2} },...,{ {a}_{r} }\},b=\{ {{b}_{1} },{ {b}_{2} },...,{ {b}_{s} }\}$ 记：${ {n}_{ij} }=\left| { {X}_{i} }\cap { {Y}_{i} } \right|$ $ARI=\frac{\sum\limits_{i,j}{C_{ {{n}_{ij} }}^{2} }-\left[ \left( \sum\limits_{i}{C_{ {{a}_{i} }}^{2} } \right)\cdot \left( \sum\limits_{i}{C_{ {{b}_{i} }}^{2} } \right) \right]/C_{n}^{2} }{\frac{1}{2}\left[ \left( \sum\limits_{i}{C_{ {{a}_{i} }}^{2} } \right)+\left( \sum\limits_{i}{C_{ {{b}_{i} }}^{2} } \right) \right]-\left[ \left( \sum\limits_{i}{C_{ {{a}_{i} }}^{2} } \right)\cdot \left( \sum\limits_{i}{C_{ {{b}_{i} }}^{2} } \right) \right]/C_{n}^{2} }$ 降维(Dimensionality Reduction)动机一：数据压缩参考视频: 14 - 1 - Motivation I_ Data Compression (10 min).mkv 这个视频，我想开始谈论第二种类型的无监督学习问题，称为降维。有几个不同的的原因使你可能想要做降维。一是数据压缩，后面我们会看了一些视频后，数据压缩不仅允许我们压缩数据，因而使用较少的计算机内存或磁盘空间，但它也让我们加快我们的学习算法。 但首先，让我们谈论降维是什么。作为一种生动的例子，我们收集的数据集，有许多，许多特征，我绘制两个在这里。 假设我们未知两个的特征：$x_1$:长度：用厘米表示；$x_2$：是用英寸表示同一物体的长度。 所以，这给了我们高度冗余表示，也许不是两个分开的特征$x_1$和$x_2$，这两个基本的长度度量，也许我们想要做的是减少数据到一维，只有一个数测量这个长度。这个例子似乎有点做作，这里厘米英寸的例子实际上不是那么不切实际的，两者并没有什么不同。 将数据从二维降至一维：假使我们要采用两种不同的仪器来测量一些东西的尺寸，其中一个仪器测量结果的单位是英寸，另一个仪器测量的结果是厘米，我们希望将测量的结果作为我们机器学习的特征。现在的问题的是，两种仪器对同一个东西测量的结果不完全相等（由于误差、精度等），而将两者都作为特征有些重复，因而，我们希望将这个二维的数据降至一维。 从这件事情我看到的东西发生在工业上的事。如果你有几百个或成千上万的特征，它是它这往往容易失去你需要的特征。有时可能有几个不同的工程团队，也许一个工程队给你二百个特征，第二工程队给你另外三百个的特征，第三工程队给你五百个特征，一千多个特征都在一起，它实际上会变得非常困难，去跟踪你知道的那些特征，你从那些工程队得到的。其实不想有高度冗余的特征一样。 多年我一直在研究直升飞机自动驾驶。诸如此类。如果你想测量——如果你想做，你知道，做一个调查或做这些不同飞行员的测试——你可能有一个特征：$x_1$，这也许是他们的技能（直升机飞行员），也许$x_2$可能是飞行员的爱好。这是表示他们是否喜欢飞行，也许这两个特征将高度相关。你真正关心的可能是这条红线的方向，不同的特征，决定飞行员的能力。 将数据从三维降至二维：这个例子中我们要将一个三维的特征向量降至一个二维的特征向量。过程是与上面类似的，我们将三维向量投射到一个二维的平面上，强迫使得所有的数据都在同一个平面上，降至二维的特征向量。 这样的处理过程可以被用于把任何维度的数据降到任何想要的维度，例如将1000维的特征降至100维。 正如我们所看到的，最后，这将使我们能够使我们的一些学习算法运行也较晚，但我们会在以后的视频提到它。 动机二：数据可视化参考视频: 14 - 2 - Motivation II_ Visualization (6 min).mkv 在许多及其学习问题中，如果我们能将数据可视化，我们便能寻找到一个更好的解决方案，降维可以帮助我们。 假使我们有有关于许多不同国家的数据，每一个特征向量都有50个特征（如GDP，人均GDP，平均寿命等）。如果要将这个50维的数据可视化是不可能的。使用降维的方法将其降至2维，我们便可以将其可视化了。 这样做的问题在于，降维的算法只负责减少维数，新产生的特征的意义就必须由我们自己去发现了。 主成分分析问题参考视频: 14 - 3 - Principal Component Analysis Problem Formulation (9 min). mkv 主成分分析(PCA)是最常见的降维算法。 在PCA中，我们要做的是找到一个方向向量（Vector direction），当我们把所有的数据都投射到该向量上时，我们希望投射平均均方误差能尽可能地小。方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。 下面给出主成分分析问题的描述： 问题是要将$n$维数据降至$k$维，目标是找到向量$u^{(1)}$,$u^{(2)}$,…,$u^{(k)}$使得总的投射误差最小。主成分分析与线性回顾的比较： 主成分分析与线性回归是两种不同的算法。主成分分析最小化的是投射误差（Projected Error），而线性回归尝试的是最小化预测误差。线性回归的目的是预测结果，而主成分分析不作任何预测。 上图中，左边的是线性回归的误差（垂直于横轴投影），右边则是主要成分分析的误差（垂直于红线投影）。 PCA将$n$个特征降维到$k$个，可以用来进行数据压缩，如果100维的向量最后可以用10维来表示，那么压缩率为90%。同样图像处理领域的KL变换使用PCA做图像压缩。但PCA 要保证降维后，还要保证数据的特性损失最小。 PCA技术的一大好处是对数据进行降维的处理。我们可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。 PCA技术的一个很大的优点是，它是完全无参数限制的。在PCA的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。 但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。 主成分分析算法参考视频: 14 - 4 - Principal Component Analysis Algorithm (15 min).mkv PCA 减少$n$维到$k$维： 第一步是均值归一化。我们需要计算出所有特征的均值，然后令 $x_j= x_j-μ_j$。如果特征是在不同的数量级上，我们还需要将其除以标准差 $σ^2$。 第二步是计算协方差矩阵（covariance matrix）$Σ$： $\sum=\dfrac {1}{m}\sum^{n}_{i=1}\left( x^{(i)}\right) \left( x^{(i)}\right) ^{T}$ 第三步是计算协方差矩阵$Σ$的特征向量（eigenvectors）: 在 Octave 里我们可以利用奇异值分解（singular value decomposition）来求解，[U, S, V]= svd(sigma)。 $$Sigma=\dfrac {1}{m}\sum^{n}_{i=1}\left( x^{(i)}\right) \left( x^{(i)}\right) ^{T}$$ 对于一个 $n×n$维度的矩阵，上式中的$U$是一个具有与数据之间最小投射误差的方向向量构成的矩阵。如果我们希望将数据从$n$维降至$k$维，我们只需要从$U$中选取前$k$个向量，获得一个$n×k$维度的矩阵，我们用$U_{reduce}$表示，然后通过如下计算获得要求的新特征向量$z^{(i)}$:$$z^{(i)}=U^{T}_{reduce}*x^{(i)}$$ 其中$x$是$n×1$维的，因此结果为$k×1$维度。注，我们不对方差特征进行处理。 选择主成分的数量参考视频: 14 - 5 - Choosing The Number Of Principal Components (13 min).mkv 主要成分分析是减少投射的平均均方误差： 训练集的方差为：$\dfrac {1}{m}\sum^{m}_{i=1}\left\| x^{\left( i\right) }\right\| ^{2}$ 我们希望在平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的$k$值。 如果我们希望这个比例小于1%，就意味着原本数据的偏差有99%都保留下来了，如果我们选择保留95%的偏差，便能非常显著地降低模型中特征的维度了。 我们可以先令$k=1$，然后进行主要成分分析，获得$U_{reduce}$和$z$，然后计算比例是否小于1%。如果不是的话再令$k=2$，如此类推，直到找到可以使得比例小于1%的最小$k$ 值（原因是各个特征之间通常情况存在某种相关性）。 还有一些更好的方式来选择$k$，当我们在Octave中调用“svd”函数的时候，我们获得三个参数：[U, S, V] = svd(sigma)。 其中的$S$是一个$n×n$的矩阵，只有对角线上有值，而其它单元都是0，我们可以使用这个矩阵来计算平均均方误差与训练集方差的比例： $$\dfrac {\dfrac {1}{m}\sum^{m}_{i=1}\left\| x^{\left( i\right) }-x^{\left( i\right) }_{approx}\right\| ^{2} }{\dfrac {1}{m}\sum^{m}_{i=1}\left\| x^{(i)}\right\| ^{2} }=1-\dfrac {\Sigma^{k}_{i=1}S_{ii} }{\Sigma^{m}_{i=1}S_{ii} }\leq 1\%$$ 也就是：$$\frac {\Sigma^{k}_{i=1}s_{ii} }{\Sigma^{n}_{i=1}s_{ii} }\geq0.99$$ 在压缩过数据后，我们可以采用如下方法来近似地获得原有的特征：$$x^{\left( i\right) }_{approx}=U_{reduce}z^{(i)}$$ 重建的压缩表示参考视频: 14 - 6 - Reconstruction from Compressed Representation (4 min).mkv 在以前的视频中，我谈论PCA作为压缩算法。在那里你可能需要把1000维的数据压缩100维特征，或具有三维数据压缩到一二维表示。所以，如果这是一个压缩算法，应该能回到这个压缩表示，回到你原有的高维数据的一种近似。 所以，给定的$z^{(i)}$，这可能100维，怎么回到你原来的表示$x^{(i)}$，这可能是1000维的数组？ PCA算法，我们可能有一个这样的样本。如图中样本$x^{(1)}$,$x^{(2)}$。我们做的是，我们把这些样本投射到图中这个一维平面。然后现在我们需要只使用一个实数，比如$z^{(1)}$，指定这些点的位置后他们被投射到这一个三维曲面。给定一个点$z^{(1)}$，我们怎么能回去这个原始的二维空间呢？$x$为2维，$z$为1维，$z=U^{T}_{reduce}x$，相反的方程为：$x_{appox}=U_{reduce}\cdot z$,$x_{appox}\approx x$。如图： 如你所知，这是一个漂亮的与原始数据相当相似。所以，这就是你从低维表示$z$回到未压缩的表示。我们得到的数据的一个之间你的原始数据 $x$，我们也把这个过程称为重建原始数据。 当我们认为试图重建从压缩表示 $x$ 的初始值。所以，给定未标记的数据集，您现在知道如何应用PCA，你的带高维特征$x$和映射到这的低维表示$z$。这个视频，希望你现在也知道如何采取这些低维表示$z$，映射到备份到一个近似你原有的高维数据。 现在你知道如何实施应用PCA，我们将要做的事是谈论一些技术在实际使用PCA很好，特别是，在接下来的视频中，我想谈一谈关于如何选择$k$。 主成分分析法的应用建议参考视频: 14 - 7 - Advice for Applying PCA (13 min).mkv 假使我们正在针对一张 100×100像素的图片进行某个计算机视觉的机器学习，即总共有10000 个特征。 第一步是运用主要成分分析将数据压缩至1000个特征 然后对训练集运行学习算法 在预测时，采用之前学习而来的$U_{reduce}$将输入的特征$x$转换成特征向量$z$，然后再进行预测 注：如果我们有交叉验证集合测试集，也采用对训练集学习而来的$U_{reduce}$。 错误的主要成分分析情况：一个常见错误使用主要成分分析的情况是，将其用于减少过拟合（减少了特征的数量）。这样做非常不好，不如尝试正则化处理。原因在于主要成分分析只是近似地丢弃掉一些特征，它并不考虑任何与结果变量有关的信息，因此可能会丢失非常重要的特征。然而当我们进行正则化处理时，会考虑到结果变量，不会丢掉重要的数据。 另一个常见的错误是，默认地将主要成分分析作为学习过程中的一部分，这虽然很多时候有效果，最好还是从所有原始特征开始，只在有必要的时候（算法运行太慢或者占用太多内存）才考虑采用主要成分分析。 异常检测(Anomaly Detection)问题的动机参考文档: 15 - 1 - Problem Motivation (8 min).mkv 在接下来的一系列视频中，我将向大家介绍异常检测(Anomaly detection)问题。这是机器学习算法的一个常见应用。这种算法的一个有趣之处在于：它虽然主要用于非监督学习问题，但从某些角度看，它又类似于一些监督学习问题。 什么是异常检测呢？为了解释这个概念，让我举一个例子吧： 假想你是一个飞机引擎制造商，当你生产的飞机引擎从生产线上流出时，你需要进行QA(质量控制测试)，而作为这个测试的一部分，你测量了飞机引擎的一些特征变量，比如引擎运转时产生的热量，或者引擎的振动等等。 这样一来，你就有了一个数据集，从$x^{(1)}$到$x^{(m)}$，如果你生产了$m$个引擎的话，你将这些数据绘制成图表，看起来就是这个样子： 这里的每个点、每个叉，都是你的无标签数据。这样，异常检测问题可以定义如下：我们假设后来有一天，你有一个新的飞机引擎从生产线上流出，而你的新飞机引擎有特征变量$x_{test}$。所谓的异常检测问题就是：我们希望知道这个新的飞机引擎是否有某种异常，或者说，我们希望判断这个引擎是否需要进一步测试。因为，如果它看起来像一个正常的引擎，那么我们可以直接将它运送到客户那里，而不需要进一步的测试。 给定数据集 $x^{(1)},x^{(2)},..,x^{(m)}$，我们假使数据集是正常的，我们希望知道新的数据 $x_{test}$ 是不是异常的，即这个测试数据不属于该组数据的几率如何。我们所构建的模型应该能根据该测试数据的位置告诉我们其属于一组数据的可能性 $p(x)$。 上图中，在蓝色圈内的数据属于该组数据的可能性较高，而越是偏远的数据，其属于该组数据的可能性就越低。 这种方法称为密度估计，表达如下： $$if \quad p(x)\begin{cases}&lt; \varepsilon &amp; anomaly \ =\varepsilon &amp; normal\end{cases}$$ 欺诈检测： $x^{(i)} = {用户的第i个活动特征}$ 模型$p(x)$ 为我们其属于一组数据的可能性，通过$p(x) &lt; \varepsilon$检测非正常用户。 异常检测主要用来识别欺骗。例如在线采集而来的有关用户的数据，一个特征向量中可能会包含如：用户多久登录一次，访问过的页面，在论坛发布的帖子数量，甚至是打字速度等。尝试根据这些特征构建一个模型，可以用这个模型来识别那些不符合该模式的用户。 再一个例子是检测一个数据中心，特征可能包含：内存使用情况，被访问的磁盘数量，CPU的负载，网络的通信量等。根据这些特征可以构建一个模型，用来判断某些计算机是不是有可能出错了。 高斯分布参考视频: 15 - 2 - Gaussian Distribution (10 min).mkv 在这个视频中，我将介绍高斯分布，也称为正态分布。回顾高斯分布的基本知识。 通常如果我们认为变量 $x$ 符合高斯分布 $x \sim N(\mu, \sigma^2)$则其概率密度函数为： $p(x,\mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$ 我们可以利用已有的数据来预测总体中的$μ$和$σ^2$的计算方法如下： $\mu=\frac{1}{m}\sum\limits_{i=1}^{m}x^{(i)}$ $\sigma^2=\frac{1}{m}\sum\limits_{i=1}^{m}(x^{(i)}-\mu)^2$ 高斯分布样例： 注：机器学习中对于方差我们通常只除以$m$而非统计学中的$(m-1)$。这里顺便提一下，在实际使用中，到底是选择使用$1/m$还是$1/(m-1)$其实区别很小，只要你有一个还算大的训练集，在机器学习领域大部分人更习惯使用$1/m$这个版本的公式。这两个版本的公式在理论特性和数学特性上稍有不同，但是在实际使用中，他们的区别甚小，几乎可以忽略不计。 算法参考视频: 15 - 3 - Algorithm (12 min).mkv 在本节视频中，我将应用高斯分布开发异常检测算法。 异常检测算法： 对于给定的数据集 $x^{(1)},x^{(2)},...,x^{(m)}$，我们要针对每一个特征计算 $\mu$ 和 $\sigma^2$ 的估计值。 $\mu_j=\frac{1}{m}\sum\limits_{i=1}^{m}x_j^{(i)}$ $\sigma_j^2=\frac{1}{m}\sum\limits_{i=1}^m(x_j^{(i)}-\mu_j)^2$ 一旦我们获得了平均值和方差的估计值，给定新的一个训练实例，根据模型计算 $p(x)$： $p(x)=\prod\limits_{j=1}^np(x_j;\mu_j,\sigma_j^2)=\prod\limits_{j=1}^1\frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-\mu_j)^2}{2\sigma_j^2})$ 当$p(x) &lt; \varepsilon$时，为异常。 下图是一个由两个特征的训练集，以及特征的分布情况： 下面的三维图表表示的是密度估计函数，$z$轴为根据两个特征的值所估计$p(x)$值： 我们选择一个$\varepsilon$，将$p(x) = \varepsilon$作为我们的判定边界，当$p(x) > \varepsilon$时预测数据为正常数据，否则为异常。 在这段视频中，我们介绍了如何拟合$p(x)$，也就是 $x$的概率值，以开发出一种异常检测算法。同时，在这节课中，我们也给出了通过给出的数据集拟合参数，进行参数估计，得到参数 $\mu$ 和 $\sigma$，然后检测新的样本，确定新样本是否是异常。 在接下来的课程中，我们将深入研究这一算法，同时更深入地介绍，怎样让算法工作地更加有效。 开发和评价一个异常检测系统参考视频: 15 - 4 - Developing and Evaluating an Anomaly Detection System (13 min). mkv 异常检测算法是一个非监督学习算法，意味着我们无法根据结果变量 $ y$ 的值来告诉我们数据是否真的是异常的。我们需要另一种方法来帮助检验算法是否有效。当我们开发一个异常检测系统时，我们从带标记（异常或正常）的数据着手，我们从其中选择一部分正常数据用于构建训练集，然后用剩下的正常数据和异常数据混合的数据构成交叉检验集和测试集。 例如：我们有10000台正常引擎的数据，有20台异常引擎的数据。 我们这样分配数据： 6000台正常引擎的数据作为训练集 2000台正常引擎和10台异常引擎的数据作为交叉检验集 2000台正常引擎和10台异常引擎的数据作为测试集 具体的评价方法如下： 根据测试集数据，我们估计特征的平均值和方差并构建$p(x)$函数 对交叉检验集，我们尝试使用不同的$\varepsilon$值作为阀值，并预测数据是否异常，根据$F1$值或者查准率与查全率的比例来选择 $\varepsilon$ 选出 $\varepsilon$ 后，针对测试集进行预测，计算异常检验系统的$F1$值，或者查准率与查全率之比 异常检测与监督学习对比参考视频: 15 - 5 - Anomaly Detection vs. Supervised Learning (8 min).mkv 之前我们构建的异常检测系统也使用了带标记的数据，与监督学习有些相似，下面的对比有助于选择采用监督学习还是异常检测： 两者比较： 异常检测 监督学习 非常少量的正向类（异常数据 $y=1$）, 大量的负向类（$y=0$） 同时有大量的正向类和负向类 许多不同种类的异常，非常难。根据非常 少量的正向类数据来训练算法。 有足够多的正向类实例，足够用于训练 算法，未来遇到的正向类实例可能与训练集中的非常近似。 未来遇到的异常可能与已掌握的异常、非常的不同。 例如： 欺诈行为检测 生产（例如飞机引擎）检测数据中心的计算机运行状况 例如：邮件过滤器 天气预报 肿瘤分类 希望这节课能让你明白一个学习问题的什么样的特征，能让你把这个问题当做是一个异常检测，或者是一个监督学习的问题。另外，对于很多技术公司可能会遇到的一些问题，通常来说，正样本的数量很少，甚至有时候是0，也就是说，出现了太多没见过的不同的异常类型，那么对于这些问题，通常应该使用的算法就是异常检测算法。 选择特征参考视频: 15 - 6 - Choosing What Features to Use (12 min).mkv 对于异常检测算法，我们使用的特征是至关重要的，下面谈谈如何选择特征： 异常检测假设特征符合高斯分布，如果数据的分布不是高斯分布，异常检测算法也能够工作，但是最好还是将数据转换成高斯分布，例如使用对数函数：$x= log(x+c)$，其中 $c$ 为非负常数； 或者 $x=x^c$，$c$为 0-1 之间的一个分数，等方法。(编者注：在python中，通常用np.log1p()函数，$log1p$就是 $log(x+1)$，可以避免出现负数结果，反向函数就是np.expm1()) 误差分析： 一个常见的问题是一些异常的数据可能也会有较高的$p(x)$值，因而被算法认为是正常的。这种情况下误差分析能够帮助我们，我们可以分析那些被算法错误预测为正常的数据，观察能否找出一些问题。我们可能能从问题中发现我们需要增加一些新的特征，增加这些新特征后获得的新算法能够帮助我们更好地进行异常检测。 异常检测误差分析： 我们通常可以通过将一些相关的特征进行组合，来获得一些新的更好的特征（异常数据的该特征值异常地大或小），例如，在检测数据中心的计算机状况的例子中，我们可以用CPU负载与网络通信量的比例作为一个新的特征，如果该值异常地大，便有可能意味着该服务器是陷入了一些问题中。 在这段视频中，我们介绍了如何选择特征，以及对特征进行一些小小的转换，让数据更像正态分布，然后再把数据输入异常检测算法。同时也介绍了建立特征时，进行的误差分析方法，来捕捉各种异常的可能。希望你通过这些方法，能够了解如何选择好的特征变量，从而帮助你的异常检测算法，捕捉到各种不同的异常情况。 多元高斯分布（选修）参考视频: 15 - 7 - Multivariate Gaussian Distribution (Optional) (14 min).mkv 假使我们有两个相关的特征，而且这两个特征的值域范围比较宽，这种情况下，一般的高斯分布模型可能不能很好地识别异常数据。其原因在于，一般的高斯分布模型尝试的是去同时抓住两个特征的偏差，因此创造出一个比较大的判定边界。 下图中是两个相关特征，洋红色的线（根据ε的不同其范围可大可小）是一般的高斯分布模型获得的判定边界，很明显绿色的X所代表的数据点很可能是异常值，但是其$p(x)$值却仍然在正常范围内。多元高斯分布将创建像图中蓝色曲线所示的判定边界。 在一般的高斯分布模型中，我们计算 $p(x)$ 的方法是：通过分别计算每个特征对应的几率然后将其累乘起来，在多元高斯分布模型中，我们将构建特征的协方差矩阵，用所有的特征一起来计算 $p(x)$。 我们首先计算所有特征的平均值，然后再计算协方差矩阵： $p(x)=\prod_{j=1}^np(x_j;\mu,\sigma_j^2)=\prod_{j=1}^n\frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-\mu_j)^2}{2\sigma_j^2})$ $\mu=\frac{1}{m}\sum_{i=1}^mx^{(i)}$ $\Sigma = \frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu)(x^{(i)}-\mu)^T=\frac{1}{m}(X-\mu)^T(X-\mu)$ 注:其中$\mu $ 是一个向量，其每一个单元都是原特征矩阵中一行数据的均值。最后我们计算多元高斯分布的$p\left( x \right)$: $p(x)=\frac{1}{(2\pi)^{\frac{n}{2} }|\Sigma|^{\frac{1}{2} }}exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)$ 其中： $|\Sigma|$是定矩阵，在 **Octave** 中用 `det(sigma)`计算 $\Sigma^{-1}$ 是逆矩阵，下面我们来看看协方差矩阵是如何影响模型的： 上图是5个不同的模型，从左往右依次分析： 是一个一般的高斯分布模型 通过协方差矩阵，令特征1拥有较小的偏差，同时保持特征2的偏差 通过协方差矩阵，令特征2拥有较大的偏差，同时保持特征1的偏差 通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的正相关性 通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的负相关性 多元高斯分布模型与原高斯分布模型的关系： 可以证明的是，原本的高斯分布模型是多元高斯分布模型的一个子集，即像上图中的第1、2、3，3个例子所示，如果协方差矩阵只在对角线的单位上有非零的值时，即为原本的高斯分布模型了。 原高斯分布模型和多元高斯分布模型的比较： 原高斯分布模型 多元高斯分布模型 不能捕捉特征之间的相关性 但可以通过将特征进行组合的方法来解决 自动捕捉特征之间的相关性 计算代价低，能适应大规模的特征 计算代价较高 训练集较小时也同样适用 必须要有 $m>n$，不然的话协方差矩阵$\Sigma$不可逆的，通常需要 $m>10n$ 另外特征冗余也会导致协方差矩阵不可逆 原高斯分布模型被广泛使用着，如果特征之间在某种程度上存在相互关联的情况，我们可以通过构造新新特征的方法来捕捉这些相关性。 如果训练集不是太大，并且没有太多的特征，我们可以使用多元高斯分布模型。 使用多元高斯分布进行异常检测（可选）参考视频: 15 - 8 - Anomaly Detection using the Multivariate Gaussian Distribution (Optional) (14 min).mkv 在我们谈到的最后一个视频，关于多元高斯分布，看到的一些建立的各种分布模型，当你改变参数，$\mu$ 和 $\Sigma$。在这段视频中，让我们用这些想法，并应用它们制定一个不同的异常检测算法。 要回顾一下多元高斯分布和多元正态分布： 分布有两个参数， $\mu$ 和 $\Sigma$。其中$\mu$这一个$n$维向量和 $\Sigma$ 的协方差矩阵，是一种$n\times n$的矩阵。而这里的公式$x$的概率，如按 $\mu$ 和参数化 $\Sigma$，和你的变量 $\mu$ 和 $\Sigma$，你可以得到一个范围的不同分布一样，你知道的，这些都是三个样本，那些我们在以前的视频看过了。 因此，让我们谈谈参数拟合或参数估计问题： 我有一组样本${ {{ x^{(1)},x^{(2)},...,x^{(m)} } } }$是一个$n$维向量，我想我的样本来自一个多元高斯分布。我如何尝试估计我的参数 $\mu$ 和 $\Sigma$ 以及标准公式？ 估计他们是你设置 $\mu$ 是你的训练样本的平均值。 $\mu=\frac{1}{m}\sum_{i=1}^{m}x^{(i)}$ 并设置$\Sigma$： $\Sigma=\frac{1}{m}\sum_{i=1}^{m}(x^{(i)}-\mu)(x^{(i)}-\mu)^T$ 这其实只是当我们使用PCA算法时候，有 $\Sigma$ 时写出来。所以你只需插入上述两个公式，这会给你你估计的参数 $\mu$ 和你估计的参数 $\Sigma$。所以，这里给出的数据集是你如何估计 $\mu$ 和 $\Sigma$。让我们以这种方法而只需将其插入到异常检测算法。那么，我们如何把所有这一切共同开发一个异常检测算法？ 首先，我们把我们的训练集，和我们的拟合模型，我们计算$p(x)$，要知道，设定$\mu$和描述的一样$\Sigma$。 如图，该分布在中央最多，越到外面的圈的范围越小。 并在该点是出路这里的概率非常低。 原始模型与多元高斯模型的关系如图： 其中：协方差矩阵$\Sigma$为： 原始模型和多元高斯分布比较如图： 推荐系统(Recommender Systems)问题形式化参考视频: 16 - 1 - Problem Formulation (8 min).mkv 在接下来的视频中，我想讲一下推荐系统。我想讲推荐系统有两个原因： 第一、仅仅因为它是机器学习中的一个重要的应用。在过去几年，我偶尔访问硅谷不同的技术公司，我常和工作在这儿致力于机器学习应用的人们聊天，我常问他们，最重要的机器学习的应用是什么，或者，你最想改进的机器学习应用有哪些。我最常听到的答案是推荐系统。现在，在硅谷有很多团体试图建立很好的推荐系统。因此，如果你考虑网站像亚马逊，或网飞公司或易趣，或iTunes Genius，有很多的网站或系统试图推荐新产品给用户。如，亚马逊推荐新书给你，网飞公司试图推荐新电影给你，等等。这些推荐系统，根据浏览你过去买过什么书，或过去评价过什么电影来判断。这些系统会带来很大一部分收入，比如为亚马逊和像网飞这样的公司。因此，对推荐系统性能的改善，将对这些企业的有实质性和直接的影响。 推荐系统是个有趣的问题，在学术机器学习中因此，我们可以去参加一个学术机器学习会议，推荐系统问题实际上受到很少的关注，或者，至少在学术界它占了很小的份额。但是，如果你看正在发生的事情，许多有能力构建这些系统的科技企业，他们似乎在很多企业中占据很高的优先级。这是我为什么在这节课讨论它的原因之一。 我想讨论推荐系统地第二个原因是：这个班视频的最后几集我想讨论机器学习中的一些大思想，并和大家分享。这节课我们也看到了，对机器学习来说，特征是很重要的，你所选择的特征，将对你学习算法的性能有很大的影响。因此，在机器学习中有一种大思想，它针对一些问题，可能并不是所有的问题，而是一些问题，有算法可以为你自动学习一套好的特征。因此，不要试图手动设计，而手写代码这是目前为止我们常干的。有一些设置，你可以有一个算法，仅仅学习其使用的特征，推荐系统就是类型设置的一个例子。还有很多其它的，但是通过推荐系统，我们将领略一小部分特征学习的思想，至少，你将能够了解到这方面的一个例子，我认为，机器学习中的大思想也是这样。因此，让我们开始讨论推荐系统问题形式化。 我们从一个例子开始定义推荐系统的问题。 假使我们是一个电影供应商，我们有 5 部电影和 4 个用户，我们要求用户为电影打分。 前三部电影是爱情片，后两部则是动作片，我们可以看出Alice和Bob似乎更倾向与爱情片， 而 Carol 和 Dave 似乎更倾向与动作片。并且没有一个用户给所有的电影都打过分。我们希望构建一个算法来预测他们每个人可能会给他们没看过的电影打多少分，并以此作为推荐的依据。 下面引入一些标记： $n_u$ 代表用户的数量 $n_m$ 代表电影的数量 $r(i, j)$ 如果用户j给电影 $i$ 评过分则 $r(i,j)=1$ $y^{(i, j)}$ 代表用户 $j$ 给电影$i$的评分 $m_j$代表用户 $j$ 评过分的电影的总数 基于内容的推荐系统参考视频: 16 - 2 - Content Based Recommendations (15 min).mkv 在一个基于内容的推荐系统算法中，我们假设对于我们希望推荐的东西有一些数据，这些数据是有关这些东西的特征。 在我们的例子中，我们可以假设每部电影都有两个特征，如$x_1$代表电影的浪漫程度，$x_2$ 代表电影的动作程度。 则每部电影都有一个特征向量，如$x^{(1)}$是第一部电影的特征向量为[0.9 0]。 下面我们要基于这些特征来构建一个推荐系统算法。假设我们采用线性回归模型，我们可以针对每一个用户都训练一个线性回归模型，如${ {\theta }^{(1)} }$是第一个用户的模型的参数。于是，我们有： $\theta^{(j)}$用户 $j$ 的参数向量 $x^{(i)}$电影 $i$ 的特征向量 对于用户 $j$ 和电影 $i$，我们预测评分为：$(\theta^{(j)})^T x^{(i)}$ 代价函数 针对用户 $j$，该线性回归模型的代价为预测误差的平方和，加上正则化项：$$\min_{\theta (j)}\frac{1}{2}\sum_{i:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2+\frac{\lambda}{2}\left(\theta_{k}^{(j)}\right)^2$$ 其中 $i:r(i,j)$表示我们只计算那些用户 $j$ 评过分的电影。在一般的线性回归模型中，误差项和正则项应该都是乘以$1/2m$，在这里我们将$m$去掉。并且我们不对方差项$\theta_0$进行正则化处理。 上面的代价函数只是针对一个用户的，为了学习所有用户，我们将所有用户的代价函数求和：$$\min_{\theta^{(1)},…,\theta^{(n_u)} } \frac{1}{2}\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2$$如果我们要用梯度下降法来求解最优解，我们计算代价函数的偏导数后得到梯度下降的更新公式为： $$\theta_k^{(j)}:=\theta_k^{(j)}-\alpha\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_{k}^{(i)} \quad (\text{for} , k = 0)$$ $$\theta_k^{(j)}:=\theta_k^{(j)}-\alpha\left(\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_{k}^{(i)}+\lambda\theta_k^{(j)}\right) \quad (\text{for} , k\neq 0)$$ 协同过滤参考视频: 16 - 3 - Collaborative Filtering (10 min).mkv 在之前的基于内容的推荐系统中，对于每一部电影，我们都掌握了可用的特征，使用这些特征训练出了每一个用户的参数。相反地，如果我们拥有用户的参数，我们可以学习得出电影的特征。 $$\mathop{min}\limits_{x^{(1)},…,x^{(n_m)} }\frac{1}{2}\sum_{i=1}^{n_m}\sum_{j{r(i,j)=1} }((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}(x_k^{(i)})^2$$但是如果我们既没有用户的参数，也没有电影的特征，这两种方法都不可行了。协同过滤算法可以同时学习这两者。 我们的优化目标便改为同时针对$x$和$\theta$进行。$$J(x^{(1)},…x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)})=\frac{1}{2}\sum_{(i:j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}(x_k^{(j)})^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2$$ 对代价函数求偏导数的结果如下： $$x_k^{(i)}:=x_k^{(i)}-\alpha\left(\sum_{j:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\theta_k^{j}+\lambda x_k^{(i)}\right)$$ $$\theta_k^{(i)}:=\theta_k^{(i)}-\alpha\left(\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}x_k^{(i)}+\lambda \theta_k^{(j)}\right)$$ 注：在协同过滤从算法中，我们通常不使用方差项，如果需要的话，算法会自动学得。协同过滤算法使用步骤如下： 初始 $x^{(1)},x^{(1)},...x^{(nm)},\ \theta^{(1)},\theta^{(2)},...,\theta^{(n_u)}$为一些随机小值 使用梯度下降算法最小化代价函数 在训练完算法后，我们预测$(\theta^{(j)})^Tx^{(i)}$为用户 $j$ 给电影 $i$ 的评分 通过这个学习过程获得的特征矩阵包含了有关电影的重要数据，这些数据不总是人能读懂的，但是我们可以用这些数据作为给用户推荐电影的依据。 例如，如果一位用户正在观看电影 $x^{(i)}$，我们可以寻找另一部电影$x^{(j)}$，依据两部电影的特征向量之间的距离$\left\| { {x}^{(i)} }-{ {x}^{(j)} } \right\|$的大小。 协同过滤算法参考视频: 16 - 4 - Collaborative Filtering Algorithm (9 min).mkv 协同过滤优化目标： 给定$x^{(1)},...,x^{(n_m)}$，估计$\theta^{(1)},...,\theta^{(n_u)}$：$$\min_{\theta^{(1)},…,\theta^{(n_u)} }\frac{1}{2}\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2$$ 给定$\theta^{(1)},...,\theta^{(n_u)}$，估计$x^{(1)},...,x^{(n_m)}$： 同时最小化$x^{(1)},...,x^{(n_m)}$和$\theta^{(1)},...,\theta^{(n_u)}$：$$J(x^{(1)},…,x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)})=\frac{1}{2}\sum_{(i,j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}(x_k^{(i)})^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2$$ $$\min_{x^{(1)},…,x^{(n_m)} \\ \theta^{(1)},…,\theta^{(n_u)} }J(x^{(1)},…,x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)})$$ 向量化：低秩矩阵分解参考视频: 16 - 5 - Vectorization_ Low Rank Matrix Factorization (8 min).mkv 在上几节视频中，我们谈到了协同过滤算法，本节视频中我将会讲到有关该算法的向量化实现，以及说说有关该算法你可以做的其他事情。 举例子： 当给出一件产品时，你能否找到与之相关的其它产品。 一位用户最近看上一件产品，有没有其它相关的产品，你可以推荐给他。 我将要做的是：实现一种选择的方法，写出协同过滤算法的预测情况。 我们有关于五部电影的数据集，我将要做的是，将这些用户的电影评分，进行分组并存到一个矩阵中。 我们有五部电影，以及四位用户，那么 这个矩阵 $Y$ 就是一个5行4列的矩阵，它将这些电影的用户评分数据都存在矩阵里： Movie Alice (1) Bob (2) Carol (3) Dave (4) Love at last 5 5 0 0 Romance forever 5 ? ? 0 Cute puppies of love ? 4 0 ? Nonstop car chases 0 0 5 4 Swords vs. karate 0 0 5 ? 推出评分： 找到相关影片： 现在既然你已经对特征参数向量进行了学习，那么我们就会有一个很方便的方法来度量两部电影之间的相似性。例如说：电影 $i$ 有一个特征向量$x^{(i)}$，你是否能找到一部不同的电影 $j$，保证两部电影的特征向量之间的距离$x^{(i)}$和$x^{(j)}$很小，那就能很有力地表明电影$i$和电影 $j$ 在某种程度上有相似，至少在某种意义上，某些人喜欢电影 $i$，或许更有可能也对电影 $j$ 感兴趣。总结一下，当用户在看某部电影 $i$ 的时候，如果你想找5部与电影非常相似的电影，为了能给用户推荐5部新电影，你需要做的是找出电影 $j$，在这些不同的电影中与我们要找的电影 $i$ 的距离最小，这样你就能给你的用户推荐几部不同的电影了。 通过这个方法，希望你能知道，如何进行一个向量化的计算来对所有的用户和所有的电影进行评分计算。同时希望你也能掌握，通过学习特征参数，来找到相关电影和产品的方法。 推行工作上的细节：均值归一化参考视频: 16 - 6 - Implementational Detail_ Mean Normalization (9 min).mkv 让我们来看下面的用户评分数据： 如果我们新增一个用户 Eve，并且 Eve 没有为任何电影评分，那么我们以什么为依据为Eve推荐电影呢？ 我们首先需要对结果 $Y $矩阵进行均值归一化处理，将每一个用户对某一部电影的评分减去所有用户对该电影评分的平均值： 然后我们利用这个新的 $Y$ 矩阵来训练算法。如果我们要用新训练出的算法来预测评分，则需要将平均值重新加回去，预测$(\theta^{(j)})^T x^{(i)}+\mu_i$，对于Eve，我们的新模型会认为她给每部电影的评分都是该电影的平均分。 大规模机器学习(Large Scale Machine Learning)大型数据集的学习参考视频: 17 - 1 - Learning With Large Datasets (6 min).mkv 如果我们有一个低方差的模型，增加数据集的规模可以帮助你获得更好的结果。我们应该怎样应对一个有100万条记录的训练集？ 以线性回归模型为例，每一次梯度下降迭代，我们都需要计算训练集的误差的平方和，如果我们的学习算法需要有20次迭代，这便已经是非常大的计算代价。 首先应该做的事是去检查一个这么大规模的训练集是否真的必要，也许我们只用1000个训练集也能获得较好的效果，我们可以绘制学习曲线来帮助判断。 随机梯度下降法参考视频: 17 - 2 - Stochastic Gradient Descent (13 min).mkv 如果我们一定需要一个大规模的训练集，我们可以尝试使用随机梯度下降法来代替批量梯度下降法。 在随机梯度下降法中，我们定义代价函数为一个单一训练实例的代价： ​ $$cost\left( \theta, \left( {x}^{(i)} , {y}^{(i)} \right) \right) = \frac{1}{2}\left( {h}_{\theta}\left({x}^{(i)}\right)-{y}^{ {(i)} } \right)^{2}$$ 随机梯度下降算法为：首先对训练集随机“洗牌”，然后：Repeat (usually anywhere between1-10){ for $i = 1:m${ ​ $\theta:={\theta}_{j}-\alpha\left( {h}_{\theta}\left({x}^{(i)}\right)-{y}^{(i)} \right){ {x}_{j} }^{(i)}$ ​ (for $j=0:n$) ​ }} 随机梯度下降算法在每一次计算之后便更新参数 ${ {\theta } }$ ，而不需要首先将所有的训练集求和，在梯度下降算法还没有完成一次迭代时，随机梯度下降算法便已经走出了很远。但是这样的算法存在的问题是，不是每一步都是朝着”正确”的方向迈出的。因此算法虽然会逐渐走向全局最小值的位置，但是可能无法站到那个最小值的那一点，而是在最小值点附近徘徊。 小批量梯度下降参考视频: 17 - 3 - Mini-Batch Gradient Descent (6 min).mkv 小批量梯度下降算法是介于批量梯度下降算法和随机梯度下降算法之间的算法，每计算常数$b$次训练实例，便更新一次参数 ${ {\theta } }$ 。Repeat { for $i = 1:m${ ​ $\theta:={\theta}_{j}-\alpha\frac{1}{b}\sum_\limits{k=i}^{i+b-1}\left( {h}_{\theta}\left({x}^{(k)}\right)-{y}^{(k)} \right){ {x}_{j} }^{(k)}$ ​ (for $j=0:n$) ​ $ i +=10 $ ​ }} 通常我们会令 $b$ 在 2-100 之间。这样做的好处在于，我们可以用向量化的方式来循环 $b$个训练实例，如果我们用的线性代数函数库比较好，能够支持平行处理，那么算法的总体表现将不受影响（与随机梯度下降相同）。 随机梯度下降收敛参考视频: 17 - 4 - Stochastic Gradient Descent Convergence (12 min). mkv 现在我们介绍随机梯度下降算法的调试，以及学习率 $α$ 的选取。 在批量梯度下降中，我们可以令代价函数$J$为迭代次数的函数，绘制图表，根据图表来判断梯度下降是否收敛。但是，在大规模的训练集的情况下，这是不现实的，因为计算代价太大了。 在随机梯度下降中，我们在每一次更新 ${ {\theta } }$ 之前都计算一次代价，然后每$x$次迭代后，求出这$x$次对训练实例计算代价的平均值，然后绘制这些平均值与$x$次迭代的次数之间的函数图表。 当我们绘制这样的图表时，可能会得到一个颠簸不平但是不会明显减少的函数图像（如上面左下图中蓝线所示）。我们可以增加$α$来使得函数更加平缓，也许便能看出下降的趋势了（如上面左下图中红线所示）；或者可能函数图表仍然是颠簸不平且不下降的（如洋红色线所示），那么我们的模型本身可能存在一些错误。 如果我们得到的曲线如上面右下方所示，不断地上升，那么我们可能会需要选择一个较小的学习率$α$。 我们也可以令学习率随着迭代次数的增加而减小，例如令： ​ $$\alpha = \frac{const1}{iterationNumber + const2}$$ 随着我们不断地靠近全局最小值，通过减小学习率，我们迫使算法收敛而非在最小值附近徘徊。但是通常我们不需要这样做便能有非常好的效果了，对$α$进行调整所耗费的计算通常不值得 总结下，这段视频中，我们介绍了一种方法，近似地监测出随机梯度下降算法在最优化代价函数中的表现，这种方法不需要定时地扫描整个训练集，来算出整个样本集的代价函数，而是只需要每次对最后1000个，或者多少个样本，求一下平均值。应用这种方法，你既可以保证随机梯度下降法正在正常运转和收敛，也可以用它来调整学习速率$α$的大小。 在线学习参考视频: 17 - 5 - Online Learning (13 min).mkv 在这个视频中，讨论一种新的大规模的机器学习机制，叫做在线学习机制。在线学习机制让我们可以模型化问题。 今天，许多大型网站或者许多大型网络公司，使用不同版本的在线学习机制算法，从大批的涌入又离开网站的用户身上进行学习。特别要提及的是，如果你有一个由连续的用户流引发的连续的数据流，进入你的网站，你能做的是使用一个在线学习机制，从数据流中学习用户的偏好，然后使用这些信息来优化一些关于网站的决策。 假定你有一个提供运输服务的公司，用户们来向你询问把包裹从A地运到B地的服务，同时假定你有一个网站，让用户们可多次登陆，然后他们告诉你，他们想从哪里寄出包裹，以及包裹要寄到哪里去，也就是出发地与目的地，然后你的网站开出运输包裹的的服务价格。比如，我会收取&lt;!–￼780–&gt;20之类的，然后根据你开给用户的这个价格，用户有时会接受这个运输服务，那么这就是个正样本，有时他们会走掉，然后他们拒绝购买你的运输服务，所以，让我们假定我们想要一个学习算法来帮助我们，优化我们想给用户开出的价格。 一个算法来从中学习的时候来模型化问题在线学习算法指的是对数据流而非离线的静态数据集的学习。许多在线网站都有持续不断的用户流，对于每一个用户，网站希望能在不将数据存储到数据库中便顺利地进行算法学习。 假使我们正在经营一家物流公司，每当一个用户询问从地点A至地点B的快递费用时，我们给用户一个报价，该用户可能选择接受（$y=1$）或不接受（$y=0$）。 现在，我们希望构建一个模型，来预测用户接受报价使用我们的物流服务的可能性。因此报价是我们的一个特征，其他特征为距离，起始地点，目标地点以及特定的用户数据。模型的输出是:$p(y=1)$。 在线学习的算法与随机梯度下降算法有些类似，我们对单一的实例进行学习，而非对一个提前定义的训练集进行循环。Repeat forever (as long as the website is running) {Get $\left(x,y\right)$ corresponding to the current user​ $\theta:={\theta}_{j}-\alpha\left( {h}_{\theta}\left({x}\right)-{y} \right){ {x}_{j} }$​ (for $j=0:n$)} 一旦对一个数据的学习完成了，我们便可以丢弃该数据，不需要再存储它了。这种方式的好处在于，我们的算法可以很好的适应用户的倾向性，算法可以针对用户的当前行为不断地更新模型以适应该用户。 每次交互事件并不只产生一个数据集，例如，我们一次给用户提供3个物流选项，用户选择2项，我们实际上可以获得3个新的训练实例，因而我们的算法可以一次从3个实例中学习并更新模型。 这些问题中的任何一个都可以被归类到标准的，拥有一个固定的样本集的机器学习问题中。或许，你可以运行一个你自己的网站，尝试运行几天，然后保存一个数据集，一个固定的数据集，然后对其运行一个学习算法。但是这些是实际的问题，在这些问题里，你会看到大公司会获取如此多的数据，真的没有必要来保存一个固定的数据集，取而代之的是你可以使用一个在线学习算法来连续的学习，从这些用户不断产生的数据中来学习。这就是在线学习机制，然后就像我们所看到的，我们所使用的这个算法与随机梯度下降算法非常类似，唯一的区别的是，我们不会使用一个固定的数据集，我们会做的是获取一个用户样本，从那个样本中学习，然后丢弃那个样本并继续下去，而且如果你对某一种应用有一个连续的数据流，这样的算法可能会非常值得考虑。当然，在线学习的一个优点就是，如果你有一个变化的用户群，又或者你在尝试预测的事情，在缓慢变化，就像你的用户的品味在缓慢变化，这个在线学习算法，可以慢慢地调试你所学习到的假设，将其调节更新到最新的用户行为。 映射化简和数据并行参考视频: 17 - 6 - Map Reduce and Data Parallelism (14 min).mkv 映射化简和数据并行对于大规模机器学习问题而言是非常重要的概念。之前提到，如果我们用批量梯度下降算法来求解大规模数据集的最优解，我们需要对整个训练集进行循环，计算偏导数和代价，再求和，计算代价非常大。如果我们能够将我们的数据集分配给不多台计算机，让每一台计算机处理数据集的一个子集，然后我们将计所的结果汇总在求和。这样的方法叫做映射简化。 具体而言，如果任何学习算法能够表达为，对训练集的函数的求和，那么便能将这个任务分配给多台计算机（或者同一台计算机的不同CPU 核心），以达到加速处理的目的。 例如，我们有400个训练实例，我们可以将批量梯度下降的求和任务分配给4台计算机进行处理： 很多高级的线性代数函数库已经能够利用多核CPU的多个核心来并行地处理矩阵运算，这也是算法的向量化实现如此重要的缘故（比调用循环快）。 应用实例：图片文字识别(Application Example: Photo OCR)问题描述和流程图参考视频: 18 - 1 - Problem Description and Pipeline (7 min).mkv 图像文字识别应用所作的事是，从一张给定的图片中识别文字。这比从一份扫描文档中识别文字要复杂的多。 为了完成这样的工作，需要采取如下步骤： 文字侦测（Text detection）——将图片上的文字与其他环境对象分离开来 字符切分（Character segmentation）——将文字分割成一个个单一的字符 字符分类（Character classification）——确定每一个字符是什么可以用任务流程图来表达这个问题，每一项任务可以由一个单独的小队来负责解决： 滑动窗口参考视频: 18 - 2 - Sliding Windows (15 min).mkv 滑动窗口是一项用来从图像中抽取对象的技术。假使我们需要在一张图片中识别行人，首先要做的是用许多固定尺寸的图片来训练一个能够准确识别行人的模型。然后我们用之前训练识别行人的模型时所采用的图片尺寸在我们要进行行人识别的图片上进行剪裁，然后将剪裁得到的切片交给模型，让模型判断是否为行人，然后在图片上滑动剪裁区域重新进行剪裁，将新剪裁的切片也交给模型进行判断，如此循环直至将图片全部检测完。 一旦完成后，我们按比例放大剪裁的区域，再以新的尺寸对图片进行剪裁，将新剪裁的切片按比例缩小至模型所采纳的尺寸，交给模型进行判断，如此循环。 滑动窗口技术也被用于文字识别，首先训练模型能够区分字符与非字符，然后，运用滑动窗口技术识别字符，一旦完成了字符的识别，我们将识别得出的区域进行一些扩展，然后将重叠的区域进行合并。接着我们以宽高比作为过滤条件，过滤掉高度比宽度更大的区域（认为单词的长度通常比高度要大）。下图中绿色的区域是经过这些步骤后被认为是文字的区域，而红色的区域是被忽略的。 以上便是文字侦测阶段。下一步是训练一个模型来完成将文字分割成一个个字符的任务，需要的训练集由单个字符的图片和两个相连字符之间的图片来训练模型。 模型训练完后，我们仍然是使用滑动窗口技术来进行字符识别。 以上便是字符切分阶段。最后一个阶段是字符分类阶段，利用神经网络、支持向量机或者逻辑回归算法训练一个分类器即可。 获取大量数据和人工数据参考视频: 18 - 3 - Getting Lots of Data and Artificial Data (16 min).mkv 如果我们的模型是低方差的，那么获得更多的数据用于训练模型，是能够有更好的效果的。问题在于，我们怎样获得数据，数据不总是可以直接获得的，我们有可能需要人工地创造一些数据。 以我们的文字识别应用为例，我们可以字体网站下载各种字体，然后利用这些不同的字体配上各种不同的随机背景图片创造出一些用于训练的实例，这让我们能够获得一个无限大的训练集。这是从零开始创造实例。 另一种方法是，利用已有的数据，然后对其进行修改，例如将已有的字符图片进行一些扭曲、旋转、模糊处理。只要我们认为实际数据有可能和经过这样处理后的数据类似，我们便可以用这样的方法来创造大量的数据。 有关获得更多数据的几种方法： 人工数据合成 手动收集、标记数据 众包 上限分析：哪部分管道的接下去做参考视频: 18 - 4 - Ceiling Analysis_ What Part of the Pipeline to Work on Next(14 min).mkv 在机器学习的应用中，我们通常需要通过几个步骤才能进行最终的预测，我们如何能够知道哪一部分最值得我们花时间和精力去改善呢？这个问题可以通过上限分析来回答。 回到我们的文字识别应用中，我们的流程图如下： 流程图中每一部分的输出都是下一部分的输入，上限分析中，我们选取一部分，手工提供100%正确的输出结果，然后看应用的整体效果提升了多少。假使我们的例子中总体效果为72%的正确率。 如果我们令文字侦测部分输出的结果100%正确，发现系统的总体效果从72%提高到了89%。这意味着我们很可能会希望投入时间精力来提高我们的文字侦测部分。 接着我们手动选择数据，让字符切分输出的结果100%正确，发现系统的总体效果只提升了1%，这意味着，我们的字符切分部分可能已经足够好了。 最后我们手工选择数据，让字符分类输出的结果100%正确，系统的总体效果又提升了10%，这意味着我们可能也会应该投入更多的时间和精力来提高应用的总体表现。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达机器学习笔记(1-5周)]]></title>
    <url>%2F2019%2F12%2F04%2Fnew_%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(1-5%E5%91%A8)%2F</url>
    <content type="text"><![CDATA[吴恩达机器学习笔记 引言监督学习参考视频: 1 - 3 - Supervised Learning (12 min).mkv我们用一个例子介绍什么是监督学习把正式的定义放在后面介绍。假如说你想预测房价。 前阵子，一个学生从波特兰俄勒冈州的研究所收集了一些房价的数据。你把这些数据画出来，看起来是这个样子：横轴表示房子的面积，单位是平方英尺，纵轴表示房价，单位是千美元。那基于这组数据，假如你有一个朋友，他有一套750平方英尺房子，现在他希望把房子卖掉，他想知道这房子能卖多少钱。 那么关于这个问题，机器学习算法将会怎么帮助你呢？ 我们应用学习算法，可以在这组数据中画一条直线，或者换句话说，拟合一条直线，根据这条线我们可以推测出，这套房子可能卖$150,000，当然这不是唯一的算法。可能还有更好的，比如我们不用直线拟合这些数据，用二次方程去拟合可能效果会更好。根据二次方程的曲线，我们可以从这个点推测出，这套房子能卖接近$200,000。稍后我们将讨论如何选择学习算法，如何决定用直线还是二次方程来拟合。两个方案中有一个能让你朋友的房子出售得更合理。这些都是学习算法里面很好的例子。以上就是监督学习的例子。 可以看出，监督学习指的就是我们给学习算法一个数据集。这个数据集由“正确答案”组成。在房价的例子中，我们给了一系列房子的数据，我们给定数据集中每个样本的正确价格，即它们实际的售价然后运用学习算法，算出更多的正确答案。比如你朋友那个新房子的价格。用术语来讲，这叫做回归问题。我们试着推测出一个连续值的结果，即房子的价格。 一般房子的价格会记到美分，所以房价实际上是一系列离散的值，但是我们通常又把房价看成实数，看成是标量，所以又把它看成一个连续的数值。 回归这个词的意思是，我们在试着推测出这一系列连续值属性。 我再举另外一个监督学习的例子。我和一些朋友之前研究过这个。假设说你想通过查看病历来推测乳腺癌良性与否，假如有人检测出乳腺肿瘤，恶性肿瘤有害并且十分危险，而良性的肿瘤危害就没那么大，所以人们显然会很在意这个问题。 让我们来看一组数据：这个数据集中，横轴表示肿瘤的大小，纵轴上，我标出1和0表示是或者不是恶性肿瘤。我们之前见过的肿瘤，如果是恶性则记为1，不是恶性，或者说良性记为0。 我有5个良性肿瘤样本，在1的位置有5个恶性肿瘤样本。现在我们有一个朋友很不幸检查出乳腺肿瘤。假设说她的肿瘤大概这么大，那么机器学习的问题就在于，你能否估算出肿瘤是恶性的或是良性的概率。用术语来讲，这是一个分类问题。 分类指的是，我们试着推测出离散的输出值：0或1良性或恶性，而事实上在分类问题中，输出可能不止两个值。比如说可能有三种乳腺癌，所以你希望预测离散输出0、1、2、3。0 代表良性，1 表示第1类乳腺癌，2表示第2类癌症，3表示第3类，但这也是分类问题。 因为这几个离散的输出分别对应良性，第一类第二类或者第三类癌症，在分类问题中我们可以用另一种方式绘制这些数据点。 现在我用不同的符号来表示这些数据。既然我们把肿瘤的尺寸看做区分恶性或良性的特征，那么我可以这么画，我用不同的符号来表示良性和恶性肿瘤。或者说是负样本和正样本现在我们不全部画X，良性的肿瘤改成用 O 表示，恶性的继续用 X 表示。来预测肿瘤的恶性与否。 在其它一些机器学习问题中，可能会遇到不止一种特征。举个例子，我们不仅知道肿瘤的尺寸，还知道对应患者的年龄。在其他机器学习问题中，我们通常有更多的特征，我朋友研究这个问题时，通常采用这些特征，比如肿块密度，肿瘤细胞尺寸的一致性和形状的一致性等等，还有一些其他的特征。这就是我们即将学到最有趣的学习算法之一。 那种算法不仅能处理2种3种或5种特征，即使有无限多种特征都可以处理。 上图中，我列举了总共5种不同的特征，坐标轴上的两种和右边的3种，但是在一些学习问题中，你希望不只用3种或5种特征。相反，你想用无限多种特征，好让你的算法可以利用大量的特征，或者说线索来做推测。那你怎么处理无限多个特征，甚至怎么存储这些特征都存在问题，你电脑的内存肯定不够用。我们以后会讲一个算法，叫支持向量机，里面有一个巧妙的数学技巧，能让计算机处理无限多个特征。 想象一下，我没有写下这两种和右边的三种特征，而是在一个无限长的列表里面，一直写一直写不停的写，写下无限多个特征，事实上，我们能用算法来处理它们。 现在来回顾一下，这节课我们介绍了监督学习。其基本思想是，我们数据集中的每个样本都有相应的“正确答案”。再根据这些样本作出预测，就像房子和肿瘤的例子中做的那样。我们还介绍了回归问题，即通过回归来推出一个连续的输出，之后我们介绍了分类问题，其目标是推出一组离散的结果。 现在来个小测验：假设你经营着一家公司，你想开发学习算法来处理这两个问题： 你有一大批同样的货物，想象一下，你有上千件一模一样的货物等待出售，这时你想预测接下来的三个月能卖多少件？ 你有许多客户，这时你想写一个软件来检验每一个用户的账户。对于每一个账户，你要判断它们是否曾经被盗过？ 那这两个问题，它们属于分类问题、还是回归问题? 问题一是一个回归问题，因为你知道，如果我有数千件货物，我会把它看成一个实数，一个连续的值。因此卖出的物品数，也是一个连续的值。 问题二是一个分类问题，因为我会把预测的值，用 0 来表示账户未被盗，用 1 表示账户曾经被盗过。所以我们根据账号是否被盗过，把它们定为0 或 1，然后用算法推测一个账号是 0 还是 1，因为只有少数的离散值，所以我把它归为分类问题。 以上就是监督学习的内容。 无监督学习参考视频: 1 - 4 - Unsupervised Learning (14 min).mkv 上个视频中，已经介绍了监督学习。回想当时的数据集，如图表所示，这个数据集中每条数据都已经标明是阴性或阳性，即是良性或恶性肿瘤。所以，对于监督学习里的每条数据，我们已经清楚地知道，训练集对应的正确答案，是良性或恶性了。 在无监督学习中，我们已知的数据。看上去有点不一样，不同于监督学习的数据的样子，即无监督学习中没有任何的标签或者是有相同的标签或者就是没标签。所以我们已知数据集，却不知如何处理，也未告知每个数据点是什么。别的都不知道，就是一个数据集。你能从数据中找到某种结构吗？针对数据集，无监督学习就能判断出数据有两个不同的聚集簇。这是一个，那是另一个，二者不同。是的，无监督学习算法可能会把这些数据分成两个不同的簇。所以叫做聚类算法。事实证明，它能被用在很多地方。 聚类应用的一个例子就是在谷歌新闻中。如果你以前从来没见过它，你可以到这个URL网址news.google.com去看看。谷歌新闻每天都在，收集非常多，非常多的网络的新闻内容。它再将这些新闻分组，组成有关联的新闻。所以谷歌新闻做的就是搜索非常多的新闻事件，自动地把它们聚类到一起。所以，这些新闻事件全是同一主题的，所以显示到一起。 事实证明，聚类算法和无监督学习算法同样还用在很多其它的问题上。 其中就有基因学的理解应用。一个DNA微观数据的例子。基本思想是输入一组不同个体，对其中的每个个体，你要分析出它们是否有一个特定的基因。技术上，你要分析多少特定基因已经表达。所以这些颜色，红，绿，灰等等颜色，这些颜色展示了相应的程度，即不同的个体是否有着一个特定的基因。你能做的就是运行一个聚类算法，把个体聚类到不同的类或不同类型的组（人）…… 所以这个就是无监督学习，因为我们没有提前告知算法一些信息，比如，这是第一类的人，那些是第二类的人，还有第三类，等等。我们只是说，是的，这是有一堆数据。我不知道数据里面有什么。我不知道谁是什么类型。我甚至不知道人们有哪些不同的类型，这些类型又是什么。但你能自动地找到数据中的结构吗？就是说你要自动地聚类那些个体到各个类，我没法提前知道哪些是哪些。因为我们没有给算法正确答案来回应数据集中的数据，所以这就是无监督学习。 无监督学习或聚集有着大量的应用。它用于组织大型计算机集群。我有些朋友在大数据中心工作，那里有大型的计算机集群，他们想解决什么样的机器易于协同地工作，如果你能够让那些机器协同工作，你就能让你的数据中心工作得更高效。第二种应用就是社交网络的分析。所以已知你朋友的信息，比如你经常发email的，或是你Facebook的朋友、谷歌+ 圈子的朋友，我们能否自动地给出朋友的分组呢？即每组里的人们彼此都熟识，认识组里的所有人？还有市场分割。许多公司有大型的数据库，存储消费者信息。所以，你能检索这些顾客数据集，自动地发现市场分类，并自动地把顾客划分到不同的细分市场中，你才能自动并更有效地销售或不同的细分市场一起进行销售。这也是无监督学习，因为我们拥有所有的顾客数据，但我们没有提前知道是什么的细分市场，以及分别有哪些我们数据集中的顾客。我们不知道谁是在一号细分市场，谁在二号市场，等等。那我们就必须让算法从数据中发现这一切。最后，无监督学习也可用于天文数据分析，这些聚类算法给出了令人惊讶、有趣、有用的理论，解释了星系是如何诞生的。这些都是聚类的例子，聚类只是无监督学习中的一种。 我现在告诉你们另一种。我先来介绍鸡尾酒宴问题。嗯，你参加过鸡尾酒宴吧？你可以想像下，有个宴会房间里满是人，全部坐着，都在聊天，这么多人同时在聊天，声音彼此重叠，因为每个人都在说话，同一时间都在说话，你几乎听不到你面前那人的声音。所以，可能在一个这样的鸡尾酒宴中的两个人，他俩同时都在说话，假设现在是在个有些小的鸡尾酒宴中。我们放两个麦克风在房间中，因为这些麦克风在两个地方，离说话人的距离不同每个麦克风记录下不同的声音，虽然是同样的两个说话人。听起来像是两份录音被叠加到一起，或是被归结到一起，产生了我们现在的这些录音。另外，这个算法还会区分出两个音频资源，这两个可以合成或合并成之前的录音，实际上，鸡尾酒算法的第一个输出结果是： 1，2，3，4，5，6，7，8，9，10, 所以，已经把英语的声音从录音中分离出来了。 第二个输出是这样： 1，2，3，4，5，6，7，8，9，10。 看看这个无监督学习算法，实现这个得要多么的复杂，是吧？它似乎是这样，为了构建这个应用，完成这个音频处理似乎需要你去写大量的代码或链接到一堆的合成器JAVA库，处理音频的库，看上去绝对是个复杂的程序，去完成这个从音频中分离出音频。事实上，这个算法对应你刚才知道的那个问题的算法可以就用一行代码来完成。 就是这里展示的代码：[W,s,v] = svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x&#39;); 研究人员花费了大量时间才最终实现这行代码。我不是说这个是简单的问题，但它证明了，当你使用正确的编程环境，许多学习算法是相当短的程序。所以，这也是为什么在本课中，我们打算使用Octave编程环境。Octave,是免费的开源软件，使用一个像Octave或Matlab的工具，许多学习算法变得只有几行代码就可实现。 后面，我会教你们一点关于如何使用Octave的知识，你就可以用Octave来实现一些算法了。或者，如果你有Matlab（盗版？），你也可以用Matlab。事实上，在硅谷里，对大量机器学习算法，我们第一步就是建原型，在Octave建软件原型，因为软件在Octave中可以令人难以置信地、快速地实现这些学习算法。这里的这些函数比如SVM（支持向量机）函数，奇异值分解，Octave里已经建好了。如果你试图完成这个工作，但借助C++或JAVA的话，你会需要很多很多行的代码，并链接复杂的C++或Java库。所以，你可以实现这些算法，借助C++或Java或Python，它只是用这些语言来实现会更加复杂。(编者注：这个是当时的情况，现在Python变主流了) 我已经见到，在我教机器学习将近十年后的现在，发现，学习可以更加高速，如果使用Octave作为编程环境，如果使用Octave作为学习工具，以及作为原型工具，它会让你对学习算法的学习和建原型快上许多。 事实上，许多人在大硅谷的公司里做的其实就是，使用一种工具像Octave来做第一步的学习算法的原型搭建，只有在你已经让它工作后，你才移植它到C++ 或Java或别的语言。事实证明，这样做通常可以让你的算法运行得比直接用C++ 实现更快，所以，我知道，作为一名指导者，我必须说“相信我”，但对你们中从未使用过Octave这种编程环境的人，我还是要告诉你们这一点一定要相信我，我想，对你们而言，我认为你们的时间，你们的开发时间是最有价值的资源。我已经见过很多人这样做了，我把你看作是机器学习研究员，或机器学习开发人员，想更加高产的话，你要学会使用这个原型工具，开始使用Octave。 我们介绍了无监督学习，它是学习策略，交给算法大量的数据，并让算法为我们从数据中找出某种结构。 好的，希望你们还记得垃圾邮件问题。如果你有标记好的数据，区别好是垃圾还是非垃圾邮件，我们把这个当作监督学习问题。 新闻事件分类的例子，就是那个谷歌新闻的例子，我们在本视频中有见到了，我们看到，可以用一个聚类算法来聚类这些文章到一起，所以是无监督学习。 细分市场的例子，我在更早一点的时间讲过，你可以当作无监督学习问题，因为我只是拿到算法数据，再让算法去自动地发现细分市场。 最后一个例子，糖尿病，这个其实就像是我们的乳腺癌，上个视频里的。只是替换了好、坏肿瘤，良性、恶性肿瘤，我们改用糖尿病或没病。所以我们把这个当作监督学习，我们能够解决它，作为一个监督学习问题，就像我们在乳腺癌数据中做的一样。 单变量线性回归(Linear Regression with One Variable)模型表示参考视频: 2 - 1 - Model Representation (8 min).mkv 让我们通过一个例子来开始：这个例子是预测住房价格的，我们要使用一个数据集，数据集包含俄勒冈州波特兰市的住房价格。在这里，我要根据不同房屋尺寸所售出的价格，画出我的数据集。比方说，如果你朋友的房子是1250平方尺大小，你要告诉他们这房子能卖多少钱。那么，你可以做的一件事就是构建一个模型，也许是条直线，从这个数据模型上来看，也许你可以告诉你的朋友，他能以大约220000(美元)左右的价格卖掉这个房子。这就是监督学习算法的一个例子。 它被称作监督学习是因为对于每个数据来说，我们给出了“正确的答案”，即告诉我们：根据我们的数据来说，房子实际的价格是多少，而且，更具体来说，这是一个回归问题。回归一词指的是，我们根据之前的数据预测出一个准确的输出值，对于这个例子就是价格，同时，还有另一种最常见的监督学习方式，叫做分类问题，当我们想要预测离散的输出值，例如，我们正在寻找癌症肿瘤，并想要确定肿瘤是良性的还是恶性的，这就是0/1离散输出的问题。更进一步来说，在监督学习中我们有一个数据集，这个数据集被称训练集。 我将在整个课程中用小写的m来表示训练样本的数目。 以之前的房屋交易问题为例，假使我们回归问题的训练集（Training Set）如下表所示： 我们将要用来描述这个回归问题的标记如下: $m$ 代表训练集中实例的数量 $x$ 代表特征/输入变量 $y$ 代表目标变量/输出变量 $\left( x,y \right)$ 代表训练集中的实例 $({ {x}^{(i)} },{ {y}^{(i)} })$ 代表第$i$ 个观察实例 $h$ 代表学习算法的解决方案或函数也称为假设（**hypothesis**） 这就是一个监督学习算法的工作方式，我们可以看到这里有我们的训练集里房屋价格我们把它喂给我们的学习算法，学习算法的工作了，然后输出一个函数，通常表示为小写 $h$ 表示。$h$ 代表hypothesis(假设)，$h$表示一个函数，输入是房屋尺寸大小，就像你朋友想出售的房屋，因此 $h$ 根据输入的 $x$值来得出 $y$ 值，$y$ 值对应房子的价格 因此，$h$ 是一个从$x$ 到 $y$ 的函数映射。 我将选择最初的使用规则$h$代表hypothesis，因而，要解决房价预测问题，我们实际上是要将训练集“喂”给我们的学习算法，进而学习得到一个假设$h$，然后将我们要预测的房屋的尺寸作为输入变量输入给$h$，预测出该房屋的交易价格作为输出变量输出为结果。那么，对于我们的房价预测问题，我们该如何表达 $h$？ 一种可能的表达方式为：$h_\theta \left( x \right)=\theta_{0} + \theta_{1}x$，因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。 代价函数参考视频: 2 - 2 - Cost Function (8 min).mkv 在线性回归中我们有一个像这样的训练集，$m$代表了训练样本的数量，比如 $m = 47$。而我们的假设函数，也就是用来进行预测的函数，是这样的线性函数形式：$h_\theta \left( x \right)=\theta_{0}+\theta_{1}x$。 接下来我们会引入一些术语我们现在要做的便是为我们的模型选择合适的参数（parameters）$\theta_{0}$ 和 $\theta_{1}$，在房价问题这个例子中便是直线的斜率和在$y$ 轴上的截距。 我们选择的参数决定了我们得到的直线相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是建模误差（modeling error）。 我们的目标便是选择出可以使得建模误差的平方和能够最小的模型参数。 即使得代价函数 $J \left( \theta_0, \theta_1 \right) = \frac{1}{2m}\sum\limits_{i=1}^m \left( h_{\theta}(x^{(i)})-y^{(i)} \right)^{2}$最小。 我们绘制一个等高线图，三个坐标分别为$\theta_{0}$和$\theta_{1}$ 和$J(\theta_{0}, \theta_{1})$： 则可以看出在三维空间中存在一个使得$J(\theta_{0}, \theta_{1})$最小的点。 代价函数也被称作平方误差函数，有时也被称为平方误差代价函数。我们之所以要求出误差的平方和，是因为误差平方代价函数，对于大多数问题，特别是回归问题，都是一个合理的选择。还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回归问题最常用的手段了。 在后续课程中，我们还会谈论其他的代价函数，但我们刚刚讲的选择是对于大多数线性回归问题非常合理的。 也许这个函数$J(\theta_{0}, \theta_{1})$有点抽象，可能你仍然不知道它的内涵，在接下来的几个视频里，我们要更进一步解释代价函数J的工作原理，并尝试更直观地解释它在计算什么，以及我们使用它的目的。 代价函数的直观理解参考视频: 2 - 3 - Cost Function - Intuition I (11 min).mkv在上一个视频中，我们给了代价函数一个数学上的定义。在这个视频里，让我们通过一些例子来获取一些直观的感受，看看代价函数到底是在干什么。 代价函数的样子，等高线图，则可以看出在三维空间中存在一个使得$J(\theta_{0}, \theta_{1})$最小的点。 通过这些图形，我希望你能更好地理解这些代价函数$ J$所表达的值是什么样的，它们对应的假设是什么样的，以及什么样的假设对应的点，更接近于代价函数$J$的最小值。 当然，我们真正需要的是一种有效的算法，能够自动地找出这些使代价函数$J$取最小值的参数$\theta_{0}$和$\theta_{1}$来。 我们也不希望编个程序把这些点画出来，然后人工的方法来读出这些点的数值，这很明显不是一个好办法。我们会遇到更复杂、更高维度、更多参数的情况，而这些情况是很难画出图的，因此更无法将其可视化，因此我们真正需要的是编写程序来找出这些最小化代价函数的$\theta_{0}$和$\theta_{1}$的值，在下一节视频中，我们将介绍一种算法，能够自动地找出能使代价函数$J$最小化的参数$\theta_{0}$和$\theta_{1}$的值。 梯度下降参考视频: 2 - 5 - Gradient Descent (11 min).mkv梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数$J(\theta_{0}, \theta_{1})$ 的最小值。 梯度下降背后的思想是：开始时我们随机选择一个参数的组合$\left( {\theta_{0} },{\theta_{1} },......,{\theta_{n} } \right)$，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到找到一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（global minimum），选择不同的初始参数组合，可能会找到不同的局部最小值。 想象一下你正站立在山的这一点上，站立在你想象的公园这座红色山上，在梯度下降算法中，我们要做的就是旋转360度，看看我们的周围，并问自己要在某个方向上，用小碎步尽快下山。这些小碎步需要朝什么方向？如果我们站在山坡上的这一点，你看一下周围，你会发现最佳的下山方向，你再看看周围，然后再一次想想，我应该从什么方向迈着小碎步下山？然后你按照自己的判断又迈出一步，重复上面的步骤，从这个新的点，你环顾四周，并决定从什么方向将会最快下山，然后又迈进了一小步，并依此类推，直到你接近局部最低点的位置。 批量梯度下降（batch gradient descent）算法的公式为： 其中$a$是学习率（learning rate），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。 在梯度下降算法中，还有一个更微妙的问题，梯度下降中，我们要更新${\theta_{0} }$和${\theta_{1} }$ ，当 $j=0$ 和$j=1$时，会产生更新，所以你将更新$J\left( {\theta_{0} } \right)$和$J\left( {\theta_{1} } \right)$。实现梯度下降算法的微妙之处是，在这个表达式中，如果你要更新这个等式，你需要同时更新${\theta_{0} }$和${\theta_{1} }$，我的意思是在这个等式中，我们要这样更新： ${\theta_{0} }$:= ${\theta_{0} }$ ，并更新${\theta_{1} }$:= ${\theta_{1} }$。 实现方法是：你应该计算公式右边的部分，通过那一部分计算出${\theta_{0} }$和${\theta_{1} }$的值，然后同时更新${\theta_{0} }$和${\theta_{1} }$。 让我进一步阐述这个过程： 在梯度下降算法中，这是正确实现同时更新的方法。我不打算解释为什么你需要同时更新，同时更新是梯度下降中的一种常用方法。我们之后会讲到，同步更新是更自然的实现方法。当人们谈到梯度下降时，他们的意思就是同步更新。 在接下来的视频中，我们要进入这个微分项的细节之中。我已经写了出来但没有真正定义，如果你已经修过微积分课程，如果你熟悉偏导数和导数，这其实就是这个微分项： $\alpha \frac{\partial }{\partial { {\theta }_{0} }}J({ {\theta }_{0} },{ {\theta }_{1} })$，$\alpha \frac{\partial }{\partial { {\theta }_{1} }}J({ {\theta }_{0} },{ {\theta }_{1} })$。 如果你不熟悉微积分，不用担心，即使你之前没有看过微积分，或者没有接触过偏导数，在接下来的视频中，你会得到一切你需要知道，如何计算这个微分项的知识。 梯度下降的直观理解参考视频: 2 - 6 - Gradient Descent Intuition (12 min).mkv在之前的视频中，我们给出了一个数学上关于梯度下降的定义，本次视频我们更深入研究一下，更直观地感受一下这个算法是做什么的，以及梯度下降算法的更新过程有什么意义。梯度下降算法如下： ${\theta_{j} }:={\theta_{j} }-\alpha \frac{\partial }{\partial {\theta_{j} }}J\left(\theta \right)$ 描述：对$\theta $赋值，使得$J\left( \theta \right)$按梯度下降最快方向进行，一直迭代下去，最终得到局部最小值。其中$a$是学习率（learning rate），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大。 对于这个问题，求导的目的，基本上可以说取这个红点的切线，就是这样一条红色的直线，刚好与函数相切于这一点，让我们看看这条红色直线的斜率，就是这条刚好与函数曲线相切的这条直线，这条直线的斜率正好是这个三角形的高度除以这个水平长度，现在，这条线有一个正斜率，也就是说它有正导数，因此，我得到的新的${\theta_{1} }$，${\theta_{1} }$更新后等于${\theta_{1} }$减去一个正数乘以$a$。 这就是我梯度下降法的更新规则：${\theta_{j} }:={\theta_{j} }-\alpha \frac{\partial }{\partial {\theta_{j} }}J\left( \theta \right)$ 让我们来看看如果$a$太小或$a$太大会出现什么情况： 如果$a$太小了，即我的学习速率太小，结果就是只能这样像小宝宝一样一点点地挪动，去努力接近最低点，这样就需要很多步才能到达最低点，所以如果$a$太小的话，可能会很慢，因为它会一点点挪动，它会需要很多步才能到达全局最低点。 如果$a$太大，那么梯度下降法可能会越过最低点，甚至可能无法收敛，下一次迭代又移动了一大步，越过一次，又越过一次，一次次越过最低点，直到你发现实际上离最低点越来越远，所以，如果$a$太大，它会导致无法收敛，甚至发散。 现在，我还有一个问题，当我第一次学习这个地方时，我花了很长一段时间才理解这个问题，如果我们预先把${\theta_{1} }$放在一个局部的最低点，你认为下一步梯度下降法会怎样工作？ 假设你将${\theta_{1} }$初始化在局部最低点，在这儿，它已经在一个局部的最优处或局部最低点。结果是局部最优点的导数将等于零，因为它是那条切线的斜率。这意味着你已经在局部最优点，它使得${\theta_{1} }$不再改变，也就是新的${\theta_{1} }$等于原来的${\theta_{1} }$，因此，如果你的参数已经处于局部最低点，那么梯度下降法更新其实什么都没做，它不会改变参数的值。这也解释了为什么即使学习速率$a$保持不变时，梯度下降也可以收敛到局部最低点。 我们来看一个例子，这是代价函数$J\left( \theta \right)$。 我想找到它的最小值，首先初始化我的梯度下降算法，在那个品红色的点初始化，如果我更新一步梯度下降，也许它会带我到这个点，因为这个点的导数是相当陡的。现在，在这个绿色的点，如果我再更新一步，你会发现我的导数，也即斜率，是没那么陡的。随着我接近最低点，我的导数越来越接近零，所以，梯度下降一步后，新的导数会变小一点点。然后我想再梯度下降一步，在这个绿点，我自然会用一个稍微跟刚才在那个品红点时比，再小一点的一步，到了新的红色点，更接近全局最低点了，因此这点的导数会比在绿点时更小。所以，我再进行一步梯度下降时，我的导数项是更小的，${\theta_{1} }$更新的幅度就会更小。所以随着梯度下降法的运行，你移动的幅度会自动变得越来越小，直到最终移动幅度非常小，你会发现，已经收敛到局部极小值。 回顾一下，在梯度下降法中，当我们接近局部最低点时，梯度下降法会自动采取更小的幅度，这是因为当我们接近局部最低点时，很显然在局部最低时导数等于零，所以当我们接近局部最低时，导数值会自动变得越来越小，所以梯度下降将自动采取较小的幅度，这就是梯度下降的做法。所以实际上没有必要再另外减小$a$。 这就是梯度下降算法，你可以用它来最小化任何代价函数$J$，不只是线性回归中的代价函数$J$。 在接下来的视频中，我们要用代价函数$J$，回到它的本质，线性回归中的代价函数。也就是我们前面得出的平方误差函数，结合梯度下降法，以及平方代价函数，我们会得出第一个机器学习算法，即线性回归算法。 梯度下降的线性回归参考视频: 2 - 7 - GradientDescentForLinearRegression (6 min).mkv在以前的视频中我们谈到关于梯度下降算法，梯度下降是很常用的算法，它不仅被用在线性回归上和线性回归模型、平方误差代价函数。在这段视频中，我们要将梯度下降和代价函数结合。我们将用到此算法，并将其应用于具体的拟合直线的线性回归算法里。 梯度下降算法和线性回归算法比较如图： 对我们之前的线性回归问题运用梯度下降法，关键在于求出代价函数的导数，即： $h_\theta \left( x \right)=\theta_{0} + \theta_{1}x$ $\frac{\partial }{\partial { {\theta }_{j} }}J({ {\theta }_{0} },{ {\theta }_{1} })=\frac{\partial }{\partial { {\theta }_{j} }}\frac{1}{2m}{ {\sum\limits_{i=1}^{m}{\left( { {h}_{\theta } }({ {x}^{(i)} })-{ {y}^{(i)} } \right)} }^{2} }$ $j=0$ 时：$\frac{\partial }{\partial { {\theta }_{0} }}J({ {\theta }_{0} },{ {\theta }_{1} })=\frac{1}{m}{ {\sum\limits_{i=1}^{m}{\left( { {h}_{\theta } }({ {x}^{(i)} })-{ {y}^{(i)} } \right)} }}$ $j=1$ 时：$\frac{\partial }{\partial { {\theta }_{1} }}J({ {\theta }_{0} },{ {\theta }_{1} })=\frac{1}{m}\sum\limits_{i=1}^{m}{\left( \left( { {h}_{\theta } }({ {x}^{(i)} })-{ {y}^{(i)} } \right)\cdot { {x}^{(i)} } \right)}$ 则算法改写成： Repeat { ​ ${\theta_{0} }:={\theta_{0} }-a\frac{1}{m}\sum\limits_{i=1}^{m}{ \left({ {h}_{\theta } }({ {x}^{(i)} })-{ {y}^{(i)} } \right)}$ ​ ${\theta_{1} }:={\theta_{1} }-a\frac{1}{m}\sum\limits_{i=1}^{m}{\left( \left({ {h}_{\theta } }({ {x}^{(i)} })-{ {y}^{(i)} } \right)\cdot { {x}^{(i)} } \right)}$ ​ } 我们刚刚使用的算法，有时也称为批量梯度下降。实际上，在机器学习中，通常不太会给算法起名字，但这个名字”批量梯度下降”，指的是在梯度下降的每一步中，我们都用到了所有的训练样本，在梯度下降中，在计算微分求导项时，我们需要进行求和运算，所以，在每一个单独的梯度下降中，我们最终都要计算这样一个东西，这个项需要对所有$m$个训练样本求和。因此，批量梯度下降法这个名字说明了我们需要考虑所有这一”批”训练样本，而事实上，有时也有其他类型的梯度下降法，不是这种”批量”型的，不考虑整个的训练集，而是每次只关注训练集中的一些小的子集。在后面的课程中，我们也将介绍这些方法。 但就目前而言，应用刚刚学到的算法，你应该已经掌握了批量梯度算法，并且能把它应用到线性回归中了，这就是用于线性回归的梯度下降法。 如果你之前学过线性代数，有些同学之前可能已经学过高等线性代数，你应该知道有一种计算代价函数$J$最小值的数值解法，不需要梯度下降这种迭代算法。在后面的课程中，我们也会谈到这个方法，它可以在不需要多步梯度下降的情况下，也能解出代价函数$J$的最小值，这是另一种称为正规方程(normal equations)的方法。实际上在数据量较大的情况下，梯度下降法比正规方程要更适用一些。 现在我们已经掌握了梯度下降，我们可以在不同的环境中使用梯度下降法，我们还将在不同的机器学习问题中大量地使用它。所以，祝贺大家成功学会你的第一个机器学习算法。 在下一段视频中，告诉你泛化的梯度下降算法，这将使梯度下降更加强大。 接下来的内容参考视频: 2 - 8 - What_’s Next (6 min).mkv在接下来的一组视频中，我会对线性代数进行一个快速的复习回顾。如果你从来没有接触过向量和矩阵，那么这课件上所有的一切对你来说都是新知识，或者你之前对线性代数有所了解，但由于隔得久了，对其有所遗忘，那就请学习接下来的一组视频，我会快速地回顾你将用到的线性代数知识。 通过它们，你可以实现和使用更强大的线性回归模型。事实上，线性代数不仅仅在线性回归中应用广泛，它其中的矩阵和向量将有助于帮助我们实现之后更多的机器学习模型，并在计算上更有效率。正是因为这些矩阵和向量提供了一种有效的方式来组织大量的数据，特别是当我们处理巨大的训练集时，如果你不熟悉线性代数，如果你觉得线性代数看上去是一个复杂、可怕的概念，特别是对于之前从未接触过它的人，不必担心，事实上，为了实现机器学习算法，我们只需要一些非常非常基础的线性代数知识。通过接下来几个视频，你可以很快地学会所有你需要了解的线性代数知识。具体来说，为了帮助你判断是否有需要学习接下来的一组视频，我会讨论什么是矩阵和向量，谈谈如何加、减 、乘矩阵和向量，讨论逆矩阵和转置矩阵的概念。 如果你十分熟悉这些概念，那么你完全可以跳过这组关于线性代数的选修视频，但是如果你对这些概念仍有些许的不确定，不确定这些数字或这些矩阵的意思，那么请看一看下一组的视频，它会很快地教你一些你需要知道的线性代数的知识，便于之后编写机器学习算法和处理大量数据。 线性代数回顾(Linear Algebra Review)矩阵和向量参考视频: 3 - 1 - Matrices and Vectors (9 min).mkv如图：这个是4×2矩阵，即4行2列，如$m$为行，$n$为列，那么$m×n$即4×2 矩阵的维数即行数×列数 矩阵元素（矩阵项）：$A=\left[ \begin{matrix} 1402 & 191 \\ 1371 & 821 \\ 949 & 1437 \\ 147 & 1448 \\\end{matrix} \right]$ $A_{ij}$指第$i$行，第$j$列的元素。 向量是一种特殊的矩阵，讲义中的向量一般都是列向量，如： $y=\left[ \begin{matrix} {460} \\ {232} \\ {315} \\ {178} \\\end{matrix} \right]$ 为四维列向量（4×1）。 如下图为1索引向量和0索引向量，左图为1索引向量，右图为0索引向量，一般我们用1索引向量。 $y=\left[ \begin{matrix} { {y}_{1} } \\ { {y}_{2} } \\ { {y}_{3} } \\ { {y}_{4} } \\\end{matrix} \right]$，$y=\left[ \begin{matrix} { {y}_{0} } \\ { {y}_{1} } \\ { {y}_{2} } \\ { {y}_{3} } \\\end{matrix} \right]$ 加法和标量乘法参考视频: 3 - 2 - Addition and Scalar Multiplication (7 min).mkv矩阵的加法：行列数相等的可以加。 例： 矩阵的乘法：每个元素都要乘 组合算法也类似。 矩阵向量乘法参考视频: 3 - 3 - Matrix Vector Multiplication (14 min).mkv 矩阵和向量的乘法如图：$m×n$的矩阵乘以$n×1$的向量，得到的是$m×1$的向量 算法举例： 矩阵乘法参考视频: 3 - 4 - Matrix Matrix Multiplication (11 min).mkv矩阵乘法： $m×n$矩阵乘以$n×o$矩阵，变成$m×o$矩阵。 如果这样说不好理解的话就举一个例子来说明一下，比如说现在有两个矩阵$A$和$B$，那么它们的乘积就可以表示为图中所示的形式。 矩阵乘法的性质参考视频: 3 - 5 - Matrix Multiplication Properties (9 min).mkv矩阵乘法的性质： 矩阵的乘法不满足交换律：$A×B≠B×A$ 矩阵的乘法满足结合律。即：$A×(B×C)=(A×B)×C$ 单位矩阵：在矩阵的乘法中，有一种矩阵起着特殊的作用，如同数的乘法中的1,我们称这种矩阵为单位矩阵．它是个方阵，一般用 $I$ 或者 $E$ 表示，本讲义都用 $I$ 代表单位矩阵，从左上角到右下角的对角线（称为主对角线）上的元素均为1以外全都为0。如： $A{ {A}^{-1} }={ {A}^{-1} }A=I$ 对于单位矩阵，有$AI=IA=A$ 逆、转置参考视频: 3 - 6 - Inverse and Transpose (11 min).mkv矩阵的逆：如矩阵$A$是一个$m×m$矩阵（方阵），如果有逆矩阵，则：$A{ {A}^{-1} }={ {A}^{-1} }A=I$ 我们一般在OCTAVE或者MATLAB中进行计算矩阵的逆矩阵。 矩阵的转置：设$A$为$m×n$阶矩阵（即$m$行$n$列），第$i $行$j $列的元素是$a(i,j)$，即：$A=a(i,j)$ 定义$A$的转置为这样一个$n×m$阶矩阵$B$，满足$B=a(j,i)$，即 $b (i,j)=a(j,i)$（$B$的第$i$行第$j$列元素是$A$的第$j$行第$i$列元素），记${ {A}^{T} }=B$。(有些书记为A’=B） 直观来看，将$A$的所有元素绕着一条从第1行第1列元素出发的右下方45度的射线作镜面反转，即得到$A$的转置。 例： ${ {\left| \begin{matrix} a& b \\ c& d \\ e& f \\\end{matrix} \right|}^{T} }=\left|\begin{matrix} a& c & e \\ b& d & f \\\end{matrix} \right|$ 矩阵的转置基本性质: $ { {\left( A\pm B \right)}^{T} }={ {A}^{T} }\pm { {B}^{T} } $ ${ {\left( A\times B \right)}^{T} }={ {B}^{T} }\times { {A}^{T} }$ ${ {\left( { {A}^{T} } \right)}^{T} }=A $ ${ {\left( KA \right)}^{T} }=K{ {A}^{T} } $ matlab中矩阵转置：直接打一撇，x=y&#39;。 多变量线性回归(Linear Regression with Multiple Variables)多维特征参考视频: 4 - 1 - Multiple Features (8 min).mkv目前为止，我们探讨了单变量/特征的回归模型，现在我们对房价模型增加更多的特征，例如房间数楼层等，构成一个含有多个变量的模型，模型中的特征为$\left( {x_{1} },{x_{2} },...,{x_{n} } \right)$。 增添更多特征后，我们引入一系列新的注释： $n$ 代表特征的数量 ${x^{\left( i \right)} }$代表第 $i$ 个训练实例，是特征矩阵中的第$i$行，是一个**向量**（**vector**）。 比方说，上图的 ${x}^{(2)}\text{=}\begin{bmatrix} 1416\\\ 3\\\ 2\\\ 40 \end{bmatrix}$， ${x}_{j}^{\left( i \right)}$代表特征矩阵中第 $i$ 行的第 $j$ 个特征，也就是第 $i$ 个训练实例的第 $j$ 个特征。 如上图的$x_{2}^{\left( 2 \right)}=3,x_{3}^{\left( 2 \right)}=2$， 支持多变量的假设 $h$ 表示为：$h_{\theta}\left( x \right)={\theta_{0} }+{\theta_{1} }{x_{1} }+{\theta_{2} }{x_{2} }+...+{\theta_{n} }{x_{n} }$， 这个公式中有$n+1$个参数和$n$个变量，为了使得公式能够简化一些，引入$x_{0}=1$，则公式转化为：$h_{\theta} \left( x \right)={\theta_{0} }{x_{0} }+{\theta_{1} }{x_{1} }+{\theta_{2} }{x_{2} }+...+{\theta_{n} }{x_{n} }$ 此时模型中的参数是一个$n+1$维的向量，任何一个训练实例也都是$n+1$维的向量，特征矩阵$X$的维度是 $m*(n+1)$。 因此公式可以简化为：$h_{\theta} \left( x \right)={\theta^{T} }X$，其中上标$T$代表矩阵转置。 多变量梯度下降参考视频: 4 - 2 - Gradient Descent for Multiple Variables (5 min).mkv与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价函数是所有建模误差的平方和，即：$J\left( {\theta_{0} },{\theta_{1} }...{\theta_{n} } \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{ {{\left( h_{\theta} \left({x}^{\left( i \right)} \right)-{y}^{\left( i \right)} \right)}^{2} }}$ ， 其中：$h_{\theta}\left( x \right)=\theta^{T}X={\theta_{0} }+{\theta_{1} }{x_{1} }+{\theta_{2} }{x_{2} }+...+{\theta_{n} }{x_{n} }$ ， 我们的目标和单变量线性回归问题中一样，是要找出使得代价函数最小的一系列参数。多变量线性回归的批量梯度下降算法为： 即： 求导数后得到： 当$n>=1$时， ${ {\theta }_{0} }:={ {\theta }_{0} }-a\frac{1}{m}\sum\limits_{i=1}^{m}{({ {h}_{\theta } }({ {x}^{(i)} })-{ {y}^{(i)} })}x_{0}^{(i)}$ ${ {\theta }_{1} }:={ {\theta }_{1} }-a\frac{1}{m}\sum\limits_{i=1}^{m}{({ {h}_{\theta } }({ {x}^{(i)} })-{ {y}^{(i)} })}x_{1}^{(i)}$ ${ {\theta }_{2} }:={ {\theta }_{2} }-a\frac{1}{m}\sum\limits_{i=1}^{m}{({ {h}_{\theta } }({ {x}^{(i)} })-{ {y}^{(i)} })}x_{2}^{(i)}$ 我们开始随机选择一系列的参数值，计算所有的预测结果后，再给所有的参数一个新的值，如此循环直到收敛。 代码示例： 计算代价函数 $J\left( \theta \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{ {{\left( {h_{\theta} }\left( {x^{(i)} } \right)-{y^{(i)} } \right)}^{2} }}$ 其中：${h_{\theta} }\left( x \right)={\theta^{T} }X={\theta_{0} }{x_{0} }+{\theta_{1} }{x_{1} }+{\theta_{2} }{x_{2} }+...+{\theta_{n} }{x_{n} }$ Python 代码： def computeCost(X, y, theta): inner = np.power(((X * theta.T) - y), 2) return np.sum(inner) / (2 * len(X)) 梯度下降法实践1-特征缩放参考视频: 4 - 3 - Gradient Descent in Practice I - Feature Scaling (9 min).mkv 在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。 以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为 0-2000平方英尺，而房间数量的值则是0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。 解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。如图： 最简单的方法是令：${ {x}_{n} }=\frac{ {{x}_{n} }-{ {\mu}_{n} }}{ {{s}_{n} }}$，其中 ${\mu_{n} }$是平均值，${s_{n} }$是标准差。 梯度下降法实践2-学习率参考视频: 4 - 4 - Gradient Descent in Practice II - Learning Rate (9 min).mkv梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，我们不能提前预知，我们可以绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛。 也有一些自动测试是否收敛的方法，例如将代价函数的变化值与某个阀值（例如0.001）进行比较，但通常看上面这样的图表更好。 梯度下降算法的每次迭代受到学习率的影响，如果学习率$a$过小，则达到收敛所需的迭代次数会非常高；如果学习率$a$过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。 通常可以考虑尝试些学习率： $\alpha=0.01，0.03，0.1，0.3，1，3，10$ 特征和多项式回归参考视频: 4 - 5 - Features and Polynomial Regression (8 min).mkv 如房价预测问题， $h_{\theta}\left( x \right)={\theta_{0} }+{\theta_{1} }\times{frontage}+{\theta_{2} }\times{depth}$ ${x_{1} }=frontage$（临街宽度），${x_{2} }=depth$（纵向深度），$x=frontage*depth=area$（面积），则：${h_{\theta} }\left( x \right)={\theta_{0} }+{\theta_{1} }x$。 线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，比如一个二次方模型：$h_{\theta}\left( x \right)={\theta_{0} }+{\theta_{1} }{x_{1} }+{\theta_{2} }{x_{2}^2}$或者三次方模型： $h_{\theta}\left( x \right)={\theta_{0} }+{\theta_{1} }{x_{1} }+{\theta_{2} }{x_{2}^2}+{\theta_{3} }{x_{3}^3}$ 通常我们需要先观察数据然后再决定准备尝试怎样的模型。 另外，我们可以令： ${ {x}_{2} }=x_{2}^{2},{ {x}_{3} }=x_{3}^{3}$，从而将模型转化为线性回归模型。 根据函数图形特性，我们还可以使： ${ {{h} }_{\theta} }(x)={ {\theta }_{0} }\text{+}{ {\theta }_{1} }(size)+{ {\theta}_{2} }{ {(size)}^{2} }$ 或者: ${ {{h} }_{\theta} }(x)={ {\theta }_{0} }\text{+}{ {\theta }_{1} }(size)+{ {\theta }_{2} }\sqrt{size}$ 注：如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要。 正规方程参考视频: 4 - 6 - Normal Equation (16 min).mkv到目前为止，我们都在使用梯度下降算法，但是对于某些线性回归问题，正规方程方法是更好的解决方案。如： 正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：$\frac{\partial}{\partial{\theta_{j} }}J\left( {\theta_{j} } \right)=0$ 。假设我们的训练集特征矩阵为 $X$（包含了 ${ {x}_{0} }=1$）并且我们的训练集结果为向量 $y$，则利用正规方程解出向量 $\theta ={ {\left( {X^T}X \right)}^{-1} }{X^{T} }y$ 。上标 T 代表矩阵转置，上标-1 代表矩阵的逆。设矩阵$A={X^{T} }X$，则：${ {\left( {X^T}X \right)}^{-1} }={A^{-1} }$以下表示数据为例： 即： 运用正规方程方法求解参数： 在 Octave 中，正规方程写作： pinv(X&#39;*X)*X&#39;*y注：对于那些不可逆的矩阵（通常是因为特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征，也有可能是特征数量大于训练集的数量），正规方程方法是不能用的。 梯度下降与正规方程的比较： 梯度下降 正规方程 需要选择学习率{% raw %}$\alpha${% endraw %} 不需要 需要多次迭代 一次运算得出 当特征数量{% raw %}$n${% endraw %}大时也能较好适用 需要计算{% raw %}${ {\left( { {X}^{T} }X \right)}^{-1} }${% endraw %} 如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为{% raw %}$O\left( { {n}^{3} } \right)${% endraw %}，通常来说当{% raw %}$n${% endraw %}小于10000 时还是可以接受的 适用于各种类型的模型 只适用于线性模型，不适合逻辑回归模型等其他模型 总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数{% raw %}$\theta ${% endraw %}的替代方法。具体地说，只要特征变量数量小于一万，我通常使用标准方程法，而不使用梯度下降法。 随着我们要讲的学习算法越来越复杂，例如，当我们讲到分类算法，像逻辑回归算法，我们会看到，实际上对于那些算法，并不能使用标准方程法。对于那些更复杂的学习算法，我们将不得不仍然使用梯度下降法。因此，梯度下降法是一个非常有用的算法，可以用在有大量特征变量的线性回归问题。或者我们以后在课程中，会讲到的一些其他的算法，因为标准方程法不适合或者不能用在它们上。但对于这个特定的线性回归模型，标准方程法是一个比梯度下降法更快的替代算法。所以，根据具体的问题，以及你的特征变量的数量，这两种算法都是值得学习的。 正规方程的python实现： import numpy as np def normalEqn(X, y): theta = np.linalg.inv(X.T@X)@X.T@y #X.T@X等价于X.T.dot(X) return theta 正规方程及不可逆性（可选）参考视频: 4 - 7 - Normal Equation Noninvertibility (Optional) (6 min).mkv在这段视频中谈谈正规方程 ( normal equation )，以及它们的不可逆性。由于这是一种较为深入的概念，并且总有人问我有关这方面的问题，因此，我想在这里来讨论它，由于概念较为深入，所以对这段可选材料大家放轻松吧，也许你可能会深入地探索下去，并且会觉得理解以后会非常有用。但即使你没有理解正规方程和线性回归的关系，也没有关系。 我们要讲的问题如下：$\theta ={ {\left( {X^{T} }X \right)}^{-1} }{X^{T} }y$ 备注：本节最后我把推导过程写下。 有些同学曾经问过我，当计算 $\theta$=inv(X&#39;X ) X&#39;y ，那对于矩阵$X'X$的结果是不可逆的情况咋办呢?如果你懂一点线性代数的知识，你或许会知道，有些矩阵可逆，而有些矩阵不可逆。我们称那些不可逆矩阵为奇异或退化矩阵。问题的重点在于$X'X$的不可逆的问题很少发生，在Octave里，如果你用它来实现$\theta$的计算，你将会得到一个正常的解。在Octave里，有两个函数可以求解矩阵的逆，一个被称为pinv()，另一个是inv()，这两者之间的差异是些许计算过程上的，一个是所谓的伪逆，另一个被称为逆。使用pinv() 函数可以展现数学上的过程，这将计算出$\theta$的值，即便矩阵$X'X$是不可逆的。 在pinv() 和 inv() 之间，又有哪些具体区别呢 ? 其中inv() 引入了先进的数值计算的概念。例如，在预测住房价格时，如果${x_{1} }$是以英尺为尺寸规格计算的房子，${x_{2} }$是以平方米为尺寸规格计算的房子，同时，你也知道1米等于3.28英尺 ( 四舍五入到两位小数 )，这样，你的这两个特征值将始终满足约束：${x_{1} }={x_{2} }*{ {\left( 3.28 \right)}^{2} }$。实际上，你可以用这样的一个线性方程，来展示那两个相关联的特征值，矩阵$X'X$将是不可逆的。 第二个原因是，在你想用大量的特征值，尝试实践你的学习算法的时候，可能会导致矩阵$X'X$的结果是不可逆的。具体地说，在$m$小于或等于n的时候，例如，有$m$等于10个的训练样本也有$n$等于100的特征数量。要找到适合的$(n +1)$ 维参数矢量$\theta$，这将会变成一个101维的矢量，尝试从10个训练样本中找到满足101个参数的值，这工作可能会让你花上一阵子时间，但这并不总是一个好主意。因为，正如我们所看到你只有10个样本，以适应这100或101个参数，数据还是有些少。 稍后我们将看到，如何使用小数据样本以得到这100或101个参数，通常，我们会使用一种叫做正则化的线性代数方法，通过删除某些特征或者是使用某些技术，来解决当$m$比$n$小的时候的问题。即使你有一个相对较小的训练集，也可使用很多的特征来找到很多合适的参数。总之当你发现的矩阵$X'X$的结果是奇异矩阵，或者找到的其它矩阵是不可逆的，我会建议你这么做。 首先，看特征值里是否有一些多余的特征，像这些${x_{1} }$和${x_{2} }$是线性相关的，互为线性函数。同时，当有一些多余的特征时，可以删除这两个重复特征里的其中一个，无须两个特征同时保留，将解决不可逆性的问题。因此，首先应该通过观察所有特征检查是否有多余的特征，如果有多余的就删除掉，直到他们不再是多余的为止，如果特征数量实在太多，我会删除些 用较少的特征来反映尽可能多内容，否则我会考虑使用正规化方法。如果矩阵$X'X$是不可逆的，（通常来说，不会出现这种情况），如果在Octave里，可以用伪逆函数pinv() 来实现。这种使用不同的线性代数库的方法被称为伪逆。即使$X'X$的结果是不可逆的，但算法执行的流程是正确的。总之，出现不可逆矩阵的情况极少发生，所以在大多数实现线性回归中，出现不可逆的问题不应该过多的关注${X^{T} }X$是不可逆的。 增加内容： $\theta ={ {\left( {X^{T} }X \right)}^{-1} }{X^{T} }y$ 的推导过程： $J\left( \theta \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{ {{\left( {h_{\theta} }\left( {x^{(i)} } \right)-{y^{(i)} } \right)}^{2} }}$ 其中：${h_{\theta} }\left( x \right)={\theta^{T} }X={\theta_{0} }{x_{0} }+{\theta_{1} }{x_{1} }+{\theta_{2} }{x_{2} }+...+{\theta_{n} }{x_{n} }$ 将向量表达形式转为矩阵表达形式，则有$J(\theta )=\frac{1}{2}{ {\left( X\theta -y\right)}^{2} }$ ，其中$X$为$m$行$n$列的矩阵（$m$为样本个数，$n$为特征个数），$\theta$为$n$行1列的矩阵，$y$为$m$行1列的矩阵，对$J(\theta )$进行如下变换 $J(\theta )=\frac{1}{2}{ {\left( X\theta -y\right)}^{T} }\left( X\theta -y \right)$ ​ $=\frac{1}{2}\left( { {\theta }^{T} }{ {X}^{T} }-{ {y}^{T} } \right)\left(X\theta -y \right)$ ​ $=\frac{1}{2}\left( { {\theta }^{T} }{ {X}^{T} }X\theta -{ {\theta}^{T} }{ {X}^{T} }y-{ {y}^{T} }X\theta -{ {y}^{T} }y \right)$ 接下来对$J(\theta )$偏导，需要用到以下几个矩阵的求导法则: $\frac{dAB}{dB}={ {A}^{T} }$ $\frac{d{ {X}^{T} }AX}{dX}=2AX$ 所以有: $\frac{\partial J\left( \theta \right)}{\partial \theta }=\frac{1}{2}\left(2{ {X}^{T} }X\theta -{ {X}^{T} }y -{}({ {y}^{T} }X )^{T}-0 \right)$ $=\frac{1}{2}\left(2{ {X}^{T} }X\theta -{ {X}^{T} }y -{ {X}^{T} }y -0 \right)$ ​ $={ {X}^{T} }X\theta -{ {X}^{T} }y$ 令$\frac{\partial J\left( \theta \right)}{\partial \theta }=0$, 则有$\theta ={ {\left( {X^{T} }X \right)}^{-1} }{X^{T} }y$ Octave教程(Octave Tutorial)基本操作参考视频: 5 - 1 - Basic Operations (14 min).mkv 在这段视频中，我将教你一种编程语言：Octave语言。你能够用它来非常迅速地实现这门课中我们已经学过的，或者将要学的机器学习算法。 过去我一直尝试用不同的编程语言来教授机器学习，包括C++、Java、Python、Numpy和Octave。我发现当使用像Octave这样的高级语言时，学生能够更快更好地学习并掌握这些算法。事实上，在硅谷，我经常看到进行大规模的机器学习项目的人，通常使用的程序语言就是Octave。(编者注：这是当时的情况，现在主要是用Python) Octave是一种很好的原始语言(prototyping language)，使用Octave你能快速地实现你的算法，剩下的事情，你只需要进行大规模的资源配置，你只用再花时间用C++或Java这些语言把算法重新实现就行了。开发项目的时间是很宝贵的，机器学习的时间也是很宝贵的。所以，如果你能让你的学习算法在Octave上快速的实现，基本的想法实现以后，再用C++或者Java去改写，这样你就能节省出大量的时间。 据我所见，人们使用最多的用于机器学习的原始语言是Octave、MATLAB、Python、NumPy 和R。 Octave很好，因为它是开源的。当然MATLAB也很好，但它不是每个人都买得起的。(貌似国内学生喜欢用收费的matlab，matlab功能要比Octave强大的多，网上有各种D版可以下载)。这次机器学习课的作业也是用matlab的。如果你能够使用matlab，你也可以在这门课里面使用。 如果你会Python、NumPy或者R语言，我也见过有人用 R的，据我所知，这些人不得不中途放弃了，因为这些语言在开发上比较慢，而且，因为这些语言如：Python、NumPy的语法相较于Octave来说，还是更麻烦一点。正因为这样，所以我强烈建议不要用NumPy或者R来完整这门课的作业，我建议在这门课中用Octave来写程序。 本视频将快速地介绍一系列的命令，目标是迅速地展示，通过这一系列Octave的命令，让你知道Octave能用来做什么。 启动Octave： 现在打开Octave，这是Octave命令行。 现在让我示范最基本的Octave代码： 输入5 + 6，然后得到11。 输入3 – 2、5×8、1/2、2^6等等，得到相应答案。 这些都是基本的数学运算。 你也可以做逻辑运算，例如 1==2，计算结果为 false (假)，这里的百分号命令表示注释，1==2 计算结果为假，这里用0表示。 请注意，不等于符号的写法是这个波浪线加上等于符号 ( ~= )，而不是等于感叹号加等号( != )，这是和其他一些编程语言中不太一样的地方。 让我们看看逻辑运算 1 &amp;&amp; 0，使用双&amp;符号表示逻辑与，1 &amp;&amp; 0判断为假，1和0的或运算 1 || 0，其计算结果为真。 还有异或运算 如XOR ( 1, 0 )，其返回值为1 从左向右写着 Octave 324.x版本，是默认的Octave提示，它显示了当前Octave的版本，以及相关的其它信息。 如果你不想看到那个提示，这里有一个隐藏的命令： 输入命令 现在命令提示已经变得简化了。 接下来，我们将谈到Octave的变量。 现在写一个变量，对变量$A$赋值为3，并按下回车键，显示变量$A$等于3。 如果你想分配一个变量，但不希望在屏幕上显示结果，你可以在命令后加一个分号，可以抑制打印输出，敲入回车后，不打印任何东西。 其中这句命令不打印任何东西。 现在举一个字符串的例子：变量$b$等于”hi“。 $c$等于3大于等于1，所以，现在$c$变量的值是真。 如果你想打印出变量，或显示一个变量，你可以像下面这么做： 设置$a$等于圆周率$π$，如果我要打印该值，那么只需键入a像这样 就打印出来了。 对于更复杂的屏幕输出，也可以用DISP命令显示： 这是一种，旧风格的C语言语法，对于之前就学过C语言的同学来说，你可以使用这种基本的语法来将结果打印到屏幕。 例如 ^{T}命令的六个小数：0.6%f ,a，这应该打印$π$的6位小数形式。 也有一些控制输出长短格式的快捷命令： 下面，让我们来看看向量和矩阵： 比方说 建立一个矩阵$A$： 对$A$矩阵进行赋值，考虑到这是一个三行两列的矩阵，你同样可以用向量。 建立向量$V$并赋值1 2 3，$V$是一个行向量，或者说是一个3 ( 列 )×1 ( 行 )的向量，或者说，一行三列的矩阵。 如果我想，分配一个列向量，我可以写“1;2;3”，现在便有了一个3 行 1 列的向量，同时这是一个列向量。 下面是一些更为有用的符号，如： V=1：0.1：2 这个该如何理解呢：这个集合{% raw %}$v${% endraw %}是一组值，从数值1开始，增量或说是步长为0.1，直到增加到2，按照这样的方法对向量{% raw %}$V${% endraw %}操作，可以得到一个行向量，这是一个1行11列的矩阵，其矩阵的元素是11.1 1.2 1.3，依此类推，直到数值2。 我也可以建立一个集合{% raw %}$v${% endraw %}并用命令“1:6”进行赋值，这样{% raw %}$V${% endraw %}就被赋值了1至6的六个整数。 这里还有一些其他的方法来生成矩阵 例如“ones(2, 3)”，也可以用来生成矩阵： 元素都为2，两行三列的矩阵，就可以使用这个命令： 你可以把这个方法当成一个生成矩阵的快速方法。 {% raw %}$w${% endraw %}为一个一行三列的零矩阵，一行三列的{% raw %}$A${% endraw %}矩阵里的元素全部是零： 还有很多的方式来生成矩阵。 如果我对{% raw %}$W${% endraw %}进行赋值，用Rand命令建立一个一行三列的矩阵，因为使用了Rand命令，则其一行三列的元素均为随机值，如“rand(3,3)”命令，这就生成了一个3×3的矩阵，并且其所有元素均为随机。 数值介于0和1之间，所以，正是因为这一点，我们可以得到数值均匀介于0和1之间的元素。 如果，你知道什么是高斯随机变量，或者，你知道什么是正态分布的随机变量，你可以设置集合{% raw %}$W${% endraw %}，使其等于一个一行三列的{% raw %}$N${% endraw %}矩阵，并且，来自三个值，一个平均值为0的高斯分布，方差或者等于1的标准偏差。 还可以设置地更复杂： 并用hist命令绘制直方图。 绘制单位矩阵： 如果对命令不清楚，建议用help命令： 以上讲解的内容都是Octave的基本操作。希望你能通过上面的讲解，自己练习一些矩阵、乘、加等操作，将这些操作在Octave中熟练运用。 在接下来的视频中，将会涉及更多复杂的命令，并使用它们在Octave中对数据进行更多的操作。 移动数据参考视频: 5 - 2 - Moving Data Around (16 min).mkv 在这段关于 Octave的辅导课视频中，我将开始介绍如何在 Octave 中移动数据。 如果你有一个机器学习问题，你怎样把数据加载到 Octave 中？ 怎样把数据存入一个矩阵？ 如何对矩阵进行相乘？ 如何保存计算结果？ 如何移动这些数据并用数据进行操作？ 进入我的 Octave 窗口， 我键入{% raw %}$A${% endraw %}，得到我们之前构建的矩阵 {% raw %}$A${% endraw %}，也就是用这个命令生成的： A = [1 2; 3 4; 5 6] 这是一个3行2列的矩阵，Octave 中的 size() 命令返回矩阵的尺寸。 所以 size(A) 命令返回3 2 实际上，size() 命令返回的是一个 1×2 的矩阵，我们可以用 {% raw %}$sz${% endraw %} 来存放。 设置 sz = size(A) 因此 {% raw %}$sz${% endraw %} 就是一个1×2的矩阵，第一个元素是3，第二个元素是2。 所以如果键入 size(sz) 看看 {% raw %}$sz${% endraw %} 的尺寸，返回的是1 2，表示是一个1×2的矩阵，1 和 2分别表示矩阵{% raw %}$sz${% endraw %}的维度 。 你也可以键入 size(A, 1)，将返回3，这个命令会返回{% raw %}$A${% endraw %}矩阵的第一个元素，{% raw %}$A${% endraw %}矩阵的第一个维度的尺寸，也就是 {% raw %}$A${% endraw %} 矩阵的行数。 同样，命令 size(A, 2)，将返回2，也就是 {% raw %}$A${% endraw %} 矩阵的列数。 如果你有一个向量 {% raw %}$v${% endraw %}，假如 v = [1 2 3 4]，然后键入length(v)，这个命令将返回最大维度的大小，返回4。 你也可以键入length(A)，由于矩阵{% raw %}$A${% endraw %}是一个3×2的矩阵，因此最大的维度应该是3，因此该命令会返回3。 但通常我们还是对向量使用 {% raw %}$length${% endraw %} 命令，而不是对矩阵使用 length 命令，比如length([1;2;3;4;5])，返回5。 如何在系统中加载数据和寻找数据： 当我们打开 Octave 时，我们通常已经在一个默认路径中，这个路径是 Octave的安装位置，pwd 命令可以显示出Octave 当前所处路径。 cd命令，意思是改变路径，我可以把路径改为C:\Users\ang\Desktop，这样当前目录就变为了桌面。 如果键入 ls，ls 来自于一个 Unix 或者 Linux 命令，ls命令将列出我桌面上的所有路径。 事实上，我的桌面上有两个文件：featuresX.dat 和priceY.dat，是两个我想解决的机器学习问题。 featuresX文件如这个窗口所示，是一个含有两列数据的文件，其实就是我的房屋价格数据，数据集中有47行，第一个房子样本，面积是2104平方英尺，有3个卧室，第二套房子面积为1600，有3个卧室等等。 priceY这个文件就是训练集中的价格数据，所以 featuresX 和priceY就是两个存放数据的文档，那么应该怎样把数据读入 Octave 呢？我们只需要键入featuresX.dat，这样我将加载了 featuresX 文件。同样地我可以加载priceY.dat。其实有好多种办法可以完成，如果你把命令写成字符串的形式load(&#39;featureX.dat&#39;)，也是可以的，这跟刚才的命令效果是相同的，只不过是把文件名写成了一个字符串的形式，现在文件名被存在一个字符串中。Octave中使用引号来表示字符串。 另外 who 命令，能显示出 在我的 Octave工作空间中的所有变量 所以我可以键入featuresX 回车，来显示 featuresX 这些就是存在里面的数据。 还可以键入 size(featuresX)，得出的结果是 47 2，代表这是一个47×2的矩阵。 类似地，输入 size(priceY)，结果是 471，表示这是一个47维的向量，是一个列矩阵，存放的是训练集中的所有价格{% raw %}$Y${% endraw %} 的值。 who 函数能让你看到当前工作空间中的所有变量，同样还有另一个 whos命令，能更详细地进行查看。 同样也列出我所有的变量，不仅如此，还列出了变量的维度。 double 意思是双精度浮点型，这也就是说，这些数都是实数，是浮点数。 如果你想删除某个变量，你可以使用 clear 命令，我们键入 clear featuresX，然后再输入 whos 命令，你会发现 featuresX 消失了。 另外，我们怎么储存数据呢？ 我们设变量 V= priceY(1:10) 这表示的是将向量 {% raw %}$Y ${% endraw %}的前10个元素存入 {% raw %}$V${% endraw %}中。 假如我们想把它存入硬盘，那么用 save hello.mat v 命令，这个命令会将变量{% raw %}$V${% endraw %}存成一个叫 hello.mat 的文件，让我们回车，现在我的桌面上就出现了一个新文件，名为hello.mat。 由于我的电脑里同时安装了 MATLAB，所以这个图标上面有 MATLAB的标识，因为操作系统把文件识别为 MATLAB文件。如果在你的电脑上图标显示的不一样的话，也没有关系。 现在我们清除所有变量，直接键入clear，这样将删除工作空间中的所有变量，所以现在工作空间中啥都没了。 但如果我载入 hello.mat 文件，我又重新读取了变量 {% raw %}$v${% endraw %}，因为我之前把变量{% raw %}$v${% endraw %}存入了hello.mat 文件中，所以我们刚才用 save命令做了什么。这个命令把数据按照二进制形式储存，或者说是更压缩的二进制形式，因此，如果{% raw %}$v${% endraw %}是很大的数据，那么压缩幅度也更大，占用空间也更小。如果你想把数据存成一个人能看懂的形式，那么可以键入： save hello.txt v -ascii 这样就会把数据存成一个文本文档，或者将数据的 ascii 码存成文本文档。 我键入了这个命令以后，我的桌面上就有了 hello.txt文件。如果打开它，我们可以发现这个文本文档存放着我们的数据。 这就是读取和储存数据的方法。 接下来我们再来讲讲操作数据的方法： 假如 {% raw %}$A${% endraw %} 还是那个矩阵 跟刚才一样还是那个 3×2 的矩阵，现在我们加上索引值，比如键入 A(3,2) 这将索引到{% raw %}$A${% endraw %} 矩阵的 (3,2) 元素。这就是我们通常书写矩阵的形式，写成 {% raw %}$A${% endraw %} 32，3和2分别表示矩阵的第三行和第二列对应的元素，因此也就对应 6。 我也可以键入A(2,:) 来返回第二行的所有元素，冒号表示该行或该列的所有元素。 类似地，如果我键入 A(:,2)，这将返回 {% raw %}$A${% endraw %} 矩阵第二列的所有元素，这将得到 2 4 6。 这表示返回{% raw %}$A${% endraw %} 矩阵的第二列的所有元素。 你也可以在运算中使用这些较为复杂的索引。 我再给你展示几个例子，可能你也不会经常使用，但我还是输入给你看 A([1 3],:)，这个命令意思是取 {% raw %}$A${% endraw %} 矩阵第一个索引值为1或3的元素，也就是说我取的是A矩阵的第一行和第三行的每一列，冒号表示的是取这两行的每一列元素，即： 可能这些比较复杂一点的索引操作你会经常用到。 我们还能做什么呢？依然是 {% raw %}$A${% endraw %} 矩阵，A(:,2) 命令返回第二列。 你也可以为它赋值，我可以取 {% raw %}$A${% endraw %} 矩阵的第二列，然后将它赋值为10 11 12，我实际上是取出了 {% raw %}$A${% endraw %} 的第二列，然后把一个列向量[10;11;12]赋给了它，因此现在 {% raw %}$A${% endraw %} 矩阵的第一列还是 1 3 5，第二列就被替换为 10 11 12。 接下来一个操作，让我们把 {% raw %}$A ${% endraw %}设为A = [A, [100, 101,102]]，这样做的结果是在原矩阵的右边附加了一个新的列矩阵，就是把 {% raw %}$A${% endraw %}矩阵设置为原来的 {% raw %}$A${% endraw %} 矩阵再在右边附上一个新添加的列矩阵。 最后，还有一个小技巧，如果你就输入 A(:)，这是一个很特别的语法结构，意思是把 {% raw %}$A${% endraw %}中的所有元素放入一个单独的列向量，这样我们就得到了一个 9×1 的向量，这些元素都是{% raw %}$A${% endraw %} 中的元素排列起来的。 再来几个例子： 我还是把 A 重新设为 [1 2; 3 4; 5 6]，我再设一个 {% raw %}$B${% endraw %}为[11 12; 13 14; 15 16]，我可以新建一个矩阵 {% raw %}$C${% endraw %}，C = [A B]，这个意思就是把这两个矩阵直接连在一起，矩阵{% raw %}$A${% endraw %} 在左边，矩阵{% raw %}$B${% endraw %} 在右边，这样组成了 {% raw %}$C${% endraw %}矩阵，就是直接把{% raw %}$A${% endraw %}和 {% raw %}$B${% endraw %} 合起来。 我还可以设C = [A; B]，这里的分号表示把分号后面的东西放到下面。所以，[A;B]的作用依然还是把两个矩阵放在一起，只不过现在是上下排列，所以现在 {% raw %}$A${% endraw %} 在上面 {% raw %}$B${% endraw %}在下面，{% raw %}$C${% endraw %} 就是一个 6×2 矩阵。 简单地说，分号的意思就是换到下一行，所以 C 就包括上面的A，然后换行到下面，然后在下面放上一个 {% raw %}$B${% endraw %}。 另外顺便说一下，这个[A B]命令跟 [A, B] 是一样的，这两种写法的结果是相同的。 通过以上这些操作，希望你现在掌握了怎样构建矩阵，也希望我展示的这些命令能让你很快地学会怎样把矩阵放到一起，怎样取出矩阵，并且把它们放到一起，组成更大的矩阵。 通过几句简单的代码，Octave能够很方便地很快速地帮助我们组合复杂的矩阵以及对数据进行移动。这就是移动数据这一节课。 我认为对你来讲，最好的学习方法是，下课后复习一下我键入的这些代码好好地看一看，从课程的网上把代码的副本下载下来，重新好好看看这些副本，然后自己在Octave 中把这些命令重新输一遍，慢慢开始学会使用这些命令。 当然，没有必要把这些命令都记住，你也不可能记得住。你要做的就是，了解一下你可以用哪些命令，做哪些事。这样在你今后需要编写学习算法时，如果你要找到某个Octave中的命令，你可能回想起你之前在这里学到过，然后你就可以查找课程中提供的程序副本，这样就能很轻松地找到你想使用的命令了。 计算数据参考视频: 5 - 3 - Computing on Data (13 min).mkv 现在，你已经学会了在Octave中如何加载或存储数据，如何把数据存入矩阵等等。在这段视频中，我将介绍如何对数据进行运算，稍后我们将使用这些运算操作来实现我们的学习算法。 这是我的 Octave窗口，我现在快速地初始化一些变量。比如设置{% raw %}$A${% endraw %}为一个3×2的矩阵，设置{% raw %}$B${% endraw %}为一个3 ×2矩阵，设置{% raw %}$C${% endraw %}为2 × 2矩阵。 我想算两个矩阵的乘积，比如说 {% raw %}$A × C${% endraw %}，我只需键入A×C，这是一个 3×2 矩阵乘以 2×2矩阵，得到这样一个3×2矩阵。 你也可以对每一个元素，做运算 方法是做点乘运算A.*B，这么做Octave将矩阵 {% raw %}$A${% endraw %}中的每一个元素与矩阵 {% raw %}$B${% endraw %} 中的对应元素相乘:A.*B 这里第一个元素1乘以11得到11，第二个元素2乘以12得到24，这就是两个矩阵的元素位运算。通常来说，在Octave中点号一般用来表示元素位运算。 这里是一个矩阵{% raw %}$A${% endraw %}，这里我输入A.^2，这将对矩阵{% raw %}$A${% endraw %}中每一个元素平方。 我们设{% raw %}$V${% endraw %}为 [1; 2; 3] 是列向量，你也可以输入1./V，得到每一个元素的倒数，所以这样一来，就会分别算出 1/1 1/2 1/3。 矩阵也可以这样操作，1./A 得到{% raw %}$A${% endraw %}中每一个元素的倒数。 同样地，这里的点号还是表示对每一个元素进行操作。 我们还可以进行求对数运算，也就是对每个元素进行求对数运算。 还有自然数{% raw %}$e${% endraw %}的幂次运算，就是以{% raw %}$e${% endraw %}为底，以这些元素为幂的运算。 我还可以用 abs来对 {% raw %}$v${% endraw %} 的每一个元素求绝对值，当然这里 {% raw %}$v${% endraw %}都是正数。我们换成另一个这样对每个元素求绝对值，得到的结果就是这些非负的元素。还有{% raw %}$–v${% endraw %}，给出{% raw %}$v${% endraw %}中每个元素的相反数，这等价于 -1 乘以 {% raw %}$v${% endraw %}，一般就直接用 {% raw %}$-v${% endraw %}就好了，其实就等于 $-1*v$。 还有一个技巧，比如说我们想对{% raw %}$v${% endraw %}中的每个元素都加1，那么我们可以这么做，首先构造一个3行1列的1向量，然后把这个1向量跟原来的向量相加，因此{% raw %}$v${% endraw %}向量从[1 2 3] 增至 [2 3 4]。我用了一个，length(v)命令，因此这样一来，ones(length(v) ,1) 就相当于ones(3,1)，然后我做的是v +ones(3,1)，也就是将 {% raw %}$v${% endraw %} 的各元素都加上这些1，这样就将{% raw %}$v${% endraw %} 的每个元素增加了1。 另一种更简单的方法是直接用 v+1，v + 1 也就等于把 {% raw %}$v${% endraw %} 中的每一个元素都加上1。 现在，让我们来谈谈更多的操作。 矩阵{% raw %}$A${% endraw %} 如果你想要求它的转置，那么方法是用A’,将得出 A 的转置矩阵。当然，如果我写(A&#39;)&#39;，也就是 {% raw %}$A${% endraw %} 转置两次，那么我又重新得到矩阵 {% raw %}$A${% endraw %}。 还有一些有用的函数，比如： a=[1 15 2 0.5]，这是一个1行4列矩阵，val=max(a)，这将返回{% raw %}$A${% endraw %}矩阵中的最大值15。 我还可以写 [val, ind] =max(a)，这将返回{% raw %}$A${% endraw %}矩阵中的最大值存入{% raw %}$val${% endraw %}，以及该值对应的索引，元素15对应的索引值为2,存入{% raw %}$ind${% endraw %}，所以 {% raw %}$ind =2${% endraw %}。 特别注意一下，如果你用命令 max(A)，{% raw %}$A${% endraw %}是一个矩阵的话，这样做就是对每一列求最大值。 我们还是用这个例子，这个 {% raw %}$a${% endraw %} 矩阵a=[1 15 2 0.5]，如果输入a&amp;lt;3，这将进行逐元素的运算，所以元素小于3的返回1，否则返回0。 因此，返回[1 1 0 1]。也就是说，对{% raw %}$a${% endraw %}矩阵的每一个元素与3进行比较，然后根据每一个元素与3的大小关系，返回1和0表示真与假。 如果我写 find(a&amp;lt;3)，这将告诉我{% raw %}$a${% endraw %} 中的哪些元素是小于3的。 设A = magic(3)，magic 函数将返回一个矩阵，称为魔方阵或幻方 (magic squares)，它们具有以下这样的数学性质：它们所有的行和列和对角线加起来都等于相同的值。 当然据我所知，这在机器学习里基本用不上，但我可以用这个方法很方便地生成一个3行3列的矩阵，而这个魔方矩阵这神奇的方形屏幕。每一行、每一列、每一个对角线三个数字加起来都是等于同一个数。 在其他有用的机器学习应用中，这个矩阵其实没多大作用。 如果我输入 [r,c] = find(A&gt;=7)，这将找出所有{% raw %}$A${% endraw %}矩阵中大于等于7的元素，因此，{% raw %}$r${% endraw %} 和{% raw %}$c${% endraw %}分别表示行和列，这就表示，第一行第一列的元素大于等于7，第三行第二列的元素大于等于7，第二行第三列的元素大于等于7。 顺便说一句，其实我从来都不去刻意记住这个 find 函数，到底是怎么用的，我只需要会用help函数就可以了，每当我在使用这个函数，忘记怎么用的时候，我就可以用 help函数，键入 help find 来找到帮助文档。 最后再讲两个内容，一个是求和函数，这是 {% raw %}$a${% endraw %} 矩阵： 键入 sum(a)，就把 a 中所有元素加起来了。 如果我想把它们都乘起来，键入 prod(a)，prod 意思是product(乘积)，它将返回这四个元素的乘积。 floor(a) 是向下四舍五入，因此对于 {% raw %}$a${% endraw %} 中的元素0.5将被下舍入变成0。 还有 ceil(a)，表示向上四舍五入，所以0.5将上舍入变为最接近的整数，也就是1。 键入 type(3)，这通常得到一个3×3的矩阵，如果键入 max(rand(3),rand(3))，这样做的结果是返回两个3×3的随机矩阵，并且逐元素比较取最大值。 假如我输入max(A,[],1)，这样做会得到每一列的最大值。 所以第一列的最大值就是8，第二列是9，第三列的最大值是7，这里的1表示取A矩阵第一个维度的最大值。 相对地，如果我键入max(A,[],2)，这将得到每一行的最大值，所以，第一行的最大值是等于8，第二行最大值是7，第三行是9。 所以你可以用这个方法来求得每一行或每一列的最值，另外，你要知道，默认情况下max(A)返回的是每一列的最大值，如果你想要找出整个矩阵A的最大值，你可以输入max(max(A))，或者你可以将{% raw %}$A${% endraw %} 矩阵转成一个向量，然后键入 max(A(:))，这样做就是把 {% raw %}$A${% endraw %} 当做一个向量，并返回 {% raw %}$A${% endraw %}向量中的最大值。 最后，让我们把 {% raw %}$A${% endraw %}设为一个9行9列的魔方阵，魔方阵具有的特性是每行每列和对角线的求和都是相等的。 这是一个9×9的魔方阵，我们来求一个 sum(A,1)，这样就得到每一列的总和，这也验证了一个9×9的魔方阵确实每一列加起来都相等，都为369。 现在我们来求每一行的和，键入sum(A,2)，这样就得到了{% raw %}$A${% endraw %} 中每一行的和加起来还是369。 现在我们来算{% raw %}$A ${% endraw %}的对角线元素的和。我们现在构造一个9×9 的单位矩阵，键入 eye(9), 然后我们要用 {% raw %}$A${% endraw %}逐点乘以这个单位矩阵，除了对角线元素外，其他元素都会得到0。 键入sum(sum(A.*eye(9)) 这实际上是求得了，这个矩阵对角线元素的和确实是369。 你也可以求另一条对角线的和也是是369。 flipup/flipud 表示向上/向下翻转。 同样地，如果你想求这个矩阵的逆矩阵，键入pinv(A)，通常称为伪逆矩阵，你就把它看成是矩阵 {% raw %}$A${% endraw %} 求逆，因此这就是 {% raw %}$A${% endraw %}矩阵的逆矩阵。 设 temp = pinv(A)，然后再用{% raw %}$temp${% endraw %} 乘以{% raw %}$A${% endraw %}，这实际上得到的就是单位矩阵，对角线为1，其他元素为0。 如何对矩阵中的数字进行各种操作，在运行完某个学习算法之后，通常一件最有用的事情是看看你的结果，或者说让你的结果可视化，在接下来的视频中，我会非常迅速地告诉你，如何很快地画图，如何只用一两行代码，你就可以快速地可视化你的数据，这样你就能更好地理解你使用的学习算法。 绘图数据参考视频: 5 - 4 - Plotting Data (10 min).mkv 当开发学习算法时，往往几个简单的图，可以让你更好地理解算法的内容，并且可以完整地检查下算法是否正常运行，是否达到了算法的目的。 例如在之前的视频中，我谈到了绘制成本函数{% raw %}$J(\theta)${% endraw %}，可以帮助确认梯度下降算法是否收敛。通常情况下，绘制数据或学习算法所有输出，也会启发你如何改进你的学习算法。幸运的是，Octave有非常简单的工具用来生成大量不同的图。当我用学习算法时，我发现绘制数据、绘制学习算法等，往往是我获得想法来改进算法的重要部分。在这段视频中，我想告诉你一些Octave的工具来绘制和可视化你的数据。 我们先来快速生成一些数据用来绘图。 如果我想绘制正弦函数，这是很容易的，我只需要输入plot(t,y1)，并回车，就出现了这个图： 横轴是{% raw %}$t${% endraw %}变量，纵轴是{% raw %}$y1${% endraw %}，也就是我们刚刚所输出的正弦函数。 让我们设置{% raw %}$y2${% endraw %} Octave将会消除之前的正弦图，并且用这个余弦图来代替它，这里纵轴{% raw %}$cos(x)${% endraw %}从1开始， 如果我要同时表示正弦和余弦曲线。 我要做的就是，输入：plot(t, y1)，得到正弦函数，我使用函数hold on，hold on函数的功能是将新的图像绘制在旧的之上。 我现在绘制{% raw %}$y2${% endraw %}，输入：plot(t, y2)。 我要以不同的颜色绘制余弦函数，所以我在这里输入带引号的r绘制余弦函数，{% raw %}$r${% endraw %}表示所使用的颜色：plot(t,y2,’r’)，再加上命令xlabel(&#39;time&#39;)，来标记X轴即水平轴，输入ylabel(&#39;value&#39;)，来标记垂直轴的值。 同时我也可以来标记我的两条函数曲线，用这个命令 legend(&#39;sin&#39;,&#39;cos&#39;)将这个图例放在右上方，表示这两条曲线表示的内容。最后输入title(&#39;myplot&#39;)，在图像的顶部显示这幅图的标题。 如果你想保存这幅图像，你输入print –dpng &#39;myplot.png&#39;，png是一个图像文件格式，如果你这样做了，它可以让你保存为一个文件。 Octave也可以保存为很多其他的格式，你可以键入help plot。 最后如果你想，删掉这个图像，用命令close会让这个图像关掉。 Octave也可以让你为图像标号 你键入figure(1); plot(t, y1);将显示第一张图，绘制了变量{% raw %}$t${% endraw %} {% raw %}$y1${% endraw %}。 键入figure(2); plot(t, y2); 将显示第一张图，绘制了变量{% raw %}$t${% endraw %} {% raw %}$y2${% endraw %}。 subplot命令，我们要使用subplot(1,2,1)，它将图像分为一个1*2的格子，也就是前两个参数，然后它使用第一个格子，也就是最后一个参数1的意思。 我现在使用第一个格子，如果键入plot(t,y1)，现在这个图显示在第一个格子。如果我键入subplot(1,2,2)，那么我就要使用第二个格子，键入plot(t,y2)；现在y2显示在右边，也就是第二个格子。 最后一个命令，你可以改变轴的刻度，比如改成[0.5 1 -1 1]，输入命令：axis([0.5 1 -1 1])也就是设置了右边图的{% raw %}$x${% endraw %}轴和{% raw %}$y${% endraw %}轴的范围。具体而言，它将右图中的横轴的范围调整至0.5到1，竖轴的范围为-1到1。 你不需要记住所有这些命令，如果你需要改变坐标轴，或者需要知道axis命令，你可以用Octave中用help命令了解细节。 最后，还有几个命令。 Clf（清除一幅图像）。 让我们设置A等于一个5×5的magic方阵： 我有时用一个巧妙的方法来可视化矩阵，也就是imagesc(A)命令，它将会绘制一个5*5的矩阵，一个5*5的彩色格图，不同的颜色对应A矩阵中的不同值。 我还可以使用函数colorbar，让我用一个更复杂的命令 imagesc(A)，colorbar，colormap gray。这实际上是在同一时间运行三个命令：运行imagesc，然后运行，colorbar，然后运行colormap gray。 它生成了一个颜色图像，一个灰度分布图，并在右边也加入一个颜色条。所以这个颜色条显示不同深浅的颜色所对应的值。 你可以看到在不同的方格，它对应于一个不同的灰度。 输入imagesc(magic(15))，colorbar，colormap gray 这将会是一幅15*15的magic方阵值的图。 最后，总结一下这段视频。你看到我所做的是使用逗号连接函数调用。如果我键入{% raw %}$a=1${% endraw %},{% raw %}$b=2${% endraw %},{% raw %}$c=3${% endraw %}然后按Enter键，其实这是将这三个命令同时执行，或者是将三个命令一个接一个执行，它将输出所有这三个结果。 这很像{% raw %}$a=1${% endraw %}; {% raw %}$b=2${% endraw %};{% raw %}$c=3${% endraw %};如果我用分号来代替逗号，则没有输出出任何东西。 这里我们称之为逗号连接的命令或函数调用。 用逗号连接是另一种Octave中更便捷的方式，将多条命令例如imagesc colorbar colormap，将这多条命令写在同一行中。 现在你知道如何绘制Octave中不同的图像，在下面的视频中，我将告诉你怎样在Octave中，写控制语句，比如if while for语句，并且定义和使用函数。 控制语句：for，while，if语句参考视频: 5 - 5 - Control Statements_ for, while, if statements (13 min).mkv 在这段视频中，我想告诉你怎样为你的 Octave 程序写控制语句。诸如：”for“ “while“ “if“ 这些语句，并且如何定义和使用方程。 我先告诉你如何使用 “for” 循环。 首先，我要将 {% raw %}$v${% endraw %} 值设为一个10行1列的零向量。 接着我要写一个 “for“ 循环，让 {% raw %}$i${% endraw %} 等于 1 到 10，写出来就是 i = 1:10。我要设{% raw %}$ v(i)${% endraw %}的值等于 2 的 {% raw %}$i${% endraw %} 次方，循环最后写上“end”。 向量{% raw %}$v${% endraw %} 的值就是这样一个集合 2的一次方、2的二次方，依此类推。这就是我的 {% raw %}$i${% endraw %} 等于 1 到 10的语句结构，让 {% raw %}$i${% endraw %} 遍历 1 到 10的值。 另外，你还可以通过设置你的 indices (索引) 等于 1一直到10，来做到这一点。这时indices 就是一个从1到10的序列。 你也可以写 i = indices，这实际上和我直接把 i 写到 1 到 10 是一样。你可以写 disp(i)，也能得到一样的结果。所以 这就是一个 “for” 循环。 如果你对 “break” 和 “continue” 语句比较熟悉，Octave里也有 “break” 和 “continue”语句，你也可以在 Octave环境里使用那些循环语句。 但是首先让我告诉你一个 while 循环是如何工作的： 这是什么意思呢：我让 {% raw %}$i${% endraw %} 取值从 1 开始，然后我要让 {% raw %}$v(i)${% endraw %} 等于 100，再让 {% raw %}$i${% endraw %} 递增 1，直到{% raw %}$i${% endraw %} 大于 5停止。 现在来看一下结果，我现在已经取出了向量的前五个元素，把他们用100覆盖掉，这就是一个while循环的句法结构。 现在我们来分析另外一个例子： 这里我将向你展示如何使用break语句。比方说 v(i) = 999，然后让 i = i+1，当 {% raw %}$i${% endraw %} 等于6的时候 break (停止循环)，结束 (end)。 当然这也是我们第一次使用一个 if 语句，所以我希望你们可以理解这个逻辑，让 {% raw %}$i${% endraw %} 等于1 然后开始下面的增量循环，while语句重复设置 {% raw %}$v(i)${% endraw %} 等于999，不断让{% raw %}$i${% endraw %}增加，然后当 {% raw %}$i${% endraw %} 达到6，做一个中止循环的命令，尽管有while循环，语句也就此中止。所以最后的结果是取出向量 {% raw %}$v${% endraw %} 的前5个元素，并且把它们设置为999。 所以，这就是if 语句和 while 语句的句法结构。并且要注意要有end，上面的例子里第一个 end 结束的是 if语句，第二个 end 结束的是 while 语句。 现在让我告诉你使用 if-else 语句： 最后，提醒一件事：如果你需要退出 Octave，你可以键入exit命令然后回车就会退出 Octave，或者命令quit也可以。 最后，让我们来说说函数 (functions)，如何定义和调用函数。 我在桌面上存了一个预先定义的文件名为 “squarethisnumber.m”，这就是在 Octave 环境下定义的函数。 让我们打开这个文件。请注意，我使用的是微软的写字板程序来打开这个文件，我只是想建议你，如果你也使用微软的Windows系统，那么可以使用写字板程序，而不是记事本来打开这些文件。如果你有别的什么文本编辑器也可以，记事本有时会把代码的间距弄得很乱。如果你只有记事本程序，那也能用。我建议你用写字板或者其他可以编辑函数的文本编辑器。 现在我们来说如何在 Octave 里定义函数： 这个文件只有三行： 第一行写着 function y = squareThisNumber(x)，这就告诉 Octave，我想返回一个 y值，我想返回一个值，并且返回的这个值将被存放于变量 {% raw %}$y${% endraw %} 里。另外，它告诉了Octave这个函数有一个参数，就是参数 {% raw %}$x${% endraw %}，还有定义的函数体，也就是 {% raw %}$y${% endraw %} 等于 {% raw %}$x${% endraw %} 的平方。 还有一种更高级的功能，这只是对那些知道“search path (搜索路径)”这个术语的人使用的。所以如果你想要修改Octave的搜索路径，你可以把下面这部分作为一个进阶知识，或者选学材料，仅适用于那些熟悉编程语言中搜索路径概念的同学。 你可以使用addpath 命令添加路径，添加路径“C:\Users\ang\desktop”将该目录添加到Octave的搜索路径，这样即使你跑到其他路径底下，Octave依然知道会在 Users\ang\desktop目录下寻找函数。这样，即使我现在在不同的目录下，它仍然知道在哪里可以找到“SquareThisNumber” 这个函数。 但是，如果你不熟悉搜索路径的概念，不用担心，只要确保在执行函数之前，先用 cd命令设置到你函数所在的目录下，实际上也是一样的效果。 Octave还有一个其他许多编程语言都没有的概念，那就是它可以允许你定义一个函数，使得返回值是多个值或多个参数。这里就是一个例子，定义一个函数叫： “SquareAndCubeThisNumber(x)” ({% raw %}$x${% endraw %}的平方以及{% raw %}$x${% endraw %}的立方) 这说的就是函数返回值是两个： {% raw %}$y1${% endraw %} 和 {% raw %}$y2${% endraw %}，接下来就是{% raw %}$y1${% endraw %}是被平方后的结果，{% raw %}$y2${% endraw %}是被立方后的结果，这就是说，函数会真的返回2个值。 有些同学可能会根据你使用的编程语言，比如你们可能熟悉的C或C++，通常情况下，认为作为函数返回值只能是一个值，但Octave 的语法结构就不一样，可以返回多个值。 如果我键入 [a,b] = SquareAndCubeThisNumber(5)，然后，{% raw %}$a${% endraw %}就等于25，{% raw %}$b${% endraw %} 就等于5的立方125。 所以说如果你需要定义一个函数并且返回多个值，这一点常常会带来很多方便。 最后，我来给大家演示一下一个更复杂一点的函数的例子。 比方说，我有一个数据集，像这样，数据点为[1,1], [2,2],[3,3]，我想做的事是定义一个 Octave 函数来计算代价函数 {% raw %}$J(\theta)${% endraw %}，就是计算不同 {% raw %}$\theta${% endraw %}值所对应的代价函数值{% raw %}$J${% endraw %}。 首先让我们把数据放到 Octave 里，我把我的矩阵设置为X = [1 1; 1 2; 1 3]; 请仔细看一下这个函数的定义，确保你明白了定义中的每一步。 现在当我在 Octave 里运行时，我键入 J = costFunctionJ (X, y, theta)，它就计算出 {% raw %}$J${% endraw %}等于0，这是因为如果我的数据集{% raw %}$x${% endraw %} 为 [1;2;3]， {% raw %}$y${% endraw %} 也为 [1;2;3] 然后设置 {% raw %}$\theta_0${% endraw %} 等于0，{% raw %}$\theta_1${% endraw %}等于1，这给了我恰好45度的斜线，这条线是可以完美拟合我的数据集的。 而相反地，如果我设置{% raw %}$\theta${% endraw %} 等于[0;0]，那么这个假设就是0是所有的预测值，和刚才一样，设置{% raw %}$\theta_0${% endraw %} = 0，{% raw %}$\theta_1${% endraw %}也等于0，然后我计算的代价函数，结果是2.333。实际上，他就等于1的平方，也就是第一个样本的平方误差，加上2的平方，加上3的平方，然后除以{% raw %}$2m${% endraw %}，也就是训练样本数的两倍，这就是2.33。 因此这也反过来验证了我们这里的函数，计算出了正确的代价函数。这些就是我们用简单的训练样本尝试的几次试验，这也可以作为我们对定义的代价函数{% raw %}$J${% endraw %}进行了完整性检查。确实是可以计算出正确的代价函数的。至少基于这里的 {% raw %}$x${% endraw %}和 {% raw %}$y${% endraw %}是成立的。也就是我们这几个简单的训练集，至少是成立的。 现在你知道如何在 Octave 环境下写出正确的控制语句，比如 for 循环、while 循环和 if语句，以及如何定义和使用函数。 在接下来的Octave 教程视频里，我会讲解一下向量化，这是一种可以使你的 Octave程序运行非常快的思想。 向量化参考视频: 5 - 6 - Vectorization (14 min).mkv 在这段视频中，我将介绍有关向量化的内容，无论你是用Octave，还是别的语言，比如MATLAB或者你正在用Python、NumPy 或 Java C C++，所有这些语言都具有各种线性代数库，这些库文件都是内置的，容易阅读和获取，他们通常写得很好，已经经过高度优化，通常是数值计算方面的博士或者专业人士开发的。 而当你实现机器学习算法时，如果你能好好利用这些线性代数库，或者数值线性代数库，并联合调用它们，而不是自己去做那些函数库可以做的事情。如果是这样的话，那么通常你会发现：首先，这样更有效，也就是说运行速度更快，并且更好地利用你的计算机里可能有的一些并行硬件系统等等；其次，这也意味着你可以用更少的代码来实现你需要的功能。因此，实现的方式更简单，代码出现问题的有可能性也就越小。 举个具体的例子：与其自己写代码做矩阵乘法。如果你只在Octave中输入{% raw %}$a${% endraw %}乘以{% raw %}$b${% endraw %}就是一个非常有效的两个矩阵相乘的程序。有很多例子可以说明，如果你用合适的向量化方法来实现，你就会有一个简单得多，也有效得多的代码。 让我们来看一些例子：这是一个常见的线性回归假设函数：{% raw %}${ {h}_{\theta } }(x)=\sum\limits_{j=0}^{n}{ {{\theta }_{j} }{ {x}_{j} }}${% endraw %} 如果你想要计算{% raw %}$h_\theta(x)${% endraw %} ，注意到右边是求和，那么你可以自己计算{% raw %}$j = 0${% endraw %} 到{% raw %}$ j = n${% endraw %} 的和。但换另一种方式来想想，把 {% raw %}$h_\theta(x)${% endraw %} 看作{% raw %}$\theta^Tx${% endraw %}，那么你就可以写成两个向量的内积，其中{% raw %}$\theta${% endraw %}就是{% raw %}$\theta_0${% endraw %}、{% raw %}$\theta_1${% endraw %}、{% raw %}$\theta_2${% endraw %}，如果你有两个特征量，如果 {% raw %}$n = 2${% endraw %}，并且如果你把 {% raw %}$x${% endraw %} 看作{% raw %}$x_0${% endraw %}、{% raw %}$x_1${% endraw %}、{% raw %}$x_2${% endraw %}，这两种思考角度，会给你两种不同的实现方式。 比如说，这是未向量化的代码实现方式： 计算{% raw %}$h_\theta(x)${% endraw %}是未向量化的，我们可能首先要初始化变量 {% raw %}$prediction${% endraw %} 的值为0.0，而这个变量{% raw %}$prediction${% endraw %} 的最终结果就是{% raw %}$h_\theta(x)${% endraw %}，然后我要用一个 for 循环，{% raw %}$j${% endraw %} 取值 0 到{% raw %}$n+1${% endraw %}，变量{% raw %}$prediction${% endraw %} 每次就通过自身加上{% raw %}$ theta(j) ${% endraw %}乘以 {% raw %}$x(j)${% endraw %}更新值，这个就是算法的代码实现。 顺便我要提醒一下，这里的向量我用的下标是0，所以我有{% raw %}$\theta_0${% endraw %}、{% raw %}$\theta_1${% endraw %}、{% raw %}$\theta_2${% endraw %}，但因为MATLAB的下标从1开始，在 MATLAB 中{% raw %}$\theta_0${% endraw %}，我们可能会用 {% raw %}$theta(1)${% endraw %} 来表示，这第二个元素最后就会变成，{% raw %}$theta(2${% endraw %}) 而第三个元素，最终可能就用{% raw %}$theta(3)${% endraw %}表示，因为MATLAB中的下标从1开始，这就是为什么这里我的 for 循环，{% raw %}$j${% endraw %}取值从 1 直到{% raw %}$n+1${% endraw %}，而不是从 0 到 {% raw %}$n${% endraw %}。这是一个未向量化的代码实现方式，我们用一个 for 循环对 {% raw %}$n${% endraw %} 个元素进行加和。 作为比较，接下来是向量化的代码实现： 你把x和{% raw %}$\theta${% endraw %}看做向量，而你只需要令变量{% raw %}$prediction${% endraw %}等于{% raw %}$theta${% endraw %}转置乘以{% raw %}$x${% endraw %}，你就可以这样计算。与其写所有这些for循环的代码，你只需要一行代码，这行代码就是利用 Octave 的高度优化的数值，线性代数算法来计算两个向量{% raw %}$\theta${% endraw %}以及{% raw %}$x${% endraw %}的内积，这样向量化的实现更简单，它运行起来也将更加高效。这就是 Octave 所做的而向量化的方法，在其他编程语言中同样可以实现。 让我们来看一个C++ 的例子： 与此相反，使用较好的C++ 数值线性代数库，你可以写出像右边这样的代码，因此取决于你的数值线性代数库的内容。你只需要在C++ 中将两个向量相乘，根据你所使用的数值和线性代数库的使用细节的不同，你最终使用的代码表达方式可能会有些许不同，但是通过一个库来做内积，你可以得到一段更简单、更有效的代码。 现在，让我们来看一个更为复杂的例子，这是线性回归算法梯度下降的更新规则： 我们用这条规则对{% raw %}$ j${% endraw %} 等于 0、1、2等等的所有值，更新对象{% raw %}$\theta_j${% endraw %}，我只是用{% raw %}$\theta_0${% endraw %}、{% raw %}$\theta_1${% endraw %}、{% raw %}$\theta_2${% endraw %}来写方程，假设我们有两个特征量，所以{% raw %}$n${% endraw %}等于2，这些都是我们需要对{% raw %}$\theta_0${% endraw %}、{% raw %}$\theta_1${% endraw %}、{% raw %}$\theta_2${% endraw %}进行更新，这些都应该是同步更新，我们用一个向量化的代码实现，这里是和之前相同的三个方程，只不过写得小一点而已。 你可以想象实现这三个方程的方式之一，就是用一个 for 循环，就是让 {% raw %}$j${% endraw %}等于0、等于1、等于2，来更新{% raw %}$\theta_j${% endraw %}。但让我们用向量化的方式来实现，看看我们是否能够有一个更简单的方法。基本上用三行代码或者一个for 循环，一次实现这三个方程。让我们来看看怎样能用这三步，并将它们压缩成一行向量化的代码来实现。做法如下： 我打算把{% raw %}$\theta${% endraw %}看做一个向量，然后我用{% raw %}$\theta${% endraw %}-{% raw %}$\alpha${% endraw %} 乘以某个别的向量{% raw %}$\delta${% endraw %} 来更新{% raw %}$\theta${% endraw %}。 这里的 {% raw %}$\delta${% endraw %} 等于 让我解释一下是怎么回事：我要把{% raw %}$\theta${% endraw %}看作一个向量，有一个 {% raw %}$n+1${% endraw %} 维向量，{% raw %}$\alpha${% endraw %} 是一个实数，{% raw %}$\delta${% endraw %}在这里是一个向量。 所以这个减法运算是一个向量减法，因为 {% raw %}$\alpha${% endraw %} 乘以 δ是一个向量，所以{% raw %}$\theta${% endraw %}就是{% raw %}$\theta${% endraw %} - {% raw %}$\alpha \delta${% endraw %}得到的向量。 那么什么是向量 {% raw %}$\delta${% endraw %} 呢 ? {% raw %}$X^{(i)}${% endraw %}是一个向量 你就会得到这些不同的式子，然后作加和。 实际上，在以前的一个小测验，如果你要解这个方程，我们说过为了向量化这段代码，我们会令u = 2v +5w因此，我们说向量{% raw %}$u${% endraw %}等于2乘以向量{% raw %}$v${% endraw %}加上5乘以向量{% raw %}$w${% endraw %}。用这个例子说明，如何对不同的向量进行相加，这里的求和是同样的道理。 这就是为什么我们能够向量化地实现线性回归。 所以，我希望步骤是有逻辑的。请务必看视频，并且保证你确实能理解它。如果你实在不能理解它们数学上等价的原因，你就直接实现这个算法，也是能得到正确答案的。所以即使你没有完全理解为何是等价的，如果只是实现这种算法，你仍然能实现线性回归算法。如果你能弄清楚为什么这两个步骤是等价的，那我希望你可以对向量化有一个更好的理解，如果你在实现线性回归的时候，使用一个或两个以上的特征量。 有时我们使用几十或几百个特征量来计算线性归回，当你使用向量化地实现线性回归，通常运行速度就会比你以前用你的for循环快的多，也就是自己写代码更新{% raw %}$\theta_0${% endraw %}、{% raw %}$\theta_1${% endraw %}、{% raw %}$\theta_2${% endraw %}。 因此使用向量化实现方式，你应该是能够得到一个高效得多的线性回归算法。而当你向量化我们将在之后的课程里面学到的算法，这会是一个很好的技巧，无论是对于Octave 或者一些其他的语言 如C++、Java 来让你的代码运行得更高效。 逻辑回归(Logistic Regression)分类问题参考文档: 6 - 1 - Classification (8 min).mkv 在这个以及接下来的几个视频中，开始介绍分类问题。 在分类问题中，你要预测的变量 {% raw %}$y${% endraw %} 是离散的值，我们将学习一种叫做逻辑回归 (Logistic Regression) 的算法，这是目前最流行使用最广泛的一种学习算法。 在分类问题中，我们尝试预测的是结果是否属于某一个类（例如正确或错误）。分类问题的例子有：判断一封电子邮件是否是垃圾邮件；判断一次金融交易是否是欺诈；之前我们也谈到了肿瘤分类问题的例子，区别一个肿瘤是恶性的还是良性的。 我们从二元的分类问题开始讨论。 我们将因变量(dependent variable)可能属于的两个类分别称为负向类（negative class）和正向类（positive class），则因变量{% raw %}$y\in { 0,1 \\}${% endraw %} ，其中 0 表示负向类，1 表示正向类。 如果我们要用线性回归算法来解决一个分类问题，对于分类， {% raw %}$y${% endraw %} 取值为 0 或者1，但如果你使用的是线性回归，那么假设函数的输出值可能远大于 1，或者远小于0，即使所有训练样本的标签 {% raw %}$y${% endraw %} 都等于 0 或 1。尽管我们知道标签应该取值0 或者1，但是如果算法得到的值远大于1或者远小于0的话，就会感觉很奇怪。所以我们在接下来的要研究的算法就叫做逻辑回归算法，这个算法的性质是：它的输出值永远在0到 1 之间。 顺便说一下，逻辑回归算法是分类算法，我们将它作为分类算法使用。有时候可能因为这个算法的名字中出现了“回归”使你感到困惑，但逻辑回归算法实际上是一种分类算法，它适用于标签 {% raw %}$y${% endraw %} 取值离散的情况，如：1 0 0 1。 在接下来的视频中，我们将开始学习逻辑回归算法的细节。 假说表示参考视频: 6 - 2 - Hypothesis Representation (7 min).mkv 在这段视频中，我要给你展示假设函数的表达式，也就是说，在分类问题中，要用什么样的函数来表示我们的假设。此前我们说过，希望我们的分类器的输出值在0和1之间，因此，我们希望想出一个满足某个性质的假设函数，这个性质是它的预测值要在0和1之间。 回顾在一开始提到的乳腺癌分类问题，我们可以用线性回归的方法求出适合数据的一条直线： 根据线性回归模型我们只能预测连续的值，然而对于分类问题，我们需要输出0或1，我们可以预测： 当{% raw %}${h_\theta}\left( x \right)>=0.5${% endraw %}时，预测 {% raw %}$y=1${% endraw %}。 当{% raw %}${h_\theta}\left( x \right)&lt;0.5${% endraw %}时，预测 {% raw %}$y=0${% endraw %} 。 对于上图所示的数据，这样的一个线性模型似乎能很好地完成分类任务。假使我们又观测到一个非常大尺寸的恶性肿瘤，将其作为实例加入到我们的训练集中来，这将使得我们获得一条新的直线。 这时，再使用0.5作为阀值来预测肿瘤是良性还是恶性便不合适了。可以看出，线性回归模型，因为其预测的值可以超越[0,1]的范围，并不适合解决这样的问题。 我们引入一个新的模型，逻辑回归，该模型的输出变量范围始终在0和1之间。逻辑回归模型的假设是： {% raw %}$h_\theta \left( x \right)=g\left(\theta^{T}X \right)${% endraw %}其中： {% raw %}$X${% endraw %} 代表特征向量 {% raw %}$g${% endraw %} 代表逻辑函数（**logistic function**)是一个常用的逻辑函数为**S**形函数（**Sigmoid function**），公式为： {% raw %}$g\left( z \right)=\frac{1}{1+{ {e}^{-z} }}${% endraw %}。 python代码实现： import numpy as np def sigmoid(z): return 1 / (1 + np.exp(-z)) 该函数的图像为： 合起来，我们得到逻辑回归模型的假设： 对模型的理解： $g\left( z \right)=\frac{1}{1+{ {e}^{-z} }}$。 $h_\theta \left( x \right)$的作用是，对于给定的输入变量，根据选择的参数计算输出变量=1的可能性（**estimated probablity**）即$h_\theta \left( x \right)=P\left( y=1|x;\theta \right)$ 例如，如果对于给定的$x$，通过已经确定的参数计算得出$h_\theta \left( x \right)=0.7$，则表示有70%的几率$y$为正向类，相应地$y$为负向类的几率为1-0.7=0.3。 判定边界参考视频: 6 - 3 - Decision Boundary (15 min).mkv 现在讲下决策边界(decision boundary)的概念。这个概念能更好地帮助我们理解逻辑回归的假设函数在计算什么。 在逻辑回归中，我们预测： 当${h_\theta}\left( x \right)>=0.5$时，预测 $y=1$。 当${h_\theta}\left( x \right)&lt;0.5$时，预测 $y=0$ 。 根据上面绘制出的 S 形函数图像，我们知道当 $z=0$ 时 $g(z)=0.5$ $z>0$ 时 $g(z)>0.5$ $z&lt;0$ 时 $g(z)&lt;0.5$ 又 $z={\theta^{T} }x$ ，即： ${\theta^{T} }x>=0$ 时，预测 $y=1$ ${\theta^{T} }x&lt;0$ 时，预测 $y=0$ 现在假设我们有一个模型： 并且参数$\theta$ 是向量[-3 1 1]。 则当$-3+{x_1}+{x_2} \geq 0$，即${x_1}+{x_2} \geq 3$时，模型将预测 $y=1$。我们可以绘制直线${x_1}+{x_2} = 3$，这条线便是我们模型的分界线，将预测为1的区域和预测为 0的区域分隔开。 假使我们的数据呈现这样的分布情况，怎样的模型才能适合呢？ 因为需要用曲线才能分隔 $y=0$ 的区域和 $y=1$ 的区域，我们需要二次方特征：${h_\theta}\left( x \right)=g\left( {\theta_0}+{\theta_1}{x_1}+{\theta_{2} }{x_{2} }+{\theta_{3} }x_{1}^{2}+{\theta_{4} }x_{2}^{2} \right)$是[-1 0 0 1 1]，则我们得到的判定边界恰好是圆点在原点且半径为1的圆形。 我们可以用非常复杂的模型来适应非常复杂形状的判定边界。 代价函数参考视频: 6 - 4 - Cost Function (11 min).mkv 在这段视频中，我们要介绍如何拟合逻辑回归模型的参数$\theta$。具体来说，我要定义用来拟合参数的优化目标或者叫代价函数，这便是监督学习问题中的逻辑回归模型的拟合问题。 对于线性回归模型，我们定义的代价函数是所有模型误差的平方和。理论上来说，我们也可以对逻辑回归模型沿用这个定义，但是问题在于，当我们将${h_\theta}\left( x \right)=\frac{1}{1+{e^{-\theta^{T}x} }}$带入到这样定义了的代价函数中时，我们得到的代价函数将是一个非凸函数（non-convexfunction）。 这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。 线性回归的代价函数为：$J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{\frac{1}{2}{ {\left( {h_\theta}\left({x}^{\left( i \right)} \right)-{y}^{\left( i \right)} \right)}^{2} }}$ 。我们重新定义逻辑回归的代价函数为：$J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{ {Cost}\left( {h_\theta}\left( {x}^{\left( i \right)} \right),{y}^{\left( i \right)} \right)}$，其中 ${h_\theta}\left( x \right)$与 $Cost\left( {h_\theta}\left( x \right),y \right)$之间的关系如下图所示： 这样构建的$Cost\left( {h_\theta}\left( x \right),y \right)$函数的特点是：当实际的 $y=1$ 且${h_\theta}\left( x \right)$也为 1 时误差为 0，当 $y=1$ 但${h_\theta}\left( x \right)$不为1时误差随着${h_\theta}\left( x \right)$变小而变大；当实际的 $y=0$ 且${h_\theta}\left( x \right)$也为 0 时代价为 0，当$y=0$ 但${h_\theta}\left( x \right)$不为 0时误差随着 ${h_\theta}\left( x \right)$的变大而变大。将构建的 $Cost\left( {h_\theta}\left( x \right),y \right)$简化如下： $Cost\left( {h_\theta}\left( x \right),y \right)=-y\times log\left( {h_\theta}\left( x \right) \right)-(1-y)\times log\left( 1-{h_\theta}\left( x \right) \right)$ 带入代价函数得到： $J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{ {y}^{(i)} }\log \left( {h_\theta}\left( { {x}^{(i)} } \right) \right)-\left( 1-{ {y}^{(i)} } \right)\log \left( 1-{h_\theta}\left( { {x}^{(i)} } \right) \right)]}$ 即：$J\left( \theta \right)=-\frac{1}{m}\sum\limits_{i=1}^{m}{[{ {y}^{(i)} }\log \left( {h_\theta}\left( { {x}^{(i)} } \right) \right)+\left( 1-{ {y}^{(i)} } \right)\log \left( 1-{h_\theta}\left( { {x}^{(i)} } \right) \right)]}$ Python代码实现： import numpy as np def cost(theta, X, y): theta = np.matrix(theta) X = np.matrix(X) y = np.matrix(y) first = np.multiply(-y, np.log(sigmoid(X* theta.T))) second = np.multiply((1 - y), np.log(1 - sigmoid(X* theta.T))) return np.sum(first - second) / (len(X)) 在得到这样一个代价函数以后，我们便可以用梯度下降算法来求得能使代价函数最小的参数了。算法为： Repeat { $\theta_j := \theta_j - \alpha \frac{\partial}{\partial\theta_j} J(\theta)$ (simultaneously update all )} 求导后得到： Repeat { $\theta_j := \theta_j - \alpha \frac{1}{m}\sum\limits_{i=1}^{m}{ {\left( {h_\theta}\left( \mathop{x}^{\left( i \right)} \right)-\mathop{y}^{\left( i \right)} \right)} }\mathop{x}_{j}^{(i)}$ (simultaneously update all )} 在这个视频中，我们定义了单训练样本的代价函数，凸性分析的内容是超出这门课的范围的，但是可以证明我们所选的代价值函数会给我们一个凸优化问题。代价函数$J(\theta)$会是一个凸函数，并且没有局部最优值。 推导过程： $J\left( \theta \right)=-\frac{1}{m}\sum\limits_{i=1}^{m}{[{ {y}^{(i)} }\log \left( {h_\theta}\left( { {x}^{(i)} } \right) \right)+\left( 1-{ {y}^{(i)} } \right)\log \left( 1-{h_\theta}\left( { {x}^{(i)} } \right) \right)]}$ 考虑： ${h_\theta}\left( { {x}^{(i)} } \right)=\frac{1}{1+{ {e}^{-{\theta^T}{ {x}^{(i)} }} }}$ 则： ${ {y}^{(i)} }\log \left( {h_\theta}\left( { {x}^{(i)} } \right) \right)+\left( 1-{ {y}^{(i)} } \right)\log \left( 1-{h_\theta}\left( { {x}^{(i)} } \right) \right)$ $={ {y}^{(i)} }\log \left( \frac{1}{1+{ {e}^{-{\theta^T}{ {x}^{(i)} }} }} \right)+\left( 1-{ {y}^{(i)} } \right)\log \left( 1-\frac{1}{1+{ {e}^{-{\theta^T}{ {x}^{(i)} }} }} \right)$ $=-{ {y}^{(i)} }\log \left( 1+{ {e}^{-{\theta^T}{ {x}^{(i)} }} } \right)-\left( 1-{ {y}^{(i)} } \right)\log \left( 1+{ {e}^{ {\theta^T}{ {x}^{(i)} }} } \right)$ 所以： $\frac{\partial }{\partial {\theta_{j} }}J\left( \theta \right)=\frac{\partial }{\partial {\theta_{j} }}[-\frac{1}{m}\sum\limits_{i=1}^{m}{[-{ {y}^{(i)} }\log \left( 1+{ {e}^{-{\theta^{T} }{ {x}^{(i)} }} } \right)-\left( 1-{ {y}^{(i)} } \right)\log \left( 1+{ {e}^{ {\theta^{T} }{ {x}^{(i)} }} } \right)]}]$ $=-\frac{1}{m}\sum\limits_{i=1}^{m}{[-{ {y}^{(i)} }\frac{-x_{j}^{(i)}{ {e}^{-{\theta^{T} }{ {x}^{(i)} }} }}{1+{ {e}^{-{\theta^{T} }{ {x}^{(i)} }} }}-\left( 1-{ {y}^{(i)} } \right)\frac{x_j^{(i)}{ {e}^{ {\theta^T}{ {x}^{(i)} }} }}{1+{ {e}^{ {\theta^T}{ {x}^{(i)} }} }} }]$ $=-\frac{1}{m}\sum\limits_{i=1}^{m}{ {y}^{(i)} }\frac{x_j^{(i)} }{1+{ {e}^{ {\theta^T}{ {x}^{(i)} }} }}-\left( 1-{ {y}^{(i)} } \right)\frac{x_j^{(i)}{ {e}^{ {\theta^T}{ {x}^{(i)} }} }}{1+{ {e}^{ {\theta^T}{ {x}^{(i)} }} }}]$ $=-\frac{1}{m}\sum\limits_{i=1}^{m}{\frac{ {{y}^{(i)} }x_j^{(i)}-x_j^{(i)}{ {e}^{ {\theta^T}{ {x}^{(i)} }} }+{ {y}^{(i)} }x_j^{(i)}{ {e}^{ {\theta^T}{ {x}^{(i)} }} }}{1+{ {e}^{ {\theta^T}{ {x}^{(i)} }} }} }$ $=-\frac{1}{m}\sum\limits_{i=1}^{m}{\frac{ {{y}^{(i)} }\left( 1\text{+}{ {e}^{ {\theta^T}{ {x}^{(i)} }} } \right)-{ {e}^{ {\theta^T}{ {x}^{(i)} }} }}{1+{ {e}^{ {\theta^T}{ {x}^{(i)} }} }}x_j^{(i)} }$ $=-\frac{1}{m}\sum\limits_{i=1}^{m}{({ {y}^{(i)} }-\frac{ {{e}^{ {\theta^T}{ {x}^{(i)} }} }}{1+{ {e}^{ {\theta^T}{ {x}^{(i)} }} }})x_j^{(i)} }$ $=-\frac{1}{m}\sum\limits_{i=1}^{m}{({ {y}^{(i)} }-\frac{1}{1+{ {e}^{-{\theta^T}{ {x}^{(i)} }} }})x_j^{(i)} }$ $=-\frac{1}{m}\sum\limits_{i=1}^{m}{[{ {y}^{(i)} }-{h_\theta}\left( { {x}^{(i)} } \right)]x_j^{(i)} }$ $=\frac{1}{m}\sum\limits_{i=1}^{m}{[{h_\theta}\left( { {x}^{(i)} } \right)-{ {y}^{(i)} }]x_j^{(i)} }$ 注：虽然得到的梯度下降算法表面上看上去与线性回归的梯度下降算法一样，但是这里的${h_\theta}\left( x \right)=g\left( {\theta^T}X \right)$与线性回归中不同，所以实际上是不一样的。另外，在运行梯度下降算法之前，进行特征缩放依旧是非常必要的。 一些梯度下降算法之外的选择：除了梯度下降算法以外，还有一些常被用来令代价函数最小的算法，这些算法更加复杂和优越，而且通常不需要人工选择学习率，通常比梯度下降算法要更加快速。这些算法有：共轭梯度（Conjugate Gradient），局部优化法(Broyden fletcher goldfarb shann,BFGS)和有限内存局部优化法(LBFGS) ，fminunc是 matlab和octave 中都带的一个最小值优化函数，使用时我们需要提供代价函数和每个参数的求导，下面是 octave 中使用 fminunc 函数的代码示例： function [jVal, gradient] = costFunction(theta) jVal = [...code to compute J(theta)...]; gradient = [...code to compute derivative of J(theta)...]; end options = optimset(&#39;GradObj&#39;, &#39;on&#39;, &#39;MaxIter&#39;, &#39;100&#39;); initialTheta = zeros(2,1); [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options); 在下一个视频中，我们会把单训练样本的代价函数的这些理念进一步发展，然后给出整个训练集的代价函数的定义，我们还会找到一种比我们目前用的更简单的写法，基于这些推导出的结果，我们将应用梯度下降法得到我们的逻辑回归算法。 简化的成本函数和梯度下降参考视频: 6 - 5 - Simplified Cost Function and Gradient Descent (10 min).mkv 在这段视频中，我们将会找出一种稍微简单一点的方法来写代价函数，来替换我们现在用的方法。同时我们还要弄清楚如何运用梯度下降法，来拟合出逻辑回归的参数。因此，听了这节课，你就应该知道如何实现一个完整的逻辑回归算法。 这就是逻辑回归的代价函数： 这个式子可以合并成： $Cost\left( {h_\theta}\left( x \right),y \right)=-y\times log\left( {h_\theta}\left( x \right) \right)-(1-y)\times log\left( 1-{h_\theta}\left( x \right) \right)$ 即，逻辑回归的代价函数： $Cost\left( {h_\theta}\left( x \right),y \right)=-y\times log\left( {h_\theta}\left( x \right) \right)-(1-y)\times log\left( 1-{h_\theta}\left( x \right) \right)$ $=-\frac{1}{m}\sum\limits_{i=1}^{m}{[{ {y}^{(i)} }\log \left( {h_\theta}\left( { {x}^{(i)} } \right) \right)+\left( 1-{ {y}^{(i)} } \right)\log \left( 1-{h_\theta}\left( { {x}^{(i)} } \right) \right)]}$ 根据这个代价函数，为了拟合出参数，该怎么做呢？我们要试图找尽量让$J\left( \theta \right)$ 取得最小值的参数$\theta $。 $\underset{\theta}{\min }J\left( \theta \right)$ 所以我们想要尽量减小这一项，这将我们将得到某个参数$\theta $。如果我们给出一个新的样本，假如某个特征 $x$，我们可以用拟合训练样本的参数$\theta $，来输出对假设的预测。另外，我们假设的输出，实际上就是这个概率值：$p(y=1|x;\theta)$，就是关于 $x$以$\theta $为参数，$y=1$ 的概率，你可以认为我们的假设就是估计 $y=1$ 的概率，所以，接下来就是弄清楚如何最大限度地最小化代价函数$J\left( \theta \right)$，作为一个关于$\theta $的函数，这样我们才能为训练集拟合出参数$\theta $。 最小化代价函数的方法，是使用梯度下降法(gradient descent)。这是我们的代价函数： $J\left( \theta \right)=-\frac{1}{m}\sum\limits_{i=1}^{m}{[{ {y}^{(i)} }\log \left( {h_\theta}\left( { {x}^{(i)} } \right) \right)+\left( 1-{ {y}^{(i)} } \right)\log \left( 1-{h_\theta}\left( { {x}^{(i)} } \right) \right)]}$ 如果我们要最小化这个关于$\theta$的函数值，这就是我们通常用的梯度下降法的模板。 我们要反复更新每个参数，用这个式子来更新，就是用它自己减去学习率 $\alpha$乘以后面的微分项。求导后得到： 如果你计算一下的话，你会得到这个等式： ${\theta_j}:={\theta_j}-\alpha \frac{1}{m}\sum\limits_{i=1}^{m}{({h_\theta}({ {x}^{(i)} })-{ {y}^{(i)} }){x_{j} }^{(i)} }$ 我把它写在这里，将后面这个式子，在 $i=1$ 到 $m$ 上求和，其实就是预测误差乘以$x_j^{(i)}$ ，所以你把这个偏导数项$\frac{\partial }{\partial {\theta_j} }J\left( \theta \right)$放回到原来式子这里，我们就可以将梯度下降算法写作如下形式： ${\theta_j}:={\theta_j}-\alpha \frac{1}{m}\sum\limits_{i=1}^{m}{({h_\theta}({ {x}^{(i)} })-{ {y}^{(i)} }){x_{j} }^{(i)} }$ 所以，如果你有 $n$ 个特征，也就是说：，参数向量$\theta $包括${\theta_{0} }$ ${\theta_{1} }$ ${\theta_{2} }$ 一直到${\theta_{n} }$，那么你就需要用这个式子： ${\theta_j}:={\theta_j}-\alpha \frac{1}{m}\sum\limits_{i=1}^{m}{({h_\theta}({ {x}^{(i)} })-{ {y}^{(i)} }){ {x}_{j} }^{(i)} }$来同时更新所有$\theta $的值。 现在，如果你把这个更新规则和我们之前用在线性回归上的进行比较的话，你会惊讶地发现，这个式子正是我们用来做线性回归梯度下降的。 那么，线性回归和逻辑回归是同一个算法吗？要回答这个问题，我们要观察逻辑回归看看发生了哪些变化。实际上，假设的定义发生了变化。 对于线性回归假设函数： ${h_\theta}\left( x \right)={\theta^T}X={\theta_{0} }{x_{0} }+{\theta_{1} }{x_{1} }+{\theta_{2} }{x_{2} }+...+{\theta_{n} }{x_{n} }$ 而现在逻辑函数假设函数： ${h_\theta}\left( x \right)=\frac{1}{1+{ {e}^{-{\theta^T}X} }}$ 因此，即使更新参数的规则看起来基本相同，但由于假设的定义发生了变化，所以逻辑函数的梯度下降，跟线性回归的梯度下降实际上是两个完全不同的东西。 在先前的视频中，当我们在谈论线性回归的梯度下降法时，我们谈到了如何监控梯度下降法以确保其收敛，我通常也把同样的方法用在逻辑回归中，来监测梯度下降，以确保它正常收敛。 当使用梯度下降法来实现逻辑回归时，我们有这些不同的参数$\theta $，就是${\theta_{0} }$ ${\theta_{1} }$ ${\theta_{2} }$ 一直到${\theta_{n} }$，我们需要用这个表达式来更新这些参数。我们还可以使用 for循环来更新这些参数值，用 for i=1 to n，或者 for i=1 to n+1。当然，不用 for循环也是可以的，理想情况下，我们更提倡使用向量化的实现，可以把所有这些 $n$个参数同时更新。 最后还有一点，我们之前在谈线性回归时讲到的特征缩放，我们看到了特征缩放是如何提高梯度下降的收敛速度的，这个特征缩放的方法，也适用于逻辑回归。如果你的特征范围差距很大的话，那么应用特征缩放的方法，同样也可以让逻辑回归中，梯度下降收敛更快。 就是这样，现在你知道如何实现逻辑回归，这是一种非常强大，甚至可能世界上使用最广泛的一种分类算法。 高级优化参考视频: 6 - 6 - Advanced Optimization (14 min).mkv 在上一个视频中，我们讨论了用梯度下降的方法最小化逻辑回归中代价函数$J\left( \theta \right)$。在本次视频中，我会教你们一些高级优化算法和一些高级的优化概念，利用这些方法，我们就能够使通过梯度下降，进行逻辑回归的速度大大提高，而这也将使算法更加适合解决大型的机器学习问题，比如，我们有数目庞大的特征量。现在我们换个角度来看什么是梯度下降，我们有个代价函数$J\left( \theta \right)$，而我们想要使其最小化，那么我们需要做的是编写代码，当输入参数 $\theta$ 时，它们会计算出两样东西：$J\left( \theta \right)$ 以及$J$ 等于 0、1直到 $n$ 时的偏导数项。 假设我们已经完成了可以实现这两件事的代码，那么梯度下降所做的就是反复执行这些更新。另一种考虑梯度下降的思路是：我们需要写出代码来计算$J\left( \theta \right)$ 和这些偏导数，然后把这些插入到梯度下降中，然后它就可以为我们最小化这个函数。对于梯度下降来说，我认为从技术上讲，你实际并不需要编写代码来计算代价函数$J\left( \theta \right)$。你只需要编写代码来计算导数项，但是，如果你希望代码还要能够监控这些$J\left( \theta \right)$ 的收敛性，那么我们就需要自己编写代码来计算代价函数$J(\theta)$和偏导数项$\frac{\partial }{\partial {\theta_j} }J\left( \theta \right)$。所以，在写完能够计算这两者的代码之后，我们就可以使用梯度下降。然而梯度下降并不是我们可以使用的唯一算法，还有其他一些算法，更高级、更复杂。如果我们能用这些方法来计算代价函数$J\left( \theta \right)$和偏导数项$\frac{\partial }{\partial {\theta_j} }J\left( \theta \right)$两个项的话，那么这些算法就是为我们优化代价函数的不同方法，共轭梯度法 BFGS (变尺度法) 和L-BFGS (限制变尺度法) 就是其中一些更高级的优化算法，它们需要有一种方法来计算 $J\left( \theta \right)$，以及需要一种方法计算导数项，然后使用比梯度下降更复杂的算法来最小化代价函数。这三种算法的具体细节超出了本门课程的范畴。实际上你最后通常会花费很多天，或几周时间研究这些算法，你可以专门学一门课来提高数值计算能力，不过让我来告诉你他们的一些特性： 这三种算法有许多优点： 一个是使用这其中任何一个算法，你通常不需要手动选择学习率 $\alpha$，所以对于这些算法的一种思路是，给出计算导数项和代价函数的方法，你可以认为算法有一个智能的内部循环，而且，事实上，他们确实有一个智能的内部循环，称为线性搜索(line search)算法，它可以自动尝试不同的学习速率 $\alpha$，并自动选择一个好的学习速率 $a$，因此它甚至可以为每次迭代选择不同的学习速率，那么你就不需要自己选择。这些算法实际上在做更复杂的事情，不仅仅是选择一个好的学习速率，所以它们往往最终比梯度下降收敛得快多了，不过关于它们到底做什么的详细讨论，已经超过了本门课程的范围。 实际上，我过去使用这些算法已经很长一段时间了，也许超过十年了，使用得相当频繁，而直到几年前我才真正搞清楚共轭梯度法 BFGS 和 L-BFGS的细节。 我们实际上完全有可能成功使用这些算法，并应用于许多不同的学习问题，而不需要真正理解这些算法的内环间在做什么，如果说这些算法有缺点的话，那么我想说主要缺点是它们比梯度下降法复杂多了，特别是你最好不要使用 L-BGFS、BFGS这些算法，除非你是数值计算方面的专家。实际上，我不会建议你们编写自己的代码来计算数据的平方根，或者计算逆矩阵，因为对于这些算法，我还是会建议你直接使用一个软件库，比如说，要求一个平方根，我们所能做的就是调用一些别人已经写好用来计算数字平方根的函数。幸运的是现在我们有Octave 和与它密切相关的 MATLAB 语言可以使用。 Octave 有一个非常理想的库用于实现这些先进的优化算法，所以，如果你直接调用它自带的库，你就能得到不错的结果。我必须指出这些算法实现得好或不好是有区别的，因此，如果你正在你的机器学习程序中使用一种不同的语言，比如如果你正在使用C、C++、Java等等，你可能会想尝试一些不同的库，以确保你找到一个能很好实现这些算法的库。因为在L-BFGS或者等高线梯度的实现上，表现得好与不太好是有差别的，因此现在让我们来说明：如何使用这些算法： 比方说，你有一个含两个参数的问题，这两个参数是${\theta_{0} }$和${\theta_{1} }$，因此，通过这个代价函数，你可以得到${\theta_{1} }$和 ${\theta_{2} }$的值，如果你将$J\left( \theta \right)$ 最小化的话，那么它的最小值将是${\theta_{1} }=5$ ，${\theta_{2} }=5$。代价函数$J\left( \theta \right)$的导数推出来就是这两个表达式： $\frac{\partial }{\partial { {\theta }_{1} }}J(\theta)=2({ {\theta }_{1} }-5)$ $\frac{\partial }{\partial { {\theta }_{2} }}J(\theta)=2({ {\theta }_{2} }-5)$ 如果我们不知道最小值，但你想要代价函数找到这个最小值，是用比如梯度下降这些算法，但最好是用比它更高级的算法，你要做的就是运行一个像这样的Octave 函数： function [jVal, gradient]=costFunction(theta) jVal=(theta(1)-5)^2+(theta(2)-5)^2; gradient=zeros(2,1); gradient(1)=2*(theta(1)-5); gradient(2)=2*(theta(2)-5); end 这样就计算出这个代价函数，函数返回的第二个值是梯度值，梯度值应该是一个2×1的向量，梯度向量的两个元素对应这里的两个偏导数项，运行这个costFunction 函数后，你就可以调用高级的优化函数，这个函数叫fminunc，它表示Octave 里无约束最小化函数。调用它的方式如下： options=optimset(&#39;GradObj&#39;,&#39;on&#39;,&#39;MaxIter&#39;,100); initialTheta=zeros(2,1); [optTheta, functionVal, exitFlag]=fminunc(@costFunction, initialTheta, options); 你要设置几个options，这个 options 变量作为一个数据结构可以存储你想要的options，所以 GradObj 和On，这里设置梯度目标参数为打开(on)，这意味着你现在确实要给这个算法提供一个梯度，然后设置最大迭代次数，比方说100，我们给出一个$\theta$ 的猜测初始值，它是一个2×1的向量，那么这个命令就调用fminunc，这个@符号表示指向我们刚刚定义的costFunction 函数的指针。如果你调用它，它就会使用众多高级优化算法中的一个，当然你也可以把它当成梯度下降，只不过它能自动选择学习速率$\alpha$，你不需要自己来做。然后它会尝试使用这些高级的优化算法，就像加强版的梯度下降法，为你找到最佳的${\theta}$值。 让我告诉你它在 Octave 里什么样： 所以我写了这个关于theta的 costFunction 函数，它计算出代价函数 jval以及梯度gradient，gradient 有两个元素，是代价函数对于theta(1) 和 theta(2)这两个参数的偏导数。 我希望你们从这个幻灯片中学到的主要内容是：写一个函数，它能返回代价函数值、梯度值，因此要把这个应用到逻辑回归，或者甚至线性回归中，你也可以把这些优化算法用于线性回归，你需要做的就是输入合适的代码来计算这里的这些东西。 现在你已经知道如何使用这些高级的优化算法，有了这些算法，你就可以使用一个复杂的优化库，它让算法使用起来更模糊一点。因此也许稍微有点难调试，不过由于这些算法的运行速度通常远远超过梯度下降。 所以当我有一个很大的机器学习问题时，我会选择这些高级算法，而不是梯度下降。有了这些概念，你就应该能将逻辑回归和线性回归应用于更大的问题中，这就是高级优化的概念。 在下一个视频，我想要告诉你如何修改你已经知道的逻辑回归算法，然后使它在多类别分类问题中也能正常运行。 多类别分类：一对多参考视频: 6 - 7 - Multiclass Classification_ One-vs-all (6 min).mkv 在本节视频中，我们将谈到如何使用逻辑回归 (logistic regression)来解决多类别分类问题，具体来说，我想通过一个叫做”一对多” (one-vs-all) 的分类算法。 先看这样一些例子。 第一个例子：假如说你现在需要一个学习算法能自动地将邮件归类到不同的文件夹里，或者说可以自动地加上标签，那么，你也许需要一些不同的文件夹，或者不同的标签来完成这件事，来区分开来自工作的邮件、来自朋友的邮件、来自家人的邮件或者是有关兴趣爱好的邮件，那么，我们就有了这样一个分类问题：其类别有四个，分别用$y=1$、$y=2$、$y=3$、$y=4$ 来代表。 第二个例子是有关药物诊断的，如果一个病人因为鼻塞来到你的诊所，他可能并没有生病，用 $y=1$ 这个类别来代表；或者患了感冒，用 $y=2$ 来代表；或者得了流感用$y=3$来代表。 第三个例子：如果你正在做有关天气的机器学习分类问题，那么你可能想要区分哪些天是晴天、多云、雨天、或者下雪天，对上述所有的例子，$y$ 可以取一个很小的数值，一个相对”谨慎”的数值，比如1 到3、1到4或者其它数值，以上说的都是多类分类问题，顺便一提的是，对于下标是0 1 2 3，还是 1 2 3 4 都不重要，我更喜欢将分类从 1 开始标而不是0，其实怎样标注都不会影响最后的结果。 然而对于之前的一个，二元分类问题，我们的数据看起来可能是像这样： 对于一个多类分类问题，我们的数据集或许看起来像这样： 我用3种不同的符号来代表3个类别，问题就是给出3个类型的数据集，我们如何得到一个学习算法来进行分类呢？ 我们现在已经知道如何进行二元分类，可以使用逻辑回归，对于直线或许你也知道，可以将数据集一分为二为正类和负类。用一对多的分类思想，我们可以将其用在多类分类问题上。 下面将介绍如何进行一对多的分类工作，有时这个方法也被称为”一对余”方法。 现在我们有一个训练集，好比上图表示的有3个类别，我们用三角形表示 $y=1$，方框表示$y=2$，叉叉表示 $y=3$。我们下面要做的就是使用一个训练集，将其分成3个二元分类问题。 我们先从用三角形代表的类别1开始，实际上我们可以创建一个，新的”伪”训练集，类型2和类型3定为负类，类型1设定为正类，我们创建一个新的训练集，如下图所示的那样，我们要拟合出一个合适的分类器。 这里的三角形是正样本，而圆形代表负样本。可以这样想，设置三角形的值为1，圆形的值为0，下面我们来训练一个标准的逻辑回归分类器，这样我们就得到一个正边界。 为了能实现这样的转变，我们将多个类中的一个类标记为正向类（$y=1$），然后将其他所有类都标记为负向类，这个模型记作$h_\theta^{\left( 1 \right)}\left( x \right)$。接着，类似地第我们选择另一个类标记为正向类（$y=2$），再将其它类都标记为负向类，将这个模型记作 $h_\theta^{\left( 2 \right)}\left( x \right)$,依此类推。最后我们得到一系列的模型简记为： $h_\theta^{\left( i \right)}\left( x \right)=p\left( y=i|x;\theta \right)$其中：$i=\left( 1,2,3....k \right)$ 最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。 总之，我们已经把要做的做完了，现在要做的就是训练这个逻辑回归分类器：$h_\theta^{\left( i \right)}\left( x \right)$， 其中 $i$ 对应每一个可能的 $y=i$，最后，为了做出预测，我们给出输入一个新的 $x$ 值，用这个做预测。我们要做的就是在我们三个分类器里面输入 $x$，然后我们选择一个让 $h_\theta^{\left( i \right)}\left( x \right)$ 最大的$ i$，即$\mathop{\max}\limits_i\,h_\theta^{\left( i \right)}\left( x \right)$。 你现在知道了基本的挑选分类器的方法，选择出哪一个分类器是可信度最高效果最好的，那么就可认为得到一个正确的分类，无论$i$值是多少，我们都有最高的概率值，我们预测$y$就是那个值。这就是多类别分类问题，以及一对多的方法，通过这个小方法，你现在也可以将逻辑回归分类器用在多类分类的问题上。 正则化(Regularization)过拟合的问题参考视频: 7 - 1 - The Problem of Overfitting (10 min).mkv 到现在为止，我们已经学习了几种不同的学习算法，包括线性回归和逻辑回归，它们能够有效地解决许多问题，但是当将它们应用到某些特定的机器学习应用时，会遇到过拟合(over-fitting)的问题，可能会导致它们效果很差。 在这段视频中，我将为你解释什么是过度拟合问题，并且在此之后接下来的几个视频中，我们将谈论一种称为正则化(regularization)的技术，它可以改善或者减少过度拟合问题。 如果我们有非常多的特征，我们通过学习得到的假设可能能够非常好地适应训练集（代价函数可能几乎为0），但是可能会不能推广到新的数据。 下图是一个回归问题的例子： 第一个模型是一个线性模型，欠拟合，不能很好地适应我们的训练集；第三个模型是一个四次方的模型，过于强调拟合原始数据，而丢失了算法的本质：预测新数据。我们可以看出，若给出一个新的值使之预测，它将表现的很差，是过拟合，虽然能非常好地适应我们的训练集但在新输入变量进行预测时可能会效果不好；而中间的模型似乎最合适。 分类问题中也存在这样的问题： 就以多项式理解，$x$ 的次数越高，拟合的越好，但相应的预测的能力就可能变差。 问题是，如果我们发现了过拟合问题，应该如何处理？ 丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如PCA） 正则化。 保留所有的特征，但是减少参数的大小（magnitude）。 代价函数参考视频: 7 - 2 - Cost Function (10 min).mkv 上面的回归问题中如果我们的模型是： ${h_\theta}\left( x \right)={\theta_{0} }+{\theta_{1} }{x_{1} }+{\theta_{2} }{x_{2}^2}+{\theta_{3} }{x_{3}^3}+{\theta_{4} }{x_{4}^4}$ 我们可以从之前的事例中看出，正是那些高次项导致了过拟合的产生，所以如果我们能让这些高次项的系数接近于0的话，我们就能很好的拟合了。所以我们要做的就是在一定程度上减小这些参数$\theta $ 的值，这就是正则化的基本方法。我们决定要减少${\theta_{3} }$和${\theta_{4} }$的大小，我们要做的便是修改代价函数，在其中${\theta_{3} }$和${\theta_{4} }$ 设置一点惩罚。这样做的话，我们在尝试最小化代价时也需要将这个惩罚纳入考虑中，并最终导致选择较小一些的${\theta_{3} }$和${\theta_{4} }$。修改后的代价函数如下：$\underset{\theta }{\mathop{\min } }\,\frac{1}{2m}[\sum\limits_{i=1}^{m}{ {{\left( { {h}_{\theta } }\left( { {x}^{(i)} } \right)-{ {y}^{(i)} } \right)}^{2} }+1000\theta _{3}^{2}+10000\theta _{4}^{2}]}$ 通过这样的代价函数选择出的${\theta_{3} }$和${\theta_{4} }$ 对预测结果的影响就比之前要小许多。假如我们有非常多的特征，我们并不知道其中哪些特征我们要惩罚，我们将对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设：$J\left( \theta \right)=\frac{1}{2m}[\sum\limits_{i=1}^{m}{ {{({h_\theta}({ {x}^{(i)} })-{ {y}^{(i)} })}^{2} }+\lambda \sum\limits_{j=1}^{n}{\theta_{j}^{2} }]}$ 其中$\lambda $又称为正则化参数（Regularization Parameter）。 注：根据惯例，我们不对${\theta_{0} }$ 进行惩罚。经过正则化处理的模型与原模型的可能对比如下图所示： 如果选择的正则化参数$\lambda$ 过大，则会把所有的参数都最小化了，导致模型变成 ${h_\theta}\left( x \right)={\theta_{0} }$，也就是上图中红色直线所示的情况，造成欠拟合。那为什么增加的一项$\lambda =\sum\limits_{j=1}^{n}{\theta_j^{2} }$ 可以使$\theta $的值减小呢？因为如果我们令 $\lambda$ 的值很大的话，为了使Cost Function 尽可能的小，所有的 $\theta $ 的值（不包括${\theta_{0} }$）都会在一定程度上减小。但若$\lambda$ 的值太大了，那么$\theta $（不包括${\theta_{0} }$）都会趋近于0，这样我们所得到的只能是一条平行于$x$轴的直线。所以对于正则化，我们要取一个合理的 $\lambda$ 的值，这样才能更好的应用正则化。回顾一下代价函数，为了使用正则化，让我们把这些概念应用到到线性回归和逻辑回归中去，那么我们就可以让他们避免过度拟合了。 正则化线性回归参考视频: 7 - 3 - Regularized Linear Regression (11 min).mkv 对于线性回归的求解，我们之前推导了两种学习算法：一种基于梯度下降，一种基于正规方程。 正则化线性回归的代价函数为： $J\left( \theta \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{[({ {({h_\theta}({ {x}^{(i)} })-{ {y}^{(i)} })}^{2} }+\lambda \sum\limits_{j=1}^{n}{\theta _{j}^{2} })]}$ 如果我们要使用梯度下降法令这个代价函数最小化，因为我们未对$\theta_0$进行正则化，所以梯度下降算法将分两种情形： $Repeat$ $until$ $convergence${ ​ ${\theta_0}:={\theta_0}-a\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({ {x}^{(i)} })-{ {y}^{(i)} })x_{0}^{(i)} })$ ​ ${\theta_j}:={\theta_j}-a[\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({ {x}^{(i)} })-{ {y}^{(i)} })x_{j}^{\left( i \right)} }+\frac{\lambda }{m}{\theta_j}]$ ​ $for$ $j=1,2,...n$ ​ } 对上面的算法中$ j=1,2,...,n$ 时的更新式子进行调整可得： ${\theta_j}:={\theta_j}(1-a\frac{\lambda }{m})-a\frac{1}{m}\sum\limits_{i=1}^{m}{({h_\theta}({ {x}^{(i)} })-{ {y}^{(i)} })x_{j}^{\left( i \right)} }$ 可以看出，正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新规则的基础上令$\theta $值减少了一个额外的值。 我们同样也可以利用正规方程来求解正则化线性回归模型，方法如下所示： 图中的矩阵尺寸为 $(n+1)*(n+1)$。 正则化的逻辑回归模型参考视频: 7 - 4 - Regularized Logistic Regression (9 min).mkv 针对逻辑回归问题，我们在之前的课程已经学习过两种优化算法：我们首先学习了使用梯度下降法来优化代价函数$J\left( \theta \right)$，接下来学习了更高级的优化算法，这些高级优化算法需要你自己设计代价函数$J\left( \theta \right)$。 自己计算导数同样对于逻辑回归，我们也给代价函数增加一个正则化的表达式，得到代价函数： $J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{ {y}^{(i)} }\log \left( {h_\theta}\left( { {x}^{(i)} } \right) \right)-\left( 1-{ {y}^{(i)} } \right)\log \left( 1-{h_\theta}\left( { {x}^{(i)} } \right) \right)]}+\frac{\lambda }{2m}\sum\limits_{j=1}^{n}{\theta _{j}^{2} }$ Python代码： import numpy as np def costReg(theta, X, y, learningRate): theta = np.matrix(theta) X = np.matrix(X) y = np.matrix(y) first = np.multiply(-y, np.log(sigmoid(X*theta.T))) second = np.multiply((1 - y), np.log(1 - sigmoid(X*theta.T))) reg = (learningRate / (2 * len(X))* np.sum(np.power(theta[:,1:theta.shape[1]],2)) return np.sum(first - second) / (len(X)) + reg 要最小化该代价函数，通过求导，得出梯度下降算法为： $Repeat$ $until$ $convergence${ ​ ${\theta_0}:={\theta_0}-a\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({ {x}^{(i)} })-{ {y}^{(i)} })x_{0}^{(i)} })$ ​ ${\theta_j}:={\theta_j}-a[\frac{1}{m}\sum\limits_{i=1}^{m}{({h_\theta}({ {x}^{(i)} })-{ {y}^{(i)} })x_{j}^{\left( i \right)} }+\frac{\lambda }{m}{\theta_j}]$ ​ $for$ $j=1,2,...n$ ​ } 注：看上去同线性回归一样，但是知道 ${h_\theta}\left( x \right)=g\left( {\theta^T}X \right)$，所以与线性回归不同。Octave 中，我们依旧可以用 fminuc 函数来求解代价函数最小化的参数，值得注意的是参数${\theta_{0} }$的更新规则与其他情况不同。注意： 虽然正则化的逻辑回归中的梯度下降和正则化的线性回归中的表达式看起来一样，但由于两者的${h_\theta}\left( x \right)$不同所以还是有很大差别。 ${\theta_{0} }$不参与其中的任何一个正则化。 目前大家对机器学习算法可能还只是略懂，但是一旦你精通了线性回归、高级优化算法和正则化技术，坦率地说，你对机器学习的理解可能已经比许多工程师深入了。现在，你已经有了丰富的机器学习知识，目测比那些硅谷工程师还厉害，或者用机器学习算法来做产品。 接下来的课程中，我们将学习一个非常强大的非线性分类器，无论是线性回归问题，还是逻辑回归问题，都可以构造多项式来解决。你将逐渐发现还有更强大的非线性分类器，可以用来解决多项式回归问题。我们接下来将将学会，比现在解决问题的方法强大N倍的学习算法。 神经网络：表述(Neural Networks: Representation)非线性假设参考视频: 8 - 1 - Non-linear Hypotheses (10 min).mkv 我们之前学的，无论是线性回归还是逻辑回归都有这样一个缺点，即：当特征太多时，计算的负荷会非常大。 下面是一个例子： 当我们使用$x_1$, $x_2$ 的多次项式进行预测时，我们可以应用的很好。之前我们已经看到过，使用非线性的多项式项，能够帮助我们建立更好的分类模型。假设我们有非常多的特征，例如大于100个变量，我们希望用这100个特征来构建一个非线性的多项式模型，结果将是数量非常惊人的特征组合，即便我们只采用两两特征的组合$(x_1x_2+x_1x_3+x_1x_4+...+x_2x_3+x_2x_4+...+x_{99}x_{100})$，我们也会有接近5000个组合而成的特征。这对于一般的逻辑回归来说需要计算的特征太多了。 假设我们希望训练一个模型来识别视觉对象（例如识别一张图片上是否是一辆汽车），我们怎样才能这么做呢？一种方法是我们利用很多汽车的图片和很多非汽车的图片，然后利用这些图片上一个个像素的值（饱和度或亮度）来作为特征。 假如我们只选用灰度图片，每个像素则只有一个值（而非 RGB值），我们可以选取图片上的两个不同位置上的两个像素，然后训练一个逻辑回归算法利用这两个像素的值来判断图片上是否是汽车： 假使我们采用的都是50x50像素的小图片，并且我们将所有的像素视为特征，则会有 2500个特征，如果我们要进一步将两两特征组合构成一个多项式模型，则会有约${ {2500}^{2} }/2$个（接近3百万个）特征。普通的逻辑回归模型，不能有效地处理这么多的特征，这时候我们需要神经网络。 神经元和大脑参考视频: 8 - 2 - Neurons and the Brain (8 min).mkv 神经网络是一种很古老的算法，它最初产生的目的是制造能模拟大脑的机器。 在这门课中，我将向你们介绍神经网络。因为它能很好地解决不同的机器学习问题。而不只因为它们在逻辑上行得通，在这段视频中，我想告诉你们一些神经网络的背景知识，由此我们能知道可以用它们来做什么。不管是将其应用到现代的机器学习问题上，还是应用到那些你可能会感兴趣的问题中。也许，这一伟大的人工智能梦想在未来能制造出真正的智能机器。另外，我们还将讲解神经网络是怎么涉及这些问题的神经网络产生的原因是人们想尝试设计出模仿大脑的算法，从某种意义上说如果我们想要建立学习系统，那为什么不去模仿我们所认识的最神奇的学习机器——人类的大脑呢？ 神经网络逐渐兴起于二十世纪八九十年代，应用得非常广泛。但由于各种原因，在90年代的后期应用减少了。但是最近，神经网络又东山再起了。其中一个原因是：神经网络是计算量有些偏大的算法。然而大概由于近些年计算机的运行速度变快，才足以真正运行起大规模的神经网络。正是由于这个原因和其他一些我们后面会讨论到的技术因素，如今的神经网络对于许多应用来说是最先进的技术。当你想模拟大脑时，是指想制造出与人类大脑作用效果相同的机器。大脑可以学会去以看而不是听的方式处理图像，学会处理我们的触觉。 我们能学习数学，学着做微积分，而且大脑能处理各种不同的令人惊奇的事情。似乎如果你想要模仿它，你得写很多不同的软件来模拟所有这些五花八门的奇妙的事情。不过能不能假设大脑做所有这些，不同事情的方法，不需要用上千个不同的程序去实现。相反的，大脑处理的方法，只需要一个单一的学习算法就可以了？尽管这只是一个假设，不过让我和你分享，一些这方面的证据。 大脑的这一部分这一小片红色区域是你的听觉皮层，你现在正在理解我的话，这靠的是耳朵。耳朵接收到声音信号，并把声音信号传递给你的听觉皮层，正因如此，你才能明白我的话。 神经系统科学家做了下面这个有趣的实验，把耳朵到听觉皮层的神经切断。在这种情况下，将其重新接到一个动物的大脑上，这样从眼睛到视神经的信号最终将传到听觉皮层。如果这样做了。那么结果表明听觉皮层将会学会“看”。这里的“看”代表了我们所知道的每层含义。所以，如果你对动物这样做，那么动物就可以完成视觉辨别任务，它们可以看图像，并根据图像做出适当的决定。它们正是通过脑组织中的这个部分完成的。下面再举另一个例子，这块红色的脑组织是你的躯体感觉皮层，这是你用来处理触觉的，如果你做一个和刚才类似的重接实验，那么躯体感觉皮层也能学会“看”。这个实验和其它一些类似的实验，被称为神经重接实验，从这个意义上说，如果人体有同一块脑组织可以处理光、声或触觉信号，那么也许存在一种学习算法，可以同时处理视觉、听觉和触觉，而不是需要运行上千个不同的程序，或者上千个不同的算法来做这些大脑所完成的成千上万的美好事情。也许我们需要做的就是找出一些近似的或实际的大脑学习算法，然后实现它大脑通过自学掌握如何处理这些不同类型的数据。在很大的程度上，可以猜想如果我们把几乎任何一种传感器接入到大脑的几乎任何一个部位的话，大脑就会学会处理它。 下面再举几个例子： 这张图是用舌头学会“看”的一个例子。它的原理是：这实际上是一个名为BrainPort的系统，它现在正在FDA(美国食品和药物管理局)的临床试验阶段，它能帮助失明人士看见事物。它的原理是，你在前额上带一个灰度摄像头，面朝前，它就能获取你面前事物的低分辨率的灰度图像。你连一根线到舌头上安装的电极阵列上，那么每个像素都被映射到你舌头的某个位置上，可能电压值高的点对应一个暗像素电压值低的点。对应于亮像素，即使依靠它现在的功能，使用这种系统就能让你我在几十分钟里就学会用我们的舌头“看”东西。 这是第二个例子，关于人体回声定位或者说人体声纳。你有两种方法可以实现：你可以弹响指，或者咂舌头。不过现在有失明人士，确实在学校里接受这样的培训，并学会解读从环境反弹回来的声波模式—这就是声纳。如果你搜索YouTube之后，就会发现有些视频讲述了一个令人称奇的孩子，他因为癌症眼球惨遭移除，虽然失去了眼球，但是通过打响指，他可以四处走动而不撞到任何东西，他能滑滑板，他可以将篮球投入篮框中。注意这是一个没有眼球的孩子。 第三个例子是触觉皮带，如果你把它戴在腰上，蜂鸣器会响，而且总是朝向北时发出嗡嗡声。它可以使人拥有方向感，用类似于鸟类感知方向的方式。 还有一些离奇的例子： 如果你在青蛙身上插入第三只眼，青蛙也能学会使用那只眼睛。因此，这将会非常令人惊奇。如果你能把几乎任何传感器接入到大脑中，大脑的学习算法就能找出学习数据的方法，并处理这些数据。从某种意义上来说，如果我们能找出大脑的学习算法，然后在计算机上执行大脑学习算法或与之相似的算法，也许这将是我们向人工智能迈进做出的最好的尝试。人工智能的梦想就是：有一天能制造出真正的智能机器。 神经网络可能为我们打开一扇进入遥远的人工智能梦的窗户，但我在这节课中讲授神经网络的原因，主要是对于现代机器学习应用。它是最有效的技术方法。因此在接下来的一些课程中，我们将开始深入到神经网络的技术细节。 模型表示1参考视频: 8 - 3 - Model Representation I (12 min).mkv 为了构建神经网络模型，我们需要首先思考大脑中的神经网络是怎样的？每一个神经元都可以被认为是一个处理单元/神经核（processing unit/Nucleus），它含有许多输入/树突（input/Dendrite），并且有一个输出/轴突（output/Axon）。神经网络是大量神经元相互链接并通过电脉冲来交流的一个网络。 下面是一组神经元的示意图，神经元利用微弱的电流进行沟通。这些弱电流也称作动作电位，其实就是一些微弱的电流。所以如果神经元想要传递一个消息，它就会就通过它的轴突，发送一段微弱电流给其他神经元，这就是轴突。 这里是一条连接到输入神经，或者连接另一个神经元树突的神经，接下来这个神经元接收这条消息，做一些计算，它有可能会反过来将在轴突上的自己的消息传给其他神经元。这就是所有人类思考的模型：我们的神经元把自己的收到的消息进行计算，并向其他神经元传递消息。这也是我们的感觉和肌肉运转的原理。如果你想活动一块肌肉，就会触发一个神经元给你的肌肉发送脉冲，并引起你的肌肉收缩。如果一些感官：比如说眼睛想要给大脑传递一个消息，那么它就像这样发送电脉冲给大脑的。 神经网络模型建立在很多神经元之上，每一个神经元又是一个个学习模型。这些神经元（也叫激活单元，activation unit）采纳一些特征作为输出，并且根据本身的模型提供一个输出。下图是一个以逻辑回归模型作为自身学习模型的神经元示例，在神经网络中，参数又可被成为权重（weight）。 我们设计出了类似于神经元的神经网络，效果如下： 其中$x_1$, $x_2$, $x_3$是输入单元（input units），我们将原始数据输入给它们。 $a_1$, $a_2$, $a_3$是中间单元，它们负责将数据进行处理，然后呈递到下一层。 最后是输出单元，它负责计算${h_\theta}\left( x \right)$。 神经网络模型是许多逻辑单元按照不同层级组织起来的网络，每一层的输出变量都是下一层的输入变量。下图为一个3层的神经网络，第一层成为输入层（Input Layer），最后一层称为输出层（Output Layer），中间一层成为隐藏层（Hidden Layers）。我们为每一层都增加一个偏差单位（bias unit）： 下面引入一些标记法来帮助描述模型： $a_{i}^{\left( j \right)}$ 代表第$j$ 层的第 $i$ 个激活单元。${ {\theta }^{\left( j \right)} }$代表从第 $j$ 层映射到第$ j+1$ 层时的权重的矩阵，例如${ {\theta }^{\left( 1 \right)} }$代表从第一层映射到第二层的权重的矩阵。其尺寸为：以第 $j+1$层的激活单元数量为行数，以第 $j$ 层的激活单元数加一为列数的矩阵。例如：上图所示的神经网络中${ {\theta }^{\left( 1 \right)} }$的尺寸为 3*4。 对于上图所示的模型，激活单元和输出分别表达为： $a_{1}^{(2)}=g(\Theta _{10}^{(1)}{ {x}_{0} }+\Theta _{11}^{(1)}{ {x}_{1} }+\Theta _{12}^{(1)}{ {x}_{2} }+\Theta _{13}^{(1)}{ {x}_{3} })$ $a_{2}^{(2)}=g(\Theta _{20}^{(1)}{ {x}_{0} }+\Theta _{21}^{(1)}{ {x}_{1} }+\Theta _{22}^{(1)}{ {x}_{2} }+\Theta _{23}^{(1)}{ {x}_{3} })$ $a_{3}^{(2)}=g(\Theta _{30}^{(1)}{ {x}_{0} }+\Theta _{31}^{(1)}{ {x}_{1} }+\Theta _{32}^{(1)}{ {x}_{2} }+\Theta _{33}^{(1)}{ {x}_{3} })$ ${ {h}_{\Theta } }(x)=g(\Theta _{10}^{(2)}a_{0}^{(2)}+\Theta _{11}^{(2)}a_{1}^{(2)}+\Theta _{12}^{(2)}a_{2}^{(2)}+\Theta _{13}^{(2)}a_{3}^{(2)})$ 上面进行的讨论中只是将特征矩阵中的一行（一个训练实例）喂给了神经网络，我们需要将整个训练集都喂给我们的神经网络算法来学习模型。 我们可以知道：每一个$a$都是由上一层所有的$x$和每一个$x$所对应的决定的。 （我们把这样从左到右的算法称为前向传播算法( FORWARD PROPAGATION )） 把$x$, $\theta$, $a$ 分别用矩阵表示： 我们可以得到$\theta \cdot X=a$ 。 模型表示2参考视频: 8 - 4 - Model Representation II (12 min).mkv ( FORWARD PROPAGATION )相对于使用循环来编码，利用向量化的方法会使得计算更为简便。以上面的神经网络为例，试着计算第二层的值： 我们令 ${ {z}^{\left( 2 \right)} }={ {\theta }^{\left( 1 \right)} }x$，则 ${ {a}^{\left( 2 \right)} }=g({ {z}^{\left( 2 \right)} })$ ，计算后添加 $a_{0}^{\left( 2 \right)}=1$。 计算输出的值为： 我们令 ${ {z}^{\left( 3 \right)} }={ {\theta }^{\left( 2 \right)} }{ {a}^{\left( 2 \right)} }$，则 $h_\theta(x)={ {a}^{\left( 3 \right)} }=g({ {z}^{\left( 3 \right)} })$。这只是针对训练集中一个训练实例所进行的计算。如果我们要对整个训练集进行计算，我们需要将训练集特征矩阵进行转置，使得同一个实例的特征都在同一列里。即： ${ {z}^{\left( 2 \right)} }={ {\Theta }^{\left( 1 \right)} }\times { {X}^{T} } $ ${ {a}^{\left( 2 \right)} }=g({ {z}^{\left( 2 \right)} })$ 为了更好了了解Neuron Networks的工作原理，我们先把左半部分遮住： 右半部分其实就是以$a_0, a_1, a_2, a_3$, 按照Logistic Regression的方式输出$h_\theta(x)$： 其实神经网络就像是logistic regression，只不过我们把logistic regression中的输入向量$\left[ x_1\sim {x_3} \right]$ 变成了中间层的$\left[ a_1^{(2)}\sim a_3^{(2)} \right]$, 即: $h_\theta(x)=g\left( \Theta_0^{\left( 2 \right)}a_0^{\left( 2 \right)}+\Theta_1^{\left( 2 \right)}a_1^{\left( 2 \right)}+\Theta_{2}^{\left( 2 \right)}a_{2}^{\left( 2 \right)}+\Theta_{3}^{\left( 2 \right)}a_{3}^{\left( 2 \right)} \right)$我们可以把$a_0, a_1, a_2, a_3$看成更为高级的特征值，也就是$x_0, x_1, x_2, x_3$的进化体，并且它们是由 $x$与$\theta$决定的，因为是梯度下降的，所以$a$是变化的，并且变得越来越厉害，所以这些更高级的特征值远比仅仅将 $x$次方厉害，也能更好的预测新数据。这就是神经网络相比于逻辑回归和线性回归的优势。 特征和直观理解1参考视频: 8 - 5 - Examples and Intuitions I (7 min).mkv 从本质上讲，神经网络能够通过学习得出其自身的一系列特征。在普通的逻辑回归中，我们被限制为使用数据中的原始特征$x_1,x_2,...,{ {x}_{n} }$，我们虽然可以使用一些二项式项来组合这些特征，但是我们仍然受到这些原始特征的限制。在神经网络中，原始特征只是输入层，在我们上面三层的神经网络例子中，第三层也就是输出层做出的预测利用的是第二层的特征，而非输入层中的原始特征，我们可以认为第二层中的特征是神经网络通过学习后自己得出的一系列用于预测输出变量的新特征。 神经网络中，单层神经元（无中间层）的计算可用来表示逻辑运算，比如逻辑与(AND)、逻辑或(OR)。 举例说明：逻辑与(AND)；下图中左半部分是神经网络的设计与output层表达式，右边上部分是sigmod函数，下半部分是真值表。 我们可以用这样的一个神经网络表示AND 函数： 其中$\theta_0 = -30, \theta_1 = 20, \theta_2 = 20$我们的输出函数$h_\theta(x)$即为：$h_\Theta(x)=g\left( -30+20x_1+20x_2 \right)$ 我们知道$g(x)$的图像是： 所以我们有：$h_\Theta(x) \approx \text{x}_1 \text{AND} \, \text{x}_2$ 所以我们的：$h_\Theta(x) $ 这就是AND函数。 接下来再介绍一个OR函数： OR与AND整体一样，区别只在于的取值不同。 样本和直观理解II参考视频: 8 - 6 - Examples and Intuitions II (10 min).mkv 二元逻辑运算符（BINARY LOGICAL OPERATORS）当输入特征为布尔值（0或1）时，我们可以用一个单一的激活层可以作为二元逻辑运算符，为了表示不同的运算符，我们只需要选择不同的权重即可。 下图的神经元（三个权重分别为-30，20，20）可以被视为作用同于逻辑与（AND）： 下图的神经元（三个权重分别为-10，20，20）可以被视为作用等同于逻辑或（OR）： 下图的神经元（两个权重分别为 10，-20）可以被视为作用等同于逻辑非（NOT）： 我们可以利用神经元来组合成更为复杂的神经网络以实现更复杂的运算。例如我们要实现XNOR 功能（输入的两个值必须一样，均为1或均为0），即 $\text{XNOR}=( \text{x}_1\, \text{AND}\, \text{x}_2 )\, \text{OR} \left( \left( \text{NOT}\, \text{x}_1 \right) \text{AND} \left( \text{NOT}\, \text{x}_2 \right) \right)$首先构造一个能表达$\left( \text{NOT}\, \text{x}_1 \right) \text{AND} \left( \text{NOT}\, \text{x}_2 \right)$部分的神经元： 然后将表示 AND 的神经元和表示$\left( \text{NOT}\, \text{x}_1 \right) \text{AND} \left( \text{NOT}\, \text{x}_2 \right)$的神经元以及表示 OR 的神经元进行组合： 我们就得到了一个能实现 $\text{XNOR}$ 运算符功能的神经网络。 按这种方法我们可以逐渐构造出越来越复杂的函数，也能得到更加厉害的特征值。 这就是神经网络的厉害之处。 多类分类参考视频: 8 - 7 - Multiclass Classification (4 min).mkv 当我们有不止两种分类时（也就是$y=1,2,3….$），比如以下这种情况，该怎么办？如果我们要训练一个神经网络算法来识别路人、汽车、摩托车和卡车，在输出层我们应该有4个值。例如，第一个值为1或0用于预测是否是行人，第二个值用于判断是否为汽车。 输入向量$x$有三个维度，两个中间层，输出层4个神经元分别用来表示4类，也就是每一个数据在输出层都会出现${ {\left[ a\text{ }b\text{ }c\text{ }d \right]}^{T} }$，且$a,b,c,d$中仅有一个为1，表示当前类。下面是该神经网络的可能结构示例： 神经网络算法的输出结果为四种可能情形之一： 神经网络的学习(Neural Networks: Learning)代价函数参考视频: 9 - 1 - Cost Function (7 min).mkv 首先引入一些便于稍后讨论的新标记方法： 假设神经网络的训练样本有$m$个，每个包含一组输入$x$和一组输出信号$y$，$L$表示神经网络层数，$S_I$表示每层的neuron个数($S_l$表示输出层神经元个数)，$S_L$代表最后一层中处理单元的个数。 将神经网络的分类定义为两种情况：二类分类和多类分类， 二类分类：$S_L=0, y=0\, or\, 1$表示哪一类； $K$类分类：$S_L=k, y_i = 1$表示分到第$i$类；$(k>2)$ 我们回顾逻辑回归问题中我们的代价函数为： $ J\left(\theta \right)=-\frac{1}{m}\left[\sum_\limits{i=1}^{m}{y}^{(i)}\log{h_\theta({x}^{(i)})}+\left(1-{y}^{(i)}\right)log\left(1-h_\theta\left({x}^{(i)}\right)\right)\right]+\frac{\lambda}{2m}\sum_\limits{j=1}^{n}{\theta_j}^{2} $ 在逻辑回归中，我们只有一个输出变量，又称标量（scalar），也只有一个因变量$y$，但是在神经网络中，我们可以有很多输出变量，我们的$h_\theta(x)$是一个维度为$K$的向量，并且我们训练集中的因变量也是同样维度的一个向量，因此我们的代价函数会比逻辑回归更加复杂一些，为：$\newcommand{\subk}[1]{ #1_k }$ $$h_\theta\left(x\right)\in \mathbb{R}^{K}$$ $${\left({h_\theta}\left(x\right)\right)}_{i}={i}^{th} \text{output}$$ $J(\Theta) = -\frac{1}{m} \left[ \sum\limits_{i=1}^{m} \sum\limits_{k=1}^{k} {y_k}^{(i)} \log \subk{(h_\Theta(x^{(i)}))} + \left( 1 - y_k^{(i)} \right) \log \left( 1- \subk{\left( h_\Theta \left( x^{(i)} \right) \right)} \right) \right] + \frac{\lambda}{2m} \sum\limits_{l=1}^{L-1} \sum\limits_{i=1}^{s_l} \sum\limits_{j=1}^{s_{l+1} } \left( \Theta_{ji}^{(l)} \right)^2$ 这个看起来复杂很多的代价函数背后的思想还是一样的，我们希望通过代价函数来观察算法预测的结果与真实情况的误差有多大，唯一不同的是，对于每一行特征，我们都会给出$K$个预测，基本上我们可以利用循环，对每一行特征都预测$K$个不同结果，然后在利用循环在$K$个预测中选择可能性最高的一个，将其与$y$中的实际数据进行比较。 正则化的那一项只是排除了每一层$\theta_0$后，每一层的$\theta$ 矩阵的和。最里层的循环$j$循环所有的行（由$s_{l+1}$ 层的激活单元数决定），循环$i$则循环所有的列，由该层（$s_l$层）的激活单元数所决定。即：$h_\theta(x)$与真实值之间的距离为每个样本-每个类输出的加和，对参数进行regularization的bias项处理所有参数的平方和。 反向传播算法参考视频: 9 - 2 - Backpropagation Algorithm (12 min).mkv 之前我们在计算神经网络预测结果的时候我们采用了一种正向传播方法，我们从第一层开始正向一层一层进行计算，直到最后一层的$h_{\theta}\left(x\right)$。 现在，为了计算代价函数的偏导数$\frac{\partial}{\partial\Theta^{(l)}_{ij} }J\left(\Theta\right)$，我们需要采用一种反向传播算法，也就是首先计算最后一层的误差，然后再一层一层反向求出各层的误差，直到倒数第二层。以一个例子来说明反向传播算法。 假设我们的训练集只有一个样本$\left({x}^{(1)},{y}^{(1)}\right)$，我们的神经网络是一个四层的神经网络，其中$K=4，S_{L}=4，L=4$： 前向传播算法： 下面的公式推导过程见：&lt;https://blog.csdn.net/qq_29762941/article/details/80343185&gt; 我们从最后一层的误差开始计算，误差是激活单元的预测（${a^{(4)} }$）与实际值（$y^k$）之间的误差，（$k=1:k$）。我们用$\delta$来表示误差，则：$\delta^{(4)}=a^{(4)}-y$我们利用这个误差值来计算前一层的误差：$\delta^{(3)}=\left({\Theta^{(3)} }\right)^{T}\delta^{(4)}\ast g'\left(z^{(3)}\right)$其中 $g'(z^{(3)})$是 $S$ 形函数的导数，$g'(z^{(3)})=a^{(3)}\ast(1-a^{(3)})$。而$(θ^{(3)})^{T}\delta^{(4)}$则是权重导致的误差的和。下一步是继续计算第二层的误差： $ \delta^{(2)}=(\Theta^{(2)})^{T}\delta^{(3)}\ast g'(z^{(2)})$ 因为第一层是输入变量，不存在误差。我们有了所有的误差的表达式后，便可以计算代价函数的偏导数了，假设$λ=0$，即我们不做任何正则化处理时有： $\frac{\partial}{\partial\Theta_{ij}^{(l)} }J(\Theta)=a_{j}^{(l)} \delta_{i}^{l+1}$ 重要的是清楚地知道上面式子中上下标的含义： $l$ 代表目前所计算的是第几层。 $j$ 代表目前计算层中的激活单元的下标，也将是下一层的第$j$个输入变量的下标。 $i$ 代表下一层中误差单元的下标，是受到权重矩阵中第$i$行影响的下一层中的误差单元的下标。 如果我们考虑正则化处理，并且我们的训练集是一个特征矩阵而非向量。在上面的特殊情况中，我们需要计算每一层的误差单元来计算代价函数的偏导数。在更为一般的情况中，我们同样需要计算每一层的误差单元，但是我们需要为整个训练集计算误差单元，此时的误差单元也是一个矩阵，我们用$\Delta^{(l)}_{ij}$来表示这个误差矩阵。第 $l$ 层的第 $i$ 个激活单元受到第 $j$ 个参数影响而导致的误差。 我们的算法表示为： 即首先用正向传播方法计算出每一层的激活单元，利用训练集的结果与神经网络预测的结果求出最后一层的误差，然后利用该误差运用反向传播法计算出直至第二层的所有误差。 在求出了$\Delta_{ij}^{(l)}$之后，我们便可以计算代价函数的偏导数了，计算方法如下： $ D_{ij}^{(l)} :=\frac{1}{m}\Delta_{ij}^{(l)}+\lambda\Theta_{ij}^{(l)}$ ${if}\; j \neq 0$ $ D_{ij}^{(l)} :=\frac{1}{m}\Delta_{ij}^{(l)}$ ${if}\; j = 0$ 在Octave 中，如果我们要使用 fminuc这样的优化算法来求解求出权重矩阵，我们需要将矩阵首先展开成为向量，在利用算法求出最优解后再重新转换回矩阵。 假设我们有三个权重矩阵，Theta1，Theta2 和 Theta3，尺寸分别为 10*11，10*11 和1*11，下面的代码可以实现这样的转换： thetaVec = [Theta1(:) ; Theta2(:) ; Theta3(:)] ...optimization using functions like fminuc... Theta1 = reshape(thetaVec(1:110, 10, 11); Theta2 = reshape(thetaVec(111:220, 10, 11); Theta1 = reshape(thetaVec(221:231, 1, 11);反向传播算法的直观理解参考视频: 9 - 3 - Backpropagation Intuition (13 min).mkv 在上一段视频中，我们介绍了反向传播算法，对很多人来说，当第一次看到这种算法时，第一印象通常是，这个算法需要那么多繁杂的步骤，简直是太复杂了，实在不知道这些步骤，到底应该如何合在一起使用。就好像一个黑箱，里面充满了复杂的步骤。如果你对反向传播算法也有这种感受的话，这其实是正常的，相比于线性回归算法和逻辑回归算法而言，从数学的角度上讲，反向传播算法似乎并不简洁，对于反向传播这种算法，其实我已经使用了很多年了，但即便如此，即使是现在，我也经常感觉自己对反向传播算法的理解并不是十分深入，对于反向传播算法究竟是如何执行的，并没有一个很直观的理解。做过编程练习的同学应该可以感受到这些练习或多或少能帮助你，将这些复杂的步骤梳理了一遍，巩固了反向传播算法具体是如何实现的，这样你才能自己掌握这种算法。 在这段视频中，我想更加深入地讨论一下反向传播算法的这些复杂的步骤，并且希望给你一个更加全面直观的感受，理解这些步骤究竟是在做什么，也希望通过这段视频，你能理解，它至少还是一个合理的算法。但可能你即使看了这段视频，你还是觉得反向传播依然很复杂，依然像一个黑箱，太多复杂的步骤，依然感到有点神奇，这也是没关系的。即使是我接触反向传播这么多年了，有时候仍然觉得这是一个难以理解的算法，但还是希望这段视频能有些许帮助，为了更好地理解反向传播算法，我们再来仔细研究一下前向传播的原理： 前向传播算法： 反向传播算法做的是： 感悟：上图中的 $\delta^{(l)}_{j}="error" \ of cost \ for \ a^{(l)}_{j} \ (unit \ j \ in \ layer \ l)$ 理解如下： $\delta^{(l)}_{j}$ 相当于是第 $l$ 层的第 $j$ 单元中得到的激活项的“误差”，即”正确“的 $a^{(l)}_{j}$ 与计算得到的 $a^{(l)}_{j}$ 的差。 而 $a^{(l)}_{j}=g(z^{(l)})$ ，（g为sigmoid函数）。我们可以想象 $\delta^{(l)}_{j}$ 为函数求导时迈出的那一丁点微分，所以更准确的说 $\delta^{(l)}_{j}=\frac{\partial}{\partial z^{(l)}_{j} }cost(i)$ 实现注意：展开参数参考视频: 9 - 4 - Implementation Note_ Unrolling Parameters (8 min).mkv 在上一段视频中，我们谈到了怎样使用反向传播算法计算代价函数的导数。在这段视频中，我想快速地向你介绍一个细节的实现过程，怎样把你的参数从矩阵展开成向量，以便我们在高级最优化步骤中的使用需要。 梯度检验参考视频: 9 - 5 - Gradient Checking (12 min).mkv 当我们对一个较为复杂的模型（例如神经网络）使用梯度下降算法时，可能会存在一些不容易察觉的错误，意味着，虽然代价看上去在不断减小，但最终的结果可能并不是最优解。 为了避免这样的问题，我们采取一种叫做梯度的数值检验（Numerical Gradient Checking）方法。这种方法的思想是通过估计梯度值来检验我们计算的导数值是否真的是我们要求的。 对梯度的估计采用的方法是在代价函数上沿着切线的方向选择离两个非常近的点然后计算两个点的平均值用以估计梯度。即对于某个特定的 $\theta$，我们计算出在 $\theta$-$\varepsilon $ 处和 $\theta$+$\varepsilon $ 的代价值（$\varepsilon $是一个非常小的值，通常选取 0.001），然后求两个代价的平均，用以估计在 $\theta$ 处的代价值。 Octave 中代码如下： gradApprox = (J(theta + eps) – J(theta - eps)) / (2*eps) 当$\theta$是一个向量时，我们则需要对偏导数进行检验。因为代价函数的偏导数检验只针对一个参数的改变进行检验，下面是一个只针对$\theta_1$进行检验的示例： $$ \frac{\partial}{\partial\theta_1}=\frac{J\left(\theta_1+\varepsilon_1,\theta_2,\theta_3...\theta_n \right)-J \left( \theta_1-\varepsilon_1,\theta_2,\theta_3...\theta_n \right)}{2\varepsilon} $$ 最后我们还需要对通过反向传播方法计算出的偏导数进行检验。 根据上面的算法，计算出的偏导数存储在矩阵 $D_{ij}^{(l)}$ 中。检验时，我们要将该矩阵展开成为向量，同时我们也将 $\theta$ 矩阵展开为向量，我们针对每一个 $\theta$ 都计算一个近似的梯度值，将这些值存储于一个近似梯度矩阵中，最终将得出的这个矩阵同 $D_{ij}^{(l)}$ 进行比较。 随机初始化参考视频: 9 - 6 - Random Initialization (7 min).mkv 任何优化算法都需要一些初始的参数。到目前为止我们都是初始所有参数为0，这样的初始方法对于逻辑回归来说是可行的，但是对于神经网络来说是不可行的。如果我们令所有的初始参数都为0，这将意味着我们第二层的所有激活单元都会有相同的值。同理，如果我们初始所有的参数都为一个非0的数，结果也是一样的。 我们通常初始参数为正负ε之间的随机值，假设我们要随机初始一个尺寸为10×11的参数矩阵，代码如下： Theta1 = rand(10, 11) * (2*eps) – eps 综合起来参考视频: 9 - 7 - Putting It Together (14 min).mkv 小结一下使用神经网络时的步骤： 网络结构：第一件要做的事是选择网络结构，即决定选择多少层以及决定每层分别有多少个单元。 第一层的单元数即我们训练集的特征数量。 最后一层的单元数是我们训练集的结果的类的数量。 如果隐藏层数大于1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。 我们真正要决定的是隐藏层的层数和每个中间层的单元数。 训练神经网络： 参数的随机初始化 利用正向传播方法计算所有的$h_{\theta}(x)$ 编写计算代价函数 $J$ 的代码 利用反向传播方法计算所有偏导数 利用数值检验方法检验这些偏导数 使用优化算法来最小化代价函数 自主驾驶参考视频: 9 - 8 - Autonomous Driving (7 min).mkv 在这段视频中，我想向你介绍一个具有历史意义的神经网络学习的重要例子。那就是使用神经网络来实现自动驾驶，也就是说使汽车通过学习来自己驾驶。接下来我将演示的这段视频是我从 Dean Pomerleau那里拿到的，他是我的同事，任职于美国东海岸的卡耐基梅隆大学。在这部分视频中，你就会明白可视化技术到底是什么？在看这段视频之前，我会告诉你可视化技术是什么。 在下面也就是左下方，就是汽车所看到的前方的路况图像。 在图中你依稀能看出一条道路，朝左延伸了一点，又向右了一点，然后上面的这幅图，你可以看到一条水平的菜单栏显示的是驾驶操作人选择的方向。就是这里的这条白亮的区段显示的就是人类驾驶者选择的方向。比如：最左边的区段，对应的操作就是向左急转，而最右端则对应向右急转的操作。因此，稍微靠左的区段，也就是中心稍微向左一点的位置，则表示在这一点上人类驾驶者的操作是慢慢的向左拐。 这幅图的第二部分对应的就是学习算法选出的行驶方向。并且，类似的，这一条白亮的区段显示的就是神经网络在这里选择的行驶方向，是稍微的左转，并且实际上在神经网络开始学习之前，你会看到网络的输出是一条灰色的区段，就像这样的一条灰色区段覆盖着整个区域这些均称的灰色区域，显示出神经网络已经随机初始化了，并且初始化时，我们并不知道汽车如何行驶，或者说我们并不知道所选行驶方向。只有在学习算法运行了足够长的时间之后，才会有这条白色的区段出现在整条灰色区域之中。显示出一个具体的行驶方向这就表示神经网络算法，在这时候已经选出了一个明确的行驶方向，不像刚开始的时候，输出一段模糊的浅灰色区域，而是输出一条白亮的区段，表示已经选出了明确的行驶方向。 ALVINN (Autonomous Land Vehicle In a Neural Network)是一个基于神经网络的智能系统，通过观察人类的驾驶来学习驾驶，ALVINN能够控制NavLab，装在一辆改装版军用悍马，这辆悍马装载了传感器、计算机和驱动器用来进行自动驾驶的导航试验。实现ALVINN功能的第一步，是对它进行训练，也就是训练一个人驾驶汽车。 然后让ALVINN观看，ALVINN每两秒将前方的路况图生成一张数字化图片，并且记录驾驶者的驾驶方向，得到的训练集图片被压缩为30x32像素，并且作为输入提供给ALVINN的三层神经网络，通过使用反向传播学习算法，ALVINN会训练得到一个与人类驾驶员操纵方向基本相近的结果。一开始，我们的网络选择出的方向是随机的，大约经过两分钟的训练后，我们的神经网络便能够准确地模拟人类驾驶者的驾驶方向，对其他道路类型，也重复进行这个训练过程，当网络被训练完成后，操作者就可按下运行按钮，车辆便开始行驶了。 每秒钟ALVINN生成12次数字化图片，并且将图像传送给神经网络进行训练，多个神经网络同时工作，每一个网络都生成一个行驶方向，以及一个预测自信度的参数，预测自信度最高的那个神经网络得到的行驶方向。比如这里，在这条单行道上训练出的网络将被最终用于控制车辆方向，车辆前方突然出现了一个交叉十字路口，当车辆到达这个十字路口时，我们单行道网络对应的自信度骤减，当它穿过这个十字路口时，前方的双车道将进入其视线，双车道网络的自信度便开始上升，当它的自信度上升时，双车道的网络，将被选择来控制行驶方向，车辆将被安全地引导进入双车道路。 这就是基于神经网络的自动驾驶技术。当然，我们还有很多更加先进的试验来实现自动驾驶技术。在美国，欧洲等一些国家和地区，他们提供了一些比这个方法更加稳定的驾驶控制技术。但我认为，使用这样一个简单的基于反向传播的神经网络，训练出如此强大的自动驾驶汽车，的确是一次令人惊讶的成就。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nlp基本概念及算法]]></title>
    <url>%2F2019%2F11%2F25%2Fnlp%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%8F%8A%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[整理的一些基本算法及概念 算法复杂度 分词最大前向匹配 最大后向匹配 依赖于词典，不能做词细分 局部最优（属于贪心算法） 效率不高（取决于max_length） 有歧义（不能考虑语义）unigram lmViterbi算法(DP算法)输入句子-&gt;生成所有可能的分割-&gt;利用语言模型，选择其中最好的，分词方法分两步进行（分割-&gt;计算unigram概率），时间复杂度很高编辑距离拼写纠错距离相似度欧式距离：d = |s1-s2|余弦相似度：d=s1s2/(∣s1∣∣s2∣) d(s1,s2)=0缺点: 只从出现频率来表示单词/句子，距离计算时，频次高的单词对结果影响较大，实际的语义分析时，并不是出现频率越高就越重要 BOW词袋模型假设我们不考虑文本中词与词之间的上下文关系，仅仅只考虑所有词的权重。而权重与词在文本中出现的频率有关。 one-hot表示单词 词典：[我们，去，爬山，今天，你们，昨天，跑步]按照单词在词典库中的顺序我们：(1,0,0,0,0,0,0)-&gt;7维=|词典|爬山：(0,0,1,0,0,0,0)跑步：(0,0,0,0,0,0,1)昨天：(0,0,0,0,0,1,0) boolean represention(不关心单词频率)表达句子 词典：[我们，又，去，爬山，今天，你们，昨天，跑步]按照单词在词典库中的顺序我们|今天|去|爬山：(1,0,1,1,1,0,0,0)-&gt;8维=|词典|你们|昨天|跑步：(0,0,0,0,0,1,1,1)你们|又|去|爬山|又|去|跑步：(0,1,1,1,0,1,0,1) count based represention(记录单词频率)表达句子 词典：[我们，又，去，爬山，今天，你们，昨天，跑步]按照单词在词典库中的顺序我们|今天|去|爬山：(1,0,1,1,1,0,0,0)-&gt;8维=|词典|你们|昨天|跑步：(0,0,0,0,0,1,1,1)你们|又|去|爬山|又|去|跑步：(0,2,2,1,0,1,0,1) tf-idf tf为词频，即一个词语在文档中的出现频率，假设一个词语在整个文档中出现了i次，而整个文档有N个词语，则tf的值为i/N.idf为逆向文件频率，假设整个文档有n篇文章，而一个词语在k篇文章中出现，则idf值为log(n/k)1.所有文档分词得到词典向量2.词典向量每个词对于当前句子进行词频统计，该词在句中出现n次，即tf3.所有文档总数m,词典向量中当前词出现在k个文档中，log(m/k)为idf4.针对词典向量生成tf-idf为m*log(m/k) 词典：[今天,上,NLP,课程,的,有,意思,数据,也]s1:今天|上|NLP|课程-&gt;[1log(3\2),1log(3\1),1log(3\1),1log(3\3),0,0,0,0,0]s2:今天|的|课程|有|意思-&gt;[1log(3\2),0,0,1log(3/3),1log(3\1),1log(3\2),1log(3\2),0,0]s3:数据|课程|也|有|意思-&gt;[0,0,0,1log(3\3),0,1log(3\2),1log(3\2),1log(3\1),1log(3\1)] distributed word 表示单词 我们:[0.4,0.6,0.8,0.1]爬山:[0.4,0.6,0.2,0.6]运动:[0.2,0.3,0.7,0.6]昨天:[0.5,0.9,0.1,0.3] 缺点：首先，它是一个词袋模型，不考虑词与词之间的顺序（文本中词的顺序信息也是很重要的）；其次，它假设词与词相互独立（在大多数情况下，词与词是相互影响的）；最后，它得到的特征是离散稀疏的。 文本语料库-&gt;预处理-&gt;文本处理集-&gt;one-hot向量输入word2vec-&gt;词向量（模型：CBOW和Skip-gram;；方法：负采样与层次softmax方法） Language Model语言模型：判断一句话是否通顺。 unigram bigram n-gram SmoothingLaplace Good-Turning]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>tf-idf</tag>
        <tag>hanlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于ML的中文短文本分类（6）]]></title>
    <url>%2F2019%2F11%2F22%2F6.%E5%9F%BA%E4%BA%8EML%E7%9A%84%E4%B8%AD%E6%96%87%E7%9F%AD%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[对每一条输入数据，判断事情的主体是谁 语料加载 分词 去停用词 抽取词向量特征 分别进行算法建模和模型训练 评估、计算 AUC 值 模型对比语料加载 import random import jieba import pandas as pd #加载停用词 stopwords=pd.read_csv(&#39;stopwords.txt&#39;,index_col=False,quoting=3,sep=&quot;\t&quot;,names=[&#39;stopword&#39;], encoding=&#39;utf-8&#39;) stopwords=stopwords[&#39;stopword&#39;].values #加载语料 laogong_df = pd.read_csv(&#39;beilaogongda.csv&#39;, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) laopo_df = pd.read_csv(&#39;beilaogongda.csv&#39;, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) erzi_df = pd.read_csv(&#39;beierzida.csv&#39;, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) nver_df = pd.read_csv(&#39;beinverda.csv&#39;, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) #删除语料的nan行 laogong_df.dropna(inplace=True) laopo_df.dropna(inplace=True) erzi_df.dropna(inplace=True) nver_df.dropna(inplace=True) #提取要分词的 content 列转换为 list 列表 laogong = laogong_df.segment.values.tolist() laopo = laopo_df.segment.values.tolist() erzi = erzi_df.segment.values.tolist() nver = nver_df.segment.values.tolist() 分词和去停用词#定义分词、去停用词和批量打标签的函数 #参数content_lines即为语料列表 上面转换的list #参数sentences是先定义的 list，用来存储分词并打标签后的结果 #参数category 是类型标签 def preprocess_text(content_lines, sentences, category): for line in content_lines: try: segs=jieba.lcut(line) segs = [v for v in segs if not str(v).isdigit()]#去数字 segs = list(filter(lambda x:x.strip(), segs)) #去左右空格 segs = list(filter(lambda x:len(x)&gt;1, segs)) #长度为1的字符 segs = list(filter(lambda x:x not in stopwords, segs)) #去掉停用词 sentences.append((&quot; &quot;.join(segs), category))# 打标签 except Exception: print(line) continue # 调用函数、生成训练数据，根据我提供的司法语料数据，分为报警人被老公打，报警人被老婆打，报警人被儿子打，报警人被女儿打，标签分别为0、1、2、3 sentences = [] preprocess_text(laogong, sentences,0) preprocess_text(laopo, sentences, 1) preprocess_text(erzi, sentences, 2) preprocess_text(nver, sentences, 3) # 将得到的数据集打散，生成更可靠的训练集分布，避免同类数据分布不均匀 random.shuffle(sentences) # 控制台输出前10条数据 for sentence in sentences[:10]: print(sentence[0], sentence[1]) #下标0是词列表，1是标签 抽取词向量特征# 抽取特征，我们定义文本抽取词袋模型特征 from sklearn.feature_extraction.text import CountVectorizer vec = CountVectorizer( analyzer=&#39;word&#39;, # tokenise by character ngrams max_features=4000, # keep the most common 1000 ngrams ) # 把语料数据切分，用 sk-learn 对数据切分，分成训练集和测试集 from sklearn.model_selection import train_test_split x, y = zip(*sentences) x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1256) # 把训练数据转换为词袋模型 vec.fit(x_train) 分别进行算法建模和模型训练# 定义朴素贝叶斯模型，然后对训练集进行模型训练，直接使用 sklearn 中的 MultinomialNB from sklearn.naive_bayes import MultinomialNB classifier = MultinomialNB() classifier.fit(vec.transform(x_train), y_train)评估、计算 AUC 值# 评分为 0.647331786543。 print(classifier.score(vec.transform(x_test), y_test)) # 测试集的预测 pre = classifier.predict(vec.transform(x_test)) 模型对比改变特征向量模型和训练模型对结果有什么变化。 ## 改变特征向量模型:把特征做得更强一点，尝试加入抽取 2-gram 和 3-gram 的统计特征，把词库的量放大一点 from sklearn.feature_extraction.text import CountVectorizer vec = CountVectorizer( analyzer=&#39;word&#39;, # tokenise by character ngrams ngram_range=(1,4), # use ngrams of size 1 and 2 max_features=20000, # keep the most common 1000 ngrams ) vec.fit(x_train) #用朴素贝叶斯算法进行模型训练 classifier = MultinomialNB() classifier.fit(vec.transform(x_train), y_train) #对结果进行评分 结果评分为：0.649651972158 print(classifier.score(vec.transform(x_test), y_test)) ## SVM 训练 from sklearn.svm import SVC svm = SVC(kernel=&#39;linear&#39;) svm.fit(vec.transform(x_train), y_train) print(svm.score(vec.transform(x_test), y_test)) ## 使用决策树、随机森林、XGBoost、神经网络 import xgboost as xgb from sklearn.model_selection import StratifiedKFold import numpy as np # xgb矩阵赋值 xgb_train = xgb.DMatrix(vec.transform(x_train), label=y_train) xgb_test = xgb.DMatrix(vec.transform(x_test)) 在 XGBoost 中，下面主要是调参指标，可以根据参数进行调参 params = { &#39;booster&#39;: &#39;gbtree&#39;, #使用gbtree &#39;objective&#39;: &#39;multi:softmax&#39;, # 多分类的问题、 # &#39;objective&#39;: &#39;multi:softprob&#39;, # 多分类概率 #&#39;objective&#39;: &#39;binary:logistic&#39;, #二分类 &#39;eval_metric&#39;: &#39;merror&#39;, #logloss &#39;num_class&#39;: 4, # 类别数，与 multisoftmax 并用 &#39;gamma&#39;: 0.1, # 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。 &#39;max_depth&#39;: 8, # 构建树的深度，越大越容易过拟合 &#39;alpha&#39;: 0, # L1正则化系数 &#39;lambda&#39;: 10, # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。 &#39;subsample&#39;: 0.7, # 随机采样训练样本 &#39;colsample_bytree&#39;: 0.5, # 生成树时进行的列采样 &#39;min_child_weight&#39;: 3, # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言 # 假设 h 在 0.01 附近，min_child_weight 为 1 叶子节点中最少需要包含 100 个样本。 &#39;silent&#39;: 0, # 设置成1则没有运行信息输出，最好是设置为0. &#39;eta&#39;: 0.03, # 如同学习率 &#39;seed&#39;: 1000, &#39;nthread&#39;: -1, # cpu 线程数 &#39;missing&#39;: 1 }]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>jieba</tag>
        <tag>auc</tag>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中文短文本分类（22）]]></title>
    <url>%2F2019%2F11%2F22%2F22.%E4%B8%AD%E6%96%87%E7%9F%AD%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[目前，随着大数据、云计算对关系型数据处理技术趋向稳定成熟，各大互联网公司对关系数据的整合也已经落地成熟，笔者预测未来数据领域的挑战将主要集中在半结构化和非结构化数据的整合，NLP 技术对个人发展越来越重要，尤其在中文文本上挑战更大。 由于是第一讲，笔者在本次 Chat 并没有提及较深入的 NLP 处理技术，通过 WordCloud 制作词云、用 LDA 主题模型获取文本关键词、以及用朴素贝叶斯算法和 SVM 分别对文本分类，目的是让大家对中文文本处理有一个直观了解，为后续实战提供基础保障。 WordCloud 制作词云最近中美贸易战炒的沸沸扬扬，笔者用网上摘取了一些文本（自己线下可以继续添加语料），下面来制作一个中美贸易战相关的词云。 jieba 分词安装jieba 俗称中文分词利器，作用是来对文本语料进行分词。 全自动安装：easy_install jieba 或者 pip install jieba / pip3 install jieba 半自动安装：先下载 https://pypi.python.org/pypi/jieba/ ，解压后运行 python setup.py install 手动安装：将 jieba 目录放置于当前目录或者 site-packages 目录。 安装完通过 import jieba 验证安装成功与否。 WordCloud 安装 下载：https://www.lfd.uci.edu/~gohlke/pythonlibs/#wordcloud 安装（window 环境安装）找的下载文件的路径：pip install wordcloud-1.3.2-cp36-cp36m-win_amd64.whl 安装完通过 from wordcloud import WordCloud 验证安装成功与否。 开始编码实现整个过程分为几个步骤： 文件加载 分词 统计词频 去停用词 构建词云 #引入所需要的包 import jieba import pandas as pd import numpy as np from scipy.misc import imread from wordcloud import WordCloud,ImageColorGenerator import matplotlib.pyplot as plt #定义文件路径 dir = &quot;D://ProgramData//PythonWorkSpace//study//&quot; #定义语料文件路径 file = &quot;&quot;.join([dir,&quot;z_m.csv&quot;]) #定义停用词文件路径 stop_words = &quot;&quot;.join([dir,&quot;stopwords.txt&quot;]) #定义wordcloud中字体文件的路径 simhei = &quot;&quot;.join([dir,&quot;simhei.ttf&quot;]) #读取语料 df = pd.read_csv(file, encoding=&#39;utf-8&#39;) df.head() #如果存在nan，删除 df.dropna(inplace=True) #将content一列转为list content=df.content.values.tolist() #用jieba进行分词操作 segment=[] for line in content: try: segs=jieba.cut_for_search(line) segs = [v for v in segs if not str(v).isdigit()]#去数字 segs = list(filter(lambda x:x.strip(), segs)) #去左右空格 #segs = list(filter(lambda x:len(x)&gt;1, segs)) #长度为1的字符 for seg in segs: if len(seg)&gt;1 and seg!=&#39;\r\n&#39;: segment.append(seg) except: print(line) continue #分词后加入一个新的DataFrame words_df=pd.DataFrame({&#39;segment&#39;:segment}) #加载停用词 stopwords=pd.read_csv(stop_words,index_col=False,quoting=3,sep=&quot;\t&quot;,names=[&#39;stopword&#39;], encoding=&#39;utf-8&#39;) #安装关键字groupby分组统计词频，并按照计数降序排序 words_stat=words_df.groupby(by=[&#39;segment&#39;])[&#39;segment&#39;].agg({&quot;计数&quot;:np.size}) words_stat=words_stat.reset_index().sort_values(by=[&quot;计数&quot;],ascending=False) #分组之后去掉停用词 words_stat=words_stat[~words_stat.segment.isin(stopwords.stopword)] #下面是重点，绘制wordcloud词云，这一提供2种方式 #第一种是默认的样式 wordcloud=WordCloud(font_path=simhei,background_color=&quot;white&quot;,max_font_size=80) word_frequence = {x[0]:x[1] for x in words_stat.head(1000).values} wordcloud=wordcloud.fit_words(word_frequence) plt.imshow(wordcloud) wordcloud.to_file(r&#39;wordcloud_1.jpg&#39;) #保存结果 #第二种是自定义图片 text = &quot; &quot;.join(words_stat[&#39;segment&#39;].head(100).astype(str)) abel_mask = imread(r&quot;china.jpg&quot;) #这里设置了一张中国地图 wordcloud2 = WordCloud(background_color=&#39;white&#39;, # 设置背景颜色 mask = abel_mask, # 设置背景图片 max_words = 3000, # 设置最大现实的字数 font_path = simhei, # 设置字体格式 width=2048, height=1024, scale=4.0, max_font_size= 300, # 字体最大值 random_state=42).generate(text) # 根据图片生成词云颜色 image_colors = ImageColorGenerator(abel_mask) wordcloud2.recolor(color_func=image_colors) # 以下代码显示图片 plt.imshow(wordcloud2) plt.axis(&quot;off&quot;) plt.show() wordcloud2.to_file(r&#39;wordcloud_2.jpg&#39;) #保存结果 这里只给出默认生产的图，自定义的图保存在 wordcloud_2.jpg，回头自己看。 LDA 提取关键字Gensim 安装Gensim 除了具备基本的语料处理功能外，Gensim 还提供了 LSI、LDA、HDP、DTM、DIM 等主题模型、TF-IDF 计算以及当前流行的深度神经网络语言模型 word2vec、paragraph2vec 等算法，可谓是方便之至。 Gensim 安装：pip install gensim 安装完通过 import gensim 验证安装成功与否。 LDA 提取关键字编码实现语料是一个关于汽车的短文本，传统的关键字提取有基于 TF-IDF 算法的关键词抽取；基于 TextRank 算法的关键词抽取等方式。下面通过 Gensim 库完成基于 LDA 的关键字提取。 整个过程步骤： 文件加载 分词 去停用词 构建词袋模型 LDA 模型训练 结果可视化 #引入库文件 import jieba.analyse as analyse import jieba import pandas as pd from gensim import corpora, models, similarities import gensim import numpy as np import matplotlib.pyplot as plt %matplotlib inline #设置文件路径 dir = &quot;D://ProgramData//PythonWorkSpace//study//&quot; file_desc = &quot;&quot;.join([dir,&#39;car.csv&#39;]) stop_words = &quot;&quot;.join([dir,&#39;stopwords.txt&#39;]) #定义停用词 stopwords=pd.read_csv(stop_words,index_col=False,quoting=3,sep=&quot;\t&quot;,names=[&#39;stopword&#39;], encoding=&#39;utf-8&#39;) stopwords=stopwords[&#39;stopword&#39;].values #加载语料 df = pd.read_csv(file_desc, encoding=&#39;gbk&#39;) #删除nan行 df.dropna(inplace=True) lines=df.content.values.tolist() #开始分词 sentences=[] for line in lines: try: segs=jieba.lcut(line) segs = [v for v in segs if not str(v).isdigit()]#去数字 segs = list(filter(lambda x:x.strip(), segs)) #去左右空格 segs = list(filter(lambda x:x not in stopwords, segs)) #去掉停用词 sentences.append(segs) except Exception: print(line) continue #构建词袋模型 dictionary = corpora.Dictionary(sentences) corpus = [dictionary.doc2bow(sentence) for sentence in sentences] #lda模型，num_topics是主题的个数，这里定义了5个 lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10) #我们查一下第1号分类，其中最常出现的5个词是： print(lda.print_topic(1, topn=5)) #我们打印所有5个主题，每个主题显示8个词 for topic in lda.print_topics(num_topics=10, num_words=8): print(topic[1]) #显示中文matplotlib plt.rcParams[&#39;font.sans-serif&#39;] = [u&#39;SimHei&#39;] plt.rcParams[&#39;axes.unicode_minus&#39;] = False # 在可视化部分，我们首先画出了九个主题的7个词的概率分布图 num_show_term = 8 # 每个主题下显示几个词 num_topics = 10 for i, k in enumerate(range(num_topics)): ax = plt.subplot(2, 5, i+1) item_dis_all = lda.get_topic_terms(topicid=k) item_dis = np.array(item_dis_all[:num_show_term]) ax.plot(range(num_show_term), item_dis[:, 1], &#39;b*&#39;) item_word_id = item_dis[:, 0].astype(np.int) word = [dictionary.id2token[i] for i in item_word_id] ax.set_ylabel(u&quot;概率&quot;) for j in range(num_show_term): ax.text(j, item_dis[j, 1], word[j], bbox=dict(facecolor=&#39;green&#39;,alpha=0.1)) plt.suptitle(u&#39;9个主题及其7个主要词的概率&#39;, fontsize=18) plt.show() 朴素贝叶斯和 SVM 文本分类最后一部分，我们通过带标签的数据： 数据是笔者曾经做过的一份司法数据，其中需求是对每一条输入数据，判断事情的主体是谁，比如报警人被老公打，报警人被老婆打，报警人被儿子打，报警人被女儿打等来进行文本有监督的分类操作。 本次主要选这 4 类标签，基本操作过程步骤： 文件加载 分词 去停用词 抽取词向量特征 分别进行朴素贝叶斯和 SVM 分类算法建模 评估打分 #引入包 import random import jieba import pandas as pd #指定文件目录 dir = &quot;D://ProgramData//PythonWorkSpace//chat//chat1//NB_SVM//&quot; #指定语料 stop_words = &quot;&quot;.join([dir,&#39;stopwords.txt&#39;]) laogong = &quot;&quot;.join([dir,&#39;beilaogongda.csv&#39;]) #被老公打 laopo = &quot;&quot;.join([dir,&#39;beilaopoda.csv&#39;]) #被老婆打 erzi = &quot;&quot;.join([dir,&#39;beierzida.csv&#39;]) #被儿子打 nver = &quot;&quot;.join([dir,&#39;beinverda.csv&#39;]) #被女儿打 #加载停用词 stopwords=pd.read_csv(stop_words,index_col=False,quoting=3,sep=&quot;\t&quot;,names=[&#39;stopword&#39;], encoding=&#39;utf-8&#39;) stopwords=stopwords[&#39;stopword&#39;].values #加载语料 laogong_df = pd.read_csv(laogong, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) laopo_df = pd.read_csv(laopo, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) erzi_df = pd.read_csv(erzi, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) nver_df = pd.read_csv(nver, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) #删除语料的nan行 laogong_df.dropna(inplace=True) laopo_df.dropna(inplace=True) erzi_df.dropna(inplace=True) nver_df.dropna(inplace=True) #转换 laogong = laogong_df.segment.values.tolist() laopo = laopo_df.segment.values.tolist() erzi = erzi_df.segment.values.tolist() nver = nver_df.segment.values.tolist() #定义分词和打标签函数preprocess_text #参数content_lines即为上面转换的list #参数sentences是定义的空list，用来储存打标签之后的数据 #参数category 是类型标签 def preprocess_text(content_lines, sentences, category): for line in content_lines: try: segs=jieba.lcut(line) segs = [v for v in segs if not str(v).isdigit()]#去数字 segs = list(filter(lambda x:x.strip(), segs)) #去左右空格 segs = list(filter(lambda x:len(x)&gt;1, segs)) #长度为1的字符 segs = list(filter(lambda x:x not in stopwords, segs)) #去掉停用词 sentences.append((&quot; &quot;.join(segs), category))# 打标签 except Exception: print(line) continue #调用函数、生成训练数据 sentences = [] preprocess_text(laogong, sentences, &#39;laogong&#39;) preprocess_text(laopo, sentences, &#39;laopo&#39;) preprocess_text(erzi, sentences, &#39;erzi&#39;) preprocess_text(nver, sentences, &#39;nver&#39;) #打散数据，生成更可靠的训练集 random.shuffle(sentences) #控制台输出前10条数据，观察一下 for sentence in sentences[:10]: print(sentence[0], sentence[1]) #用sk-learn对数据切分，分成训练集和测试集 from sklearn.model_selection import train_test_split x, y = zip(*sentences) x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1234) #抽取特征，我们对文本抽取词袋模型特征 from sklearn.feature_extraction.text import CountVectorizer vec = CountVectorizer( analyzer=&#39;word&#39;, # tokenise by character ngrams max_features=4000, # keep the most common 1000 ngrams ) vec.fit(x_train) #用朴素贝叶斯算法进行模型训练 from sklearn.naive_bayes import MultinomialNB classifier = MultinomialNB() classifier.fit(vec.transform(x_train), y_train) #对结果进行评分 print(classifier.score(vec.transform(x_test), y_test)) 这时输出结果为：0.99284009546539376。 我们看到，这个时候的结果评分已经很高了，当然我们的训练数据集中，噪声都已经提前处理完了，使得数据集在很少数据量下，模型得分就可以很高。 下面我们继续进行优化： #可以把特征做得更棒一点，试试加入抽取2-gram和3-gram的统计特征，比如可以把词库的量放大一点。 from sklearn.feature_extraction.text import CountVectorizer vec = CountVectorizer( analyzer=&#39;word&#39;, # tokenise by character ngrams ngram_range=(1,4), # use ngrams of size 1 and 2 max_features=20000, # keep the most common 1000 ngrams ) vec.fit(x_train) #用朴素贝叶斯算法进行模型训练 from sklearn.naive_bayes import MultinomialNB classifier = MultinomialNB() classifier.fit(vec.transform(x_train), y_train) #对结果进行评分 print(classifier.score(vec.transform(x_test), y_test)) 这时输出结果为：0.97852028639618138。 我们看到，这个时候的结果稍微有点下降，这与我们语料本身有关系，但是如果随着数据量更多，语料文本长点，我认为或许第二种方式应该比第一种方式效果更好。 可见使用 scikit-learn 包，算法模型开发非常简单，下面看看 SVM 的应用： from sklearn.svm import SVC svm = SVC(kernel=&#39;linear&#39;) svm.fit(vec.transform(x_train), y_train) print(svm.score(vec.transform(x_test), y_test)) 这时输出结果为：0.997613365155，比朴素贝叶斯效果更好。 语料数据下载地址：https://github.com/sujeek/chat_list 数据科学比赛大杀器 XGBoost 实战文本分类在说 XGBoost 之前，我们先简单从树模型说起，典型的决策树模型。决策树的学习过程主要包括： 特征选择： 从训练数据的特征中选择一个特征作为当前节点的分裂标准（特征选择的标准不同产生了不同的特征决策树算法，如根据信息增益、信息增益率和gini等）。 决策树生成： 根据所选特征评估标准，从上至下递归地生成子节点，直到数据集不可分则停止决策树生长。 剪枝： 决策树容易过拟合，需通过剪枝来预防过拟合（包括预剪枝和后剪枝）。 常见的决策树算法有 ID3、C4.5、CART 等。 在 sklearn 中决策树分类模型如下，可以看到默认通过 gini 计算实现。 sklearn.tree.DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False) 尽管决策树分类算法模型在应用中能够得到很好的结果，并通过剪枝操作提高模型泛化能力，但一棵树的生成肯定不如多棵树，因此就有了随机森林，并成功解决了决策树泛化能力弱的缺点，随机森林就是集成学习的一种方式。 在西瓜书中对集成学习的描述：集成学习通过将多个学习器进行结合，可获得比单一学习器显著优越的泛化性能，对“弱学习器” 尤为明显。弱学习器常指泛化性能略优于随机猜测的学习器。集成学习的结果通过投票法产生，即“少数服从多数”。个体学习不能太坏，并且要有“多样性”，即学习器间具有差异，即集成个体应“好而不同”。 假设基分类器的错误率相互独立，则由 Hoeffding 不等式可知，随着集成中个体分类器数目 T 的增大，集成的错误率将指数级下降，最终趋向于零。 但是这里有一个关键假设：基学习器的误差相互独立，而现实中个体学习器是为解决同一个问题训练出来的，所以不可能相互独立。因此，如何产生并结合“好而不同”的个体学习器是集成学习研究的核心。 集成学习大致分为两大类： Boosting：个体学习器间存在强依赖关系，必须串行生成的序列化方法。代表：AdaBoost、GBDT、XGBoost Bagging：个体学习器间不存在强依赖关系，可同时生成的并行化方法。代表：随机森林（Random Forest） 在 sklearn 中，对于 Random Forest、AdaBoost、GBDT 都有实现，下面我们重点说说在 kaggle、阿里天池等数据科学比赛经常会用到的大杀器 XGBoost，来实战文本分类 。 关于分类数据，还是延用《NLP 中文短文本分类项目实践（上）》中朴素贝叶斯算法的数据，这里对数据的标签做个修改，标签由 str 换成 int 类型，并从 0 开始，0、1、2、3 代表四类，所以是一个多分类问题： preprocess_text(laogong, sentences,0) #laogong 分类0 preprocess_text(laopo, sentences, 1) #laopo 分类1 preprocess_text(erzi, sentences, 2) #erzi 分类2 preprocess_text(nver, sentences,3) #nver 分类3 接着我们引入 XGBoost 的库（默认你已经安装好 XGBoost），整个代码加了注释，可以当做模板来用，每次使用只需微调即可使用。 import xgboost as xgb from sklearn.model_selection import StratifiedKFold import numpy as np # xgb矩阵赋值 xgb_train = xgb.DMatrix(vec.transform(x_train), label=y_train) xgb_test = xgb.DMatrix(vec.transform(x_test)) 上面在引入库和构建完 DMatrix 矩阵之后，下面主要是调参指标，可以根据参数进行调参： params = { &#39;booster&#39;: &#39;gbtree&#39;, #使用gbtree &#39;objective&#39;: &#39;multi:softmax&#39;, # 多分类的问题、 # &#39;objective&#39;: &#39;multi:softprob&#39;, # 多分类概率 #&#39;objective&#39;: &#39;binary:logistic&#39;, #二分类 &#39;eval_metric&#39;: &#39;merror&#39;, #logloss &#39;num_class&#39;: 4, # 类别数，与 multisoftmax 并用 &#39;gamma&#39;: 0.1, # 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。 &#39;max_depth&#39;: 8, # 构建树的深度，越大越容易过拟合 &#39;alpha&#39;: 0, # L1正则化系数 &#39;lambda&#39;: 10, # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。 &#39;subsample&#39;: 0.7, # 随机采样训练样本 &#39;colsample_bytree&#39;: 0.5, # 生成树时进行的列采样 &#39;min_child_weight&#39;: 3, # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言 # 假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。 # 这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。 &#39;silent&#39;: 0, # 设置成1则没有运行信息输出，最好是设置为0. &#39;eta&#39;: 0.03, # 如同学习率 &#39;seed&#39;: 1000, &#39;nthread&#39;: -1, # cpu 线程数 &#39;missing&#39;: 1 #&#39;scale_pos_weight&#39;: (np.sum(y==0)/np.sum(y==1)) # 用来处理正负样本不均衡的问题,通常取：sum(negative cases) / sum(positive cases) } 这里进行迭代次数设置和 k 折交叉验证，训练模型，并进行模型保存和预测结果。 plst = list(params.items()) num_rounds = 200 # 迭代次数 watchlist = [(xgb_train, &#39;train&#39;)] # 交叉验证 result = xgb.cv(plst, xgb_train, num_boost_round=200, nfold=4, early_stopping_rounds=200, verbose_eval=True, folds=StratifiedKFold(n_splits=4).split(vec.transform(x_train), y_train)) # 训练模型并保存 # early_stopping_rounds 当设置的迭代次数较大时，early_stopping_rounds 可在一定的迭代次数内准确率没有提升就停止训练 model = xgb.train(plst, xgb_train, num_rounds, watchlist, early_stopping_rounds=200) #model.save_model(&#39;../data/model/xgb.model&#39;) # 用于存储训练出的模型 #predicts = model.predict(xgb_test) #预测 词向量 Word2Vec 和 FastText 实战深度学习带给自然语言处理最令人兴奋的突破是词向量（Word Embedding）技术。词向量技术是将词语转化成为稠密向量。在自然语言处理应用中，词向量作为机器学习、深度学习模型的特征进行输入。因此，最终模型的效果很大程度上取决于词向量的效果。 Word2Vec 词向量在 Word2Vec 出现之前，自然语言处理经常把字词进行独热编码，也就是 One-Hot Encoder。 大数据 [0,0,0,0,0,0,0,1,0,……，0,0,0,0,0,0,0] 云计算[0,0,0,0,1,0,0,0,0,……，0,0,0,0,0,0,0] 机器学习[0,0,0,1,0,0,0,0,0,……，0,0,0,0,0,0,0] 人工智能[0,0,0,0,0,0,0,0,0,……，1,0,0,0,0,0,0] 比如上面的例子中，大数据 、云计算、机器学习和人工智能各对应一个向量，向量中只有一个值为 1，其余都为 0。所以使用 One-Hot Encoder 有以下问题：第一，词语编码是随机的，向量之间相互独立，看不出词语之间可能存在的关联关系。第二，向量维度的大小取决于语料库中词语的多少，如果语料包含的所有词语对应的向量合为一个矩阵的话，那这个矩阵过于稀疏，并且会造成维度灾难。 而解决这个问题的手段，就是使用向量表示（Vector Representations）。 Word2Vec 是 Google 团队 2013 年推出的，自提出后被广泛应用在自然语言处理任务中，并且受到它的启发，后续出现了更多形式的词向量模型。Word2Vec 主要包含两种模型：Skip-Gram 和 CBOW，值得一提的是，Word2Vec 词向量可以较好地表达不同词之间的相似和类比关系。 下面我们通过代码实战来体验一下 Word2Vec，pip install gensim 安装好库后，即可导入使用： 训练模型定义 from gensim.models import Word2Vec model = Word2Vec(sentences, sg=1, size=100, window=5, min_count=5, negative=3, sample=0.001, hs=1, workers=4) 参数解释： sg=1 是 skip-gram 算法，对低频词敏感；默认 sg=0 为 CBOW 算法。 size 是输出词向量的维数，值太小会导致词映射因为冲突而影响结果，值太大则会耗内存并使算法计算变慢，一般值取为 100 到 200 之间。 window 是句子中当前词与目标词之间的最大距离，3 表示在目标词前看 3-b 个词，后面看 b 个词（b 在 0-3 之间随机）。 min_count 是对词进行过滤，频率小于 min-count 的单词则会被忽视，默认值为 5。 negative 和 sample 可根据训练结果进行微调，sample 表示更高频率的词被随机下采样到所设置的阈值，默认值为 1e-3。 hs=1 表示层级 softmax 将会被使用，默认 hs=0 且 negative 不为 0，则负采样将会被选择使用。 详细参数说明可查看 Word2Vec 源代码。训练后的模型保存与加载，可以用来计算相似性，最大匹配程度等。 model.save(model) #保存模型 model = Word2Vec.load(model) #加载模型 model.most_similar(positive=[&#39;女人&#39;, &#39;女王&#39;], negative=[&#39;男人&#39;]) #输出[(&#39;女王&#39;, 0.50882536), ...] model.similarity(&#39;女人&#39;, &#39;男人&#39;) #输出0.73723527 FastText词向量FastText 是 facebook 开源的一个词向量与文本分类工具，模型简单，训练速度非常快。FastText 做的事情，就是把文档中所有词通过 lookup table 变成向量，取平均后直接用线性分类器得到分类结果。 FastText python 包的安装： pip install fasttext FastText 做文本分类要求文本是如下的存储形式： __label__1 ，内容。 __label__2，内容。 __label__3 ，内容。 __label__4，内容。 其中前面的 label 是前缀，也可以自己定义，label 后接的为类别，之后接的就是我是我们的文本内容。 调用 fastText 训练生成模型，对模型效果进行评估。 import fasttext classifier = fasttext.supervised(&#39;train_data.txt&#39;, &#39;classifier.model&#39;, label_prefix=&#39;__label__&#39;) result = classifier.test(&#39;train_data.txt&#39;) print(result.precision) print(result.recall) 实际预测过程： label_to_cate = {1:&#39;科技&#39;, 2:&#39;财经&#39;, 3:&#39;体育&#39;, 4:&#39;生活&#39;, 5:&#39;美食&#39;} texts = [&#39;现如今 机器 学习 和 深度 学习 带动 人工智能 飞速 的 发展 并 在 图片 处理 语音 识别 领域 取得 巨大成功&#39;] labels = classifier.predict(texts) print(labels) print(label_to_cate[int(labels[0][0])]) [[u&#39;1&#39;]] 科技 文本分类之神经网络 CNN 和 LSTM 实战CNN 做文本分类实战CNN 在图像上的巨大成功，使得大家都有在文本上试一把的冲动。CNN 的原理这里就不赘述了，关键看看怎样用于文本分类的，下图是一个 TextCNN 的结构（来源：网络）： 具体结构介绍： 输入层可以把输入层理解成把一句话转化成了一个二维的图像：每一排是一个词的 Word2Vec 向量，纵向是这句话的每个词按序排列。输入数据的 size，也就是图像的 size，n×k，n 代表训练数据中最长的句子的词个数，k 是 embbeding 的维度。从输入层还可以看出 kernel 的 size。很明显 kernel 的高 (h) 会有不同的值，有的是 2，有的是 3。这很容易理解，不同的 kernel 想获取不同范围内词的关系；和图像不同的是，NLP 中的 CNN 的 kernel 的宽 (w) 一般都是图像的宽，也就是 Word2Vec 的维度，这也可以理解，因为我们需要获得的是纵向的差异信息，也就是不同范围的词出现会带来什么信息。 卷积层由于 kernel 的特殊形状，因此卷积后的 feature map 是一个宽度是 1 的长条。 池化层这里使用 MaxPooling，并且一个 feature map 只选一个最大值留下。这被认为是按照这个 kernel 卷积后的最重要的特征。 全连接层这里的全连接层是带 dropout 的全连接层和 softmax。 下面我们看看自己用 Tensorflow 如何实现一个文本分类器： 超参数定义： #文档最长长度 MAX_DOCUMENT_LENGTH = 100 #最小词频数 MIN_WORD_FREQUENCE = 2 #词嵌入的维度 EMBEDDING_SIZE = 20 #filter个数 N_FILTERS = 10 #感知野大小 WINDOW_SIZE = 20 #filter的形状 FILTER_SHAPE1 = [WINDOW_SIZE, EMBEDDING_SIZE] FILTER_SHAPE2 = [WINDOW_SIZE, N_FILTERS] #池化 POOLING_WINDOW = 4 POOLING_STRIDE = 2 n_words = 0 网络结构定义，2 层的卷积神经网络，用于短文本分类： def cnn_model(features, target): # 先把词转成词嵌入 # 我们得到一个形状为[n_words, EMBEDDING_SIZE]的词表映射矩阵 # 接着我们可以把一批文本映射成[batch_size, sequence_length, EMBEDDING_SIZE]的矩阵形式 target = tf.one_hot(target, 15, 1, 0) word_vectors = tf.contrib.layers.embed_sequence( features, vocab_size=n_words, embed_dim=EMBEDDING_SIZE, scope=&#39;words&#39;) word_vectors = tf.expand_dims(word_vectors, 3) with tf.variable_scope(&#39;CNN_Layer1&#39;): # 添加卷积层做滤波 conv1 = tf.contrib.layers.convolution2d( word_vectors, N_FILTERS, FILTER_SHAPE1, padding=&#39;VALID&#39;) # 添加RELU非线性 conv1 = tf.nn.relu(conv1) # 最大池化 pool1 = tf.nn.max_pool( conv1, ksize=[1, POOLING_WINDOW, 1, 1], strides=[1, POOLING_STRIDE, 1, 1], padding=&#39;SAME&#39;) # 对矩阵进行转置，以满足形状 pool1 = tf.transpose(pool1, [0, 1, 3, 2]) with tf.variable_scope(&#39;CNN_Layer2&#39;): # 第2个卷积层 conv2 = tf.contrib.layers.convolution2d( pool1, N_FILTERS, FILTER_SHAPE2, padding=&#39;VALID&#39;) # 抽取特征 pool2 = tf.squeeze(tf.reduce_max(conv2, 1), squeeze_dims=[1]) # 全连接层 logits = tf.contrib.layers.fully_connected(pool2, 15, activation_fn=None) loss = tf.losses.softmax_cross_entropy(target, logits) train_op = tf.contrib.layers.optimize_loss( loss, tf.contrib.framework.get_global_step(), optimizer=&#39;Adam&#39;, learning_rate=0.01) return ({ &#39;class&#39;: tf.argmax(logits, 1), &#39;prob&#39;: tf.nn.softmax(logits) }, loss, train_op) LSTM 做文本分类实战上面实现了基于 CNN 的文本分类器之后，再来做一个基于 LSTM 分类器，会觉得非常容易，因为变化的部分只偶遇中间把 CNN 换成了 LSTM 模型。关于 RNN 和 LSTM 的理论知识，请自行解决，这里主要给出思路和代码实现。 具体结构，参照下面这幅图（来源：网络）： 上面我们用的 Tensorflow 实现的，这次我们用 Keras 更简单方便的实现其核心代码： #超参数定义 MAX_SEQUENCE_LENGTH = 100 EMBEDDING_DIM = 200 VALIDATION_SPLIT = 0.16 TEST_SPLIT = 0.2 epochs = 10 batch_size = 128 #模型网络定义 model = Sequential() model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)) model.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2)) model.add(Dropout(0.2)) model.add(Dense(labels.shape[1], activation=&#39;softmax&#39;)) model.summary() 可见，基于 Keras 实现神经网络比 Tensorflow 要简单和容易很多，Keras 搭建神经网络俗称“堆积木”，这里有所体现。所以笔者也推荐，如果想快速实现一个神经网络，建议先用 Keras 快速搭建验证，之后再尝试用 Tensorflow 去实现。 总结，本次 Chat 就将分享到这里，从 XGBoost 到词向量以及神经网络，内容非常多也非常重要，笔者并没有过多的讲述理论并不代表不重要，相反理论很重要，但笔者希望能够先通过代码实战进行体验，然后静下心来完成理论部分的学习，最后代码和理论相结合，效率更高。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>jieba</tag>
        <tag>lstm</tag>
        <tag>lda</tag>
        <tag>cnn</tag>
        <tag>svm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HMM和CRF（8）]]></title>
    <url>%2F2019%2F11%2F22%2F8.HMM%E5%92%8CCRF%2F</url>
    <content type="text"><![CDATA[HMM（隐马尔可夫模型）和 CRF（条件随机场）算法常常被用于分词、句法分析、命名实体识别、词性标注等。由于两者之间有很大的共同点，所以在很多应用上往往是重叠的，但在命名实体、句法分析等领域 CRF 似乎更胜一筹。 在机器学习中，生成式模型和判别式模型都用于有监督学习，有监督学习的任务就是从数据中学习一个模型（也叫分类器），应用这一模型，对给定的输入 X 预测相应的输出 Y。这个模型的一般形式为：决策函数 Y=f(X) 或者条件概率分布 P(Y|X)。 首先，简单从贝叶斯定理说起，若记 P(A)、P(B) 分别表示事件 A 和事件 B 发生的概率，则 P(A|B) 表示事件 B 发生的情况下事件 A 发生的概率；P(AB)表示事件 A 和事件 B 同时发生的概率。 生成式模型：估计的是联合概率分布，P(Y, X)=P(Y|X)*P(X)，由联合概率密度分布 P(X,Y)，然后求出条件概率分布 P(Y|X) 作为预测的模型，即生成模型公式为：P(Y|X)= P(X,Y)/ P(X)。基本思想是首先建立样本的联合概率密度模型 P(X,Y)，然后再得到后验概率 P(Y|X)，再利用它进行分类，其主要关心的是给定输入 X 产生输出 Y 的生成关系。 判别式模型：估计的是条件概率分布， P(Y|X)，是给定观测变量 X 和目标变量 Y 的条件模型。由数据直接学习决策函数 Y=f(X) 或者条件概率分布 P(Y|X) 作为预测的模型，其主要关心的是对于给定的输入 X，应该预测什么样的输出 Y。 HMM 使用隐含变量生成可观测状态，其生成概率有标注集统计得到，是一个生成模型。其他常见的生成式模型有：Gaussian、 Naive Bayes、Mixtures of multinomials 等。 CRF 就像一个反向的隐马尔可夫模型（HMM），通过可观测状态判别隐含变量，其概率亦通过标注集统计得来，是一个判别模型。其他常见的判别式模型有：K 近邻法、感知机、决策树、逻辑斯谛回归模型、最大熵模型、支持向量机、提升方法等。 基于 HMM 训练自己的 Python 中文分词器HMM 模型是由一个“五元组”组成的集合： StatusSet：状态值集合，状态值集合为 (B, M, E, S)，其中 B 为词的首个字，M 为词中间的字，E 为词语中最后一个字，S 为单个字，B、M、E、S 每个状态代表的是该字在词语中的位置。 举个例子，对“中国的人工智能发展进入高潮阶段”，分词可以标注为：“中B国E的S人B工E智B能E发B展E进B入E高B潮E阶B段E”，最后的分词结果为：[‘中国’, ‘的’, ‘人工’, ‘智能’, ‘发展’, ‘进入’, ‘高潮’, ‘阶段’]。 ObservedSet：观察值集合，观察值集合就是所有语料的汉字，甚至包括标点符号所组成的集合。 TransProbMatrix：转移概率矩阵，状态转移概率矩阵的含义就是从状态 X 转移到状态 Y 的概率，是一个4×4的矩阵，即 {B,E,M,S}×{B,E,M,S}。 EmitProbMatrix：发射概率矩阵，发射概率矩阵的每个元素都是一个条件概率，代表 P(Observed[i]|Status[j]) 概率。 InitStatus：初始状态分布，初始状态概率分布表示句子的第一个字属于 {B,E,M,S} 这四种状态的概率。 将 HMM 应用在分词上，要解决的问题是：参数（ObservedSet、TransProbMatrix、EmitRobMatrix、InitStatus）已知的情况下，求解状态值序列。解决这个问题的最有名的方法是 Viterbi 算法。 本次训练使用的预料 syj_trainCorpus_utf8.txt ,整个语料大小 264M，包含1116903条数据，UTF-8 编码，词与词之间用空格隔开，用来训练分词模型。语料格式，用空格隔开的： 如果 继续 听任 资产阶级 自由化 的 思潮 泛滥 ， 党 就 失去 了 凝聚力 和 战斗力 ， 怎么 能 成为 全国 人民 的 领导 核心 ？ 中国 又 会 成为 一盘散沙 ， 那 还有 什么 希望 ？预定义# 用来模型保存 import pickle import json # 定义 HMM 中的状态，初始化概率，以及中文停顿词 STATES = {&#39;B&#39;, &#39;M&#39;, &#39;E&#39;, &#39;S&#39;} EPS = 0.0001 #定义停顿标点 seg_stop_words = {&quot; &quot;,&quot;，&quot;,&quot;。&quot;,&quot;“&quot;,&quot;”&quot;,&#39;“&#39;, &quot;？&quot;, &quot;！&quot;, &quot;：&quot;, &quot;《&quot;, &quot;》&quot;, &quot;、&quot;, &quot;；&quot;, &quot;·&quot;, &quot;‘ &quot;, &quot;’&quot;, &quot;──&quot;, &quot;,&quot;, &quot;.&quot;, &quot;?&quot;, &quot;!&quot;, &quot;`&quot;, &quot;~&quot;, &quot;@&quot;, &quot;#&quot;, &quot;$&quot;, &quot;%&quot;, &quot;^&quot;, &quot;&amp;&quot;, &quot;*&quot;, &quot;(&quot;, &quot;)&quot;, &quot;-&quot;, &quot;_&quot;, &quot;+&quot;, &quot;=&quot;, &quot;[&quot;, &quot;]&quot;, &quot;{&quot;, &quot;}&quot;, &#39;&quot;&#39;, &quot;&#39;&quot;, &quot;&lt;&quot;, &quot;&gt;&quot;, &quot;\\&quot;, &quot;|&quot; &quot;\r&quot;, &quot;\n&quot;,&quot;\t&quot;} 编码实现将 HMM 模型封装为独立的类 HMM_Model class HMM_Model: def __init__(self): pass #初始化 def setup(self): pass #模型保存 def save(self, filename, code): pass #模型加载 def load(self, filename, code): pass #模型训练 def do_train(self, observes, states): pass #HMM计算 def get_prob(self): pass #模型预测 def do_predict(self, sequence): pass 第一个方法 __init__()是一种特殊的方法，被称为类的构造函数或初始化方法，当创建了这个类的实例时就会调用该方法，其中定义了数据结构和初始变量，实现如下： def __init__(self): self.trans_mat = {} self.emit_mat = {} self.init_vec = {} self.state_count = {} self.states = {} self.inited = False 其中的数据结构定义： trans_mat：状态转移矩阵，trans_mat[state1][state2] 表示训练集中由 state1 转移到 state2 的次数。 emit_mat：观测矩阵，emit_mat[state][char] 表示训练集中单字 char 被标注为 state 的次数。 init_vec：初始状态分布向量，init_vec[state] 表示状态 state 在训练集中出现的次数。 state_count：状态统计向量，state_count[state]表示状态 state 出现的次数。 word_set：词集合，包含所有单词。第二个方法 setup()，初始化第一个方法中的数据结构，具体实现如下：#初始化数据结构 def setup(self): for state in self.states: # build trans_mat self.trans_mat[state] = {} for target in self.states: self.trans_mat[state][target] = 0.0 self.emit_mat[state] = {} self.init_vec[state] = 0 self.state_count[state] = 0 self.inited = True 第三个方法 save()，用来保存训练好的模型，filename 指定模型名称，默认模型名称为 hmm.json，这里提供两种格式的保存类型，JSON 或者pickle 格式，通过参数 code 来决定，code 的值为 code=&#39;json&#39; 或者 code = &#39;pickle&#39;，默认为code=&#39;json&#39;，具体实现如下：#模型保存 def save(self, filename=&quot;hmm.json&quot;, code=&#39;json&#39;): fw = open(filename, &#39;w&#39;, encoding=&#39;utf-8&#39;) data = { &quot;trans_mat&quot;: self.trans_mat, &quot;emit_mat&quot;: self.emit_mat, &quot;init_vec&quot;: self.init_vec, &quot;state_count&quot;: self.state_count } if code == &quot;json&quot;: txt = json.dumps(data) txt = txt.encode(&#39;utf-8&#39;).decode(&#39;unicode-escape&#39;) fw.write(txt) elif code == &quot;pickle&quot;: pickle.dump(data, fw) fw.close() 第四个方法 load()，与第三个 save() 方法对应，用来加载模型，filename 指定模型名称，默认模型名称为hmm.json，这里提供两种格式的保存类型，JSON 或者 pickle 格式，通过参数 code 来决定，code 的值为 code=&#39;json&#39;或者 code = &#39;pickle&#39;，默认为 code=&#39;json&#39;，具体实现如下： #模型加载 def load(self, filename=&quot;hmm.json&quot;, code=&quot;json&quot;): fr = open(filename, &#39;r&#39;, encoding=&#39;utf-8&#39;) if code == &quot;json&quot;: txt = fr.read() model = json.loads(txt) elif code == &quot;pickle&quot;: model = pickle.load(fr) self.trans_mat = model[&quot;trans_mat&quot;] self.emit_mat = model[&quot;emit_mat&quot;] self.init_vec = model[&quot;init_vec&quot;] self.state_count = model[&quot;state_count&quot;] self.inited = True fr.close() 第五个方法 do_train()，用来训练模型，因为使用的标注数据集， 因此可以使用更简单的监督学习算法，训练函数输入观测序列和状态序列进行训练，依次更新各矩阵数据。类中维护的模型参数均为频数而非频率，这样的设计使得模型可以进行在线训练，使得模型随时都可以接受新的训练数据继续训练，不会丢失前次训练的结果。具体实现如下： #模型训练 def do_train(self, observes, states): if not self.inited: self.setup() for i in range(len(states)): if i == 0: self.init_vec[states[0]] += 1 self.state_count[states[0]] += 1 else: self.trans_mat[states[i - 1]][states[i]] += 1 self.state_count[states[i]] += 1 if observes[i] not in self.emit_mat[states[i]]: self.emit_mat[states[i]][observes[i]] = 1 else: self.emit_mat[states[i]][observes[i]] += 1 第六个方法 get_prob()，在进行预测前，需将数据结构的频数转换为频率，具体实现如下： #频数转频率 def get_prob(self): init_vec = {} trans_mat = {} emit_mat = {} default = max(self.state_count.values()) for key in self.init_vec: if self.state_count[key] != 0: init_vec[key] = float(self.init_vec[key]) / self.state_count[key] else: init_vec[key] = float(self.init_vec[key]) / default for key1 in self.trans_mat: trans_mat[key1] = {} for key2 in self.trans_mat[key1]: if self.state_count[key1] != 0: trans_mat[key1][key2] = float(self.trans_mat[key1][key2]) / self.state_count[key1] else: trans_mat[key1][key2] = float(self.trans_mat[key1][key2]) / default for key1 in self.emit_mat: emit_mat[key1] = {} for key2 in self.emit_mat[key1]: if self.state_count[key1] != 0: emit_mat[key1][key2] = float(self.emit_mat[key1][key2]) / self.state_count[key1] else: emit_mat[key1][key2] = float(self.emit_mat[key1][key2]) / default return init_vec, trans_mat, emit_mat 第七个方法 do_predict()，预测采用 Viterbi 算法求得最优路径， 具体实现如下： #模型预测 def do_predict(self, sequence): tab = [{}] path = {} init_vec, trans_mat, emit_mat = self.get_prob() # 初始化 for state in self.states: tab[0][state] = init_vec[state] * emit_mat[state].get(sequence[0], EPS) path[state] = [state] # 创建动态搜索表 for t in range(1, len(sequence)): tab.append({}) new_path = {} for state1 in self.states: items = [] for state2 in self.states: if tab[t - 1][state2] == 0: continue prob = tab[t - 1][state2] * trans_mat[state2].get(state1, EPS) * emit_mat[state1].get(sequence[t], EPS) items.append((prob, state2)) best = max(items) tab[t][state1] = best[0] new_path[state1] = path[best[1]] + [state1] path = new_path # 搜索最有路径 prob, state = max([(tab[len(sequence) - 1][state], state) for state in self.states]) return path[state] 上面实现了类 HMM_Model 的7个方法，接下来我们来实现分词器，这里先定义两个函数，这两个函数是独立的，不在类中。 定义一个工具函数，对输入的训练语料中的每个词进行标注，因为训练数据是空格隔开的，可以进行转态标注，该方法用在训练数据的标注，具体实现如下： def get_tags(src): tags = [] if len(src) == 1: tags = [&#39;S&#39;] elif len(src) == 2: tags = [&#39;B&#39;, &#39;E&#39;] else: m_num = len(src) - 2 tags.append(&#39;B&#39;) tags.extend([&#39;M&#39;] * m_num) tags.append(&#39;E&#39;) return tags 定义一个工具函数,根据预测得到的标注序列将输入的句子分割为词语列表，也就是预测得到的状态序列，解析成一个 list 列表进行返回，具体实现如下： def cut_sent(src, tags): word_list = [] start = -1 started = False if len(tags) != len(src): return None if tags[-1] not in {&#39;S&#39;, &#39;E&#39;}: if tags[-2] in {&#39;S&#39;, &#39;E&#39;}: tags[-1] = &#39;S&#39; else: tags[-1] = &#39;E&#39; for i in range(len(tags)): if tags[i] == &#39;S&#39;: if started: started = False word_list.append(src[start:i]) word_list.append(src[i]) elif tags[i] == &#39;B&#39;: if started: word_list.append(src[start:i]) start = i started = True elif tags[i] == &#39;E&#39;: started = False word = src[start:i+1] word_list.append(word) elif tags[i] == &#39;M&#39;: continue return word_list 最后，我们来定义分词器类 HMMSoyoger，继承 HMM_Model 类并实现中文分词器训练、分词功能，先给出 HMMSoyoger 类的结构定义： class HMMSoyoger(HMM_Model): def __init__(self, *args, **kwargs): pass #加载训练数据 def read_txt(self, filename): pass #模型训练函数 def train(self): pass #模型分词预测 def lcut(self, sentence): pass 第一个方法 init()，构造函数，定义了初始化变量，具体实现如下： def __init__(self, *args, **kwargs): super(HMMSoyoger, self).__init__(*args, **kwargs) self.states = STATES self.data = None 第二个方法 read_txt()，加载训练语料，读入文件为 txt，并且 UTF-8 编码，防止中文出现乱码，具体实现如下： #加载语料 def read_txt(self, filename): self.data = open(filename, &#39;r&#39;, encoding=&quot;utf-8&quot;) 第三个方法 train()，根据单词生成观测序列和状态序列，并通过父类的 do_train() 方法进行训练，具体实现如下： def train(self): if not self.inited: self.setup() for line in self.data: line = line.strip() if not line: continue #观测序列 observes = [] for i in range(len(line)): if line[i] == &quot; &quot;: continue observes.append(line[i]) #状态序列 words = line.split(&quot; &quot;) states = [] for word in words: if word in seg_stop_words: continue states.extend(get_tags(word)) #开始训练 if(len(observes) &gt;= len(states)): self.do_train(observes, states) else: pass 第四个方法 lcut()，模型训练好之后，通过该方法进行分词测试，具体实现如下： def lcut(self, sentence): try: tags = self.do_predict(sentence) return cut_sent(sentence, tags) except: return sentence 通过上面两个类和两个方法，就完成了基于 HMM 的中文分词器编码，下面我们来进行模型训练和测试。 训练模型首先实例化 HMMSoyoger 类，然后通过 read_txt() 方法加载语料，再通过 train()进行在线训练，如果训练语料比较大，可能需要等待一点时间，具体实现如下： soyoger = HMMSoyoger() soyoger.read_txt(&quot;syj_trainCorpus_utf8.txt&quot;) soyoger.train() 模型测试模型训练完成之后，我们就可以进行测试： soyoger.lcut(&quot;中国的人工智能发展进入高潮阶段。&quot;) 得到结果为： [‘中国’, ‘的’, ‘人工’, ‘智能’, ‘发展’, ‘进入’, ‘高潮’, ‘阶段’, ‘。’] soyoger.lcut(&quot;中文自然语言处理是人工智能技术的一个重要分支。&quot;) 得到结果为： [‘中文’, ‘自然’, ‘语言’, ‘处理’, ‘是人’, ‘工智’, ‘能技’, ‘术的’, ‘一个’, ‘重要’, ‘分支’, ‘。’] 可见，最后的结果还是不错的，如果想取得更好的结果，可自行制备更大更丰富的训练数据集。 基于 CRF 的开源中文分词工具 Genius 实践Genius 是一个基于 CRF 的开源中文分词工具，采用了 Wapiti 做训练与序列标注，支持 Python 2.x、Python 3.x。 安装（1）下载源码 在 Github 上下载源码地址，解压源码，然后通过 python setup.py install 安装。 （2）Pypi 安装 通过执行命令：easy_install genius 或者 pip install genius 安装。 分词首先引入 Genius，然后对 text 文本进行分词。 import genius text = u&quot;&quot;&quot;中文自然语言处理是人工智能技术的一个重要分支。&quot;&quot;&quot; seg_list = genius.seg_text( text, use_combine=True, use_pinyin_segment=True, use_tagging=True, use_break=True ) print(&#39; &#39;.join([word.text for word in seg_list]) 其中，genius.seg_text 函数接受5个参数，其中 text 是必填参数： text 第一个参数为需要分词的字。 use_break 代表对分词结构进行打断处理，默认值 True。 use_combine 代表是否使用字典进行词合并，默认值 False。 use_tagging 代表是否进行词性标注，默认值 True。 use_pinyin_segment 代表是否对拼音进行分词处理，默认值 True。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>crf</tag>
        <tag>hmm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4j从入门到构建一个简单知识图谱（20）]]></title>
    <url>%2F2019%2F11%2F22%2F20.Neo4j%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%2F</url>
    <content type="text"><![CDATA[Neo4j 对于大多数人来说，可能是比较陌生的。其实，Neo4j 是一个图形数据库，就像传统的关系数据库中的 Oracel 和MySQL一样，用来持久化数据。Neo4j 是最近几年发展起来的新技术，属于 NoSQL 数据库中的一种。 本文主要从 Neo4j 为什么被用来做知识图谱，Neo4j 的简单安装，在 Neo4j 浏览器中创建节点和关系，Neo4j 的 Python 接口操作以及用Neo4j 构建一个简单的农业知识图谱五个方面来讲。 Neo4j 为什么被用来做知识图谱从第19课《知识挖掘与知识图谱概述》中，我们已经明白，知识图谱是一种基于图的数据结构，由节点和边组成。其中节点即实体，由一个全局唯一的 ID标示，关系（也称属性）用于连接两个节点。通俗地讲，知识图谱就是把所有不同种类的信息连接在一起而得到一个关系网络，提供了从“关系”的角度去分析问题的能力。 而 Neo4j 作为一种经过特别优化的图形数据库，有以下优势： 数据存储 ：不像传统数据库整条记录来存储数据，Neo4j 以图的结构存储，可以存储图的节点、属性和边。属性、节点都是分开存储的，属性与节点的关系构成边，这将大大有助于提高数据库的性能。 数据读写 ：在 Neo4j 中，存储节点时使用了 Index-free Adjacency 技术，即每个节点都有指向其邻居节点的指针，可以让我们在时间复杂度为 O(1) 的情况下找到邻居节点。另外，按照官方的说法，在 Neo4j 中边是最重要的，是 First-class Entities，所以单独存储，更有利于在图遍历时提高速度，也可以很方便地以任何方向进行遍历。 资源丰富 ：Neo4j 作为较早的一批图形数据库之一，其文档和各种技术博客较多。 同类对比 ：Flockdb 安装过程中依赖太多，安装复杂；Orientdb，Arangodb 与 Neo4j 做对比，从易用性来说都差不多，但是从稳定性来说，neo4j 是最好的。 综合上述以及因素，我认为 Neo4j 是做知识图谱比较简单、灵活、易用的图形数据库。 Neo4j 的简单安装Neo4j 是基于 Java 的图形数据库，运行 Neo4j 需要启动 JVM 进程，因此必须安装 Java SE 的 JDK。从 Oracle官方网站下载 Java SEJDK，选择版本JDK8 以上版本即可。 下面简单介绍下 Neo4j 在 Linux 和 Windows的安装过程。首先去官网下载对应版本。解压之后，Neo4j 应用程序有如下主要的目录结构： bin 目录：用于存储 Neo4j 的可执行程序； conf 目录：用于控制 Neo4j 启动的配置文件； data 目录：用于存储核心数据库文件； plugins 目录：用于存储 Neo4j 的插件。 Linux 系统下的安装通过 tar 解压命令解压到一个目录下： tar -xzvf neo4j-community-3.3.1-unix.tar.gz 然后进入 Neo4j 解压目录： cd /usr/local/neo4j/neo4j-community-3.1.0 通过启动命令，可以实现启动、控制台、停止服务： bin/neo4j start/console/stop（启动/控制台/停止） 通过 cypher-shell 命令，可以进入命令行： bin/cypher-shell Windows 系统下的安装启动 DOS 命令行窗口，切换到解压目录 bin 下，以管理员身份运行命令，分别为启动服务、停止服务、重启服务和查询服务的状态： bin\neo4j start bin\neo4j stop bin\neo4j restart bin\neo4j status 把 Neo4j 安装为服务（Windows Services），可通过以下命令： bin\neo4j install-service bin\neo4j uninstall-service Neo4j 的配置文档存储在 conf 目录下，Neo4j 通过配置文件 neo4j.conf控制服务器的工作。默认情况下，不需要进行任意配置，就可以启动服务器。 下面我们在 Windows 环境下启动 Neo4j： Neo4j 服务器具有一个集成的浏览器，在一个运行的服务器实例上访问： http://localhost:7474/，打开浏览器，显示启动页面： 默认的 Host 是 bolt://localhost:7687，默认的用户是 neo4j，其默认的密码是 neo4j，第一次成功登录到 Neo4j服务器之后，需要重置密码。访问 Graph Database 需要输入身份验证，Host 是 Bolt 协议标识的主机。登录成功后界面： 到此为止，我们就完成了 Neo4j 的基本安装过程，更详细的参数配置，可以参考官方文档。 在 Neo4j 浏览器中创建节点和关系下面，我们简单编写 Cypher 命令，Cypher 命令可以通过 Neo4j教程学习，在浏览器中通过 Neo4j 创建两个节点和两个关系。 在 $ 命令行中，编写 Cypher 脚本代码，点击 Play 按钮完成创建，依次执行下面的语句： CREATE (n:Person { name: &#39;Andres&#39;, title: &#39;Developer&#39; }) return n; 作用是创建一个 Person，并包含属性名字和职称。 下面这条语句也创建了一个 Person 对象，属性中只是名字和职称不一样。 CREATE (n:Person { name: &#39;Vic&#39;, title: &#39;Developer&#39; }) return n; 紧接着，通过下面两行命令进行两个 Person 的关系匹配： match(n:Person{name:&quot;Vic&quot;}),(m:Person{name:&quot;Andres&quot;}) create (n)-[r:Friend]-&gt;(m) return r; match(n:Person{name:&quot;Vic&quot;}),(m:Person{name:&quot;Andres&quot;}) create (n)&lt;-[r:Friend]-(m) return r; 最后，在创建完两个节点和关系之后，查看数据库中的图形： match(n) return n; 如下图，返回两个 Person 节点，以及其关系网，两个 Person 之间组成 Friend 关系： Neo4j 的 Python 操作既然 Neo4j 作为一个图库数据库，那我们在项目中使用的时候，必然不能通过上面那种方式完成任务，一般都要通过代码来完成数据的持久化操作。其中，对于Java 编程者来说，可通过 Spring Data Neo4j 达到这一目的。 而对于 Python 开发者来说，Py2neo 库也可以完成对 Neo4j 的操作，操作过程如下。 首先 安装 Py2neo。Py2neo 的安装过程非常简单，在命令行通过下面命令即可安装成功。 pip install py2neo 安装好之后，我们来看一下简单的图关系构建，看下面代码： from py2neo.data import Node, Relationship a = Node(&quot;Person&quot;, name=&quot;Alice&quot;) b = Node(&quot;Person&quot;, name=&quot;Bob&quot;) ab = Relationship(a, &quot;KNOWS&quot;, b) 第一行代码，首先引入 Node 和 Relationship 对象，紧接着，创建 a 和 b 节点对象，最后一行匹配 a 和 b之间的工作雇佣关系。接着来看看 ab 对象的内容是什么： print(ab) 通过 print 打印出 ab 的内容： (Alice)-[:KNOWS {}]-&gt;(Bob) 通过这样，就完成了 Alice 和 Bob 之间的工作关系，如果有多组关系将构建成 Person 之间的一个关系网。 了解更多 Py2neo 的使用方法，建议查看官方文档。 用 Neo4j 构建一个简单的农业知识图谱我们来看一个基于开源语料的简单农业知识图谱，由于过程比较繁杂，数据和知识图谱数据预处理过程这里不再赘述，下面，我们重点看基于 Neo4j来创建知识图谱的过程。 整个过程主要包含以下步骤： 环境准备 语料准备 语料加载 知识图谱查询展示 Neo4j 环境准备。根据上面对 Neo4j 环境的介绍，这里默认你已经搭建好 Neo4j 的环境，并能正常访问，如果没有环境，请自行搭建好 Neo4j 的可用环境。 数据语料介绍。本次提供的语料是已经处理好的数据，包含6个 csv 文件，文件内容和描述如下。 attributes.csv：文件大小 2M，内容是通过互动百科页面得到的部分实体的属性，包含字段：Entity、AttributeName、Attribute，分别表示实体、属性名称、属性值。文件前5行结构如下： Entity,AttributeName,Attribute 密度板,别名,纤维板 葡萄蔓枯病,主要为害部位,枝蔓 坎德拉,性别,男 坎德拉,国籍,法国 坎德拉,场上位置,后卫 hudong_pedia.csv：文件大小 94.6M，内容是已经爬好的农业实体的百科页面的结构化数据，包含字段：title、url、image、openTypeList、detail、baseInfoKeyList、baseInfoValueList，分别表示名称、百科 URL 地址、图片、分类类型、详情、关键字、依据来源。文件前2行结构如下： &quot;title&quot;,&quot;url&quot;,&quot;image&quot;,&quot;openTypeList&quot;,&quot;detail&quot;,&quot;baseInfoKeyList&quot;,&quot;baseInfoValueList&quot; &quot;菊糖&quot;,&quot;http://www.baike.com/wiki/菊糖&quot;,&quot;http://a0.att.hudong.com/72/85/20200000013920144736851207227_s.jpg&quot;,&quot;健康科学##分子生物学##化学品##有机物##科学##自然科学##药品##药学名词##药物中文名称列表&quot;,&quot;[药理作用] 诊断试剂 人体内不含菊糖，静注后，不被机体分解、结合、利用和破坏，经肾小球滤过，通过测定血中和尿中的菊糖含量，可以准确计算肾小球的滤过率。菊糖广泛存在于植物组织中,约有3.6万种植物中含有菊糖,尤其是菊芋、菊苣块根中含有丰富的菊糖[6,8]。菊芋(Jerusalem artichoke)又名洋姜,多年生草本植物,在我国栽种广泛,其适应性广、耐贫瘠、产量高、易种植,一般亩产菊芋块茎为2 000～4 000 kg,菊芋块茎除水分外,还含有15%～20%的菊糖,是加工生产菊糖及其制品的良好原料。&quot;,&quot;中文名：&quot;,&quot;菊糖&quot; &quot;密度板&quot;,&quot;http://www.baike.com/wiki/密度板&quot;,&quot;http://a0.att.hudong.com/64/31/20200000013920144728317993941_s.jpg&quot;,&quot;居家##巧克力包装##应用科学##建筑材料##珠宝盒##礼品盒##科学##糖果盒##红酒盒##装修##装饰材料##隔断##首饰盒&quot;,&quot;密度板（英文：Medium Density Fiberboard (MDF)）也称纤维板，是以木质纤维或其他植物纤维为原料，施加脲醛树脂或其他适用的胶粘剂制成的人造板材。按其密度的不同，分为高密度板、中密度板、低密度板。密度板由于质软耐冲击，也容易再加工，在国外是制作家私的一种良好材料，但由于国家关于高密度板的标准比国际标准低数倍，所以，密度板在中国的使用质量还有待提高。&quot;,&quot;中文名：##全称：##别名：##主要材料：##分类：##优点：&quot;,&quot;密度板##中密度板纤维板##纤维板##以木质纤维或其他植物纤维##高密度板、中密度板、低密度板##表面光滑平整、材质细密性能稳定&quot; hudong_pedia2.csv：文件大小 41M，内容结构和 hudong_pedia.csv 文件保持一致，只是增加数据量，作为 hudong_pedia.csv 数据的补充。 new_node.csv：文件大小 2.28M，内容是节点名称和标签，包含字段：title、lable，分别表示节点名称、标签，文件前5行结构如下： title,lable 药物治疗,newNode 膳食纤维,newNode Boven Merwede,newNode 亚美尼亚苏维埃百科全书,newNode wikidata_relation.csv：文件大小 1.83M，内容是实体和关系，包含字段 HudongItem1、relation、HudongItem2，分别表示实体1、关系、实体2，文件前5行结构如下： HudongItem1,relation,HudongItem2 菊糖,instance of,化合物 菊糖,instance of,多糖 瓦尔,instance of,河流 菊糖,subclass of,食物 瓦尔,origin of the watercourse,莱茵河 wikidata_relation2.csv：大小 7.18M，内容结构和 wikidata_relation.csv 一致，作为 wikidata_relation.csv 数据的补充。 语料加载。语料加载，利用 Neo4j 的 LOAD CSV WITH HEADERS FROM... 功能进行加载，具体操作过程如下。 首先，依次执行以下命令： // 将hudong_pedia.csv 导入 LOAD CSV WITH HEADERS FROM &quot;file:///hudong_pedia.csv&quot; AS line CREATE (p:HudongItem{title:line.title,image:line.image,detail:line.detail,url:line.url,openTypeList:line.openTypeList,baseInfoKeyList:line.baseInfoKeyList,baseInfoValueList:line.baseInfoValueList}) 执行成功之后，控制台显示成功： 上面这张图，表示数据加载成功，并显示加载的数据条数和耗费的时间。 // 新增了hudong_pedia2.csv LOAD CSV WITH HEADERS FROM &quot;file:///hudong_pedia2.csv&quot; AS line CREATE (p:HudongItem{title:line.title,image:line.image,detail:line.detail,url:line.url,openTypeList:line.openTypeList,baseInfoKeyList:line.baseInfoKeyList,baseInfoValueList:line.baseInfoValueList}) // 创建索引 CREATE CONSTRAINT ON (c:HudongItem) ASSERT c.title IS UNIQUE 以上命令的意思是，将 hudong_pedia.csv 和 hudong_pedia2.csv 导入 Neo4j 作为结点，然后对 titile属性添加 UNIQUE（唯一约束/索引）。 注意： 如果导入的时候出现 Neo4j JVM 内存溢出错误，可以在导入前，先把 Neo4j 下的 conf/neo4j.conf 中的dbms.memory.heap.initial_size 和 dbms.memory.heap.max_size调大点。导入完成后再把值改回去即可。 下面继续执行数据导入命令： // 导入新的节点 LOAD CSV WITH HEADERS FROM &quot;file:///new_node.csv&quot; AS line CREATE (:NewNode { title: line.title }) //添加索引 CREATE CONSTRAINT ON (c:NewNode) ASSERT c.title IS UNIQUE //导入hudongItem和新加入节点之间的关系 LOAD CSV WITH HEADERS FROM &quot;file:///wikidata_relation2.csv&quot; AS line MATCH (entity1:HudongItem{title:line.HudongItem}) , (entity2:NewNode{title:line.NewNode}) CREATE (entity1)-[:RELATION { type: line.relation }]-&gt;(entity2) LOAD CSV WITH HEADERS FROM &quot;file:///wikidata_relation.csv&quot; AS line MATCH (entity1:HudongItem{title:line.HudongItem1}) , (entity2:HudongItem{title:line.HudongItem2}) CREATE (entity1)-[:RELATION { type: line.relation }]-&gt;(entity2) 执行完这些命令后，我们导入 new_node.csv 新节点，并对 titile 属性添加 UNIQUE（唯一约束/索引），导入wikidata_relation.csv 和 wikidata_relation2.csv，并给节点之间创建关系。 紧接着，继续导入实体属性，并创建实体之间的关系： LOAD CSV WITH HEADERS FROM &quot;file:///attributes.csv&quot; AS line MATCH (entity1:HudongItem{title:line.Entity}), (entity2:HudongItem{title:line.Attribute}) CREATE (entity1)-[:RELATION { type: line.AttributeName }]-&gt;(entity2); LOAD CSV WITH HEADERS FROM &quot;file:///attributes.csv&quot; AS line MATCH (entity1:HudongItem{title:line.Entity}), (entity2:NewNode{title:line.Attribute}) CREATE (entity1)-[:RELATION { type: line.AttributeName }]-&gt;(entity2); LOAD CSV WITH HEADERS FROM &quot;file:///attributes.csv&quot; AS line MATCH (entity1:NewNode{title:line.Entity}), (entity2:NewNode{title:line.Attribute}) CREATE (entity1)-[:RELATION { type: line.AttributeName }]-&gt;(entity2); LOAD CSV WITH HEADERS FROM &quot;file:///attributes.csv&quot; AS line MATCH (entity1:NewNode{title:line.Entity}), (entity2:HudongItem{title:line.Attribute}) CREATE (entity1)-[:RELATION { type: line.AttributeName }]-&gt;(entity2) 这里注意，建索引的时候带了 label，因此只有使用 label 时才会使用索引，这里我们的实体有两个 label，所以一共做 2*2=4次。当然也可以建立全局索引，即对于不同的 label 使用同一个索引。 以上过程，我们就完成了语料加载，并创建了实体之间的关系和属性匹配，下面我们来看看 Neo4j 图谱关系展示。 知识图谱查询展示最后通过 cypher 语句查询来看看农业图谱展示。 首先，展示 HudongItem 实体，执行如下命令： MATCH (n:HudongItem) RETURN n LIMIT 25 对 HudongItem 实体进行查询，返回结果的25条数据，结果如下图： 接着，展示 NewNode 实体，执行如下命令： MATCH (n:NewNode) RETURN n LIMIT 25 对 NewNode 实体进行查询，返回结果的25条数据，结果如下图： 之后，展示 RELATION 直接的关系，执行如下命令： MATCH p=()-[r:RELATION]-&gt;() RETURN p LIMIT 25 展示实体属性关系，结果如下图： 总结本节内容到此结束，回顾下整篇文章，主要讲了以下内容： 解释了 Neo4j 被用来做知识图谱的原因； Neo4j 的简单安装以及在 Neo4j 浏览器中创建节点和关系； Neo4j 的 Python 接口操作及使用； 从五个方面讲解了如何使用 Neo4j 构建一个简单的农业知识图谱。 最后，强调一句，知识图谱未来会通过自然语言处理技术和搜索技术结合应用会越来越广，工业界所出的地位也会越来越重要。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于情感词典的文本情感分析（12）]]></title>
    <url>%2F2019%2F11%2F22%2F12.%E5%9F%BA%E4%BA%8E%E6%83%85%E6%84%9F%E8%AF%8D%E5%85%B8%E7%9A%84%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[目前情感分析在中文自然语言处理中比较火热，很多场景下，我们都需要用到情感分析。比如，做金融产品量化交易，需要根据爬取的舆论数据来分析政策和舆论对股市或者基金期货的态度；电商交易，根据买家的评论数据，来分析商品的预售率等等。 下面我们通过以下几点来介绍中文自然语言处理情感分析： 中文情感分析方法简介； SnowNLP 快速进行评论数据情感分析； 基于标注好的情感词典来计算情感值； pytreebank 绘制情感树； 股吧数据情感分类。 中文情感分析方法简介情感倾向可认为是主体对某一客体主观存在的内心喜恶，内在评价的一种倾向。它由两个方面来衡量：一个情感倾向方向，一个是情感倾向度。 目前，情感倾向分析的方法主要分为两类：一种是基于情感词典的方法；一种是基于机器学习的方法，如基于大规模语料库的机器学习。前者需要用到标注好的情感词典；后者则需要大量的人工标注的语料作为训练集，通过提取文本特征，构建分类器来实现情感的分类。 文本情感分析的分析粒度可以是词语、句子、段落或篇章。 段落篇章级情感分析主要是针对某个主题或事件进行情感倾向判断，一般需要构建对应事件的情感词典，如电影评论的分析，需要构建电影行业自己的情感词典，这样效果会比通用情感词典更好；也可以通过人工标注大量电影评论来构建分类器。句子级的情感分析大多通过计算句子里包含的所有情感词的值来得到。 篇章级的情感分析，也可以通过聚合篇章中所有的句子的情感倾向来计算得出。因此，针对句子级的情感倾向分析，既能解决短文本的情感分析，同时也是篇章级文本情感分析的基础。 中文情感分析的一些难点，比如句子是由词语根据一定的语言规则构成的，应该把句子中词语的依存关系纳入到句子情感的计算过程中去，不同的依存关系，进行情感倾向计算是不一样的。文档的情感，根据句子对文档的重要程度赋予不同权重，调整其对文档情感的贡献程度等。 SnowNLP 快速进行评论数据情感分析如果有人问，有没有比较快速简单的方法能判断一句话的情感倾向，那么 SnowNLP 库就是答案。 SnowNLP 主要可以进行中文分词、词性标注、情感分析、文本分类、转换拼音、繁体转简体、提取文本关键词、提取摘要、分割句子、文本相似等。 需要注意的是，用 SnowNLP进行情感分析，官网指出进行电商评论的准确率较高，其实是因为它的语料库主要是电商评论数据，但是可以自己构建相关领域语料库，替换单一的电商评论语料，准确率也挺不错的。 1. SnowNLP 安装。 （1） 使用 pip 安装： pip install snownlp==0.11.1 （2）使用 Github 源码安装。 首先，下载 SnowNLP 的 Github源码并解压，在解压目录，通过下面命令安装： python setup.py install 以上方式，二选一安装完成之后，就可以引入 SnowNLP 库使用了。 from snownlp import SnowNLP 2. 评论语料获取情感值。 首先，SnowNLP 对情感的测试值为0到1，值越大，说明情感倾向越积极。下面我们通过 SnowNLP 测试在京东上找的好评、中评、差评的结果。 首先，引入 SnowNLP 库： from snownlp import SnowNLP （1） 测试一条京东的好评数据： SnowNLP(u&#39;本本已收到，体验还是很好，功能方面我不了解，只看外观还是很不错很薄，很轻，也有质感。&#39;).sentiments 得到的情感值很高，说明买家对商品比较认可，情感值为： 0.999950702449061 （2）测试一条京东的中评数据： SnowNLP(u&#39;屏幕分辨率一般，送了个极丑的鼠标。&#39;).sentiments 得到的情感值一般，说明买家对商品看法一般，甚至不喜欢，情感值为： 0.03251402883400323 （3）测试一条京东的差评数据： SnowNLP(u&#39;很差的一次购物体验，细节做得极差了，还有发热有点严重啊，散热不行，用起来就是烫得厉害，很垃圾！！！&#39;).sentiments 得到的情感值一般，说明买家对商品不认可，存在退货嫌疑，情感值为： 0.0036849517156107847 以上就完成了简单快速的情感值计算，对评论数据是不是很好用呀！！！ 使用 SnowNLP 来计算情感值，官方推荐的是电商评论数据计算准确度比较高，难道非评论数据就不能使用 SnowNLP 来计算情感值了吗？当然不是，虽然SnowNLP 默认提供的模型是用评论数据训练的，但是它还支持我们根据现有数据训练自己的模型。 首先我们来看看自定义训练模型的 源码 Sentiment 类 ，代码定义如下： class Sentiment(object): def __init__(self): self.classifier = Bayes() def save(self, fname, iszip=True): self.classifier.save(fname, iszip) def load(self, fname=data_path, iszip=True): self.classifier.load(fname, iszip) def handle(self, doc): words = seg.seg(doc) words = normal.filter_stop(words) return words def train(self, neg_docs, pos_docs): data = [] for sent in neg_docs: data.append([self.handle(sent), &#39;neg&#39;]) for sent in pos_docs: data.append([self.handle(sent), &#39;pos&#39;]) self.classifier.train(data) def classify(self, sent): ret, prob = self.classifier.classify(self.handle(sent)) if ret == &#39;pos&#39;: return prob return 1-prob 通过源代码，我们可以看到，可以使用 train方法训练数据，并使用 save 方法和 load 方法保存与加载模型。下面训练自己的模型，训练集pos.txt 和 neg.txt 分别表示积极和消极情感语句，两个 TXT 文本中每行表示一句语料。 下面代码进行自定义模型训练和保存： from snownlp import sentiment sentiment.train(&#39;neg.txt&#39;, &#39;pos.txt&#39;) sentiment.save(&#39;sentiment.marshal&#39;) 基于标注好的情感词典来计算情感值这里我们使用一个行业标准的情感词典——玻森情感词典，来自定义计算一句话、或者一段文字的情感值。 整个过程如下： 加载玻森情感词典； jieba 分词； 获取句子得分。 首先引入包： import pandas as pd import jieba 接下来加载情感词典： df = pd.read_table(&quot;bosonnlp//BosonNLP_sentiment_score.txt&quot;,sep= &quot; &quot;,names=[&#39;key&#39;,&#39;score&#39;]) 查看一下情感词典前5行： 将词 key 和对应得分 score 转成2个 list 列表，目的是找到词 key 的时候，能对应获取到 score 值： key = df[&#39;key&#39;].values.tolist() score = df[&#39;score&#39;].values.tolist() 定义分词和统计得分函数： def getscore(line): segs = jieba.lcut(line) #分词 score_list = [score[key.index(x)] for x in segs if(x in key)] return sum(score_list) #计算得分 最后来进行结果测试： line = &quot;今天天气很好，我很开心&quot; print(round(getscore(line),2)) line = &quot;今天下雨，心情也受到影响。&quot; print(round(getscore(line),2)) 获得的情感得分保留2位小数： 5.26 -0.96 pytreebank 绘制情感树1. 安装 pytreebank。 在 Github 上下载 pytreebank源码，解压之后，进入解压目录命令行，执行命令： python setup.py install 最后通过引入命令，判断是否安装成功： import pytreebank 提示，如果在 Windows 下安装之后，报错误： UnicodeDecodeError: &#39;gbk&#39; codec can&#39;t decode byte 0x92 in position 24783: illegal multibyte sequence 这是由于编码问题引起的，可以在安装目录下报错的文件中报错的代码地方加个 encoding=&#39;utf-8&#39; 编码： import_tag( &quot;script&quot;, contents=format_replacements(open(scriptname,encoding=&#39;utf-8&#39;).read(), replacements), type=&quot;text/javascript&quot; ) 2. 绘制情感树。首先引入 pytreebank 包： import pytreebank 然后，加载用来可视化的 JavaScript 和 CSS 脚本： pytreebank.LabeledTree.inject_visualization_javascript() 绘制情感树，把句子首先进行组合再绘制图形： line = &#39;(4 (0 你) (3 (2 是) (3 (3 (3 谁) (2 的)) (2 谁))))&#39; pytreebank.create_tree_from_string(line).display() 得到的情感树如下： 股吧数据情感分类但在7月15日之前，随着中美贸易战不断升级，中兴股价又上演了一场“跌跌不休”的惨状，我以中美贸易战背景下中兴通讯在股吧解禁前一段时间的评论数据，来进行情感数据人工打标签和分类。其中，把消极、中性 、积极分别用0、1、2来表示。 整个文本分类流程主要包括以下6个步骤： 中文语料； 分词； 复杂规则； 特征向量； 算法建模； 情感分析。 本次分类算法采用 CNN，首先引入需要的包： import pandas as pd import numpy as np import jieba import random import keras from keras.preprocessing import sequence from keras.models import Sequential from keras.layers import Dense, Dropout, Activation from keras.layers import Embedding from keras.layers import Conv1D, GlobalMaxPooling1D from keras.datasets import imdb from keras.models import model_from_json from keras.utils import np_utils import matplotlib.pyplot as plt 继续引入停用词和语料文件： dir = &quot;D://ProgramData//PythonWorkSpace//chat//chat8//&quot; stopwords=pd.read_csv(dir +&quot;stopwords.txt&quot;,index_col=False,quoting=3,sep=&quot;\t&quot;,names=[&#39;stopword&#39;], encoding=&#39;utf-8&#39;) stopwords=stopwords[&#39;stopword&#39;].values df_data1 = pd.read_csv(dir+&quot;data1.csv&quot;,encoding=&#39;utf-8&#39;) df_data1.head() 下图展示数据的前5行：接着进行数据预处理，把消极、中性、积极分别为0、1、2的预料分别拿出来： #把内容有缺失值的删除 df_data1.dropna(inplace=True) #抽取文本数据和标签 data_1 = df_data1.loc[:,[&#39;content&#39;,&#39;label&#39;]] #把消极 中性 积极分别为0、1、2的预料分别拿出来 data_label_0 = data_1.loc[data_1[&#39;label&#39;] ==0,:] data_label_1 = data_1.loc[data_1[&#39;label&#39;] ==1,:] data_label_2 = data_1.loc[data_1[&#39;label&#39;] ==2,:] 接下来，定义中文分词函数： #定义分词函数 def preprocess_text(content_lines, sentences, category): for line in content_lines: try: segs=jieba.lcut(line) segs = filter(lambda x:len(x)&gt;1, segs) segs = [v for v in segs if not str(v).isdigit()]#去数字 segs = list(filter(lambda x:x.strip(), segs)) #去左右空格 segs = filter(lambda x:x not in stopwords, segs) temp = &quot; &quot;.join(segs) if(len(temp)&gt;1): sentences.append((temp, category)) except Exception: print(line) continue 生成训练的分词数据，并进行打散，使其分布均匀： #获取数据 data_label_0_content = data_label_0[&#39;content&#39;].values.tolist() data_label_1_content = data_label_1[&#39;content&#39;].values.tolist() data_label_2_content = data_label_2[&#39;content&#39;].values.tolist() #生成训练数据 sentences = [] preprocess_text(data_label_0_content, sentences, 0) preprocess_text(data_label_1_content, sentences, 1) preprocess_text(data_label_2_content, sentences,2) #我们打乱一下顺序，生成更可靠的训练集 random.shuffle(sentences) 对数据集进行切分，按照训练集合测试集7:3的比例： #所以把原数据集分成训练集的测试集，咱们用sklearn自带的分割函数。 from sklearn.model_selection import train_test_split x, y = zip(*sentences) x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3,random_state=1234) 然后，对特征构造词向量： #抽取特征，我们对文本抽取词袋模型特征 from sklearn.feature_extraction.text import CountVectorizer vec = CountVectorizer( analyzer=&#39;word&#39;, #tokenise by character ngrams max_features=4000, #keep the most common 1000 ngrams ) vec.fit(x_train) 定义模型参数： # 设置参数 max_features = 5001 maxlen = 100 batch_size = 32 embedding_dims = 50 filters = 250 kernel_size = 3 hidden_dims = 250 epochs = 10 nclasses = 3 输入特征转成 Array 和标签处理，打印训练集和测试集的 shape： x_train = vec.transform(x_train) x_test = vec.transform(x_test) x_train = x_train.toarray() x_test = x_test.toarray() y_train = np_utils.to_categorical(y_train,nclasses) y_test = np_utils.to_categorical(y_test,nclasses) x_train = sequence.pad_sequences(x_train, maxlen=maxlen) x_test = sequence.pad_sequences(x_test, maxlen=maxlen) print(&#39;x_train shape:&#39;, x_train.shape) print(&#39;x_test shape:&#39;, x_test.shape) 定义一个绘制 Loss 曲线的类： class LossHistory(keras.callbacks.Callback): def on_train_begin(self, logs={}): self.losses = {&#39;batch&#39;:[], &#39;epoch&#39;:[]} self.accuracy = {&#39;batch&#39;:[], &#39;epoch&#39;:[]} self.val_loss = {&#39;batch&#39;:[], &#39;epoch&#39;:[]} self.val_acc = {&#39;batch&#39;:[], &#39;epoch&#39;:[]} def on_batch_end(self, batch, logs={}): self.losses[&#39;batch&#39;].append(logs.get(&#39;loss&#39;)) self.accuracy[&#39;batch&#39;].append(logs.get(&#39;acc&#39;)) self.val_loss[&#39;batch&#39;].append(logs.get(&#39;val_loss&#39;)) self.val_acc[&#39;batch&#39;].append(logs.get(&#39;val_acc&#39;)) def on_epoch_end(self, batch, logs={}): self.losses[&#39;epoch&#39;].append(logs.get(&#39;loss&#39;)) self.accuracy[&#39;epoch&#39;].append(logs.get(&#39;acc&#39;)) self.val_loss[&#39;epoch&#39;].append(logs.get(&#39;val_loss&#39;)) self.val_acc[&#39;epoch&#39;].append(logs.get(&#39;val_acc&#39;)) def loss_plot(self, loss_type): iters = range(len(self.losses[loss_type])) plt.figure() # acc plt.plot(iters, self.accuracy[loss_type], &#39;r&#39;, label=&#39;train acc&#39;) # loss plt.plot(iters, self.losses[loss_type], &#39;g&#39;, label=&#39;train loss&#39;) if loss_type == &#39;epoch&#39;: # val_acc plt.plot(iters, self.val_acc[loss_type], &#39;b&#39;, label=&#39;val acc&#39;) # val_loss plt.plot(iters, self.val_loss[loss_type], &#39;k&#39;, label=&#39;val loss&#39;) plt.grid(True) plt.xlabel(loss_type) plt.ylabel(&#39;acc-loss&#39;) plt.legend(loc=&quot;upper right&quot;) plt.show() 然后，初始化上面类的对象，并作为模型的回调函数输入，训练模型： history = LossHistory() print(&#39;Build model...&#39;) model = Sequential() model.add(Embedding(max_features, embedding_dims, input_length=maxlen)) model.add(Dropout(0.5)) model.add(Conv1D(filters, kernel_size, padding=&#39;valid&#39;, activation=&#39;relu&#39;, strides=1)) model.add(GlobalMaxPooling1D()) model.add(Dense(hidden_dims)) model.add(Dropout(0.5)) model.add(Activation(&#39;relu&#39;)) model.add(Dense(nclasses)) model.add(Activation(&#39;softmax&#39;)) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test),callbacks=[history]) 得到的模型迭代次数为10轮的训练过程： 最后绘制 Loss 图像： 关于本次分类，这里重点讨论的一个知识点就是数据分布不均匀的情况，我们都知道，本次贸易战中兴公司受影响很大，导致整个股票价格处于下跌趋势，所以整个舆论上，大多数评论都是消极的态度，导致数据分布极不均匀。 那数据分布不均匀一般怎么处理呢？从以下几个方面考虑： 数据采样，包括上采样、下采样和综合采样； 改变分类算法，在传统分类算法的基础上对不同类别采取不同的加权方式，使得模型更看重少数类； 采用合理的性能评价指标； 代价敏感。 总结，本文通过第三方、基于词典等方式计算中文文本情感值，以及通过情感树来进行可视化，然而这些内容只是情感分析的入门知识，情感分析还涉及句法依存等，最后通过一个CNN 分类模型，提供一种有监督的情感分类思路。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>cnn</tag>
        <tag>snownlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于CNN的电影推荐系统（10）]]></title>
    <url>%2F2019%2F11%2F22%2F10.%E5%9F%BA%E4%BA%8ECNN%E7%9A%84%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[本文从深度学习卷积神经网络入手，基于 Github 的开源项目来完成 MovieLens 数据集的电影推荐系统。 什么是推荐系统呢？什么是推荐系统呢？首先我们来看看几个常见的推荐场景。 如果你经常通过豆瓣电影评分来找电影，你会发现下图所示的推荐： 如果你喜欢购物，根据你的选择和购物行为，平台会给你推荐相似商品： 在互联网的很多场景下都可以看到推荐的影子。因为推荐可以帮助用户和商家满足不同的需求： 对用户而言：找到感兴趣的东西，帮助发现新鲜、有趣的事物。 对商家而言：提供个性化服务，提高信任度和粘性，增加营收。 常见的推荐系统主要包含两个方面的内容，基于用户的推荐系统（UserCF）和基于物品的推荐系统（ItemCF）。两者的区别在于，UserCF给用户推荐那些和他有共同兴趣爱好的用户喜欢的商品，而 ItemCF 给用户推荐那些和他之前喜欢的商品类似的商品。这两种方式都会遭遇冷启动问题。 下面是 UserCF 和 ItemCF 的对比： CNN 是如何应用在文本处理上的？提到卷积神经网络（CNN），相信大部分人首先想到的是图像分类，比如 MNIST 手写体识别，CAFRI10 图像分类。CNN已经在图像识别方面取得了较大的成果，随着近几年的不断发展，在文本处理领域，基于文本挖掘的文本卷积神经网络被证明是有效的。 首先，来看看 CNN 是如何应用到 NLP 中的，下面是一个简单的过程图： 和图像像素处理不一样，自然语言通常是一段文字，那么在特征矩阵中，矩阵的每一个行向量（比如 word2vec 或者 doc2vec）代表一个Token，包括词或者字符。如果一段文字包含有 n 个词，每个词有 m 维的词向量，那么我们可以构造出一个 n*m 的词向量矩阵，在 NLP处理过程中，让过滤器宽度和矩阵宽度保持一致整行滑动。 动手实战基于 CNN 的电影推荐系统将 CNN 的技术应用到自然语言处理中并与电影推荐相结合，来训练一个基于文本的卷积神经网络，实现电影个性化推荐系统。 首先感谢作者 chengstone 的分享，源码请访问下面网址： Github 在验证了 CNN 应用在自然语言处理上是有效的之后，从推荐系统的个性化推荐入手，在文本上，把 CNN成果应用到电影的个性化推荐上。并在特征工程中，对训练集和测试集做了相应的特征处理，其中有部分字段是类型性变量，特征工程上可以采用 one-hot编码，但是对于 UserID、MovieID 这样非常稀疏的变量，如果使用 one-hot，那么数据的维度会急剧膨胀，对于这份数据集来说是不合适的。 具体算法设计如下： 1. 定义用户嵌入矩阵。 用户的特征矩阵主要是通过用户信息嵌入网络来生成的，在预处理数据的时候，我们将UserID、MovieID、性别、年龄、职业特征全部转成了数字类型，然后把这个数字当作嵌入矩阵的索引，在网络的第一层就使用嵌入层，这样数据输入的维度保持在（N，32）和（N，16）。然后进行全连接层，转成（N，128）的大小，再进行全连接层，转成（N，200）的大小，这样最后输出的用户特征维度相对比较高，也保证了能把每个用户所带有的特征充分携带并通过特征表达。 具体流程如下： 2. 生成用户特征。 生成用户特征是在用户嵌入矩阵网络输出结果的基础上，通过2层全连接层实现的。第一个全连接层把特征矩阵转成（N，128）的大小，再进行第二次全连接层，转成（N，200）的大小，这样最后输出的用户特征维度相对比较高，也保证了能把每个用户所带有的特征充分携带并通过特征表达。 具体流程如下： 3. 定义电影 ID 嵌入矩阵。 通过电影 ID 和电影类型分别生成电影 ID 和电影类型特征，电影类型的多个嵌入向量做加和输出。电影 ID的实现过程和上面一样，但是对于电影类型的处理相较于上面，稍微复杂一点。因为电影类型有重叠性，一个电影可以属于多个类别，当把电影类型从嵌入矩阵索引出来之后是一个（N，32）形状的矩阵，因为有多个类别，这里采用的处理方式是矩阵求和，把类别加上去，变成（1，32）形状，这样使得电影的类别信息不会丢失。 具体流程如下： 4. 文本卷积神经网络设计。 文本卷积神经网络和单纯的 CNN网络结构有点不同，因为自然语言通常是一段文字与图片像素组成的矩阵是不一样的。在电影文本特征矩阵中，矩阵的每一个行构成的行向量代表一个Token，包括词或者字符。如果一段文字有 n 个词，每个词有 m 维的词向量，那么我们可以构造出一个 n*m 的矩阵。而且 NLP处理过程中，会有多个不同大小的过滤器串行执行，且过滤器宽度和矩阵宽度保持一致，是整行滑动。在执行完卷积操作之后采用了 ReLU激活函数，然后采用最大池化操作，最后通过全连接并 Dropout 操作和 Softmax输出。这里电影名称的处理比较特殊，并没有采用循环神经网络，而采用的是文本在 CNN 网络上的应用。 对于电影数据集，我们对电影名称做 CNN处理，其大致流程，从嵌入矩阵中得到电影名对应的各个单词的嵌入向量，由于电影名称比较特殊一点，名称长度有一定限制，这里过滤器大小使用时，就选择2、3、4、5长度。然后对文本嵌入层使用滑动2、3、4、5个单词尺寸的卷积核做卷积和最大池化，然后Dropout 操作，全连接层输出。 具体流程如下： 具体过程描述： （1）首先输入一个 32*32 的矩阵； （2）第一次卷积核大小为 2*2，得到 31*31 的矩阵，然后通过 [1,14,1,1] 的 max-pooling 操作，得到的矩阵为18*31； （3）第二次卷积核大小为 3*3，得到 16*29的矩阵，然后通过[1,13,1,1] 的 max-pooling 操作，得到的矩阵为4*29； （4）第三次卷积核大小 4*4，得到 1*26 的矩阵，然后通过 [1,12,1,1] 的 max-pooling 操作，得到的矩阵为1*26； （5）第四次卷积核大小 5*5，得到 1*22 的矩阵，然后通过 [1,11,1,1] 的 max-pooling 操作，得到的矩阵为1*22； （6）最后通过 Dropout 和全连接层，len(window_sizes) * filter_num =32，得到 1*32的矩阵。 5. 电影各层做一个全连接层。 将上面几步生成的特征向量，通过2个全连接层连接在一起，第一个全连接层是电影 ID 特征和电影类型特征先全连接，之后再和 CNN生成的电影名称特征全连接，生成最后的特征集。 具体流程如下： 6. 完整的基于 CNN 的电影推荐流程。 把以上实现的模块组合成整个算法，将网络模型作为回归问题进行训练，得到训练好的用户特征矩阵和电影特征矩阵进行推荐。 基于 CNN 的电影推荐系统代码调参过程在训练过程中，我们需要对算法预先设置一些超参数，这里给出的最终的设置结果： # 设置迭代次数 num_epochs = 5 # 设置BatchSize大小 batch_size = 256 #设置dropout保留比例 dropout_keep = 0.5 # 设置学习率 learning_rate = 0.0001 # 设置每轮显示的batches大小 show_every_n_batches = 20 首先对数据集进行划分，按照 4:1 的比例划分为训练集和测试集，下面给出的是算法模型最终训练集合测试集使用的划分结果： #将数据集分成训练集和测试集，随机种子不固定 train_X,test_X, train_y, test_y = train_test_split(features, targets_values, test_size = 0.3, random_state = 0) 接下来是具体模型训练过程。训练过程，要不断调参，根据经验调参粒度可以选择从粗到细分阶段进行。 调参过程对比： （1）第一步，先固定，learning_rate=0.01 和 num_epochs=10，测试 batch_size=128 对迭代时间和Loss 的影响； （2）第二步，先固定，learning_rate=0.01 和 num_epochs=10，测试 batch_size=256 对迭代时间和Loss 的影响； （3）第三步，先固定，learning_rate=0.01 和 num_epochs=10，测试 batch_size=512 对迭代时间和Loss 的影响； （4）第四步，先固定，learning_rate=0.01 和 num_epochs=5，测试 batch_size=128 对迭代时间和Loss 的影响； （5）第五步，先固定，learning_rate=0.01 和 num_epochs=5，测试 batch_size=256 对迭代时间和Loss 的影响； （6）第六步，先固定，learning_rate=0.01 和 num_epochs=5，测试 batch_size=512 对迭代时间和Loss 的影响； （7）第七步，先固定，batch_size=256 和 num_epochs=5，测试 learning_rate=0.001 对 Loss的影响； （8）第八步，先固定，batch_size=256 和 num_epochs=5，测试 learning_rate=0.0005 对 Loss的影响； （9）第九步，先固定，batch_size=256 和 num_epochs=5，测试 learning_rate=0.0001 对 Loss的影响； （10）第十步，先固定，batch_size=256 和 num_epochs=5，测试 learning_rate=0.00005 对Loss 的影响。 得到的调参结果对比表如下： 通过上面（1）-（6）步调参比较，在 learning_rate、batch_size 相同的情况下，num_epochs对于训练时间影响较大；而在 learning_rate、num_epochs 相同情况下，batch_size 对 Loss的影响较大，batch_size 选择512，Loss 有抖动情况，权衡之下，最终确定后续调参固定采用batch_size=256、num_epochs=5 的超参数值，后续（7）-（10）步，随着 learning_rate 逐渐减小，发现Loss 是先逐渐减小，而在 learning_rate=0.00005 时反而增大，最终选择出学习率为 learning_rate=0.0001的超参数值。 基于 CNN 的电影推荐系统电影推荐在上面，完成模型训练验证之后，实际来进行推荐电影，这里使用生产的用户特征矩阵和电影特征矩阵做电影推荐，主要有三种方式的推荐。 1. 推荐同类型的电影。 思路是：计算当前看的电影特征向量与整个电影特征矩阵的余弦相似度，取相似度最大的 top_k 个，这里加了些随机选择在里面，保证每次的推荐稍稍有些不同。 def recommend_same_type_movie(movie_id_val, top_k = 20): loaded_graph = tf.Graph() # with tf.Session(graph=loaded_graph) as sess: # # Load saved model loader = tf.train.import_meta_graph(load_dir + &#39;.meta&#39;) loader.restore(sess, load_dir) norm_movie_matrics = tf.sqrt(tf.reduce_sum(tf.square(movie_matrics), 1, keep_dims=True)) normalized_movie_matrics = movie_matrics / norm_movie_matrics #推荐同类型的电影 probs_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200]) probs_similarity = tf.matmul(probs_embeddings, tf.transpose(normalized_movie_matrics)) sim = (probs_similarity.eval()) print(&quot;您看的电影是：{}&quot;.format(movies_orig[movieid2idx[movie_id_val]])) print(&quot;以下是给您的推荐：&quot;) p = np.squeeze(sim) p[np.argsort(p)[:-top_k]] = 0 p = p / np.sum(p) results = set() while len(results) != 5: c = np.random.choice(3883, 1, p=p)[0] results.add(c) for val in (results): print(val) print(movies_orig[val]) return result 2. 推荐您喜欢的电影。 思路是：使用用户特征向量与电影特征矩阵计算所有电影的评分，取评分最高的 top_k 个，同样加了些随机选择部分。 def recommend_your_favorite_movie(user_id_val, top_k = 10): loaded_graph = tf.Graph() # with tf.Session(graph=loaded_graph) as sess: # # Load saved model loader = tf.train.import_meta_graph(load_dir + &#39;.meta&#39;) loader.restore(sess, load_dir) #推荐您喜欢的电影 probs_embeddings = (users_matrics[user_id_val-1]).reshape([1, 200]) probs_similarity = tf.matmul(probs_embeddings, tf.transpose(movie_matrics)) sim = (probs_similarity.eval()) print(&quot;以下是给您的推荐：&quot;) p = np.squeeze(sim) p[np.argsort(p)[:-top_k]] = 0 p = p / np.sum(p) results = set() while len(results) != 5: c = np.random.choice(3883, 1, p=p)[0] results.add(c) for val in (results): print(val) print(movies_orig[val]) return results 3. 看过这个电影的人还看了（喜欢）哪些电影。 （1）首先选出喜欢某个电影的 top_k 个人，得到这几个人的用户特征向量； （2）然后计算这几个人对所有电影的评分 ； （3）选择每个人评分最高的电影作为推荐； （4）同样加入了随机选择。 def recommend_other_favorite_movie(movie_id_val, top_k = 20): loaded_graph = tf.Graph() # with tf.Session(graph=loaded_graph) as sess: # # Load saved model loader = tf.train.import_meta_graph(load_dir + &#39;.meta&#39;) loader.restore(sess, load_dir) probs_movie_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200]) probs_user_favorite_similarity = tf.matmul(probs_movie_embeddings, tf.transpose(users_matrics)) favorite_user_id = np.argsort(probs_user_favorite_similarity.eval())[0][-top_k:] print(&quot;您看的电影是：{}&quot;.format(movies_orig[movieid2idx[movie_id_val]])) print(&quot;喜欢看这个电影的人是：{}&quot;.format(users_orig[favorite_user_id-1])) probs_users_embeddings = (users_matrics[favorite_user_id-1]).reshape([-1, 200]) probs_similarity = tf.matmul(probs_users_embeddings, tf.transpose(movie_matrics)) sim = (probs_similarity.eval()) p = np.argmax(sim, 1) print(&quot;喜欢看这个电影的人还喜欢看：&quot;) results = set() while len(results) != 5: c = p[random.randrange(top_k)] results.add(c) for val in (results): print(val) print(movies_orig[val]) return results 基于 CNN 的电影推荐系统不足这里讨论一下基于上述方法所带来的不足： 由于一个新的用户在刚开始的时候并没有任何行为记录，所以系统会出现冷启动的问题； 由于神经网络是一个黑盒子过程，我们并不清楚在反向传播的过程中的具体细节，也不知道每一个卷积层抽取的特征细节，所以此算法缺乏一定的可解释性； 一般来说，在工业界，用户的数据量是海量的，而卷积神经网络又要耗费大量的计算资源，所以进行集群计算是非常重要的。但是由于本课程所做实验环境有限，还是在单机上运行，所以后期可以考虑在服务器集群上全量跑数据，这样获得的结果也更准确。 总结上面通过 Github 上一个开源的项目，梳理了CNN 在文本推荐上的应用，并通过模型训练调参，给出一般的模型调参思路，最后建议大家自己把源码下载下来跑跑模型，效果更好。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>cnn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动手实战中文句法依存分析（16）]]></title>
    <url>%2F2019%2F11%2F22%2F16.%E5%8A%A8%E6%89%8B%E5%AE%9E%E6%88%98%E4%B8%AD%E6%96%87%E5%8F%A5%E6%B3%95%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[句法分析被用在很多场景中，比如搜索引擎用户日志分析和关键词识别，比如信息抽取、自动问答、机器翻译等其他自然语言处理相关的任务。 语法体系句法分析需要遵循某一语法体系，根据该体系的语法确定语法树的表示形式，我们看下面这个句子： 西门子将努力参与中国的三峡工程建设。 用可视化的工具 Stanford Parser来看看句法分析的整个过程： 短语结构树由终节点、非终结点以及短语标记三部分组成。句子分裂的语法规则为若干终结点构成一个短语，作为非终结点参与下一次规约，直至结束。如下图： 句法分析技术依存句法分析依存句法依存句法（Dependency Parsing， DP）通过分析语言单位内成分之间的依存关系揭示其句法结构。 直观来讲，依存句法的目的在于分析识别句子中的“主谓宾”、“定状补”这些语法成分，并分析各成分之间的关系。 依存句法的结构没有非终结点，词与词之间直接发生依存关系，构成一个依存对，其中一个是核心词，也叫支配词，另一个叫修饰词，也叫从属词。 依存关系用一个有向弧表示，叫做依存弧。依存弧的方向为由从属词指向支配词，当然反过来也是可以的，按个人习惯统一表示即可。 例如，下面这个句子： 国务院总理李克强调研上海外高桥时提出，支持上海积极探索新机制。 依存句法的分析结果见下（利用哈工大 LTP）： 从分析结果中我们可以看到，句子的核心谓词为“提出”，主语是“李克强”，提出的宾语是“支持上海……”，“调研……时”是“提出”的（时间）状语，“李克强”的修饰语是“国务院总理”，“支持”的宾语是“探索新机制”。 有了上面的依存句法分析结果，我们就可以比较容易的看到，“提出者”是“李克强”，而不是“上海”或“外高桥”，即使它们都是名词，而且距离“提出”更近。 依存关系依存句法通过分析语言单位内成分之前的依存关系解释其句法结构，主张句子中核心动词是支配其他成分的中心成分。而它本身却不受其他任何成分的支配，所有受支配成分都以某种关系从属于支配者。 在20世纪70年代，Robinson 提出依存句法中关于依存关系的四条公理，在处理中文信息的研究中，中国学者提出了依存关系的第五条公理，分别如下： 一个句子中只有一个成分是独立的； 句子的其他成分都从属于某一成分； 任何一个成分都不能依存于两个或两个以上的成分； 如果成分 A 直接从属成分 B，而成分 C 在句子中位于 A 和 B 之间，那么，成分 C 或者从属于 A，或者从属于 B，或者从属于 A 和 B 之间的某一成分； 中心成分左右两边的其他成分相互不发生关系。 句子成分之间相互支配与被支配、依存与被依存的现象，普遍存在于汉语的词汇（合成语）、短语、单句、段落、篇章等能够独立运用和表达的语言之中，这一特点体现了依存关系的普遍性。依存句法分析可以反映出句子各成分之间的语义修饰关系，它可以获得长距离的搭配信息，并与句子成分的物理位置无关。 依存句法分析标注关系（共14种）及含义如下表所示： 语义依存分析语义依存分析（Semantic Dependency Parsing，SDP），分析句子各个语言单位之间的语义关联，并将语义关联以依存结构呈现。使用语义依存刻画句子语义，好处在于不需要去抽象词汇本身，而是通过词汇所承受的语义框架来描述该词汇，而论元的数目相对词汇来说数量总是少了很多。 语义依存分析目标是跨越句子表层句法结构的束缚，直接获取深层的语义信息。例如以下三个句子，用不同的表达方式表达了同一个语义信息，即张三实施了一个吃的动作，吃的动作是对苹果实施的。 语义依存分析不受句法结构的影响，将具有直接语义关联的语言单元直接连接依存弧并标记上相应的语义关系。这也是语义依存分析与依存句法分析的重要区别。 语义依存关系分为三类，分别是主要语义角色，每一种语义角色对应存在一个嵌套关系和反关系；事件关系，描述两个事件间的关系；语义依附标记，标记说话者语气等依附性信息。 Pyhanlp 实战依存句法最后，我们通过 Pyhanlp 库实现依存句法的实战练习。这个过程中，我们选用 Dependency Viewer 工具进行可视化展示。可视化时， txt文档需要采用 UTF-8 编码。 首先，引入包，然后可直接进行分析： from pyhanlp import * sentence = &quot;徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。&quot; print(HanLP.parseDependency(sentence)) 得到的结果： 然后，我们将结果保存在 txt 文件中： f = open(&quot;D://result.txt&quot;,&#39;a+&#39;) print((HanLP.parseDependency(sentence )),file = f) 最后，通过 Dependency Viewer 工具进行可视化，如果出现乱码，记得把 txt 文档保存为 UTF-8 式即可，得到的可视化结果如下图所示： 总结本文，首先为大家介绍了语法体系，以及如何根据语法体系确定一个句子的语法树，为后面的句法分析奠定基础。 接着，介绍了依存句法，它的目的是通过分析语言单位内成分之间的依存关系揭示其句法结构，随之讲解了依存句法中的五大依存关系。 最后，进一步介绍了区别于依存句法的语义依存，其目的是分析句子各个语言单位之间的语义关联，并将语义关联以依存结构呈现。 文章结尾，通过 Pyhanlp 实战以及可视化，带大家进一步加深对中文依存句法的了解。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[nlp基础（1）]]></title>
    <url>%2F2019%2F11%2F22%2F1.nlp%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[目前，随着大数据、云计算对关系型数据处理技术趋向稳定成熟，各大互联网公司对关系数据的整合也已经落地成熟，笔者预测未来数据领域的挑战将主要集中在半结构化和非结构化数据的整合，NLP 技术对个人发展越来越重要，尤其在中文文本上挑战更大。 技术知识点 获取语料 语料是构成语料库的基本单元。所以，人们简单地用文本作为替代，并把文本中的上下文关系作为现实世界中语言的上下文关系的替代品。我们把一个文本集合称为语料库（Corpus），当有几个这样的文本集合的时候，我们称之为语料库集合(Corpora) 已有语料很多业务部门、公司等组织随着业务发展都会积累有大量的纸质或者电子文本资料。那么，对于这些资料，在允许的条件下我们稍加整合，把纸质的文本全部电子化就可以作为我们的语料库。 网络抓取获取国内外标准开放数据集，比如国内的中文汉语有搜狗语料、人民日报语料。国外的因为大都是英文或者外文，这里暂时用不到。也可以选择通过爬虫自己去抓取一些数据，然后来进行后续内容。 语料预处理在一个完整的中文自然语言处理工程应用中，语料预处理大概会占到整个50%-70%的工作量，所以开发人员大部分时间就在进行语料预处理。下面通过数据洗清、分词、词性标注、去停用词四个大的方面来完成语料的预处理工作。 语料清洗数据清洗，顾名思义就是在语料中找到我们感兴趣的东西，把不感兴趣的、视为噪音的内容清洗删除，包括对于原始文本提取标题、摘要、正文等信息，对于爬取的网页内容，去除广告、标签、HTML、JS 等代码和注释等。常见的数据清洗方式有：人工去重、对齐、删除和标注等，或者规则提取内容、正则表达式匹配、根据词性和命名实体提取、编写脚本或者代码批处理等。 分词中文语料数据为一批短文本或者长文本，比如：句子，文章摘要，段落或者整篇文章组成的一个集合。一般句子、段落之间的字、词语是连续的，有一定含义。而进行文本挖掘分析时，我们希望文本处理的最小单位粒度是词或者词语，所以这个时候就需要分词来将文本全部进行分词。 常见的分词算法有：基于字符串匹配的分词方法、基于理解的分词方法、基于统计的分词方法和基于规则的分词方法。 词性标注词性标注，就是给每个词或者词语打词类标签，如形容词、动词、名词等。这样做可以让文本在后面的处理中融入更多有用的语言信息。词性标注是一个经典的序列标注问题，不过对于有些中文自然语言处理来说，词性标注不是非必需的。比如，常见的文本分类就不用关心词性问题，但是类似情感分析、知识推理却是需要的，下图是常见的中文词性整理。 常见的词性标注方法可以分为基于规则和基于统计的方法。其中基于统计的方法，如基于最大熵的词性标注、基于统计最大概率输出词性和基于 HMM 的词性标注。 去停用词停用词一般指对文本特征没有任何贡献作用的字词，比如标点符号、语气、人称等一些词。所以在一般性的文本处理中，分词之后，接下来一步就是去停用词。但是对于中文来说，去停用词操作不是一成不变的，停用词词典是根据具体场景来决定的，比如在情感分析中，语气词、感叹号是应该保留的，因为他们对表示语气程度、感情色彩有一定的贡献和意义。 特征工程做完语料预处理之后，接下来需要考虑如何把分词之后的字和词语表示成计算机能够计算的类型。显然，如果要计算我们至少需要把中文分词的字符串转换成数字，确切的说应该是数学中的向量。有两种常用的表示模型分别是词袋模型和词向量。 词袋模型（Bag of Word, BOW)，即不考虑词语原本在句子中的顺序，直接将每一个词语或者符号统一放置在一个集合（如 list），然后按照计数的方式对出现的次数进行统计。统计词频这只是最基本的方式，TF-IDF 是词袋模型的一个经典用法。 词向量是将字、词语转换成向量矩阵的计算模型。目前为止最常用的词表示方法是 One-hot，这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。还有 Google 团队的 Word2Vec，其主要包含两个模型：跳字模型（Skip-Gram）和连续词袋模型（Continuous Bag of Words，简称 CBOW），以及两种高效训练的方法：负采样（Negative Sampling）和层序 Softmax（Hierarchical Softmax）。值得一提的是，Word2Vec 词向量可以较好地表达不同词之间的相似和类比关系。除此之外，还有一些词向量的表示方式，如 Doc2Vec、WordRank 和 FastText 等。 特征选择同数据挖掘一样，在文本挖掘相关问题中，特征工程也是必不可少的。在一个实际问题中，构造好的特征向量，是要选择合适的、表达能力强的特征。文本特征一般都是词语，具有语义信息，使用特征选择能够找出一个特征子集，其仍然可以保留语义信息；但通过特征提取找到的特征子空间，将会丢失部分语义信息。所以特征选择是一个很有挑战的过程，更多的依赖于经验和专业知识，并且有很多现成的算法来进行特征的选择。目前，常见的特征选择方法主要有 DF、 MI、 IG、 CHI、WLLR、WFO 六种。 模型训练在特征向量选择好之后，接下来要做的事情当然就是训练模型，对于不同的应用需求，我们使用不同的模型，传统的有监督和无监督等机器学习模型， 如 KNN、SVM、Naive Bayes、决策树、GBDT、K-means 等模型；深度学习模型比如 CNN、RNN、LSTM、 Seq2Seq、FastText、TextCNN 等。这些模型在后续的分类、聚类、神经序列、情感分析等示例中都会用到，这里不再赘述。下面是在模型训练时需要注意的几个点。 注意过拟合、欠拟合问题，不断提高模型的泛化能力。 过拟合：模型学习能力太强，以至于把噪声数据的特征也学习到了，导致模型泛化能力下降，在训练集上表现很好，但是在测试集上表现很差。 常见的解决方法有： 增大数据的训练量； 增加正则化项，如 L1 正则和 L2 正则； 特征选取不合理，人工筛选特征和使用特征选择算法； 采用 Dropout 方法等。 欠拟合：就是模型不能够很好地拟合数据，表现在模型过于简单。 常见的解决方法有： 添加其他特征项； 增加模型复杂度，比如神经网络加更多的层、线性模型通过添加多项式使模型泛化能力更强； 减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。 对于神经网络，注意梯度消失和梯度爆炸问题。 评价指标训练好的模型，上线之前要对模型进行必要的评估，目的让模型对语料具备较好的泛化能力。具体有以下这些指标可以参考。 错误率、精度、准确率、精确度、召回率、F1 衡量。 错误率：是分类错误的样本数占样本总数的比例。对样例集 D，分类错误率计算公式如下： 精度：是分类正确的样本数占样本总数的比例。这里的分类正确的样本数指的不仅是正例分类正确的个数还有反例分类正确的个数。对样例集 D，精度计算公式如下： 对于二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为真正例（True Positive）、假正例（False Positive）、真反例（True Negative)、假反例（False Negative）四种情形，令 TP、FP、TN、FN 分别表示其对应的样例数，则显然有 TP+FP++TN+FN=样例总数。分类结果的“混淆矩阵”（Confusion Matrix）如下： 准确率，缩写表示用 P。准确率是针对我们预测结果而言的，它表示的是预测为正的样例中有多少是真正的正样例。定义公式如下： 精确度，缩写表示用 A。精确度则是分类正确的样本数占样本总数的比例。Accuracy 反应了分类器对整个样本的判定能力（即能将正的判定为正的，负的判定为负的）。定义公式如下： 召回率，缩写表示用 R。召回率是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确。定义公式如下： F1 衡量，表达出对查准率/查全率的不同偏好。定义公式如下： ROC 曲线、AUC 曲线。 ROC 全称是“受试者工作特征”（Receiver Operating Characteristic）曲线。我们根据模型的预测结果，把阈值从0变到最大，即刚开始是把每个样本作为正例进行预测，随着阈值的增大，学习器预测正样例数越来越少，直到最后没有一个样本是正样例。在这一过程中，每次计算出两个重要量的值，分别以它们为横、纵坐标作图，就得到了 ROC 曲线。 ROC 曲线的纵轴是“真正例率”（True Positive Rate, 简称 TPR)，横轴是“假正例率”（False Positive Rate,简称FPR），两者分别定义为： ROC 曲线的意义有以下几点： ROC 曲线能很容易的查出任意阈值对模型的泛化性能影响；有助于选择最佳的阈值；可以对不同的模型比较性能，在同一坐标中，靠近左上角的 ROC 曲所代表的学习器准确性最高。如果两条 ROC 曲线没有相交，我们可以根据哪条曲线最靠近左上角哪条曲线代表的学习器性能就最好。但是实际任务中，情况很复杂，若两个模型的 ROC 曲线发生交叉，则难以一般性的断言两者孰优孰劣。此时如果一定要进行比较，则比较合理的判断依据是比较 ROC 曲线下的面积，即AUC（Area Under ROC Curve）。 AUC 就是 ROC 曲线下的面积，衡量学习器优劣的一种性能指标。AUC 是衡量二分类模型优劣的一种评价指标，表示预测的正例排在负例前面的概率。 前面我们所讲的都是针对二分类问题，那么如果实际需要在多分类问题中用 ROC 曲线的话，一般性的转化为多个“一对多”的问题。即把其中一个当作正例，其余当作负例来看待，画出多个 ROC 曲线。 模型上线应用模型线上应用，目前主流的应用方式就是提供服务或者将模型持久化。 第一就是线下训练模型，然后将模型做线上部署，发布成接口服务以供业务系统使用。 第二种就是在线训练，在线训练完成之后把模型 pickle 持久化，然后在线服务接口模板通过读取 pickle 而改变接口服务。 模型重构随着时间和变化，可能需要对模型做一定的重构，包括根据业务不同侧重点对上面提到的一至七步骤也进行调整，重新训练模型进行上线。 jieba安装pip install jieba git clone https://github.com/fxsjy/jieba.gitpython setup.py install 分词算法 基于统计词典，构造前缀词典，基于前缀词典对句子进行切分，得到所有切分可能，根据切分位置，构造一个有向无环图（DAG）； 基于DAG图，采用动态规划计算最大概率路径（最有可能的分词结果），根据最大概率路径分词； 对于新词(词库中没有的词），采用有汉字成词能力的 HMM 模型进行切分。 api参数 jieba.cut 方法接受三个输入参数: 需要分词的字符串；cut_all 参数用来控制是否采用全模式；HMM 参数用来控制是否使用 HMM 模型。 jieba.cut_for_search 方法接受两个参数：需要分词的字符串；是否使用 HMM 模型。该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细 精确分词精确模式试图将句子最精确地切开 import jieba content = &quot;现如今，机器学习和深度学习带动人工智能飞速的发展，并在图片处理、语音识别领域取得巨大成功。&quot; segs_1 = jieba.cut(content, cut_all=False) print(&quot;/&quot;.join(segs_1)) 输出：现如今/，/机器/学习/和/深度/学习/带动/人工智能/飞速/的/发展/，/并/在/图片/处理/、/语音/识别/领域/取得/巨大成功/。 全模式把句子中所有的可能是词语的都扫描出来，速度非常快，但不能解决歧义 segs_3 = jieba.cut(content, cut_all=True) print(&quot;/&quot;.join(segs_3)) 输出： 现如今/如今///机器/学习/和/深度/学习/带动/动人/人工/人工智能/智能/飞速/的/发展///并/在/图片/处理///语音/识别/领域/取得/巨大/巨大成功/大成/成功// 搜索引擎模式在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。 segs_4 = jieba.cut_for_search(content) print(&quot;/&quot;.join(segs_4)) 输出： 如今/现如今/，/机器/学习/和/深度/学习/带动/人工/智能/人工智能/飞速/的/发展/，/并/在/图片/处理/、/语音/识别/领域/取得/巨大/大成/成功/巨大成功/。 lcut生成 list segs_5 = jieba.lcut(content) print(segs_5) 输出： [‘现如今’, ‘，’, ‘机器’, ‘学习’, ‘和’, ‘深度’, ‘学习’, ‘带动’, ‘人工智能’, ‘飞速’, ‘的’, ‘发展’, ‘，’, ‘并’, ‘在’, ‘图片’, ‘处理’, ‘、’, ‘语音’, ‘识别’, ‘领域’, ‘取得’, ‘巨大成功’, ‘。’] 获取词性jieba.posseg 模块实现词性标注 import jieba.posseg as psg print([(x.word, x.flag) for x in psg.lcut(content)]) 输出：[(‘现如今’, ‘t’), (‘，’, ‘x’), (‘机器’, ‘n’), (‘学习’, ‘v’), (‘和’, ‘c’), (‘深度’, ‘ns’), (‘学习’, ‘v’), (‘带动’, ‘v’), (‘人工智能’, ‘n’), (‘飞速’, ‘n’), (‘的’, ‘uj’), (‘发展’, ‘vn’), (‘，’, ‘x’), (‘并’, ‘c’), (‘在’, ‘p’), (‘图片’, ‘n’), (‘处理’, ‘v’), (‘、’, ‘x’), (‘语音’, ‘n’), (‘识别’, ‘v’), (‘领域’, ‘n’), (‘取得’, ‘v’), (‘巨大成功’, ‘nr’), (‘。’, ‘x’)] 并行分词为文本按行分隔后，分配到多个 Python 进程并行分词，最后归并结果,默认分词器 jieba.dt 和 jieba.posseg.dt暂不支持 Windows。 jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数 。 jieba.disable_parallel() # 关闭并行分词模式 。 Counter获取分词结果中词列表的 top n from collections import Counter top5 = Counter(segs_5).most_common(5) print(top5) 输出：[(‘，’, 2), (‘学习’, 2), (‘现如今’, 1), (‘机器’, 1), (‘和’, 1)] 自定义添加词和字典txt = &quot;铁甲网是中国最大的工程机械交易平台。&quot; jieba.add_word(&quot;铁甲网&quot;) print(jieba.lcut(txt)) jieba.load_userdict(&#39;user_dict.txt&#39;) # 添加词典 print(jieba.lcut(txt)) 输出：[‘铁甲网’, ‘是’, ‘中国’, ‘最大’, ‘的’, ‘工程机械’, ‘交易平台’, ‘。’] hanlp安装 pip install pyhanlp 如报错building ‘_jpype’ extensionerror: Microsoft Visual C++ 14.0 is required，则conda install -c conda-forge jpype1;pip install pyhanlp 如ValueError: 配置错误: 数据包/pyhanlp/static\data 不存在，请修改配置文件中的root，则到https://github.com/hankcs/HanLP/releases下载data-for-1.7.2.zip下载后放入f:/anaconda3/envs/learn/lib/site-packages/pyhanlp/static\data 安装jdk环境 hanlp segment 交互分词模式 hanlp serve 内置http服务器 http://localhost:8765 分词from pyhanlp import * content = &quot;现如今，机器学习和深度学习带动人工智能飞速的发展，并在图片处理、语音识别领域取得巨大成功。&quot; print(HanLP.segment(content)) 输出：[现如今/t, ，/w, 机器学习/gi, 和/cc, 深度/n, 学习/v, 带动/v, 人工智能/n, 飞速/d, 的/ude1, 发展/vn, ，/w, 并/cc, 在/p, 图片/n, 处理/vn, 、/w, 语音/n, 识别/vn, 领域/n, 取得/v, 巨大/a, 成功/a, 。/w] 自定义词典分词txt = &quot;铁甲网是中国最大的工程机械交易平台。&quot; CustomDictionary.add(&quot;铁甲网&quot;) CustomDictionary.insert(&quot;工程机械&quot;, &quot;nz 1024&quot;) CustomDictionary.add(&quot;交易平台&quot;, &quot;nz 1024 n 1&quot;) print(HanLP.segment(txt)) 输出： [铁甲网/nz, 是/vshi, 中国/ns, 最大/gm, 的/ude1, 工程机械/nz, 交易平台/nz, 。/w] 以上两种工具可以做关键词提取、自动摘要、依存句法分析、情感分析等]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>jieba</tag>
        <tag>hanlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据可视化（4）]]></title>
    <url>%2F2019%2F11%2F22%2F4.%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[文本可视化依赖于自然语言处理，因此词袋模型、命名实体识别、关键词抽取、主题分析、情感分析等是较常用的文本分析技术。文本分析的过程主要包括特征提取，通过分词、抽取、归一化等操作提取出文本词汇级的内容，利用特征构建向量空间模型并进行降维，以便将其呈现在低维空间，或者利用主题模型处理特征，最终以灵活有效的形式表示这些处理过的数据，以便进行可视化呈现 介绍文本可视化类型，除了包含常规的图表类，如柱状图、饼图、折线图等的表现形式，在文本领域用的比较多的可视化类型有： 基于文本内容的可视化基于文本内容的可视化研究包括基于词频的可视化和基于词汇分布的可视化，常用的有词云、分布图和 Document Cards 等。 基于文本关系的可视化。基于文本关系的可视化研究文本内外关系，帮助人们理解文本内容和发现规律。常用的可视化形式有树状图、节点连接的网络图、力导向图、叠式图和 Word Tree 等。 基于多层面信息的可视化基于多层面信息的可视化主要研究如何结合信息的多个方面帮助用户从更深层次理解文本数据，发现其内在规律。其中，包含时间信息和地理坐标的文本可视化近年来受到越来越多的关注。常用的有地理热力图、ThemeRiver、SparkClouds、TextFlow 和基于矩阵视图的情感分析可视化等。 词云*第一种是默认的样式** wordcloud=WordCloud(font_path=simhei,background_color=&quot;white&quot;,max_font_size=80) word_frequence = {x[0]:x[1] for x in words_stat.head(1000).values} wordcloud=wordcloud.fit_words(word_frequence) #**第二种是自定义图片** text = &quot; &quot;.join(words_stat[&#39;segment&#39;].head(100).astype(str)) abel_mask = imread(r&quot;china.jpg&quot;) #这里设置了一张中国地图 wordcloud2 = WordCloud(background_color=&#39;white&#39;, # 设置背景颜色 mask = abel_mask, # 设置背景图片 max_words = 3000, # 设置最大现实的字数 font_path = simhei, # 设置字体格式 width=2048, height=1024, scale=4.0, max_font_size= 300, # 字体最大值 random_state=42).generate(text) # 根据图片生成词云颜色 image_colors = ImageColorGenerator(abel_mask) wordcloud2.recolor(color_func=image_colors) # 以下代码显示图片 plt.imshow(wordcloud2) plt.axis(&quot;off&quot;) plt.show() wordcloud2.to_file(r&#39;wordcloud_2.jpg&#39;) #保存结果 关系图关系图法，是指用连线图来表示事物相互关系的一种方法。最常见的关系图是数据库里的 E-R 图，表示实体、关系、属性三者之间的关系。在文本可视化里面，关系图也经常被用来表示有相互关系、原因与结果和目的与手段等复杂关系 安装 Matplotlib、NetworkX； 解决 Matplotlib 无法写中文问题。 NetworkX 绘制关系图的数据组织结构，节点和边都是 list 格式，边的 list 里面是成对的节点 classes= df[&#39;class&#39;].values.tolist() classrooms=df[&#39;classroom&#39;].values.tolist() nodes = list(set(classes + classrooms)) weights = [(df.loc[index,&#39;class&#39;],df.loc[index,&#39;classroom&#39;])for index in df.index] weights = list(set(weights)) # 设置matplotlib正常显示中文 plt.rcParams[&#39;font.sans-serif&#39;]=[&#39;SimHei&#39;] # 用黑体显示中文 plt.rcParams[&#39;axes.unicode_minus&#39;]=False colors = [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;, &#39;yellow&#39;] #有向图 DG = nx.DiGraph() #一次性添加多节点，输入的格式为列表 DG.add_nodes_from(nodes) #添加边，数据格式为列表 DG.add_edges_from(weights) #作图，设置节点名显示,节点大小，节点颜色 nx.draw(DG,with_labels=True, node_size=1000, node_color = colors) plt.show() 地理热力图地理热力图，是以特殊高亮的形式显示用户的地理位置，借助热力图，可以直观地观察到用户的总体情况和偏好。 安装 Folium； 将地理名词通过百度转换成经纬度。 在通过分词得到城市名称后，将地理名词通过百度转换成经纬度。首先注册密钥，使用百度 Web 服务 API 下的 Geocoding API 接口来获取你所需要地址的经纬度坐标，并转化为 JSON 结构的数据（个人接口，百度每天限制调用6000次），接下来定义经纬度获取函数： #经纬度转换 def getlnglat(address): url = &#39;http://api.map.baidu.com/geocoder/v2/&#39; output = &#39;json&#39; ak = &#39;sqGDDvCDEZPSz24bt4b0BpKLnMk1dv6d&#39; add = quote(address) #由于本文城市变量为中文，为防止乱码，先用quote进行编码 uri = url + &#39;?&#39; + &#39;address=&#39; + add + &#39;&amp;output=&#39; + output + &#39;&amp;ak=&#39; + ak req = urlopen(uri) res = req.read().decode() #将其他编码的字符串解码成unicode temp = json.loads(res) #对json数据进行解析 return temp 输出：北京,116.39564503787867,39.92998577808024,840成都,104.06792346330406,30.679942845419564,291重庆,106.53063501341296,29.54460610888615,261昆明,102.71460113878045,25.049153100453157,238潍坊,119.14263382297052,36.71611487305138,214济南,117.02496706629023,36.68278472716141,212 使用 Folium 库进行热力图绘制地图lat = np.array(cities[&quot;lat&quot;][0:num]) # 获取维度之维度值 lon = np.array(cities[&quot;lng&quot;][0:num]) # 获取经度值 pop = np.array(cities[&quot;count&quot;][0:num],dtype=float) # 获取人口数，转化为numpy浮点型 data1 = [[lat[i],lon[i],pop[i]] for i in range(num)] #将数据制作成[lats,lons,weights]的形式 map_osm = folium.Map(location=[35,110],zoom_start=5) #绘制Map，开始缩放程度是5倍 HeatMap(data1).add_to(map_osm) # 将热力图添加到前面建立的map里 file_path = dir + &quot;heatmap.html&quot; map_osm.save(file_path) 可视化技术栈 第一个是百度的 Echarts，基于 Canvas，适合刚入门的新手，遵循了数据可视化的一些经典范式，只要把数据组织好，就可以轻松得到很漂亮的图表； 第二个推荐 D3.js，基于 SVG 方便自己定制，D3 V4 支持 Canvas+SVG，D3.js 比 Echarts 稍微难点，适合有一定开发经验的人； 第三个 three.js，是一个基于 WebGL 的 3D 图形的框架，可以让用户通过 JavaScript 搭建 WebGL 项目]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>wordcloud</tag>
        <tag>folium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RNN-GRU-LSTM（9）]]></title>
    <url>%2F2019%2F11%2F22%2F9.RNN-GRU-LSTM%2F</url>
    <content type="text"><![CDATA[序列数据的处理，我们从语言模型 N-gram 模型说起，然后着重谈谈 RNN，并通过 RNN 的变种 LSTM 和 GRU 来实战文本分类。 首先，我们来思考下，当人工神经网络从浅层发展到深层；从全连接到卷积神经网络。在此过程中，人类在图片分类、语音识别等方面都取得了非常好的结果，那么我们为什么还需要循环神经网络呢？ 因为，上面提到的这些网络结构的层与层之间是全连接或部分连接的，但在每层之间的节点是无连接的，这样的网络结构并不能很好的处理序列数据。 语言模型 N-gram 模型通过前面的课程，我们了解到一般自然语言处理的传统方法是将句子处理为一个词袋模型（Bag-of-Words，BoW），而不考虑每个词的顺序，比如用朴素贝叶斯算法进行垃圾邮件识别或者文本分类。在中文里有时候这种方式没有问题，因为有些句子即使把词的顺序打乱，还是可以看懂这句话在说什么，比如： T：研究表明，汉字的顺序并不一定能影响阅读，比如当你看完这句话后。F：研表究明，汉字的序顺并不定一能影阅响读，比如当你看完这句话后。 但有时候不行，词的顺序打乱，句子意思就变得让人不可思议了，例如： T：我喜欢吃烧烤。F：烧烤喜欢吃我。 那么，有没有模型是考虑句子中词与词之间的顺序的呢？有，语言模型中的 N-gram 就是一种。 N-gram 模型是一种语言模型（Language Model，LM），是一个基于概率的判别模型，它的输入是一句话（词的顺序序列），输出是这句话的概率，即这些词的联合概率（JointProbability）。使用 N-gram语言模型思想，一般是需要知道当前词以及前面的词，因为一个句子中每个词的出现并不是独立的。比如，如果第一个词是“空气”，接下来的词是“很”，那么下一个词很大概率会是“新鲜”。类似于我们人的联想，N-gram模型知道的信息越多，得到的结果也越准确。 在前面课程中讲解的文本分类中，我们曾用到基于 sklearn 的词袋模型，尝试加入抽取 2-gram 和 3-gram的统计特征，把词库的量放大，获得更强的特征。 通过 ngram_range 参数来控制，代码如下： from sklearn.feature_extraction.text import CountVectorizer vec = CountVectorizer( analyzer=&#39;word&#39;, # tokenise by character ngrams ngram_range=(1,4), # use ngrams of size 1 and 2 max_features=20000, # keep the most common 1000 ngrams ) 因此，N-gram 模型，在自然语言处理中主要应用在如词性标注、垃圾短信分类、分词器、机器翻译和语音识别、语音识别等领域。 然而 N-gram 模型并不是完美的，它存在如下优缺点： 优点：包含了前 N-1 个词所能提供的全部信息，这些词对于当前词的出现概率具有很强的约束力； 缺点：需要很大规模的训练文本来确定模型的参数，当 N 很大时，模型的参数空间过大。所以常见的 N 值一般为1，2，3等。还有因数据稀疏而导致的数据平滑问题，解决方法主要是拉普拉斯平滑和内插与回溯。 所以，根据 N-gram 的优缺点，它的进化版 NNLM（Neural Network based Language Model）诞生了。 NNLM 由 Bengio 在2003年提出，它是一个很简单的模型，由四层组成，输入层、嵌入层、隐层和输出层，模型结构如下图（来自百度图片）： NNLM 接收的输入是长度为 N 的词序列，输出是下一个词的类别。首先，输入是词序列的 index 序列，例如词“我”在字典（大小为|V|）中的 index是10，词“是”的 index 是23， “小明”的 index 是65，则句子“我是小明”的 index 序列就是 10、23、65。嵌入层（Embedding）是一个大小为 |V|×K 的矩阵，从中取出第10、23、65行向量拼成 3×K 的矩阵就是 Embedding层的输出了。隐层接受拼接后的 Embedding 层输出作为输入，以 tanh 为激活函数，最后送入带 softmax 的输出层，输出概率。 NNLM 最大的缺点就是参数多，训练慢，要求输入定长 N 这一点很不灵活，同时不能利用完整的历史信息。 因此，针对 NNLM 存在的问题，Mikolov 在2010年提出了RNNLM，有兴趣可以阅读相关论文，其结构实际上是用RNN 代替 NNLM 里的隐层，这样做的好处，包括减少模型参数、提高训练速度、接受任意长度输入、利用完整的历史信息。同时，RNN 的引入意味着可以使用RNN 的其他变体，像 LSTM、BLSTM、GRU 等等，从而在序列建模上进行更多更丰富的优化。 以上，从词袋模型说起，引出语言模型 N-gram 以及其优化模型 NNLM 和 RNNLM，后续内容从 RNN 说起，来看看其变种 LSTM 和 GRU模型如何处理类似序列数据。 RNN 以及变种 LSTM 和 GRU 原理RNN 为序列数据而生RNN 称为循环神经网路，因为这种网络有“记忆性”，主要应用在自然语言处理（NLP）和语音领域。RNN具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。 理论上，RNN 能够对任何长度的序列数据进行处理，但由于该网络结构存在“梯度消失”问题，所以在实际应用中，解决梯度消失的方法有：梯度裁剪（ClippingGradient）和 LSTM（Long Short-Term Memory）。 下图是一个简单的 RNN 经典结构： RNN 包含输入单元（Input Units），输入集标记为{x0,x1,…,xt,xt…}\{x_0,x_1,…,x_t,x_t…\}{x0​,x1​,…,xt​,xt​…}；输出单元（Output Units）的输出集则被标记为{y0,y1,…,yt,…}\{y_0,y_1,…,y_t,…\}{y0​,y1​,…,yt​,…}；RNN还包含隐藏单元（Hidden Units），我们将其输出集标记为{h0,h1,…,ht,…}\{h_0,h_1,…,h_t,…\}{h0​,h1​,…,ht​,…}，这些隐藏单元完成了最为主要的工作。 LSTM 结构LSTM 在1997年由“Hochreiter &amp; Schmidhuber”提出，目前已经成为 RNN 中的标准形式，用来解决上面提到的 RNN模型存在“长期依赖”的问题。 LSTM 通过三个“门”结构来控制不同时刻的状态和输出。所谓的“门”结构就是使用了 Sigmoid激活函数的全连接神经网络和一个按位做乘法的操作，Sigmoid激活函数会输出一个0~1之间的数值，这个数值代表当前有多少信息能通过“门”，0表示任何信息都无法通过，1表示全部信息都可以通过。其中，“遗忘门”和“输入门”是LSTM 单元结构的核心。下面我们来详细分析下三种“门”结构。 遗忘门，用来让 LSTM“忘记”之前没有用的信息。它会根据当前时刻节点的输入 XtX_tXt​、上一时刻节点的状态 $C_{t -1 } $ 和上一时刻节点的输出 ht−1h_{t-1}ht−1​ 来决定哪些信息将被遗忘。 输入门，LSTM 来决定当前输入数据中哪些信息将被留下来。在 LSTM 使用遗忘门“忘记”部分信息后需要从当前的输入留下最新的记忆。输入门会根据当前时刻节点的输入 XtX_tXt​、上一时刻节点的状态 Ct−1C_{t-1}Ct−1​ 和上一时刻节点的输出 ht−1h_{t-1}ht−1​ 来决定哪些信息将进入当前时刻节点的状态 CtC_tCt​，模型需要记忆这个最新的信息。 输出门，LSTM 在得到最新节点状态 CtC_tCt​ 后，结合上一时刻节点的输出 ht−1h_{t-1}ht−1​ 和当前时刻节点的输入 XtX_tXt​ 来决定当前时刻节点的输出。 GRU 结构GRU（Gated Recurrent Unit）是2014年提出来的新的 RNN 架构，它是简化版的 LSTM。下面是 LSTM 和 GRU的结构比较图（来自于网络）： 在超参数均调优的前提下，据说效果和 LSTM 差不多，但是参数少了1/3，不容易过拟合。如果发现 LSTM 训练出来的模型过拟合比较严重，可以试试 GRU。 实战基于 Keras 的 LSTM 和 GRU 文本分类上面讲了那么多，但是 RNN 的知识还有很多，比如双向 RNN 等，这些需要自己去学习，下面，我们来实战一下基于 LSTM 和 GRU 的文本分类。 本次开发使用 Keras 来快速构建和训练模型，使用的数据集还是第06课使用的司法数据。 整个过程包括： 语料加载 分词和去停用词 数据预处理 使用 LSTM 分类 使用 GRU 分类 第一步，引入数据处理库，停用词和语料加载： #引入包 import random import jieba import pandas as pd #加载停用词 stopwords=pd.read_csv(&#39;stopwords.txt&#39;,index_col=False,quoting=3,sep=&quot;\t&quot;,names=[&#39;stopword&#39;], encoding=&#39;utf-8&#39;) stopwords=stopwords[&#39;stopword&#39;].values #加载语料 laogong_df = pd.read_csv(&#39;beilaogongda.csv&#39;, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) laopo_df = pd.read_csv(&#39;beilaogongda.csv&#39;, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) erzi_df = pd.read_csv(&#39;beierzida.csv&#39;, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) nver_df = pd.read_csv(&#39;beinverda.csv&#39;, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) #删除语料的nan行 laogong_df.dropna(inplace=True) laopo_df.dropna(inplace=True) erzi_df.dropna(inplace=True) nver_df.dropna(inplace=True) #转换 laogong = laogong_df.segment.values.tolist() laopo = laopo_df.segment.values.tolist() erzi = erzi_df.segment.values.tolist() nver = nver_df.segment.values.tolist() 第二步，分词和去停用词： #定义分词和打标签函数preprocess_text #参数content_lines即为上面转换的list #参数sentences是定义的空list，用来储存打标签之后的数据 #参数category 是类型标签 def preprocess_text(content_lines, sentences, category): for line in content_lines: try: segs=jieba.lcut(line) segs = [v for v in segs if not str(v).isdigit()]#去数字 segs = list(filter(lambda x:x.strip(), segs)) #去左右空格 segs = list(filter(lambda x:len(x)&gt;1, segs))#长度为1的字符 segs = list(filter(lambda x:x not in stopwords, segs)) #去掉停用词 sentences.append((&quot; &quot;.join(segs), category))# 打标签 except Exception: print(line) continue #调用函数、生成训练数据 sentences = [] preprocess_text(laogong, sentences,0) preprocess_text(laopo, sentences, 1) preprocess_text(erzi, sentences, 2) preprocess_text(nver, sentences, 3) 第三步，先打散数据，使数据分布均匀，然后获取特征和标签列表： #打散数据，生成更可靠的训练集 random.shuffle(sentences) #控制台输出前10条数据，观察一下 for sentence in sentences[:10]: print(sentence[0], sentence[1]) #所有特征和对应标签 all_texts = [ sentence[0] for sentence in sentences] all_labels = [ sentence[1] for sentence in sentences] 第四步，使用 LSTM 对数据进行分类： #引入需要的模块 from keras.preprocessing.text import Tokenizer from keras.preprocessing.sequence import pad_sequences from keras.utils import to_categorical from keras.layers import Dense, Input, Flatten, Dropout from keras.layers import LSTM, Embedding,GRU from keras.models import Sequential #预定义变量 MAX_SEQUENCE_LENGTH = 100 #最大序列长度 EMBEDDING_DIM = 200 #embdding 维度 VALIDATION_SPLIT = 0.16 #验证集比例 TEST_SPLIT = 0.2 #测试集比例 #keras的sequence模块文本序列填充 tokenizer = Tokenizer() tokenizer.fit_on_texts(all_texts) sequences = tokenizer.texts_to_sequences(all_texts) word_index = tokenizer.word_index print(&#39;Found %s unique tokens.&#39; % len(word_index)) data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) labels = to_categorical(np.asarray(all_labels)) print(&#39;Shape of data tensor:&#39;, data.shape) print(&#39;Shape of label tensor:&#39;, labels.shape) #数据切分 p1 = int(len(data)*(1-VALIDATION_SPLIT-TEST_SPLIT)) p2 = int(len(data)*(1-TEST_SPLIT)) x_train = data[:p1] y_train = labels[:p1] x_val = data[p1:p2] y_val = labels[p1:p2] x_test = data[p2:] y_test = labels[p2:] #LSTM训练模型 model = Sequential() model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)) model.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2)) model.add(Dropout(0.2)) model.add(Dense(64, activation=&#39;relu&#39;)) model.add(Dense(labels.shape[1], activation=&#39;softmax&#39;)) model.summary() #模型编译 model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;rmsprop&#39;, metrics=[&#39;acc&#39;]) print(model.metrics_names) model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10, batch_size=128) model.save(&#39;lstm.h5&#39;) #模型评估 print(model.evaluate(x_test, y_test)) 训练过程结果为： 第五步，使用 GRU 进行文本分类，上面就是完整的使用 LSTM 进行 文本分类，如果使用 GRU 只需要改变模型训练部分： model = Sequential() model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)) model.add(GRU(200, dropout=0.2, recurrent_dropout=0.2)) model.add(Dropout(0.2)) model.add(Dense(64, activation=&#39;relu&#39;)) model.add(Dense(labels.shape[1], activation=&#39;softmax&#39;)) model.summary() model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;rmsprop&#39;, metrics=[&#39;acc&#39;]) print(model.metrics_names) model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10, batch_size=128) model.save(&#39;lstm.h5&#39;) print(model.evaluate(x_test, y_test)) 训练过程结果： 总结本文从词袋模型谈起，旨在引出语言模型 N-gram 以及其优化模型 NNLM 和 RNNLM，并通过 RNN 以及其变种 LSTM 和 GRU模型，理解其如何处理类似序列数据的原理，并实战基于 LSTM 和 GRU 的中文文本分类。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>gru</tag>
        <tag>lstm</tag>
        <tag>n-gram</tag>
        <tag>rnn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于LSTM轻松生成各种古诗（11）]]></title>
    <url>%2F2019%2F11%2F22%2F11.%E5%9F%BA%E4%BA%8ELSTM%E8%BD%BB%E6%9D%BE%E7%94%9F%E6%88%90%E5%90%84%E7%A7%8D%E5%8F%A4%E8%AF%97%2F</url>
    <content type="text"><![CDATA[目前循环神经网络（RNN）已经广泛用于自然语言处理中，可以处理大量的序列数据，可以说是最强大的神经网络模型之一。人们已经给 RNN找到了越来越多的事情做，比如画画和写诗，微软的小冰都已经出版了一本诗集了。 而其实训练一个能写诗的神经网络并不难，下面我们就介绍如何简单快捷地建立一个会写诗的网络模型。 本次开发环境如下： Python 3.6 Keras 环境 Jupyter Notebook 整个过程分为以下步骤完成： 语料准备 语料预处理 模型参数配置 构建模型 训练模型 模型作诗 绘制模型网络结构图 下面一步步来构建和训练一个会写诗的模型。 第一 ，语料准备。一共四万多首古诗，每行一首诗，标题在预处理的时候已经去掉了。 第二 ，文件预处理。首先，机器并不懂每个中文汉字代表的是什么，所以要将文字转换为机器能理解的形式，这里我们采用 One-Hot的形式，这样诗句中的每个字都能用向量来表示，下面定义函数 preprocess_file() 来处理。 puncs = [&#39;]&#39;, &#39;[&#39;, &#39;（&#39;, &#39;）&#39;, &#39;{&#39;, &#39;}&#39;, &#39;：&#39;, &#39;《&#39;, &#39;》&#39;] def preprocess_file(Config): # 语料文本内容 files_content = &#39;&#39; with open(Config.poetry_file, &#39;r&#39;, encoding=&#39;utf-8&#39;) as f: for line in f: # 每行的末尾加上&quot;]&quot;符号代表一首诗结束 for char in puncs: line = line.replace(char, &quot;&quot;) files_content += line.strip() + &quot;]&quot; words = sorted(list(files_content)) words.remove(&#39;]&#39;) counted_words = {} for word in words: if word in counted_words: counted_words[word] += 1 else: counted_words[word] = 1 # 去掉低频的字 erase = [] for key in counted_words: if counted_words[key] &lt;= 2: erase.append(key) for key in erase: del counted_words[key] del counted_words[&#39;]&#39;] wordPairs = sorted(counted_words.items(), key=lambda x: -x[1]) words, _ = zip(*wordPairs) # word到id的映射 word2num = dict((c, i + 1) for i, c in enumerate(words)) num2word = dict((i, c) for i, c in enumerate(words)) word2numF = lambda x: word2num.get(x, 0) return word2numF, num2word, words, files_content 在每行末尾加上 ]符号是为了标识这首诗已经结束了。我们给模型学习的方法是，给定前六个字，生成第七个字，所以在后面生成训练数据的时候，会以6的跨度，1的步长截取文字，生成语料。如果出现了] 符号，说明 ] 符号之前的语句和之后的语句是两首诗里面的内容，两首诗之间是没有关联关系的，所以我们后面会舍弃掉包含 ] 符号的训练数据。 第三 ，模型参数配置。预先定义模型参数和加载语料以及模型保存名称，通过类 Config 实现。 class Config(object): poetry_file = &#39;poetry.txt&#39; weight_file = &#39;poetry_model.h5&#39; # 根据前六个字预测第七个字 max_len = 6 batch_size = 512 learning_rate = 0.001 第四 ，构建模型，通过 PoetryModel 类实现，类的代码结构如下： class PoetryModel(object): def __init__(self, config): pass def build_model(self): pass def sample(self, preds, temperature=1.0): pass def generate_sample_result(self, epoch, logs): pass def predict(self, text): pass def data_generator(self): pass def train(self): pass 类中定义的方法具体实现功能如下： （1）init 函数定义，通过加载 Config 配置信息，进行语料预处理和模型加载，如果模型文件存在则直接加载模型，否则开始训练。 def __init__(self, config): self.model = None self.do_train = True self.loaded_model = False self.config = config # 文件预处理 self.word2numF, self.num2word, self.words, self.files_content = preprocess_file(self.config) if os.path.exists(self.config.weight_file): self.model = load_model(self.config.weight_file) self.model.summary() else: self.train() self.do_train = False self.loaded_model = True （2）build_model 函数主要用 Keras 来构建网络模型，这里使用 LSTM 的 GRU 来实现，当然直接使用 LSTM 也没问题。 def build_model(self): &#39;&#39;&#39;建立模型&#39;&#39;&#39; input_tensor = Input(shape=(self.config.max_len,)) embedd = Embedding(len(self.num2word)+1, 300, input_length=self.config.max_len)(input_tensor) lstm = Bidirectional(GRU(128, return_sequences=True))(embedd) dropout = Dropout(0.6)(lstm) lstm = Bidirectional(GRU(128, return_sequences=True))(embedd) dropout = Dropout(0.6)(lstm) flatten = Flatten()(lstm) dense = Dense(len(self.words), activation=&#39;softmax&#39;)(flatten) self.model = Model(inputs=input_tensor, outputs=dense) optimizer = Adam(lr=self.config.learning_rate) self.model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=optimizer, metrics=[&#39;accuracy&#39;]) （3）sample 函数，在训练过程的每个 epoch 迭代中采样。 def sample(self, preds, temperature=1.0): &#39;&#39;&#39; 当temperature=1.0时，模型输出正常 当temperature=0.5时，模型输出比较open 当temperature=1.5时，模型输出比较保守 在训练的过程中可以看到temperature不同，结果也不同 &#39;&#39;&#39; preds = np.asarray(preds).astype(&#39;float64&#39;) preds = np.log(preds) / temperature exp_preds = np.exp(preds) preds = exp_preds / np.sum(exp_preds) probas = np.random.multinomial(1, preds, 1) return np.argmax(probas) （4）训练过程中，每个 epoch 打印出当前的学习情况。 def generate_sample_result(self, epoch, logs): print(&quot;\n==================Epoch {}=====================&quot;.format(epoch)) for diversity in [0.5, 1.0, 1.5]: print(&quot;------------Diversity {}--------------&quot;.format(diversity)) start_index = random.randint(0, len(self.files_content) - self.config.max_len - 1) generated = &#39;&#39; sentence = self.files_content[start_index: start_index + self.config.max_len] generated += sentence for i in range(20): x_pred = np.zeros((1, self.config.max_len)) for t, char in enumerate(sentence[-6:]): x_pred[0, t] = self.word2numF(char) preds = self.model.predict(x_pred, verbose=0)[0] next_index = self.sample(preds, diversity) next_char = self.num2word[next_index] generated += next_char sentence = sentence + next_char print(sentence) （5）predict 函数，用于根据给定的提示，来进行预测。 根据给出的文字，生成诗句，如果给的 text 不到四个字，则随机补全。 def predict(self, text): if not self.loaded_model: return with open(self.config.poetry_file, &#39;r&#39;, encoding=&#39;utf-8&#39;) as f: file_list = f.readlines() random_line = random.choice(file_list) # 如果给的text不到四个字，则随机补全 if not text or len(text) != 4: for _ in range(4 - len(text)): random_str_index = random.randrange(0, len(self.words)) text += self.num2word.get(random_str_index) if self.num2word.get(random_str_index) not in [&#39;,&#39;, &#39;。&#39;, &#39;，&#39;] else self.num2word.get( random_str_index + 1) seed = random_line[-(self.config.max_len):-1] res = &#39;&#39; seed = &#39;c&#39; + seed for c in text: seed = seed[1:] + c for j in range(5): x_pred = np.zeros((1, self.config.max_len)) for t, char in enumerate(seed): x_pred[0, t] = self.word2numF(char) preds = self.model.predict(x_pred, verbose=0)[0] next_index = self.sample(preds, 1.0) next_char = self.num2word[next_index] seed = seed[1:] + next_char res += seed return res （6） data_generator 函数，用于生成数据，提供给模型训练时使用。 def data_generator(self): i = 0 while 1: x = self.files_content[i: i + self.config.max_len] y = self.files_content[i + self.config.max_len] puncs = [&#39;]&#39;, &#39;[&#39;, &#39;（&#39;, &#39;）&#39;, &#39;{&#39;, &#39;}&#39;, &#39;：&#39;, &#39;《&#39;, &#39;》&#39;, &#39;:&#39;] if len([i for i in puncs if i in x]) != 0: i += 1 continue if len([i for i in puncs if i in y]) != 0: i += 1 continue y_vec = np.zeros( shape=(1, len(self.words)), dtype=np.bool ) y_vec[0, self.word2numF(y)] = 1.0 x_vec = np.zeros( shape=(1, self.config.max_len), dtype=np.int32 ) for t, char in enumerate(x): x_vec[0, t] = self.word2numF(char) yield x_vec, y_vec i += 1 （7）train 函数，用来进行模型训练，其中迭代次数 number_of_epoch ，是根据训练语料长度除以 batch_size计算的，如果在调试中，想用更小一点的 number_of_epoch ，可以自定义大小，把 train 函数的第一行代码注释即可。 def train(self): #number_of_epoch = len(self.files_content) // self.config.batch_size number_of_epoch = 10 if not self.model: self.build_model() self.model.summary() self.model.fit_generator( generator=self.data_generator(), verbose=True, steps_per_epoch=self.config.batch_size, epochs=number_of_epoch, callbacks=[ keras.callbacks.ModelCheckpoint(self.config.weight_file, save_weights_only=False), LambdaCallback(on_epoch_end=self.generate_sample_result) ] ) 第五 ，整个模型构建好以后，接下来进行模型训练。 model = PoetryModel(Config) 训练过程中的第1-2轮迭代： 训练过程中的第9-10轮迭代： 虽然训练过程写出的诗句不怎么能看得懂，但是可以看到模型从一开始标点符号都不会用 ，到最后写出了有一点点模样的诗句，能看到模型变得越来越聪明了。 第六 ，模型作诗，模型迭代10次之后的测试，首先输入几个字，模型根据输入的提示，做出诗句。 text = input(&quot;text:&quot;) sentence = model.predict(text) print(sentence) 比如输入：小雨，模型做出的诗句为： 输入：text：小雨 结果：小妃侯里守。雨封即客寥。俘剪舟过槽。傲老槟冬绛。 第七 ，绘制网络结构图。 模型结构绘图，采用 Keras自带的功能实现： plot_model(model.model, to_file=&#39;model.png&#39;) 得到的模型结构图如下： 本节使用 LSTM 的变形 GRU 训练出一个能作诗的模型，当然大家可以替换训练语料为歌词或者小说，让机器人自动创作不同风格的歌曲或者小说。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>gru</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊天机器人（13）]]></title>
    <url>%2F2019%2F11%2F22%2F13.%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%2F</url>
    <content type="text"><![CDATA[自动聊天机器人，也称为自动问答系统，由于所使用的场景不同，叫法也不一样。自动问答（Question Answering，QA）是指利用计算机自动回答用户所提出的问题以满足用户知识需求的任务。不同于现有搜索引擎，问答系统是信息服务的一种高级形式，系统返回用户的不再是基于关键词匹配排序的文档列表，而是精准的自然语言答案。近年来，随着人工智能的飞速发展，自动问答已经成为倍受关注且发展前景广泛的研究方向。 自动问答简介自动问答主要研究的内容和关键科学问题如下： 问句理解 ：给定用户问题，自动问答首先需要理解用户所提问题。用户问句的语义理解包含词法分析、句法分析、语义分析等多项关键技术，需要从文本的多个维度理解其中包含的语义内容。 文本信息抽取 ：自动问答系统需要在已有语料库、知识库或问答库中匹配相关的信息，并抽取出相应的答案。 知识推理 ：自动问答中，由于语料库、知识库和问答库本身的覆盖度有限，并不是所有问题都能直接找到答案。这就需要在已有的知识体系中，通过知识推理的手段获取这些隐含的答案。 纵观自动问答研究的发展态势和技术现状，以下研究方向或问题将可能成为未来整个领域和行业重点关注的方向：基于深度学习的端到端自动问答，多领域、多语言的自动问答，面向问答的深度推理，篇章阅读理解、对话等。 基于 Chatterbot 制作中文聊天机器人ChatterBot 是一个构建在 Python 上，基于一系列规则和机器学习算法完成的聊天机器人，具有结构清晰，可扩展性好，简单实用的特点。 Chatterbot 安装有两种方式： 使用 pip install chatterbot 安装； 直接在 Github Chatterbot 下载这个项目，通过 python setup.py install 安装，其中 examples 文件夹中包含几个例子，可以根据例子加深自己的理解。 安装过程如果出现错误，主要是需要安装这些依赖库： chatterbot-corpus&gt;=1.1,&lt;1.2 mathparse&gt;=0.1,&lt;0.2 nltk&gt;=3.2,&lt;4.0 pymongo&gt;=3.3,&lt;4.0 python-dateutil&gt;=2.6,&lt;2.7 python-twitter&gt;=3.0,&lt;4.0 sqlalchemy&gt;=1.2,&lt;1.3 pint&gt;=0.8.1 1. 手动设置一点语料，体验基于规则的聊天机器人回答。 from chatterbot import ChatBot from chatterbot.trainers import ListTrainer Chinese_bot = ChatBot(&quot;Training demo&quot;) #创建一个新的实例 Chinese_bot.set_trainer(ListTrainer) Chinese_bot.train([ &#39;亲，在吗？&#39;, &#39;亲，在呢&#39;, &#39;这件衣服的号码大小标准吗？&#39;, &#39;亲，标准呢，请放心下单吧。&#39;, &#39;有红色的吗？&#39;, &#39;有呢，目前有白红蓝3种色调。&#39;, ]) 下面进行测试： # 测试一下 question = &#39;亲，在吗&#39; print(question) response = Chinese_bot.get_response(question) print(response) print(&quot;\n&quot;) question = &#39;有红色的吗？&#39; print(question) response = Chinese_bot.get_response(question) print(response) 从得到的结果可以看出，这应该完全是基于规则的判断： 亲，在吗 亲，在呢 有红色的吗？ 有呢，目前有白红蓝3种色调。 2. 训练自己的语料。 本次使用的语料来自 QQ 群的聊天记录，导出的 QQ 聊天记录稍微处理一下即可使用，整个过程如下。 （1）首先载入语料，第二行代码主要是想把每句话后面的换行 \n 去掉。 lines = open(&quot;QQ.txt&quot;,&quot;r&quot;,encoding=&#39;gbk&#39;).readlines() sec = [ line.strip() for line in lines] （2）接下来就可以训练模型了，由于整个语料比较大，训练过程也比较耗时。 from chatterbot import ChatBot from chatterbot.trainers import ListTrainer Chinese_bot = ChatBot(&quot;Training&quot;) Chinese_bot.set_trainer(ListTrainer) Chinese_bot.train(sec) 这里需要注意，如果训练过程很慢，可以在第一步中加入如下代码，即只取前1000条进行训练： sec = sec[0:1000] （3）最后，对训练好的模型进行测试，可见训练数据是 QQ 群技术对话，也看得出程序员们都很努力，整体想的都是学习。 以上只是简单的 Chatterbot 演示，如果想看更好的应用，推荐看官方文档。 基于 Seq2Seq 制作中文聊天机器人序列数据处理模型，从N-gram 语言模型到 RNN 及其变种。这里我们讲另外一个基于深度学习的 Seq2Seq 模型。 从 RNN 结构说起，根据输出和输入序列不同数量 RNN ，可以有多种不同的结构，不同结构自然就有不同的引用场合。 One To One 结构，仅仅只是简单的给一个输入得到一个输出，此处并未体现序列的特征，例如图像分类场景。 One To Many 结构，给一个输入得到一系列输出，这种结构可用于生产图片描述的场景。 Many To One 结构，给一系列输入得到一个输出，这种结构可用于文本情感分析，对一些列的文本输入进行分类，看是消极还是积极情感。 Many To Many 结构，给一系列输入得到一系列输出，这种结构可用于翻译或聊天对话场景，将输入的文本转换成另外一系列文本。 同步 Many To Many 结构，它是经典的 RNN 结构，前一输入的状态会带到下一个状态中，而且每个输入都会对应一个输出，我们最熟悉的应用场景是字符预测，同样也可以用于视频分类，对视频的帧打标签。 在 Many To Many 的两种模型中，第四和第五种是有差异的，经典 RNN结构的输入和输出序列必须要等长，它的应用场景也比较有限。而第四种，输入和输出序列可以不等长，这种模型便是 Seq2Seq 模型，即 Sequence toSequence。它实现了从一个序列到另外一个序列的转换，比如 Google 曾用 Seq2Seq 模型加 Attention模型实现了翻译功能，类似的还可以实现聊天机器人对话模型。经典的 RNN 模型固定了输入序列和输出序列的大小，而 Seq2Seq 模型则突破了该限制。 Seq2Seq 属于 Encoder-Decoder 结构，这里看看常见的 Encoder-Decoder 结构。基本思想就是利用两个 RNN，一个RNN 作为 Encoder，另一个 RNN 作为 Decoder。Encoder负责将输入序列压缩成指定长度的向量，这个向量就可以看成是这个序列的语义，这个过程称为编码，如下图，获取语义向量最简单的方式就是直接将最后一个输入的隐状态作为语义向量。也可以对最后一个隐含状态做一个变换得到语义向量，还可以将输入序列的所有隐含状态做一个变换得到语义变量。 具体理论知识这里不再赘述，下面重点看看，如何通过 Keras 实现一个 LSTM_Seq2Seq 自动问答机器人。 1. 语料准备。 语料我们使用 Tab 键 \t 把问题和答案区分，每一对为一行。其中，语料为爬虫爬取的工程机械网站的问答。 2. 模型构建和训练。 第一步，引入需要的包： from keras.models import Model from keras.layers import Input, LSTM, Dense import numpy as np import pandas as pd 第二步，定义模型超参数、迭代次数、语料路径： #Batch size 的大小 batch_size = 32 # 迭代次数epochs epochs = 100 # 编码空间的维度Latent dimensionality latent_dim = 256 # 要训练的样本数 num_samples = 5000 #设置语料的路径 data_path = &#39;D://nlp//ch13//files.txt&#39; 第三步，把语料向量化： #把数据向量话 input_texts = [] target_texts = [] input_characters = set() target_characters = set() with open(data_path, &#39;r&#39;, encoding=&#39;utf-8&#39;) as f: lines = f.read().split(&#39;\n&#39;) for line in lines[: min(num_samples, len(lines) - 1)]: #print(line) input_text, target_text = line.split(&#39;\t&#39;) # We use &quot;tab&quot; as the &quot;start sequence&quot; character # for the targets, and &quot;\n&quot; as &quot;end sequence&quot; character. target_text = target_text[0:100] target_text = &#39;\t&#39; + target_text + &#39;\n&#39; input_texts.append(input_text) target_texts.append(target_text) for char in input_text: if char not in input_characters: input_characters.add(char) for char in target_text: if char not in target_characters: target_characters.add(char) input_characters = sorted(list(input_characters)) target_characters = sorted(list(target_characters)) num_encoder_tokens = len(input_characters) num_decoder_tokens = len(target_characters) max_encoder_seq_length = max([len(txt) for txt in input_texts]) max_decoder_seq_length = max([len(txt) for txt in target_texts]) print(&#39;Number of samples:&#39;, len(input_texts)) print(&#39;Number of unique input tokens:&#39;, num_encoder_tokens) print(&#39;Number of unique output tokens:&#39;, num_decoder_tokens) print(&#39;Max sequence length for inputs:&#39;, max_encoder_seq_length) print(&#39;Max sequence length for outputs:&#39;, max_decoder_seq_length) input_token_index = dict( [(char, i) for i, char in enumerate(input_characters)]) target_token_index = dict( [(char, i) for i, char in enumerate(target_characters)]) encoder_input_data = np.zeros( (len(input_texts), max_encoder_seq_length, num_encoder_tokens),dtype=&#39;float32&#39;) decoder_input_data = np.zeros( (len(input_texts), max_decoder_seq_length, num_decoder_tokens),dtype=&#39;float32&#39;) decoder_target_data = np.zeros( (len(input_texts), max_decoder_seq_length, num_decoder_tokens),dtype=&#39;float32&#39;) for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)): for t, char in enumerate(input_text): encoder_input_data[i, t, input_token_index[char]] = 1. for t, char in enumerate(target_text): # decoder_target_data is ahead of decoder_input_data by one timestep decoder_input_data[i, t, target_token_index[char]] = 1. if t &gt; 0: # decoder_target_data will be ahead by one timestep # and will not include the start character. decoder_target_data[i, t - 1, target_token_index[char]] = 1. 第四步，LSTM_Seq2Seq 模型定义、训练和保存： encoder_inputs = Input(shape=(None, num_encoder_tokens)) encoder = LSTM(latent_dim, return_state=True) encoder_outputs, state_h, state_c = encoder(encoder_inputs) # 输出 `encoder_outputs` encoder_states = [state_h, state_c] # 状态 `encoder_states` decoder_inputs = Input(shape=(None, num_decoder_tokens)) decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states) decoder_dense = Dense(num_decoder_tokens, activation=&#39;softmax&#39;) decoder_outputs = decoder_dense(decoder_outputs) # 定义模型 model = Model([encoder_inputs, decoder_inputs], decoder_outputs) # 训练 model.compile(optimizer=&#39;rmsprop&#39;, loss=&#39;categorical_crossentropy&#39;) model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=epochs, validation_split=0.2) # 保存模型 model.save(&#39;s2s.h5&#39;) 第五步，Seq2Seq 的 Encoder 操作： encoder_model = Model(encoder_inputs, encoder_states) decoder_state_input_h = Input(shape=(latent_dim,)) decoder_state_input_c = Input(shape=(latent_dim,)) decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c] decoder_outputs, state_h, state_c = decoder_lstm( decoder_inputs, initial_state=decoder_states_inputs) decoder_states = [state_h, state_c] decoder_outputs = decoder_dense(decoder_outputs) decoder_model = Model( [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states) 第六步，把索引和分词转成序列： reverse_input_char_index = dict( (i, char) for char, i in input_token_index.items()) reverse_target_char_index = dict( (i, char) for char, i in target_token_index.items()) 第七步，定义预测函数，先使用预模型预测，然后编码成汉字结果： def decode_sequence(input_seq): # Encode the input as state vectors. states_value = encoder_model.predict(input_seq) #print(states_value) # Generate empty target sequence of length 1. target_seq = np.zeros((1, 1, num_decoder_tokens)) # Populate the first character of target sequence with the start character. target_seq[0, 0, target_token_index[&#39;\t&#39;]] = 1. # Sampling loop for a batch of sequences # (to simplify, here we assume a batch of size 1). stop_condition = False decoded_sentence = &#39;&#39; while not stop_condition: output_tokens, h, c = decoder_model.predict( [target_seq] + states_value) # Sample a token sampled_token_index = np.argmax(output_tokens[0, -1, :]) sampled_char = reverse_target_char_index[sampled_token_index] decoded_sentence += sampled_char if (sampled_char == &#39;\n&#39; or len(decoded_sentence) &gt; max_decoder_seq_length): stop_condition = True # Update the target sequence (of length 1). target_seq = np.zeros((1, 1, num_decoder_tokens)) target_seq[0, 0, sampled_token_index] = 1. # 更新状态 states_value = [h, c] return decoded_sentence 3. 模型预测。首先，定义一个预测函数： def predict_ans(question): inseq = np.zeros((len(question), max_encoder_seq_length, num_encoder_tokens),dtype=&#39;float16&#39;) decoded_sentence = decode_sequence(inseq) return decoded_sentence 然后就可以预测了： print(&#39;Decoded sentence:&#39;, predict_ans(&quot;挖机履带掉了怎么装上去&quot;)) 总结本文我们首先基于 Chatterbot 制作了中文聊天机器人，并用 QQ 群对话语料自己尝试训练。然后通过 LSTM 和 Seq2Seq模型，根据爬取的语料，训练了一个自动问答的模型，通过以上两种方式，我们们对自动问答有了一个简单的入门。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>chatterbot</tag>
        <tag>seq2seq</tag>
        <tag>lstm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中文命名实体提取（14）]]></title>
    <url>%2F2019%2F11%2F22%2F14.%E4%B8%AD%E6%96%87%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E6%8F%90%E5%8F%96%2F</url>
    <content type="text"><![CDATA[命名实体识别（NamedEntitiesRecognition，NER）是自然语言处理的一个基础任务。其目的是识别语料中人名、地名、组织机构名等命名实体，比如，[2015年中国国家海洋局对124个国际海底地理实体的命名]https://baike.baidu.com/item/2015%E5%B9%B4%E4%B8%AD%E5%9B%BD%E5%91%BD%E5%90%8D%E7%9A%84124%E4%B8%AA%E5%9B%BD%E9%99%85%E6%B5%B7%E5%BA%95%E5%9C%B0%E7%90%86%E5%AE%9E%E4%BD%93%E5%90%8D%E7%A7%B0%E4%BF%A1%E6%81%AF/18705238)。 由于命名实体数量不断增加，通常不可能在词典中穷尽列出，且其构成方法具有各自的一些规律性，因而，通常把对这些词的识别从词汇形态处理（如汉语切分）任务中独立处理，称为命名实体识别。 命名实体识别技术是信息抽取、信息检索、机器翻译、问答系统等多种自然语言处理技术必不可少的组成部分。 常见的命名实体识别方法综述命名实体是命名实体识别的研究主体，一般包括三大类（实体类、时间类和数字类）和七小类（人名、地名、机构名、时间、日期、货币和百分比）命名实体。评判一个命名实体是否被正确识别包括两个方面：实体的边界是否正确和实体的类型是否标注正确。 命名实体识别的主要技术方法分为：基于规则和词典的方法、基于统计的方法、二者混合的方法等。 1.基于规则和词典的方法。 基于规则的方法多采用语言学专家手工构造规则模板，选用特征包括统计信息、标点符号、关键字、指示词和方向词、位置词（如尾字）、中心词等方法，以模式和字符串相匹配为主要手段，这类系统大多依赖于知识库和词典的建立。基于规则和词典的方法是命名实体识别中最早使用的方法，一般而言，当提取的规则能比较精确地反映语言现象时，基于规则的方法性能要优于基于统计的方法。但是这些规则往往依赖于具体语言、领域和文本风格，编制过程耗时且难以涵盖所有的语言现象，特别容易产生错误，系统可移植性不好，对于不同的系统需要语言学专家重新书写规则。基于规则的方法的另外一个缺点是代价太大，存在系统建设周期长、移植性差而且需要建立不同领域知识库作为辅助以提高系统识别能力等问题。 2.基于统计的方法。 基于统计机器学习的方法主要包括隐马尔可夫模型（HiddenMarkovMode，HMM）、最大熵（MaxmiumEntropy，ME）、支持向量机（SupportVectorMachine，SVM）、条件随机场（ConditionalRandom Fields，CRF）等。 在基于统计的这四种学习方法中，最大熵模型结构紧凑，具有较好的通用性，主要缺点是训练时间长复杂性高，有时甚至导致训练代价难以承受，另外由于需要明确的归一化计算，导致开销比较大。而条件随机场为命名实体识别提供了一个特征灵活、全局最优的标注框架，但同时存在收敛速度慢、训练时间长的问题。一般说来，最大熵和支持向量机在正确率上要比隐马尔可夫模型高一些，但隐马尔可夫模型在训练和识别时的速度要快一些，主要是由于在利用Viterbi算法求解命名实体类别序列时的效率较高。隐马尔可夫模型更适用于一些对实时性有要求以及像信息检索这样需要处理大量文本的应用，如短文本命名实体识别。 基于统计的方法对特征选取的要求较高，需要从文本中选择对该项任务有影响的各种特征，并将这些特征加入到特征向量中。依据特定命名实体识别所面临的主要困难和所表现出的特性，考虑选择能有效反映该类实体特性的特征集合。主要做法是通过对训练语料所包含的语言信息进行统计和分析，从训练语料中挖掘出特征。有关特征可以分为具体的单词特征、上下文特征、词典及词性特征、停用词特征、核心词特征以及语义特征等。 基于统计的方法对语料库的依赖也比较大，而可以用来建设和评估命名实体识别系统的大规模通用语料库又比较少。 3.混合方法。 自然语言处理并不完全是一个随机过程，单独使用基于统计的方法使状态搜索空间非常庞大，必须借助规则知识提前进行过滤修剪处理。目前几乎没有单纯使用统计模型而不使用规则知识的命名实体识别系统，在很多情况下是使用混合方法： 统计学习方法之间或内部层叠融合。 规则、词典和机器学习方法之间的融合，其核心是融合方法技术。在基于统计的学习方法中引入部分规则，将机器学习和人工知识结合起来。 将各类模型、算法结合起来，将前一级模型的结果作为下一级的训练数据，并用这些训练数据对模型进行训练，得到下一级模型。 命名实体识别的一般流程如下图所示，一般的命名实体流程主要分为四个步骤： 对需要进行提取的文本语料进行分词； 获取需要识别的领域标签，并对分词结果进行标签标注； 对标签标注的分词进行抽取； 将抽取的分词组成需要的领域的命名实体。 动手实战命名实体识别下面通过jieba 分词包和 pyhanlp 来实战命名实体识别和提取。 1.jieba 进行命名实体识别和提取。 第一步，引入 jieba 包： import jieba import jieba.analyse import jieba.posseg as posg 第二步，使用 jieba 进行词性切分，allowPOS 指定允许的词性，这里选择名词 n 和地名 ns： sentence=u&#39;&#39;&#39;上线三年就成功上市,拼多多上演了互联网企业的上市奇迹,却也放大平台上存在的诸多问题，拼多多在美国上市。&#39;&#39;&#39; kw=jieba.analyse.extract_tags(sentence,topK=10,withWeight=True,allowPOS=(&#39;n&#39;,&#39;ns&#39;)) for item in kw: print(item[0],item[1]) 在这里，我们可以得到打印出来的结果： 上市 1.437080435586 上线 0.820694551317 奇迹 0.775434839431 互联网 0.712189275429 平台 0.6244340485550001 企业 0.422177218495 美国 0.415659623166 问题 0.39635135730800003 可以看得出，上市和上线应该是动词，这里给出的结果不是很准确。接下来，我们使用 textrank 算法来试试： kw=jieba.analyse.textrank(sentence,topK=20,withWeight=True,allowPOS=(&#39;ns&#39;,&#39;n&#39;)) for item in kw: print(item[0],item[1]) 这次得到的结果如下，可见，两次给出的结果还是不一样的。 上市 1.0 奇迹 0.572687398431635 企业 0.5710407272273452 互联网 0.5692560484441649 上线 0.23481844682115297 美国 0.23481844682115297 2.pyhanlp 进行命名实体识别和提取。 第一步，引入pyhanlp包： from pyhanlp import * 第二步，进行词性切分： sentence=u&#39;&#39;&#39;上线三年就成功上市,拼多多上演了互联网企业的上市奇迹,却也放大平台上存在的诸多问题，拼多多在美国上市。&#39;&#39;&#39; analyzer = PerceptronLexicalAnalyzer() segs = analyzer.analyze(sentence) arr = str(segs).split(&quot; &quot;) 第三步，定义一个函数，从得到的结果中，根据词性获取指定词性的词： def get_result(arr): re_list = [] ner = [&#39;n&#39;,&#39;ns&#39;] for x in arr: temp = x.split(&quot;/&quot;) if(temp[1] in ner): re_list.append(temp[0]) return re_list 第四步，我们获取结果： result = get_result(arr) print(result) 得到的结果如下，可见比 jieba 更准确： [&#39;互联网&#39;, &#39;企业&#39;, &#39;奇迹&#39;, &#39;平台&#39;, &#39;问题&#39;, &#39;美国&#39;] 总结本文对命名实体识别的方法进行了总结，并给出一般的处理流程，最后通过简单的 jieba 分词和 pyhanlp分词根据词性获取实体对象，后续大家也可以尝试通过哈工大和斯坦福的包来处理，下篇我们通过条件随机场 CRF 来训练一个命名实体识别模型。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>jieba</tag>
        <tag>pyhanlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于CRF的中文命名实体识别模型实现（15）]]></title>
    <url>%2F2019%2F11%2F22%2F15.%E5%9F%BA%E4%BA%8ECRF%E7%9A%84%E4%B8%AD%E6%96%87%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[命名实体识别在越来越多的场景下被应用，如自动问答、知识图谱等。非结构化的文本内容有很多丰富的信息，但找到相关的知识始终是一个具有挑战性的任务，命名实体识别也不例外。 前面我们用隐马尔可夫模型（HMM）自己尝试训练过一个分词器，其实 HMM也可以用来训练命名实体识别器，但在本文，我们讲另外一个算法——条件随机场（CRF），来训练一个命名实体识别器。 浅析条件随机场（CRF）条件随机场（Conditional Random Fields，简称CRF）是给定一组输入序列条件下另一组输出序列的条件概率分布模型，在自然语言处理中得到了广泛应用。 首先，我们来看看什么是随机场。“随机场”的名字取的很玄乎，其实理解起来不难。随机场是由若干个位置组成的整体，当按照某种分布给每一个位置随机赋予一个值之后，其全体就叫做随机场。 还是举词性标注的例子。假如我们有一个十个词形成的句子需要做词性标注。这十个词每个词的词性可以在我们已知的词性集合（名词，动词……)中去选择。当我们为每个词选择完词性后，这就形成了一个随机场。 了解了随机场，我们再来看看马尔科夫随机场。马尔科夫随机场是随机场的特例，它假设随机场中某一个位置的赋值仅仅与和它相邻的位置的赋值有关，和与其不相邻的位置的赋值无关。 继续举十个词的句子词性标注的例子。如果我们假设所有词的词性只和它相邻的词的词性有关时，这个随机场就特化成一个马尔科夫随机场。比如第三个词的词性除了与自己本身的位置有关外，还只与第二个词和第四个词的词性有关。 理解了马尔科夫随机场，再理解 CRF 就容易了。CRF 是马尔科夫随机场的特例，它假设马尔科夫随机场中只有 X 和 Y 两种变量，X 一般是给定的，而 Y一般是在给定 X 的条件下我们的输出。这样马尔科夫随机场就特化成了条件随机场。 在我们十个词的句子词性标注的例子中，X 是词，Y 是词性。因此，如果我们假设它是一个马尔科夫随机场，那么它也就是一个 CRF。 对于 CRF，我们给出准确的数学语言描述：设 X 与 Y 是随机变量，P(Y|X) 是给定 X 时 Y 的条件概率分布，若随机变量 Y构成的是一个马尔科夫随机场，则称条件概率分布 P(Y|X) 是条件随机场。 基于 CRF 的中文命名实体识别模型实现在常规的命名实体识别中，通用场景下最常提取的是时间、人物、地点及组织机构名，因此本模型也将提取以上四种实体。 1.开发环境。 本次开发所选用的环境为： Sklearn_crfsuite Python 3.6 Jupyter Notebook 2.数据预处理。 本模型使用人民日报1998年标注数据，进行预处理。语料库词性标记中，对应的实体词依次为 t、nr、ns、nt。对语料需要做以下处理： 将语料全角字符统一转为半角； 合并语料库分开标注的姓和名，例如：温/nr 家宝/nr； 合并语料库中括号中的大粒度词，例如：[国家/n 环保局/n]nt； 合并语料库分开标注的时间，例如：（/w 一九九七年/t 十二月/t 三十一日/t ）/w。 首先引入需要用到的库： import re import sklearn_crfsuite from sklearn_crfsuite import metrics from sklearn.externals import joblib 数据预处理，定义 CorpusProcess 类，我们还是先给出类实现框架： class CorpusProcess(object): def __init__(self): &quot;&quot;&quot;初始化&quot;&quot;&quot; pass def read_corpus_from_file(self, file_path): &quot;&quot;&quot;读取语料&quot;&quot;&quot; pass def write_corpus_to_file(self, data, file_path): &quot;&quot;&quot;写语料&quot;&quot;&quot; pass def q_to_b(self,q_str): &quot;&quot;&quot;全角转半角&quot;&quot;&quot; pass def b_to_q(self,b_str): &quot;&quot;&quot;半角转全角&quot;&quot;&quot; pass def pre_process(self): &quot;&quot;&quot;语料预处理 &quot;&quot;&quot; pass def process_k(self, words): &quot;&quot;&quot;处理大粒度分词,合并语料库中括号中的大粒度分词,类似：[国家/n 环保局/n]nt &quot;&quot;&quot; pass def process_nr(self, words): &quot;&quot;&quot; 处理姓名，合并语料库分开标注的姓和名，类似：温/nr 家宝/nr&quot;&quot;&quot; pass def process_t(self, words): &quot;&quot;&quot;处理时间,合并语料库分开标注的时间词，类似： （/w 一九九七年/t 十二月/t 三十一日/t ）/w &quot;&quot;&quot; pass def pos_to_tag(self, p): &quot;&quot;&quot;由词性提取标签&quot;&quot;&quot; pass def tag_perform(self, tag, index): &quot;&quot;&quot;标签使用BIO模式&quot;&quot;&quot; pass def pos_perform(self, pos): &quot;&quot;&quot;去除词性携带的标签先验知识&quot;&quot;&quot; pass def initialize(self): &quot;&quot;&quot;初始化 &quot;&quot;&quot; pass def init_sequence(self, words_list): &quot;&quot;&quot;初始化字序列、词性序列、标记序列 &quot;&quot;&quot; pass def extract_feature(self, word_grams): &quot;&quot;&quot;特征选取&quot;&quot;&quot; pass def segment_by_window(self, words_list=None, window=3): &quot;&quot;&quot;窗口切分&quot;&quot;&quot; pass def generator(self): &quot;&quot;&quot;训练数据&quot;&quot;&quot; pass 由于整个代码实现过程较长，我这里给出重点步骤，最后会在 Github 上连同语料代码一同给出 ，下面是关键过程实现。 对语料中的句子、词性，实体分类标记进行区分。标签采用“BIO”体系，即实体的第一个字为 B_*，其余字为 I_*，非实体字统一标记为O。大部分情况下，标签体系越复杂，准确度也越高，但这里采用简单的 BIO 体系也能达到相当不错的效果。这里模型采用 tri-gram形式，所以在字符列中，要在句子前后加上占位符。 def init_sequence(self, words_list): &quot;&quot;&quot;初始化字序列、词性序列、标记序列 &quot;&quot;&quot; words_seq = [[word.split(u&#39;/&#39;)[0] for word in words] for words in words_list] pos_seq = [[word.split(u&#39;/&#39;)[1] for word in words] for words in words_list] tag_seq = [[self.pos_to_tag(p) for p in pos] for pos in pos_seq] self.pos_seq = [[[pos_seq[index][i] for _ in range(len(words_seq[index][i]))] for i in range(len(pos_seq[index]))] for index in range(len(pos_seq))] self.tag_seq = [[[self.tag_perform(tag_seq[index][i], w) for w in range(len(words_seq[index][i]))] for i in range(len(tag_seq[index]))] for index in range(len(tag_seq))] self.pos_seq = [[u&#39;un&#39;]+[self.pos_perform(p) for pos in pos_seq for p in pos]+[u&#39;un&#39;] for pos_seq in self.pos_seq] self.tag_seq = [[t for tag in tag_seq for t in tag] for tag_seq in self.tag_seq] self.word_seq = [[u&#39;&lt;BOS&gt;&#39;]+[w for word in word_seq for w in word]+[u&#39;&lt;EOS&gt;&#39;] for word_seq in words_seq] 处理好语料之后，紧接着进行模型定义和训练，定义 CRF_NER 类，我们还是采用先给出类实现框架，再具体讲解其实现： class CRF_NER(object): def __init__(self): &quot;&quot;&quot;初始化参数&quot;&quot;&quot; pass def initialize_model(self): &quot;&quot;&quot;初始化&quot;&quot;&quot; pass def train(self): &quot;&quot;&quot;训练&quot;&quot;&quot; pass def predict(self, sentence): &quot;&quot;&quot;预测&quot;&quot;&quot; pass def load_model(self): &quot;&quot;&quot;加载模型 &quot;&quot;&quot; pass def save_model(self): &quot;&quot;&quot;保存模型&quot;&quot;&quot; pass 在 CRF_NER 类中，分别完成了语料预处理和模型训练、保存、预测功能，具体实现如下。 第一步，init 函数实现了模型参数定义和 CorpusProcess 的实例化和语料预处理： def __init__(self): &quot;&quot;&quot;初始化参数&quot;&quot;&quot; self.algorithm = &quot;lbfgs&quot; self.c1 =&quot;0.1&quot; self.c2 = &quot;0.1&quot; self.max_iterations = 100 #迭代次数 self.model_path = dir + &quot;model.pkl&quot; self.corpus = CorpusProcess() #Corpus 实例 self.corpus.pre_process() #语料预处理 self.corpus.initialize() #初始化语料 self.model = None 第二步，给出模型定义，了解 sklearn_crfsuite.CRF 详情可查该[文档](https://sklearn-crfsuite.readthedocs.io/en/latest/api.html#sklearn_crfsuite.CRF)。 def initialize_model(self): &quot;&quot;&quot;初始化&quot;&quot;&quot; algorithm = self.algorithm c1 = float(self.c1) c2 = float(self.c2) max_iterations = int(self.max_iterations) self.model = sklearn_crfsuite.CRF(algorithm=algorithm, c1=c1, c2=c2, max_iterations=max_iterations, all_possible_transitions=True) 第三步，模型训练和保存，分为训练集和测试集： def train(self): &quot;&quot;&quot;训练&quot;&quot;&quot; self.initialize_model() x, y = self.corpus.generator() x_train, y_train = x[500:], y[500:] x_test, y_test = x[:500], y[:500] self.model.fit(x_train, y_train) labels = list(self.model.classes_) labels.remove(&#39;O&#39;) y_predict = self.model.predict(x_test) metrics.flat_f1_score(y_test, y_predict, average=&#39;weighted&#39;, labels=labels) sorted_labels = sorted(labels, key=lambda name: (name[1:], name[0])) print(metrics.flat_classification_report(y_test, y_predict, labels=sorted_labels, digits=3)) self.save_model() 第四至第六步中 predict、load_model、save_model 方法的实现，大家可以在文末给出的地址中查看源码，这里就不堆代码了。 最后，我们来看看模型训练和预测的过程和结果： ner = CRF_NER() model = ner.train() 经过模型训练，得到的准确率和召回率如下： 进行模型预测，其结果还不错，如下： 基于 CRF的中文命名实体识别模型实现先讲到这儿，项目源码和涉及到的语料，大家可以到：Github上查看。 总结本文浅析了条件随机场，并使用 sklearn_crfsuite.CRF模型，对人民日报1998年标注数据进行了模型训练和预测，以帮助大家加强对条件随机场的理解。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>crf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于CRF的中文句法依存分析模型实现（17）]]></title>
    <url>%2F2019%2F11%2F22%2F17.%E5%9F%BA%E4%BA%8ECRF%E7%9A%84%E4%B8%AD%E6%96%87%E5%8F%A5%E6%B3%95%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[句法分析是自然语言处理中的关键技术之一，其基本任务是确定句子的句法结构或者句子中词汇之间的依存关系。主要包括两方面的内容，一是确定语言的语法体系，即对语言中合法句子的语法结构给予形式化的定义；另一方面是句法分析技术，即根据给定的语法体系，自动推导出句子的句法结构，分析句子所包含的句法单位和这些句法单位之间的关系。 依存关系本身是一个树结构，每一个词看成一个节点，依存关系就是一条有向边。本文主要通过清华大学的句法标注语料库，来实现基于 CRF 的中文句法依存分析模型。 清华大学句法标注语料库清华大学的句法标注语料，包括训练集（train.conll）和开发集合文件（dev.conll）。训练集大小 5.41M，共185541条数据。测试集大小为578kb，共19302条数据。 语料本身格式如下图所示： 通过上图，我们可以看出，每行语料包括有8个标签，分别是ID、FROM、lEMMA、CPOSTAG、POSTAG、FEATS、HEAD、DEPREL。详细介绍如下图： 模型的实现通过上面对句法依存关键技术的定义，我们明白了，句法依存的基本任务是确定句子的句法结构或者句子中词汇之间的依存关系。同时，我们也对此次模型实现的语料有了基本了解。 有了这些基础内容，我们便可以开始着手开发了。 本模型的实现过程，我们将主要分为训练集和测试集数据预处理、语料特征生成、模型训练及预测三大部分来实现，最终将通过模型预测得到正确的预测结果。 本次实战演练，我们选择以下模型和软件： Sklearn_crfsuite Python3.6 Jupyter Notebook 训练集和测试集数据预处理由于上述给定的语料，在模型中，我们不能直接使用，必须先经过预处理，把上述语料格式重新组织成具有词性、方向和距离的格式。 首先，我们通过一个 Python 脚本 get_parser_train_test_input.py，生成所需要的训练集和测试集，执行如下命令即可： cat train.conll | python get_parser_train_test_input.py &gt; train.data cat dev.conll | python get_parser_train_test_input.py &gt; dev.data 上面的脚本通过 cat 命令和管道符把内容传递给脚本进行处理。这里需要注意的是，脚本需要在 Linux 环境下执行，且语料和脚本应放在同一目录下。 get_parser_train_test_input.py 这一脚本的目的，就是重新组织语料，组织成可以使用 CRF算法的格式，具有词性、方向和距离的格式。我们认为，如果词 A 依赖词 B，A 就是孩子，B就是父亲。按照这种假设得到父亲节点的粗词性和详细词性，以及和依赖次之间的距离。 我们打开该脚本，看看它的代码，如下所示，重要的代码给出了注释。 #coding=utf-8 &#39;&#39;&#39;词A依赖词B，A就是孩子，B就是父亲&#39;&#39;&#39; import sys sentence = [&quot;Root&quot;] def do_parse(sentence): if len(sentence) == 1:return for line in sentence[1:]: line_arr = line.strip().split(&quot;\t&quot;) c_id = int(line_arr[0]) f_id = int(line_arr[6]) if f_id == 0: print(&quot;\t&quot;.join(line_arr[2:5])+&quot;\t&quot; + &quot;0_Root&quot;) continue f_post,f_detail_post = sentence[f_id].strip().split(&quot;\t&quot;)[3:5] #得到父亲节点的粗词性和详细词性 c_edge_post = f_post #默认是依赖词的粗粒度词性，但是名词除外；名词取细粒度词性 if f_post == &quot;n&quot;: c_edge_post = f_detail_post #计算是第几个出现这种词行 diff = f_id - c_id #确定要走几步 step = 1 if f_id &gt; c_id else -1 #确定每一步方向 same_post_num = 0 #中间每一步统计多少个一样的词性 cmp_idx = 4 if f_post == &quot;n&quot; else 3 #根据是否是名词决定取的是粗or详细词性 for i in range(0, abs(diff)): idx = c_id + (i+1)*step if sentence[idx].strip().split(&quot;\t&quot;)[cmp_idx] == c_edge_post: same_post_num += step print(&quot;\t&quot;.join(line_arr[2:5])+&quot;\t&quot; + &quot;%d_%s&quot;%(same_post_num, c_edge_post)) print(&quot;&quot;) for line in sys.stdin: line = line.strip() line_arr = line.split(&quot;\t&quot;) if line == &quot;&quot; or line_arr[0] == &quot;1&quot;: do_parse(sentence) sentence = [&quot;Root&quot;] if line ==&quot;&quot;:continue sentence.append(line) 整个脚本按行读入，每行按 Tab 键分割，首先得到父亲节点的词性，然后根据词性是否是名词 n 进行判断，默认是依赖词的粗粒度词性，如果是名词取细粒度词性。 脚本处理完，数据集的格式如下： 根据依存文法，决定两个词之间依存关系的主要有两个因素：方向和距离。正如上图中第四列类别标签所示，该列可以定义为以下形式： [+|-]dPOS 其中，[+|-] 表示中心词在句子中相对坐标轴的方向；POS 代表中心词具有的词性类别；d 表示与中心词词性相同的词的数量，即距离。 语料特征生成语料特征提取，主要采用 N-gram 模型来完成。这里我们使用 3-gram完成提取，将词性与词语两两进行匹配，分别返回特征集合和标签集合，需要注意整个语料采用的是 UTF-8 编码格式。 整个编码过程中，我们首先需要引入需要的库，然后对语料进行读文件操作。语料采用 UTF-8 编码格式，以句子为单位，按 Tab 键作分割处理，从而实现句子3-gram 模型的特征提取。具体实现如下。 import sklearn_crfsuite from sklearn_crfsuite import metrics from sklearn.externals import joblib 首先引入需要用到的库，如上面代码所示。其目的是使用模型 sklearn_crfsuite .CRF，metrics 用来进行模型性能测试，joblib用来保存和加载训练好的模型。 接着，定义包含特征处理方法的类，命名为 CorpusProcess，类结构定义如下： class CorpusProcess(object): def __init__(self): &quot;&quot;&quot;初始化&quot;&quot;&quot; pass def read_corpus_from_file(self, file_path): &quot;&quot;&quot;读取语料&quot;&quot;&quot; pass def write_corpus_to_file(self, data, file_path): &quot;&quot;&quot;写语料&quot;&quot;&quot; pass def process_sentence(self,lines): &quot;&quot;&quot;处理句子&quot;&quot;&quot; pass def initialize(self): &quot;&quot;&quot;语料初始化&quot;&quot;&quot; pass def generator(self, train=True): &quot;&quot;&quot;特征生成器&quot;&quot;&quot; pass def extract_feature(self, sentences): &quot;&quot;&quot;提取特征&quot;&quot;&quot; pass 下面介绍下 CorpusProcess 类中各个方法的具体实现。 第1步， 实现 init 构造函数，目的初始化预处理好的语料的路径： def __init__(self): &quot;&quot;&quot;初始化&quot;&quot;&quot; self.train_process_path = dir + &quot;data//train.data&quot; #预处理之后的训练集 self.test_process_path = dir + &quot;data//dev.data&quot; #预处理之后的测试集 这里的路径可以自定义，这里的语料之前已经完成了预处理过程。第2-3步，read_corpus_from_file 方法和 write_corpus_to_file 方法，分别定义了语料文件的读和写操作： def read_corpus_from_file(self, file_path): &quot;&quot;&quot;读取语料&quot;&quot;&quot; f = open(file_path, &#39;r&#39;,encoding=&#39;utf-8&#39;) lines = f.readlines() f.close() return lines def write_corpus_to_file(self, data, file_path): &quot;&quot;&quot;写语料&quot;&quot;&quot; f = open(file_path, &#39;w&#39;) f.write(str(data)) f.close() 这一步，主要用 open 函数来实现语料文件的读和写。 第4-5步，process_sentence 方法和 initialize 方法，用来处理句子和初始化语料，把语料按句子结构用 list存储起来，存储到内存中： def process_sentence(self,lines): &quot;&quot;&quot;处理句子&quot;&quot;&quot; sentence = [] for line in lines: if not line.strip(): yield sentence sentence = [] else: lines = line.strip().split(u&#39;\t&#39;) result = [line for line in lines] sentence.append(result) def initialize(self): &quot;&quot;&quot;语料初始化&quot;&quot;&quot; train_lines = self.read_corpus_from_file(self.train_process_path) test_lines = self.read_corpus_from_file(self.test_process_path) self.train_sentences = [sentence for sentence in self.process_sentence(train_lines)] self.test_sentences = [sentence for sentence in self.process_sentence(test_lines)] 这一步，通过 process_sentence 把句子收尾的空格去掉，然后通过 initialize 函数调用上面read_corpus_from_file 方法读取语料，分别加载训练集和测试集。 第6步，特征生成器，分别用来指定生成训练集或者测试集的特征集： def generator(self, train=True): &quot;&quot;&quot;特征生成器&quot;&quot;&quot; if train: sentences = self.train_sentences else: sentences = self.test_sentences return self.extract_feature(sentences) 这一步，对训练集和测试集分别处理，如果参数 train 为 True，则表示处理训练集，如果是 False，则表示处理测试集。第7步，特征提取，简单的进行 3-gram 的抽取，将词性与词语两两进行匹配，分别返回特征集合和标签集合： def extract_feature(self, sentences): &quot;&quot;&quot;提取特征&quot;&quot;&quot; features, tags = [], [] for index in range(len(sentences)): feature_list, tag_list = [], [] for i in range(len(sentences[index])): feature = {&quot;w0&quot;: sentences[index][i][0], &quot;p0&quot;: sentences[index][i][1], &quot;w-1&quot;: sentences[index][i-1][0] if i != 0 else &quot;BOS&quot;, &quot;w+1&quot;: sentences[index][i+1][0] if i != len(sentences[index])-1 else &quot;EOS&quot;, &quot;p-1&quot;: sentences[index][i-1][1] if i != 0 else &quot;un&quot;, &quot;p+1&quot;: sentences[index][i+1][1] if i != len(sentences[index])-1 else &quot;un&quot;} feature[&quot;w-1:w0&quot;] = feature[&quot;w-1&quot;]+feature[&quot;w0&quot;] feature[&quot;w0:w+1&quot;] = feature[&quot;w0&quot;]+feature[&quot;w+1&quot;] feature[&quot;p-1:p0&quot;] = feature[&quot;p-1&quot;]+feature[&quot;p0&quot;] feature[&quot;p0:p+1&quot;] = feature[&quot;p0&quot;]+feature[&quot;p+1&quot;] feature[&quot;p-1:w0&quot;] = feature[&quot;p-1&quot;]+feature[&quot;w0&quot;] feature[&quot;w0:p+1&quot;] = feature[&quot;w0&quot;]+feature[&quot;p+1&quot;] feature_list.append(feature) tag_list.append(sentences[index][i][-1]) features.append(feature_list) tags.append(tag_list) return features, tags 经过第6步，确定处理的是训练集还是测试集之后，通过 extract_feature 对句子进行特征抽取，使用 3-gram模型，得到特征集合和标签集合的对应关系。 模型训练及预测在完成特征工程和特征提取之后，接下来，我们要进行模型训练和预测，要预定义模型需要的一些参数，并初始化模型对象，进而完成模型训练和预测，以及模型的保存与加载。 首先，我们定义模型 ModelParser 类，进行初始化参数、模型初始化，以及模型训练、预测、保存和加载，类的结构定义如下： class ModelParser(object): def __init__(self): &quot;&quot;&quot;初始化参数&quot;&quot;&quot; pass def initialize_model(self): &quot;&quot;&quot;模型初始化&quot;&quot;&quot; pass def train(self): &quot;&quot;&quot;训练&quot;&quot;&quot; pass def predict(self, sentences): &quot;&quot;&quot;模型预测&quot;&quot;&quot; pass def load_model(self, name=&#39;model&#39;): &quot;&quot;&quot;加载模型 &quot;&quot;&quot; pass def save_model(self, name=&#39;model&#39;): &quot;&quot;&quot;保存模型&quot;&quot;&quot; pass 接下来，我们分析 ModelParser 类中方法的具体实现。第1步，init 方法实现算法模型参数和语料预处理 CorpusProcess 类的实例化和初始化： def __init__(self): &quot;&quot;&quot;初始化参数&quot;&quot;&quot; self.algorithm = &quot;lbfgs&quot; self.c1 = 0.1 self.c2 = 0.1 self.max_iterations = 100 self.model_path = &quot;model.pkl&quot; self.corpus = CorpusProcess() #初始化CorpusProcess类 self.corpus.initialize() #语料预处理 self.model = None 这一步，init 方法初始化参数以及 CRF 模型的参数，算法选用 LBFGS，c1 和 c2分别为0.1，最大迭代次数100次。然后定义模型保存的文件名称，以及完成对 CorpusProcess 类 的初始化。 第2-3步，initialize_model 方法和 train 实现模型定义和训练： def initialize_model(self): &quot;&quot;&quot;模型初始化&quot;&quot;&quot; algorithm = self.algorithm c1 = float(self.c1) c2 = float(self.c2) max_iterations = int(self.max_iterations) self.model = sklearn_crfsuite.CRF(algorithm=algorithm, c1=c1, c2=c2, max_iterations=max_iterations, all_possible_transitions=True) def train(self): &quot;&quot;&quot;训练&quot;&quot;&quot; self.initialize_model() x_train, y_train = self.corpus.generator() self.model.fit(x_train, y_train) labels = list(self.model.classes_) x_test, y_test = self.corpus.generator(train=False) y_predict = self.model.predict(x_test) metrics.flat_f1_score(y_test, y_predict, average=&#39;weighted&#39;, labels=labels) sorted_labels = sorted(labels, key=lambda name: (name[1:], name[0])) print(metrics.flat_classification_report(y_test, y_predict, labels=sorted_labels, digits=3)) self.save_model() 这一步，initialize_model 方法实现 了 sklearn_crfsuite.CRF 模型的初始化。然后在 train 方法中，先通过fit 方法训练模型，再通过 metrics.flat_f1_score 对测试集进行 F1 性能测试，最后将模型保存。 第4-6步，分别实现模型预测、保存和加载方法 最后，实例化类，并进行模型训练： model = ModelParser() model.train() 对模型进行预测，预测数据输入格式为三维，表示完整的一句话： [[[‘坚决’, ‘a’, ‘ad’, ‘1_v’], [&#39;惩治&#39;, &#39;v&#39;, &#39;v&#39;, &#39;0_Root&#39;], [&#39;贪污&#39;, &#39;v&#39;, &#39;v&#39;, &#39;1_v&#39;], [&#39;贿赂&#39;, &#39;n&#39;, &#39;n&#39;, &#39;-1_v&#39;], [&#39;等&#39;, &#39;u&#39;, &#39;udeng&#39;, &#39;-1_v&#39;], [&#39;经济&#39;, &#39;n&#39;, &#39;n&#39;, &#39;1_v&#39;], [&#39;犯罪&#39;, &#39;v&#39;, &#39;vn&#39;, &#39;-2_v&#39;]]] 模型预测的结果如下图所示： 预测的结果，和原始语料预处理得到的标签格式保持一致。 总结本文通过清华大学的句法标注语料库，实现了基于 CRF 的中文句法依存分析模型。借此实例，相信大家对句法依存已有了一个完整客观的认识。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>crf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型部署上线的几种服务发布方式（18）]]></title>
    <url>%2F2019%2F11%2F22%2F18.%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E4%B8%8A%E7%BA%BF%E7%9A%84%E5%87%A0%E7%A7%8D%E6%9C%8D%E5%8A%A1%E5%8F%91%E5%B8%83%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[在前面所有的模型训练和预测中，我们训练好的模型都是直接通过控制台或者 Jupyter Notebook来进行预测和交互的，在一个系统或者项目中使用这种方式显然不可能，那在 Web 应用中如何使用我们训练好的模型呢？ 本文将通过以下四个方面对该问题进行讲解： 微服务架构简介； 模型的持久化与加载方式； Flask 和 Bottle 微服务框架； Tensorflow Serving 模型部署和服务。 微服务架构简介微服务是指开发一个单个小型的但有业务功能的服务，每个服务都有自己的处理和轻量通讯机制，可以部署在单个或多个服务器上。微服务也指一种松耦合的、有一定的有界上下文的面向服务架构。也就是说，如果每个服务都要同时修改，那么它们就不是微服务，因为它们紧耦合在一起；如果你需要掌握一个服务太多的上下文场景使用条件，那么它就是一个有上下文边界的服务，这个定义来自DDD 领域驱动设计。 相对于单体架构和 SOA，它的主要特点是组件化、松耦合、自治、去中心化，体现在以下几个方面： 一组小的服务：服务粒度要小，而每个服务是针对一个单一职责的业务能力的封装，专注做好一件事情； 独立部署运行和扩展：每个服务能够独立被部署并运行在一个进程内。这种运行和部署方式能够赋予系统灵活的代码组织方式和发布节奏，使得快速交付和应对变化成为可能。 独立开发和演化：技术选型灵活，不受遗留系统技术约束。合适的业务问题选择合适的技术可以独立演化。服务与服务之间采取与语言无关的 API 进行集成。相对单体架构，微服务架构是更面向业务创新的一种架构模式。 独立团队和自治：团队对服务的整个生命周期负责，工作在独立的上下文中，自己决策自己治理，而不需要统一的指挥中心。团队和团队之间通过松散的社区部落进行衔接。 由此，我们可以看到整个微服务的思想，与我们现在面对信息爆炸、知识爆炸做事情的思路是相通的：通过解耦我们所做的事情，分而治之以减少不必要的损耗，使得整个复杂的系统和组织能够快速地应对变化。 我们为什么采用微服务呢？ “让我们的系统尽可能快地响应变化” ——Rebecca Parson 下面是一个简单的微服务模型架构设计： 模型的持久化与加载方式开发过 J2EE应用的人应该对持久化的概念很清楚。通俗得讲，就是临时数据（比如内存中的数据，是不能永久保存的）持久化为持久数据（比如持久化至数据库中，能够长久保存）。 那我们训练好的模型一般都是存储在内存中，这个时候就需要用到持久化方式，在 Python 中，常用的模型持久化方式有三种，并且都是以文件的方式持久化。 1.JSON（JavaScript Object Notation）格式。 JSON 是一种轻量级的数据交换格式，易于人们阅读和编写。使用 JSON 函数需要导入 JSON 库： import json 它拥有两个格式处理函数： json.dumps：将 Python 对象编码成 JSON 字符串； json.loads：将已编码的 JSON 字符串解码为 Python 对象。 下面看一个例子。 首先我们创建一个 List 对象 data，然后把 data 编码成 JSON 字符串保存在 data.json 文件中，之后再读取 data.json文件中的字符串解码成 Python 对象，代码如下： 2. pickle 模块 pickle 提供了一个简单的持久化功能。可以将对象以文件的形式存放在磁盘上。pickle 模块只能在 Python 中使用，Python中几乎所有的数据类型（列表、字典、集合、类等）都可以用 pickle 来序列化。pickle 序列化后的数据，可读性差，人一般无法识别。 使用的时候需要引入库： import pickle 它有以下两个方法： pickle.dump(obj, file[, protocol])：序列化对象，并将结果数据流写入到文件对象中。参数 protocol 是序列化模式，默认值为0，表示以文本的形式序列化。protocol 的值还可以是1或2，表示以二进制的形式序列化。 pickle.load(file)：反序列化对象。将文件中的数据解析为一个 Python 对象。 我们继续延用上面的例子。实现的不同点在于，这次文件打开时用了 with...as... 语法，使用 pickle 保存结果，文件保存为data.pkl，代码如下。3. sklearn 中的 joblib 模块。使用 joblib，首先需要引入包： from sklearn.externals import joblib 使用方法如下，基本和 JSON、pickle一样，这里不再详细讲解。第17课中，进行模型保存时使用的就是这种方式，可以看代码，回顾一下。 joblib.dump(model, model_path) #模型保存 joblib.load(model_path) #模型加载 Flask 和 Bottle 微服务框架通过上面，我们对微服务和 Python 中三种模型持久化和加载方式有了基本了解。下面我们看看，Python 中如何把模型发布成一个微服务的。 这里给出两个微服务框架 Bottle 和Flask。 Bottle 是一个非常小巧但高效的微型 Python Web 框架，它被设计为仅仅只有一个文件的 Python 模块，并且除 Python标准库外，它不依赖于任何第三方模块。 Bottle 本身主要包含以下四个模块，依靠它们便可快速开发微 Web 服务： 路由（Routing）：将请求映射到函数，可以创建十分优雅的 URL； 模板（Templates）：可以快速构建 Python 内置模板引擎，同时还支持 Mako、Jinja2、Cheetah 等第三方模板引擎； 工具集（Utilites）：用于快速读取 form 数据，上传文件，访问 Cookies，Headers 或者其它 HTTP 相关的 metadata； 服务器（Server）：内置 HTTP 开发服务器，并且支持 paste、fapws3、 bjoern、Google App Engine、Cherrypy 或者其它任何 WSGI HTTP 服务器。 Flask 也是一个 Python 编写的 Web 微框架，可以让我们使用 Python 语言快速实现一个网站或 Web 服务。并使用方式和 Bottle相似，Flask 依赖 Jinja2 模板和 Werkzeug WSGI 服务。Werkzeug 本质是 Socket 服务端，其用于接收 HTTP请求并对请求进行预处理，然后触发 Flask 框架，开发人员基于 Flask框架提供的功能对请求进行相应的处理，并返回给用户，如果返回给用户的内容比较复杂时，需要借助 Jinja2模板来实现对模板的处理，即将模板和数据进行渲染，将渲染后的字符串返回给用户浏览器。 Bottle 和 Flask 在使用上相似，而且 Flask 的文档资料更全，发布的服务更稳定，因此下面重点以 Flask为例，来说明模型的微服务发布过程。 如果大家想进一步了解这两个框架，可以参考说明文档。 1.安装。 对 Bottle 和 Flask 进行安装，分别执行如下命令即可安装成功： pip install bottle pip install Flask 安装好之后，分别进入需要的包就可以写微服务程序了。这两个框架在使用时，用法、语法结构都差不多，网上 Flask 的中文资料相对多一些，所以这里用 Flask来举例。 2. 第一个最小的 Flask 应用。 第一个最小的 Flask 应用看起来会是这样: from flask import Flask app = Flask(__name__) @app.route(&#39;/&#39;) def hello_world(): return &#39;Hello World!&#39; if __name__ == &#39;__main__&#39;: app.run() 把它保存为 hello.py（或是类似的），然后用 Python 解释器来运行： python hello.py 或者直接在 Jupyter Notebook 里面执行，都没有问题。服务启动将在控制台打印如下消息： Running on http://127.0.0.1:5000/ 意思就是，可以通过 localhost 和 5000 端口，在浏览器访问： 这时我们就得到了服务在浏览器上的返回结果，于是也成功构建了与浏览器交互的服务。 如果要修改服务对应的 IP 地址和端口怎么办？只需要修改这行代码，即可修改 IP 地址和端口： app.run(host=&#39;192.168.31.19&#39;,port=8088) 3. Flask 发布一个预测模型。 首先，我们这里使用第17课保存的模型“model.pkl”。如果不使用浏览器，常规的控制台交互，我们这样就可以实现： from sklearn.externals import joblib model_path = &quot;D://达人课//中文自然语言处理入门实战课程//ch18//model.pkl&quot; model = joblib.load(model_path) sen =[[[&#39;坚决&#39;, &#39;a&#39;, &#39;ad&#39;, &#39;1_v&#39;], [&#39;惩治&#39;, &#39;v&#39;, &#39;v&#39;, &#39;0_Root&#39;], [&#39;贪污&#39;, &#39;v&#39;, &#39;v&#39;, &#39;1_v&#39;], [&#39;贿赂&#39;, &#39;n&#39;, &#39;n&#39;, &#39;-1_v&#39;], [&#39;等&#39;, &#39;u&#39;, &#39;udeng&#39;, &#39;-1_v&#39;], [&#39;经济&#39;, &#39;n&#39;, &#39;n&#39;, &#39;1_v&#39;], [&#39;犯罪&#39;, &#39;v&#39;, &#39;vn&#39;, &#39;-2_v&#39;]]] print(model.predict(sen)) 如果你现在有个需求，要求你的模型和浏览器进行交互，那 Flask 就可以实现。在第一个最小的 Flask 应用基础上，我们增加模型预测接口，这里注意： 启动之前把 IP 地址修改为自己本机的地址或者服务器工作站所在的 IP地址。 完整的代码如下，首先在启动之前先把模型预加载到内存中，然后重新定义 predict 函数，接受一个参数 sen： from sklearn.externals import joblib from flask import Flask,request app = Flask(__name__) @app.route(&#39;/&#39;) def hello_world(): return &#39;Hello World!&#39; @app.route(&#39;/predict/&lt;sen&gt;&#39;) def predict(sen): result = model.predict(sen) return str(result) if __name__ == &#39;__main__&#39;: model_path = &quot;D://ch18//model.pkl&quot; model = joblib.load(model_path) app.run(host=&#39;192.168.31.19&#39;) 启动 Flask 服务之后，在浏览器地址中输入： http://192.168.31.19:5000/predict/[[[&#39;坚决&#39;, ‘a’, ‘ad’, ‘1 v’], [‘惩治’, ‘v’,’v’, ‘0 Root’], [‘贪污’, ‘v’, ‘v’, ‘1 v’], [‘贿赂’, ‘n’, ‘n’, ‘-1 v’], [‘等’,’u’, ‘udeng’, ‘-1 v’], [‘经济’, ‘n’, ‘n’, ‘1 v’], [‘犯罪’, ‘v’, ‘vn’, ‘-2_v’]]] 得到预测结果，这样就完成了微服务的发布，并实现了模型和前端浏览器的交互。 Tensorflow Serving 模型部署和服务TensorFlow Serving 是一个用于机器学习模型 Serving 的高性能开源库。它可以将训练好的机器学习模型部署到线上，使用 gRPC作为接口接受外部调用。更加让人眼前一亮的是，它支持模型热更新与自动模型版本管理。这意味着一旦部署 TensorFlow Serving后，你再也不需要为线上服务操心，只需要关心你的线下模型训练。 同样，TensorFlow Serving 可以将模型部署在移动端，如安卓或者 iOS 系统的 App 应用上。关于 Tensorflow Serving模型部署和服务，这里不在列举示例，直接参考文末的推荐阅读。 总结本节对微服务架构做了简单介绍，并介绍了三种机器学习模型持久化和加载的方式，接着介绍了 Python 的两个轻量级微服务框架 Bottle 和Flask。随后，我们通过 Flask 制作了一个简单的微服务预测接口，实现模型的预测和浏览器交互功能，最后简单介绍了 TensorFlow Servin模型的部署和服务功能。 学完上述内容，读者可轻易实现自己训练的模型和 Web 应用的结合，提供微服务接口，实现模型上线应用。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>flask</tag>
        <tag>bottle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[知识挖掘与知识图谱概述（19）]]></title>
    <url>%2F2019%2F11%2F22%2F19.%E7%9F%A5%E8%AF%86%E6%8C%96%E6%8E%98%E4%B8%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[搜索技术日新月异，如今它不再是搜索框中输入几个单词那么简单了。不仅输入方式多样化，并且还要在非常短的时间内给出一个精准而又全面的答案。目前，谷歌给出的解决方案就是——知识图谱（KnowledgeGraph）。 知识图谱能做什么？知识图谱想做的，就是在不同数据（来自现实世界）之间建立联系，从而带给我们更有意义的搜索结果。 比如，在上图中，用 Google搜索自然语言处理，右侧会显示研究领域和相关概念。点击这些知识点，又可以深入了解；再比如，搜索一个人名时，右侧会给出此人的生平、背景、居住位置、作品等信息。 这就是知识图谱，它不再是单一的信息，而是一个多元的信息网络。 知识图谱的源头知识图谱的雏形好几年前就已出现，一家名为 Metaweb的小公司，将现实世界中实体（人或事）的各种数据信息存储在系统中，并在数据之间建立起联系，从而发展出有别于传统关键词搜索的技术。 谷歌认为这一系统很有发展潜力，于2010年收购了 Metaweb。那时 Metawab 已经存储了1200万个节点（ReferencePoint，相当于一个词条或者一个页面），谷歌收购后的两年中，大大加速这一进程，现已有超过5.7亿个节点并在它们之间建了180亿个有效连接（这可是一个相当大的数字，维基百科英文版也才有大约400万个节点）。 知识图谱的通用表示方法本质上，知识图谱是一种揭示实体之间关系的语义网络 ，可以对现实世界的事物及其相互关系进行形式化地描述 。现在的知识图谱己被用来泛指各种大规模的知识库 。 三元组是知识图谱的一种通用表示方式，即 ${G=(E，R，S)}$，其中 $E={e_1，e_2，…，e_{|E|}}$ 是知识库中的实体集合，共包含$|E|$ 种不同实体，$R={r_1，r_2，…,r_{|E|}}$ 是知识库中的关系集合，共包含 $|R|$ 种不同关系，$S \subseteqE×R×E$ 代表知识库中的三元组集合。 三元组的基本形式主要包括实体 A、关系、实体 B和概念、属性、属性值等，实体是知识图谱中的最基本元素，不同的实体间存在不同的关系。概念主要指集合、类别、对象类型、事物的种类，例如人物、地理等；属性主要指对象可能具有的属性、特征、特性、特点以及参数，例如国籍、生日等；属性值主要指对象指定属性的值，例如中国、1988—09—08等。每个实体（概念的外延）可用一个全局唯一确定的ID 来标识，每个属性—属性值对可用来刻画实体的内在特性，而关系可用来连接两个实体，刻画它们之间的关联。 如下图是实体 A 与实体 B 组成的一个简单三元组形式。 知识图谱的架构知识图谱的架构主要包括自身的逻辑结构以及体系架构，分别说明如下。 1. 知识图谱的逻辑结构。 知识图谱在逻辑上可分为模式层与数据层两个层次，数据层主要是由一系列的事实组成，而知识将以事实为单位进行存储。如果用（实体 A，关系，实体B）、（实体、属性，属性值）这样的三元组来表达事实，可选择图数据库作为存储介质，例如开源的 Neo4j、Twitter 的 FlockDB、Sones 的GraphDB等。模式层构建在数据层之上，主要是通过本体库来规范数据层的一系列事实表达。本体是结构化知识库的概念模板，通过本体库而形成的知识库不仅层次结构较强，并且冗余程度较小。 2. 知识图谱的体系架构。 知识图谱的体系架构是指其构建模式结构，如图下图所示。 知识图谱主要有自顶向下与自底向上两种构建方式。自顶向下指的是先为知识图谱定义好本体与数据模式，再将实体加入到知识库。该构建方式需要利用一些现有的结构化知识库作为其基础知识库，例如Freebase项目就是采用这种方式，它的绝大部分数据是从维基百科中得到的。自底向上指的是从一些开放链接数据中提取出实体，选择其中置信度较高的加入到知识库，再构建顶层的本体模式。目前，大多数知识图谱都采用自底向上的方式进行构建，其中最典型就是Google 的 Knowledge Vault。 知识图谱的关键技术大规模知识库的构建与应用需要多种智能信息处理技术的支持。这就涉及到当下异常火爆的人工智能中的自然语言处理（NLP）技术。 所谓自然语言，就是我们平时所说的话（包括语音或文字），但这些话计算机如何能“理解”？过程很复杂，下面是其中的几个关键步骤。 1. 知识抽取。 知识抽取技术，可以从一些公开的半结构化、非结构化的数据中提取出实体、关系、属性等知识要素。 知识抽取主要包含实体抽取、关系抽取、属性抽取等，涉及到的 NLP 技术有命名实体识别、句法依存、实体关系识别等。 2. 知识表示。 知识表示形成的综合向量对知识库的构建、推理、融合以及应用均具有重要的意义。 基于三元组的知识表示形式受到了人们广泛的认可，但是其在计算效率、数据稀疏性等方面却面临着诸多问题。近年来，以深度学习为代表的表示学习技术取得了重要的进展，可以将实体的语义信息表示为稠密低维实值向量，进而在低维空间中高效计算实体、关系及其之间的复杂语义关联。 知识表示学习主要包含的 NLP 技术有语义相似度计算、复杂关系模型，知识代表模型如距离模型、双线性模型、神经张量模型、矩阵分解模型、翻译模型等。 3.知识融合。 由于知识图谱中的知识来源广泛，存在知识质量良莠不齐、来自不同数据源的知识重复、知识间的关联不够明确等问题，所以必须要进行知识的融合。知识融合是高层次的知识组织，使来自不同知识源的知识在同一框架规范下进行异构数据整合、消歧、加工、推理验证、更新等步骤，达到数据、信息、方法、经验以及人的思想的融合，形成高质量的知识库。 在知识融合过程中，实体对齐、知识加工是两个重要的过程。 4.知识推理。 知识推理则是在已有的知识库基础上进一步挖掘隐含的知识，从而丰富、扩展知识库。在推理的过程中，往往需要关联规则的支持。由于实体、实体属性以及关系的多样性，人们很难穷举所有的推理规则，一些较为复杂的推理规则往往是手动总结的。对于推理规则的挖掘，主要还是依赖于实体以及关系间的丰富情况。知识推理的对象可以是实体、实体的属性、实体间的关系、本体库中概念的层次结构等。 知识推理方法主要可分为基于逻辑的推理与基于图的推理两种类别。 大规模开放知识库互联网的发展为知识工程提供了新的机遇。从一定程度上看，是互联网的出现帮助突破了传统知识工程在知识获取方面的瓶颈。从1998年 Tim Berners Lee提出语义网至今，涌现出大量以互联网资源为基础的新一代知识库。这类知识库的构建方法可以分为三类：互联网众包、专家协作和互联网挖掘，如下图所示： 下面介绍几个知名的中文知识图谱资源： OpenKG.CN：中文开放知识图谱联盟旨在通过建设开放的社区来促进中文知识图谱数据的开放与互联，促进中文知识图谱工具的标准化和技术普及。 Zhishi.me ：Zhishi.me 是中文常识知识图谱。主要通过从开放的百科数据中抽取结构化数据，已融合了百度百科，互动百科以及维基百科中的中文数据。 CN-DBPeidia：CN-DBpedia 是由复旦大学知识工场实验室研发并维护的大规模通用领域结构化百科。 cnSchema.org: cnSchema.org 是一个基于社区维护的开放的知识图谱 Schema 标准。cnSchema 的词汇集包括了上千种概念分类、数据类型、属性和关系等常用概念定义，以支持知识图谱数据的通用性、复用性和流动性。 知识图谱的典型应用知识图谱为互联网上海量、异构、动态的大数据表达、组织、管理以及利用提供了一种更为有效的方式，使得网络的智能化水平更高，更加接近于人类的认知思维。 基于大规模开放知识库或知识图谱的应用，目前尚处在持续不断的发展与探索的阶段。下面列出了一些国内外比较出色的应用。 1. 语义检索。 谷歌公司通过建立 Google KnowledgeGraph，实现了对知识的体系化组织与展示，试图从用户搜索意图感知、以及查询扩展的角度，直接提供给用户想要的知识。 2. 智能问答。 IBM 公司通过搭建知识图谱，并通过自然语言处理和机器学习等技术，开发出了 Watson系统。在2011年2月的美国问答节目《Jeopardy!》上，Watson 战胜了这一节目的两位冠军选手，可与1996年同样来自 IBM的“深蓝”战胜国际象棋大师卡斯帕罗夫产生的影响相提并论，被认为是人工智能历史上的一个里程碑。 3. 领域专家快速生成。 构建面向特定领域、特定主题的大规模知识库是实现对某一领域深度分析和计算的重要基础，OpenKN通过实现端到端的开放知识库构建工具集，实现了在给定部分种子（Seed）的情况下，从无到有的生成领域知识库，进而形成领域专家。 4. 行业生态深度分析与预测。 利用开放大数据可以帮助企业发现潜伏在数据中的威胁，将结构化网络日志、文本数据、开源和第三方数据整合进一个单一的环境，屏蔽可疑的信号与噪声，有效保护用户网络，可在信用卡欺诈行为识别、医疗行业疾病预测、电商商品推荐、强化组织数据安全、不一致性验证、异常分析、金融量化交易、法律分析服务等多方面提供有价值的服务。 知识图谱的前景与挑战在关注到知识图谱在自然语言处理、人工智能等领域展现巨大潜力的同时，也不难发现知识图谱中的知识获取、知识表示、知识推理等技术依然面临着一些困难与挑战，在未来的一段时间内，知识图谱将是大数据智能的前沿研究问题，有很多重要的开放性问题亟待学术界和产业界协力解决。我们认为，未来知识图谱研究有以下几个重要挑战： 知识类型与表示。知识图谱主要采用（实体1、关系、实体2）三元组的形式来表示知识，这种方法可以较好地表示很多事实性知识。然而，人类知识类型多样，面对很多复杂知识，三元组就束手无策了。例如，人们的购物记录信息、新闻事件等，包含大量实体及其之间的复杂关系，更不用说人类大量的涉及主观感受、主观情感和模糊的知识了。 知识获取。如何从互联网大数据萃取知识，是构建知识图谱的重要问题。目前已经提出各种知识获取方案，并已成功抽取大量有用的知识。但在抽取知识的准确率、覆盖率和效率等方面，都仍不如人意，有极大的提升空间。 知识融合。来自不同数据的抽取知识可能存在大量噪音和冗余，或者使用了不同的语言。如何将这些知识有机融合起来，建立更大规模的知识图谱，是实现大数据智能的必由之路。 知识应用。目前大规模知识图谱的应用场景和方式还比较有限，如何有效实现知识图谱的应用，利用知识图谱实现深度知识推理，提高大规模知识图谱计算效率，需要人们不断锐意发掘用户需求，探索更重要的应用场景，提出新的应用算法。 总结本文对知识图谱的起源、定义、架构、大规模知识库、应用以及未来挑战等内容，进行了全面阐述。 知识抽取、知识表示、知识融合以及知识推理为构建知识图谱的四大核心技术，本文就当前产业界的需求介绍了它在智能搜索、深度问答、社交网络以及一些垂直行业中的实际应用。此外，还总结了目前知识图谱面临的主要挑战，并对其未来的研究方向进行了展望。 知识图谱的重要性不仅在于它是一个拥有强大语义处理能力与开放互联能力的知识库，并且还是一把开启智能机器大脑的钥匙，能够打开 Web3.0时代的知识宝库，为相关学科领域开启新的发展方向。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中文自然语言处理的应用、现状和未来（21）]]></title>
    <url>%2F2019%2F11%2F22%2F21.%E4%B8%AD%E6%96%87%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%9A%84%E5%BA%94%E7%94%A8%E3%80%81%E7%8E%B0%E7%8A%B6%E5%92%8C%E6%9C%AA%E6%9D%A5%2F</url>
    <content type="text"><![CDATA[自然语言理解和自然语言生成是自然语言处理的两大内核，机器翻译是自然语言理解方面最早的研究工作。自然语言处理的主要任务是：研究表示语言能力和语言应用的模型，建立和实现计算框架并提出相应的方法不断地完善模型，根据这样的语言模型设计有效地实现自然语言通信的计算机系统，并研讨关于系统的评测技术，最终实现用自然语言与计算机进行通信。目前，具有一定自然语言处理能力的典型应用包括计算机信息检索系统、多语种翻译系统等。 微软创始人比尔·盖茨曾经表示，“语言理解是人工智能领域皇冠上的明珠”。 语言是逻辑思维和交流的工具，宇宙万物中，只有人类才具有这种高级功能。要实现人与计算机间采用自然语言通信，必须使计算机同时具备自然语言理解和自然语言生成两大功能。 因此，NLP作为人工智能的一个子领域，其主要目的就包括两个方面：自然语言理解，让计算机理解自然语言文本的意义；自然语言生成，让计算机能以自然语言文本来表达给定的意图、思想等。自然语言是人类智慧的结晶，自然语言处理是人工智能中最为困难的问题之一，而对自然语言处理的研究也是充满魅力和挑战的。 NLP 领域发展现状如何？近年来，自然语言处理处于快速发展阶段。各种词表、语义语法词典、语料库等数据资源的日益丰富，词语切分、词性标注、句法分析等技术的快速进步，各种新理论、新方法、新模型的出现推动了自然语言处理研究的繁荣。互联网与移动互联网和世界经济社会一体化的潮流对自然语言处理技术的迫切需求，为自然语言处理研究发展提供了强大的市场动力。 我国直到上世纪80年代中期才开始较大规模和较系统的自然语言处理研究，尽管较国际水平尚有较大差距，但已经有了比较稳定的研究内容，包括语料库、知识库等数据资源建设，词语切分、句法分析等基础技术，以及信息检索、机器翻译等应用技术。 当前国内外出现了一批基于 NLP 技术的应用系统，例如 IBM 的 Watson 在电视问答节目中战胜人类冠军；苹果公司的 Siri个人助理被大众广为测试；谷歌、微软、百度等公司纷纷发布个人智能助理；科大讯飞牵头研发高考机器人……但相比于性能趋于饱和的计算机视觉和语音识别技术，自然语言处理因技术难度太大、应用场景太复杂，研究成果还未达到足够的高度。 自然语言处理中句子级分析技术目前，自然语言处理的对象有词、句子、篇章和段落、文本等，但是大多归根到底在句子的处理上，自然语言处理中的自然语言句子级分析技术，可以大致分为词法分析、句法分析、语义分析三个层面。 第一层面的词法分析包括汉语分词和词性标注两部分。和大部分西方语言不同，汉语书面语词语之间没有明显的空格标记，文本中的句子以字串的形式出现。因此汉语自然语言处理的首要工作就是要将输入的字串切分为单独的词语，然后在此基础上进行其他更高级的分析，这一步骤称为分词。 除了分词，词性标注也通常认为是词法分析的一部分。给定一个切好词的句子，词性标注的目的是为每一个词赋予一个类别，这个类别称为词性标记，比如，名词（Noun）、动词（Verb）、形容词（Adjective）等。一般来说，属于相同词性的词，在句法中承担类似的角色。 第二个层面的句法分析是对输入的文本句子进行分析以得到句子的句法结构的处理过程。对句法结构进行分析，一方面是语言理解的自身需求，句法分析是语言理解的重要一环，另一方面也为其它自然语言处理任务提供支持。例如句法驱动的统计机器翻译需要对源语言或目标语言（或者同时两种语言）进行句法分析；语义分析通常以句法分析的输出结果作为输入以便获得更多的指示信息。 根据句法结构表示形式的不同，最常见的句法分析任务可以分为以下三种： 短语结构句法分析，该任务也被称作成分句法分析，作用是识别出句子中的短语结构以及短语之间的层次句法关系； 依存句法分析，作用是识别句子中词汇与词汇之间的相互依存关系； 深层文法句法分析，即利用深层文法，例如词汇化树邻接文法、词汇功能文法、组合范畴文法等，对句子进行深层的句法以及语义分析。 上述几种句法分析任务比较而言，依存句法分析属于浅层句法分析。其实现过程相对简单，比较适合在多语言环境下的应用，但是依存句法分析所能提供的信息也相对较少。深层文法句法分析可以提供丰富的句法和语义信息，但是采用的文法相对复杂，分析器的运行复杂度也较高，这使得深层句法分析当前不适合处理大规模数据。短语结构句法分析介于依存句法分析和深层文法句法分析之间。 第三个层面是语义分析。语义分析的最终目的是理解句子表达的真实语义。但是，语义应该采用什么表示形式一直困扰着研究者们，至今这个问题也没有一个统一的答案。 语义角色标注是目前比较成熟的浅层语义分析技术。基于逻辑表达的语义分析也得到学术界的长期关注。出于机器学习模型复杂度、效率的考虑，自然语言处理系统通常采用级联的方式，即分词、词性标注、句法分析、语义分析分别训练模型。实际使用时，给定输入句子，逐一使用各个模块进行分析，最终得到所有结果。 深度学习背景下的自然语言处理近年来，随着研究工作的深入，研究者们开始从传统机器学习转向深度学习。2006年开始，有人利用深层神经网络在大规模无标注语料上无监督的为每个词学到了一个分布式表示，形式上把每个单词表示成一个固定维数的向量，当作词的底层特征。在此特征基础上，完成了词性标注、命名实体识别和语义角色标注等多个任务，后来有人利用递归神经网络完成了句法分析、情感分析和句子表示等多个任务，这也为语言表示提供了新的思路。 面向自然语言处理的深度学习研究工作，目前尚处于起步阶段，尽管已有的深度学习算法模型如循环神经网络、递归神经网络和卷积神经网络等已经有较为显著的应用，但还没有重大突破。围绕适合自然语言处理领域的深度学习模型构建等研究应该有着非常广阔的空间。 在当前已有的深度学习模型研究中，难点是在模型构建过程中参数的优化调整方面。主要有深度网络层数、正则化问题及网络学习速率等，可能的解决方案比如有采用多核机提升网络训练速度，针对不同应用场合，选择合适的优化算法等。 自然语言处理未来的研究方向纵观自然语言处理技术研究发展的态势和现状，以下研究方向或问题将可能成为自然语言处理未来研究必须攻克的堡垒： 词法和句法分析方面：包括多粒度分词、新词发现、词性标注等； 语义分析方面：包括词义消歧、非规范文本的语义分析。其中，非规范划化文本主要指社交平台上比较口语化、弱规范甚至不规范的短文本，因其数据量巨大和实时性而具有研究和应用价值，被广泛用于舆情监控、情感分析和突发事件发现等任务； 语言认知模型方面：比如使用深度神经网络处理自然语言，建立更有效、可解释的语言计算模型，例如，词嵌入的发现。还有目前词的表示是通过大量的语料库学习得到的，如何通过基于少量样本来发现新词、低频词也急需探索； 知识图谱方面：如何构建能够融合符号逻辑和表示学习的大规模高精度的知识图谱； 文本分类与聚类方面：通过有监督、半监督和无监督学习，能够准确进行分类和聚类。当下大多数语料都是没有标签的，未来在无监督或者半监督方面更有需求； 信息抽取方面：对于多源异构信息，如何准确进行关系、事件的抽取等。信息抽取主要从面向开放域的可扩展信息抽取技术、自学习与自适应和自演化的信息抽取系统以及面向多源异构数据的信息融合技术方向发展； 情感分析方面：包括基于上下文感知的情感分析、跨领域跨语言情感分析、基于深度学习的端到端情感分析、情感解释、反讽分析、立场分析等； 自动文摘方面：如何表达要点信息？如何评估信息单元的重要性？这些都要随着语义分析、篇章理解、深度学习等技术快速发展； 信息检索方面：包括意图搜索、语义搜索等，都将有可能出现在各种场景的垂直领域，将以知识化推理为检索运行方式，以自然语言多媒体交互为手段的智能化搜索与推荐技术； 自动问答方面：包括深度推理问答、多轮问答等各种形式的自动问答系统； 机器翻译方面：包括面向小数据的机器翻译、非规范文本的机器翻译和篇章级机器翻译等。 总结本文，从 NLP 的概念出发，首先指出了自然语言处理的两大内核：自然语言理解和自然语言生成；然后简单介绍了国内外 NLP研究发展现状；紧接着重点介绍了最常用、应用最广的自然语言处理中句子级分析技术，最后在深度学习背景下，指出了自然语言处理未来可能遇到的挑战和重点研究方向，为后期的学习提供指导和帮助。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关键字提取（3）]]></title>
    <url>%2F2019%2F11%2F22%2F3.%E5%85%B3%E9%94%AE%E5%AD%97%E6%8F%90%E5%8F%96%2F</url>
    <content type="text"><![CDATA[关键词提取就是从文本里面把跟这篇文章意义最相关的一些词语抽取出来。关键词在文献检索、自动文摘、文本聚类/分类等方面有着重要的应用，它不仅是进行这些工作不可或缺的基础和前提，也是互联网上信息建库的一项重要工作。 简介关键词抽取从方法来说主要有两种： 第一种是关键词分配：就是给定一个已有的关键词库，对于新来的文档从该词库里面匹配几个词语作为这篇文档的关键词。 第二种是关键词提取：针对新文档，通过算法分析，提取文档中一些词语作为该文档的关键词。 目前大多数应用领域的关键词抽取算法都是基于后者实现的，从逻辑上说，后者比前者在实际应用中更准确。 下面介绍一些关于关键词抽取的常用和经典的算法实现。 基于 TF-IDF 算法进行关键词提取在信息检索理论中，TF-IDF 是 Term Frequency - Inverse Document Frequency 的简写。TF-IDF 是一种数值统计，用于反映一个词对于语料中某篇文档的重要性。在信息检索和文本挖掘领域，它经常用于因子加权。TF-IDF 的主要思想就是：如果某个词在一篇文档中出现的频率高，也即 TF 高；并且在语料库中其他文档中很少出现，即 DF 低，也即 IDF 高，则认为这个词具有很好的类别区分能力。 TF 为词频（Term Frequency），表示词 t 在文档 d 中出现的频率，计算公式：IDF 为逆文档频率（Inverse Document Frequency），表示语料库中包含词 t 的文档的数目的倒数，计算公式：TF-IDF 在实际中主要是将二者相乘，也即 TF * IDF， 计算公式：因此，TF-IDF 倾向于过滤掉常见的词语，保留重要的词语。例如，某一特定文件内的高频率词语，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的 TF-IDF。好在 jieba 已经实现了基于 TF-IDF 算法的关键词抽取，通过命令 import jieba.analyse 引入，函数参数解释如下： jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=()) sentence：待提取的文本语料； topK：返回 TF/IDF 权重最大的关键词个数，默认值为 20； withWeight：是否需要返回关键词权重值，默认值为 False； allowPOS：仅包括指定词性的词，默认值为空，即不筛选 import jieba.analyse sentence = &quot;人工智能（Artificial Intelligence），英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，可以设想，未来人工智能带来的科技产品，将会是人类智慧的“容器”。人工智能可以对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。人工智能是一门极富挑战性的科学，从事这项工作的人必须懂得计算机知识，心理学和哲学。人工智能是包括十分广泛的科学，它由不同的领域组成，如机器学习，计算机视觉等等，总的说来，人工智能研究的一个主要目标是使机器能够胜任一些通常需要人类智能才能完成的复杂工作。但不同的时代、不同的人对这种“复杂工作”的理解是不同的。2017年12月，人工智能入选“2017年度中国媒体十大流行语”。&quot; keywords = &quot; &quot;.join(jieba.analyse.extract_tags(sentence , topK=20, withWeight=False, allowPOS=())) print(keywords) 输出：人工智能 智能 2017 机器 不同 人类 科学 模拟 一门 技术 计算机 研究 工作 Artificial Intelligence AI 图像识别 12 复杂 流行语 keywords =(jieba.analyse.extract_tags(sentence , topK=10, withWeight=True, allowPOS=([&#39;n&#39;,&#39;v&#39;]))) print(keywords) 输出：[(‘人工智能’, 0.9750542675762887), (‘智能’, 0.5167124540885567), (‘机器’, 0.20540911929525774), (‘人类’, 0.17414426566082475), (‘科学’, 0.17250169374402063), (‘模拟’, 0.15723537382948452), (‘技术’, 0.14596259315164947), (‘计算机’, 0.14030483362639176), (‘图像识别’, 0.12324502580309278), (‘流行语’, 0.11242211730309279)] 基于 TextRank 算法进行关键词提取TextRank 是由 PageRank 改进而来，核心思想将文本中的词看作图中的节点，通过边相互连接，不同的节点会有不同的权重，权重高的节点可以作为关键词。这里给出 TextRank 的公式：节点 i 的权重取决于节点 i 的邻居节点中 i-j 这条边的权重 / j 的所有出度的边的权重 * 节点 j 的权重，将这些邻居节点计算的权重相加，再乘上一定的阻尼系数，就是节点 i 的权重，阻尼系数 d 一般取 0.85。 TextRank 用于关键词提取的算法如下： （1）把给定的文本 T 按照完整句子进行分割，即:（2）对于每个句子，进行分词和词性标注处理，并过滤掉停用词，只保留指定词性的单词，如名词、动词、形容词，其中 ti,j 是保留后的候选关键词。（3）构建候选关键词图 G = (V,E)，其中 V 为节点集，由（2）生成的候选关键词组成，然后采用共现关系（Co-Occurrence）构造任两点之间的边，两个节点之间存在边仅当它们对应的词汇在长度为 K 的窗口中共现，K 表示窗口大小，即最多共现 K 个单词。 （4）根据 TextRank 的公式，迭代传播各节点的权重，直至收敛。 （5）对节点权重进行倒序排序，从而得到最重要的 T 个单词，作为候选关键词。 （6）由（5）得到最重要的 T 个单词，在原始文本中进行标记，若形成相邻词组，则组合成多词关键词。 同样 jieba 已经实现了基于 TextRank 算法的关键词抽取，通过命令 import jieba.analyse 引用，函数参数解释如下： jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=(&#39;ns&#39;, &#39;n&#39;, &#39;vn&#39;, &#39;v&#39;)) 直接使用，接口参数同 TF-IDF 相同，注意默认过滤词性。接下来，我们继续看例子，语料继续使用上例中的句子。 result = &quot; &quot;.join(jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=(&#39;ns&#39;, &#39;n&#39;, &#39;vn&#39;, &#39;v&#39;))) print(result) 输出：智能 人工智能 机器 人类 研究 技术 模拟 包括 科学 工作 领域 理论 计算机 年度 需要 语言 相似 方式 做出 心理学 修改词性 result = &quot; &quot;.join(jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=(&#39;n&#39;,&#39;v&#39;))) print(result) 输出： 智能 人工智能 机器 人类 技术 模拟 包括 科学 理论 计算机 领域 年度 需要 心理学 信息 语言 识别 带来 过程 延伸 基于 LDA 主题模型进行关键词提取通过 Gensim 库完成基于 LDA 的关键字提取。整个过程的步骤为：文件加载 -&gt; jieba 分词 -&gt; 去停用词 -&gt; 构建词袋模型 -&gt; LDA 模型训练 -&gt; 结果可视化。 数据处理训练#引入库文件 import jieba.analyse as analyse import jieba import pandas as pd from gensim import corpora, models, similarities import gensim import numpy as np import matplotlib.pyplot as plt %matplotlib inline #设置文件路径 dir = &quot;./&quot; file_desc = &quot;&quot;.join([dir,&#39;car.csv&#39;]) stop_words = &quot;&quot;.join([dir,&#39;stopwords.txt&#39;]) #定义停用词 stopwords=pd.read_csv(stop_words,index_col=False,quoting=3,sep=&quot;\t&quot;,names=[&#39;stopword&#39;], encoding=&#39;utf-8&#39;) stopwords=stopwords[&#39;stopword&#39;].values #加载语料 df = pd.read_csv(file_desc, encoding=&#39;gbk&#39;) #删除nan行 df.dropna(inplace=True) lines=df.content.values.tolist() #开始分词 sentences=[] for line in lines: try: segs=jieba.lcut(line) segs = [v for v in segs if not str(v).isdigit()]#去数字 segs = list(filter(lambda x:x.strip(), segs)) #去左右空格 segs = list(filter(lambda x:x not in stopwords, segs)) #去掉停用词 sentences.append(segs) except Exception: print(line) continue #构建词袋模型 dictionary = corpora.Dictionary(sentences) corpus = [dictionary.doc2bow(sentence) for sentence in sentences] #lda模型，num_topics是主题的个数，这里定义了5个 lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10) #我们查一下第1号分类，其中最常出现的5个词是： print(lda.print_topic(1, topn=5)) #我们打印所有5个主题，每个主题显示8个词 for topic in lda.print_topics(num_topics=10, num_words=8): print(topic[1]) 结果可视化#显示中文matplotlib plt.rcParams[&#39;font.sans-serif&#39;] = [u&#39;SimHei&#39;] plt.rcParams[&#39;axes.unicode_minus&#39;] = False # 在可视化部分，我们首先画出了九个主题的7个词的概率分布图 num_show_term = 8 # 每个主题下显示几个词 num_topics = 10 for i, k in enumerate(range(num_topics)): ax = plt.subplot(2, 5, i+1) item_dis_all = lda.get_topic_terms(topicid=k) item_dis = np.array(item_dis_all[:num_show_term]) ax.plot(range(num_show_term), item_dis[:, 1], &#39;b*&#39;) item_word_id = item_dis[:, 0].astype(np.int) word = [dictionary.id2token[i] for i in item_word_id] ax.set_ylabel(u&quot;概率&quot;) for j in range(num_show_term): ax.text(j, item_dis[j, 1], word[j], bbox=dict(facecolor=&#39;green&#39;,alpha=0.1)) plt.suptitle(u&#39;9个主题及其7个主要词的概率&#39;, fontsize=18) plt.show() 基于 pyhanlp 进行关键词提取 HanLP 来完成关键字提取，内部采用 TextRankKeyword 实现 from pyhanlp import * result = HanLP.extractKeyword(sentence, 20) print(result) 输出：[人工智能, 智能, 领域, 人类, 研究, 不同, 工作, 包括, 模拟, 新的, 机器, 计算机, 门, 科学, 应用, 系统, 理论, 技术, 入选, 复杂]]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>tf-idf</tag>
        <tag>pyhanlp</tag>
        <tag>textrank</tag>
        <tag>lda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[词袋与词向量（5）]]></title>
    <url>%2F2019%2F11%2F22%2F5.%E8%AF%8D%E8%A2%8B%E4%B8%8E%E8%AF%8D%E5%90%91%E9%87%8F%2F</url>
    <content type="text"><![CDATA[词袋和词向量模型可以将文本数据如转换成计算机能够计算的数据。 词袋模型（Bag of Words Model）词袋模型把文本（段落或者文档）被看作是无序的词汇集合，忽略语法甚至是单词的顺序，把每一个单词都进行统计，同时计算每个单词出现的次数，常常被用在文本分类中，如贝叶斯算法、LDA 和 LSA 等。 实战词袋模型import jieba #定义停用词、标点符号 punctuation = [&quot;，&quot;,&quot;。&quot;, &quot;：&quot;, &quot;；&quot;, &quot;？&quot;] #定义语料 content = [&quot;机器学习带动人工智能飞速的发展。&quot;, &quot;深度学习带动人工智能飞速的发展。&quot;, &quot;机器学习和深度学习带动人工智能飞速的发展。&quot; ] #分词 segs_1 = [jieba.lcut(con) for con in content] print(segs_1) 输出： [[‘机器’, ‘学习’, ‘带动’, ‘人工智能’, ‘飞速’, ‘的’, ‘发展’, ‘。’], [‘深度’, ‘学习’, ‘带动’, ‘人工智能’, ‘飞速’, ‘的’, ‘发展’, ‘。’], [‘机器’, ‘学习’, ‘和’, ‘深度’, ‘学习’, ‘带动’, ‘人工智能’, ‘飞速’, ‘的’, ‘发展’, ‘。’]] # 去标点符号 tokenized = [] for sentence in segs_1: words = [] for word in sentence: if word not in punctuation: words.append(word) tokenized.append(words) print(tokenized) 输出：[[‘机器’, ‘学习’, ‘带动’, ‘人工智能’, ‘飞速’, ‘的’, ‘发展’], [‘深度’, ‘学习’, ‘带动’, ‘人工智能’, ‘飞速’, ‘的’, ‘发展’], [‘机器’, ‘学习’, ‘和’, ‘深度’, ‘学习’, ‘带动’, ‘人工智能’, ‘飞速’, ‘的’, ‘发展’]] # 把所有的分词结果放到一个袋子（List）里面，也就是取并集，再去重，获取对应的特征词。 #求并集 bag_of_words = [ x for item in segs_1 for x in item if x not in punctuation] #去重 bag_of_words = list(set(bag_of_words)) print(bag_of_words) 输出： [‘飞速’, ‘的’, ‘深度’, ‘人工智能’, ‘发展’, ‘和’, ‘机器’, ‘学习’, ‘带动’] # 以上面特征词的顺序，完成词袋化，得到词袋向量 bag_of_word2vec = [] for sentence in tokenized: tokens = [1 if token in sentence else 0 for token in bag_of_words ] bag_of_word2vec.append(tokens) 输出：[[1, 1, 0, 1, 1, 0, 1, 1, 1], [1, 1, 1, 1, 1, 0, 0, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]] Gensim 构建词袋模型from gensim import corpora import gensim #tokenized是去标点之后的 dictionary = corpora.Dictionary(tokenized) #保存词典 dictionary.save(&#39;deerwester.dict&#39;) print(dictionary) 输出： Dictionary(9 unique tokens: [‘人工智能’, ‘发展’, ‘学习’, ‘带动’, ‘机器’]…) #查看词典和下标 id 的映射 print(dictionary.token2id) 输出： {‘人工智能’: 0, ‘发展’: 1, ‘学习’: 2, ‘带动’: 3, ‘机器’: 4, ‘的’: 5, ‘飞速’: 6, ‘深度’: 7, ‘和’: 8} # doc2bow()，作用只是计算每个不同单词的出现次数，将单词转换为其整数单词 id 并将结果作为稀疏向量返回。 corpus = [dictionary.doc2bow(sentence) for sentence in segs_1] print(corpus ) 输出： [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(0, 1), (1, 1), (2, 1), (3, 1), (5, 1), (6, 1), (7, 1)], [(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]] 词向量 （Word Embedding）词向量技术是将词语转化成为稠密向量。在自然语言处理应用中，词向量作为机器学习、深度学习模型的特征进行输入。因此，最终模型的效果很大程度上取决于词向量的效果。在 Word2Vec 出现之前，自然语言处理经常把字词进行独热编码，也就是 One-Hot Encoder。 大数据 [0,0,0,0,0,0,0,1,0,……，0,0,0,0,0,0,0] 云计算[0,0,0,0,1,0,0,0,0,……，0,0,0,0,0,0,0] 机器学习[0,0,0,1,0,0,0,0,0,……，0,0,0,0,0,0,0] 人工智能[0,0,0,0,0,0,0,0,0,……，1,0,0,0,0,0,0] 比如上面的例子中，大数据 、云计算、机器学习和人工智能各对应一个向量，向量中只有一个值为1，其余都为0。所以使用 One-Hot Encoder有以下问题： 第一，词语编码是随机的，向量之间相互独立，看不出词语之间可能存在的关联关系。第二，向量维度的大小取决于语料库中词语的多少，如果语料包含的所有词语对应的向量合为一个矩阵的话，那这个矩阵过于稀疏，并且会造成维度灾难。而解决这个问题的手段，就是使用向量表示（Vector Representations）。比如 Word2Vec 可以将 One-Hot Encoder 转化为低维度的连续值，也就是稠密向量，并且其中意思相近的词也将被映射到向量空间中相近的位置。经过降维，在二维空间中，相似的单词在空间中的距离也很接近。 这里简单给词向量一个定义，词向量就是要用某个固定维度的向量去表示单词。也就是说要把单词变成固定维度的向量，作为机器学习（Machine Learning）或深度学习模型的特征向量输入。 词向量实战Word2VecWord2Vec 主要包含两种模型：Skip-Gram 和 CBOW，值得一提的是，Word2Vec 词向量可以较好地表达不同词之间的相似和类比关系。 pip install gensim from gensim.models import Word2Vec import jieba #定义停用词、标点符号 punctuation = [&quot;,&quot;,&quot;。&quot;, &quot;:&quot;, &quot;;&quot;, &quot;.&quot;, &quot;&#39;&quot;, &#39;&quot;&#39;, &quot;’&quot;, &quot;?&quot;, &quot;/&quot;, &quot;-&quot;, &quot;+&quot;, &quot;&amp;&quot;, &quot;(&quot;, &quot;)&quot;] sentences = [ &quot;长江是中国第一大河，干流全长6397公里（以沱沱河为源），一般称6300公里。流域总面积一百八十余万平方公里，年平均入海水量约九千六百余亿立方米。以干流长度和入海水量论，长江均居世界第三位。&quot;, &quot;黄河，中国古代也称河，发源于中华人民共和国青海省巴颜喀拉山脉，流经青海、四川、甘肃、宁夏、内蒙古、陕西、山西、河南、山东9个省区，最后于山东省东营垦利县注入渤海。干流河道全长5464千米，仅次于长江，为中国第二长河。黄河还是世界第五长河。&quot;, &quot;黄河,是中华民族的母亲河。作为中华文明的发祥地,维系炎黄子孙的血脉.是中华民族民族精神与民族情感的象征。&quot;, &quot;黄河被称为中华文明的母亲河。公元前2000多年华夏族在黄河领域的中原地区形成、繁衍。&quot;, &quot;在兰州的“黄河第一桥”内蒙古托克托县河口镇以上的黄河河段为黄河上游。&quot;, &quot;黄河上游根据河道特性的不同，又可分为河源段、峡谷段和冲积平原三部分。 &quot;, &quot;黄河,是中华民族的母亲河。&quot; ] # 定义好语料，接下来进行分词，去标点符号 sentences = [jieba.lcut(sen) for sen in sentences] tokenized = [] for sentence in sentences: words = [] for word in sentence: if word not in punctuation: words.append(word) tokenized.append(words) # 模型训练 model = Word2Vec(tokenized, sg=1, size=100, window=5, min_count=2, negative=1, sample=0.001, hs=1, workers=4) sg=1 是 skip-gram 算法，对低频词敏感；默认 sg=0 为 CBOW 算法。 size 是输出词向量的维数，值太小会导致词映射因为冲突而影响结果，值太大则会耗内存并使算法计算变慢，一般值取为100到200之间。 window 是句子中当前词与目标词之间的最大距离，3表示在目标词前看3-b 个词，后面看 b 个词（b 在0-3之间随机）。 min_count 是对词进行过滤，频率小于 min-count 的单词则会被忽视，默认值为5。 negative 和 sample 可根据训练结果进行微调，sample 表示更高频率的词被随机下采样到所设置的阈值，默认值为 1e-3。 hs=1 表示层级 softmax 将会被使用，默认 hs=0 且 negative 不为0，则负采样将会被选择使用。 # 训练后的模型可以保存与加载 model.save(&#39;model&#39;) #保存模型 model = Word2Vec.load(&#39;model&#39;) #加载模型 # 模型训练好之后，接下来就可以使用模型，可以用来计算句子或者词的相似性、最大匹配程度等。 # 计算相似度 print(model.similarity(&#39;黄河&#39;, &#39;长江&#39;)) # 预测最接近的词，预测与黄河和母亲河最接近，而与长江不接近的词 print(model.most_similar(positive=[&#39;黄河&#39;, &#39;母亲河&#39;], negative=[&#39;长江&#39;])) 输出： [(‘是’, 0.14632007479667664), (‘以’, 0.14630728960037231), (‘长河’, 0.13878652453422546), (‘河道’, 0.13716217875480652), (‘在’, 0.11577725410461426), (‘全长’, 0.10969121754169464), (‘内蒙古’, 0.07590540498495102), (‘入海’, 0.06970417499542236), (‘民族’, 0.06064444035291672), (‘中华文明’, 0.057667165994644165)] Word2Vec 是一种将词变成词向量的工具。通俗点说，只有这样文本预料才转化为计算机能够计算的矩阵向量。 Doc2Vec在 Gensim 库中，Doc2Vec 与 Word2Vec 都极为相似。但两者在对输入数据的预处理上稍有不同，Doc2vec 接收一个由 LabeledSentence 对象组成的迭代器作为其构造函数的输入参数。其中，LabeledSentence 是 Gensim 内建的一个类，它接收两个 List 作为其初始化的参数：word list 和 label list。 Doc2Vec 也包括两种实现方式：DBOW（Distributed Bag of Words）和 DM （Distributed Memory）。DBOW 和 DM 的实现，二者在 gensim 库中的实现用的是同一个方法，该方法中参数 dm = 0 或者 dm=1 决定调用 DBOW 还是 DM。Doc2Vec 将文档语料通过一个固定长度的向量表达。 下面是 Gensim 中 Doc2Vec 模型的实战，我们把上述语料每一句话当做一个文本，添加上对应的标签。接下来，定义数据预处理类，作用是给每个文章添加对应的标签： #定义数据预处理类，作用是给每个文章添加对应的标签 from gensim.models.doc2vec import Doc2Vec,LabeledSentence doc_labels = [&quot;长江&quot;,&quot;黄河&quot;,&quot;黄河&quot;,&quot;黄河&quot;,&quot;黄河&quot;,&quot;黄河&quot;,&quot;黄河&quot;] class LabeledLineSentence(object): def __init__(self, doc_list, labels_list): self.labels_list = labels_list self.doc_list = doc_list def __iter__(self): for idx, doc in enumerate(self.doc_list): yield LabeledSentence(words=doc,tags=[self.labels_list[idx]]) model = Doc2Vec(documents,dm=1, size=100, window=8, min_count=5, workers=4) model.save(&#39;model&#39;) model = Doc2Vec.load(&#39;model&#39;) # 定义好了数据预处理函数，我们将 Word2Vec 中分词去标点后的数据，进行转换 iter_data = LabeledLineSentence(tokenized, doc_labels) # 开始定义模型参数，这里 dm=1，采用了 Gensim 中的 DM 实现 model = Doc2Vec(dm=1, size=100, window=8, min_count=5, workers=4) model.build_vocab(iter_data) # 训练模型， 设置迭代次数1000次，start_alpha 为开始学习率，end_alpha 与 start_alpha 线性递减。 model.train(iter_data,total_examples=model.corpus_count,epochs=1000,start_alpha=0.01,end_alpha =0.001) # 进行一些预测 根据标签找最相似的，这里只有黄河和长江，所以结果为长江，并计算出了相似度 print(model.docvecs.most_similar(&#39;黄河&#39;)) 输出： [(‘长江’, 0.25543850660324097)] print(model.docvecs.similarity(&#39;黄河&#39;,&#39;长江&#39;)) 输出： 0.25543848271351405 最终影响模型准确率的因素有：文档的数量越多，文档的相似性越好，也就是基于大数据量的模型训练。在工业界，Word2Vec 和 Doc2Vec 常见的应用有：做相似词计算；相关词挖掘，在推荐系统中用在品牌、用户、商品挖掘中；上下文预测句子；机器翻译；作为特征输入其他模型等。 总结，本文只是简单的介绍了词袋和词向量模型的典型应用，对于两者的理论和其他词向量模型，比如 TextRank 、FastText 和 GloVe 等。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>doc2vec</tag>
        <tag>word2vec</tag>
        <tag>gensim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于ML的中文短文本聚类（7）]]></title>
    <url>%2F2019%2F11%2F22%2F7.%E5%9F%BA%E4%BA%8EML%E7%9A%84%E4%B8%AD%E6%96%87%E7%9F%AD%E6%96%87%E6%9C%AC%E8%81%9A%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[文本聚类是将一个个文档由原有的自然语言文字信息转化成数学信息，以高维空间点的形式展现出来，通过计算哪些点距离比较近，从而将那些点聚成一个簇，簇的中心叫做簇心。一个好的聚类要保证簇内点的距离尽量的近，但簇与簇之间的点要尽量的远。 文本聚类如下图，以 K、M、N 三个点分别为聚类的簇心，将结果聚为三类，使得簇内点的距离尽量的近，但簇与簇之间的点尽量的远。 文本无监督聚类步骤： 语料加载 分词 去停用词 抽取词向量特征 实战 TF-IDF 的中文文本 K-means 聚类 实战 word2Vec 的中文文本 K-means 聚类 语料加载进行语料加载，在这之前，引入所需要的 Python 依赖包，并将全部语料和停用词字典读入内存中 #依赖库，有随机数库、jieba 分词、pandas 库 import random import jieba import pandas as pd import numpy as np from sklearn.feature_extraction.text import TfidfTransformer from sklearn.feature_extraction.text import TfidfVectorizer import matplotlib.pyplot as plt from sklearn.decomposition import PCA from sklearn.cluster import KMeans import gensim from gensim.models import Word2Vec from sklearn.preprocessing import scale import multiprocessing #加载停用词 stopwords.txt topwords=pd.read_csv(&#39;stopwords.txt&#39;,index_col=False,quoting=3,sep=&quot;\t&quot;,names=[&#39;stopword&#39;], encoding=&#39;utf-8&#39;) topwords=stopwords[&#39;stopword&#39;].values # 加载语料，语料是4个已经分好类的 csv 文件，直接用 pandas 加载即可，加载之后可以首先删除 nan 行，并提取要分词的 content 列转换为 list 列表 #加载语料 laogong_df = pd.read_csv(&#39;beilaogongda.csv&#39;, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) laopo_df = pd.read_csv(&#39;beilaogongda.csv&#39;, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) erzi_df = pd.read_csv(&#39;beierzida.csv&#39;, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) nver_df = pd.read_csv(&#39;beinverda.csv&#39;, encoding=&#39;utf-8&#39;, sep=&#39;,&#39;) #删除语料的nan行 laogong_df.dropna(inplace=True) laopo_df.dropna(inplace=True) erzi_df.dropna(inplace=True) nver_df.dropna(inplace=True) #转换 laogong = laogong_df.segment.values.tolist() laopo = laopo_df.segment.values.tolist() erzi = erzi_df.segment.values.tolist() nver = nver_df.segment.values.tolist() 分词和去停用词定义分词、去停用词的函数，函数包含两个参数：content_lines 参数为语料列表；sentences 参数为预先定义的 list，用来存储分词后的结果 def preprocess_text(content_lines, sentences): for line in content_lines: try: segs=jieba.lcut(line) segs = [v for v in segs if not str(v).isdigit()]#去数字 segs = list(filter(lambda x:x.strip(), segs)) #去左右空格 segs = list(filter(lambda x:len(x)&gt;1, segs)) #长度为1的字符 segs = list(filter(lambda x:x not in stopwords, segs)) #去掉停用词 sentences.append(&quot; &quot;.join(segs)) except Exception: print(line) continue # 调用函数、生成训练数据，根据我提供的司法语料数据，分为报警人被老公打，报警人被老婆打，报警人被儿子打，报警人被女儿打 sentences = [] preprocess_text(laogong, sentences) preprocess_text(laopo, sentences) preprocess_text(erzi, sentences) preprocess_text(nver, sentences) # 将得到的数据集打散，生成更可靠的训练集分布，避免同类数据分布不均匀 random.shuffle(sentences) # 我们控制台输出前10条数据，观察一下（因为上面进行了随机打散，你看到的前10条可能不一样） for sentence in sentences[:10]: print(sentenc) 得到的结果聚类和分类是不同的，这里没有标签 抽取词向量特征抽取特征，将文本中的词语转换为词频矩阵，统计每个词语的 tf-idf 权值，获得词在对应文本中的 tf-idf 权重 #将文本中的词语转换为词频矩阵 矩阵元素a[i][j] 表示j词在i类文本下的词频 vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5) #统计每个词语的tf-idf权值 transformer = TfidfTransformer() # 第一个fit_transform是计算tf-idf 第二个fit_transform是将文本转为词频矩阵 tfidf = transformer.fit_transform(vectorizer.fit_transform(sentences)) # 获取词袋模型中的所有词语 word = vectorizer.get_feature_names() # 将tf-idf矩阵抽取出来，元素w[i][j]表示j词在i类文本中的tf-idf权重 weight = tfidf.toarray() #查看特征大小 print (&#39;Features length: &#39; + str(len(word)))TF-IDF 的中文文本 K-means 聚类使用 k-means++ 来初始化模型，当然也可以选择随机初始化，即 init=”random”，然后通过 PCA 降维把上面的权重 weight 降到10维，进行聚类模型训练 numClass=4 #聚类分几簇 clf = KMeans(n_clusters=numClass, max_iter=10000, init=&quot;k-means++&quot;, tol=1e-6) #这里也可以选择随机初始化init=&quot;random&quot; pca = PCA(n_components=10) # 降维 TnewData = pca.fit_transform(weight) # 载入N维 s = clf.fit(TnewData)第二步，定义聚类结果可视化函数 plot_cluster(result,newData,numClass)，该函数包含3个参数，其中 result 表示聚类拟合的结果集；newData 表示权重 weight 降维的结果，这里需要降维到2维，即平面可视化；numClass 表示聚类分为几簇，绘制代码第一部分绘制结果 newData，第二部分绘制聚类的中心点 def plot_cluster(result,newData,numClass): plt.figure(2) Lab = [[] for i in range(numClass)] index = 0 for labi in result: Lab[labi].append(index) index += 1 color = [&#39;oy&#39;, &#39;ob&#39;, &#39;og&#39;, &#39;cs&#39;, &#39;ms&#39;, &#39;bs&#39;, &#39;ks&#39;, &#39;ys&#39;, &#39;yv&#39;, &#39;mv&#39;, &#39;bv&#39;, &#39;kv&#39;, &#39;gv&#39;, &#39;y^&#39;, &#39;m^&#39;, &#39;b^&#39;, &#39;k^&#39;, &#39;g^&#39;] * 3 for i in range(numClass): x1 = [] y1 = [] for ind1 in newData[Lab[i]]: # print ind1 try: y1.append(ind1[1]) x1.append(ind1[0]) except: pass plt.plot(x1, y1, color[i]) #绘制初始中心点 x1 = [] y1 = [] for ind1 in clf.cluster_centers_: try: y1.append(ind1[1]) x1.append(ind1[0]) except: pass plt.plot(x1, y1, &quot;rv&quot;) #绘制中心 plt.show() 对数据降维到2维，然后获得结果，最后绘制聚类结果图 pca = PCA(n_components=2) # 输出两维 newData = pca.fit_transform(weight) # 载入N维 result = list(clf.predict(TnewData)) plot_cluster(result,newData,numClass) 得到的聚类结果图，4个中心点和4个簇，我们看到结果还比较好，簇的边界很清楚上面演示的可视化过程，降维使用了 PCA，我们还可以试试 TSNE，两者同为降维工具，主要区别在于，所在的包不同因为原理不同，导致 TSNE 保留下的属性信息，更具代表性，也即最能体现样本间的差异，但是 TSNE 运行极慢，PCA 则相对较快，下面看看 TSNE 运行的可视化结果 from sklearn.decomposition import PCA from sklearn.manifold import TSNE ts =TSNE(2) newData = ts.fit_transform(weight) result = list(clf.predict(TnewData)) plot_cluster(result,newData,numClass) 得到的可视化结果，为一个中心点，不同簇落在围绕中心点的不同半径之内，得到的可视化结果，为一个中心点，不同簇落在围绕中心点的不同半径之内，我们看到在这里结果并不是很好为了更好的表达和获取更具有代表性的信息，在展示（可视化）高维数据时，更为一般的处理，常常先用 PCA 进行降维，再使用 TSNE from sklearn.manifold import TSNE newData = PCA(n_components=4).fit_transform(weight) # 载入N维 newData =TSNE(2).fit_transform(newData) result = list(clf.predict(TnewData)) plot_cluster(result,newData,numClass) 得到的可视化结果，不同簇落在围绕中心点的不同半径之内从优化和提高模型准确率来说，主要有两方面可以尝试： 特征向量的构建，除了词袋模型，可以考虑使用 word2vec 和 doc2vec 等； 模型上可以采用基于密度的 DBSCAN、层次聚类等算法。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>jieba</tag>
        <tag>tf-idf</tag>
        <tag>gensim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql使用实战]]></title>
    <url>%2F2019%2F10%2F29%2Fmysql%2F</url>
    <content type="text"><![CDATA[mysql使用实战 中文转拼音模糊查询CREATE TABLE IF NOT EXISTS `t_base_pinyin` ( `pin_yin_` VARCHAR (1000) CHARACTER SET gbk NOT NULL, `code_` INT (11) NOT NULL, PRIMARY KEY (`code_`) ) ENGINE = INNODB DEFAULT CHARSET = latin1; INSERT INTO t_base_pinyin (pin_yin_, code_) VALUES (&quot;a&quot;, 20319), (&quot;ai&quot;, 20317), (&quot;an&quot;, 20304), (&quot;ang&quot;, 20295), (&quot;ao&quot;, 20292), (&quot;ba&quot;, 20283), (&quot;bai&quot;, 20265), (&quot;ban&quot;, 20257), (&quot;bang&quot;, 20242), (&quot;bao&quot;, 20230), (&quot;bei&quot;, 20051), (&quot;ben&quot;, 20036), (&quot;beng&quot;, 20032), (&quot;bi&quot;, 20026), (&quot;bian&quot;, 20002), (&quot;biao&quot;, 19990), (&quot;bie&quot;, 19986), (&quot;bin&quot;, 19982), (&quot;bing&quot;, 19976), (&quot;bo&quot;, 19805), (&quot;bu&quot;, 19784), (&quot;ca&quot;, 19775), (&quot;cai&quot;, 19774), (&quot;can&quot;, 19763), (&quot;cang&quot;, 19756), (&quot;cao&quot;, 19751), (&quot;ce&quot;, 19746), (&quot;ceng&quot;, 19741), (&quot;cha&quot;, 19739), (&quot;chai&quot;, 19728), (&quot;chan&quot;, 19725), (&quot;chang&quot;, 19715), (&quot;chao&quot;, 19540), (&quot;che&quot;, 19531), (&quot;chen&quot;, 19525), (&quot;cheng&quot;, 19515), (&quot;chi&quot;, 19500), (&quot;chong&quot;, 19484), (&quot;chou&quot;, 19479), (&quot;chu&quot;, 19467), (&quot;chuai&quot;, 19289), (&quot;chuan&quot;, 19288), (&quot;chuang&quot;, 19281), (&quot;chui&quot;, 19275), (&quot;chun&quot;, 19270), (&quot;chuo&quot;, 19263), (&quot;ci&quot;, 19261), (&quot;cong&quot;, 19249), (&quot;cou&quot;, 19243), (&quot;cu&quot;, 19242), (&quot;cuan&quot;, 19238), (&quot;cui&quot;, 19235), (&quot;cun&quot;, 19227), (&quot;cuo&quot;, 19224), (&quot;da&quot;, 19218), (&quot;dai&quot;, 19212), (&quot;dan&quot;, 19038), (&quot;dang&quot;, 19023), (&quot;dao&quot;, 19018), (&quot;de&quot;, 19006), (&quot;deng&quot;, 19003), (&quot;di&quot;, 18996), (&quot;dian&quot;, 18977), (&quot;diao&quot;, 18961), (&quot;die&quot;, 18952), (&quot;ding&quot;, 18783), (&quot;diu&quot;, 18774), (&quot;dong&quot;, 18773), (&quot;dou&quot;, 18763), (&quot;du&quot;, 18756), (&quot;duan&quot;, 18741), (&quot;dui&quot;, 18735), (&quot;dun&quot;, 18731), (&quot;duo&quot;, 18722), (&quot;e&quot;, 18710), (&quot;en&quot;, 18697), (&quot;er&quot;, 18696), (&quot;fa&quot;, 18526), (&quot;fan&quot;, 18518), (&quot;fang&quot;, 18501), (&quot;fei&quot;, 18490), (&quot;fen&quot;, 18478), (&quot;feng&quot;, 18463), (&quot;fo&quot;, 18448), (&quot;fou&quot;, 18447), (&quot;fu&quot;, 18446), (&quot;ga&quot;, 18239), (&quot;gai&quot;, 18237), (&quot;gan&quot;, 18231), (&quot;gang&quot;, 18220), (&quot;gao&quot;, 18211), (&quot;ge&quot;, 18201), (&quot;gei&quot;, 18184), (&quot;gen&quot;, 18183), (&quot;geng&quot;, 18181), (&quot;gong&quot;, 18012), (&quot;gou&quot;, 17997), (&quot;gu&quot;, 17988), (&quot;gua&quot;, 17970), (&quot;guai&quot;, 17964), (&quot;guan&quot;, 17961), (&quot;guang&quot;, 17950), (&quot;gui&quot;, 17947), (&quot;gun&quot;, 17931), (&quot;guo&quot;, 17928), (&quot;ha&quot;, 17922), (&quot;hai&quot;, 17759), (&quot;han&quot;, 17752), (&quot;hang&quot;, 17733), (&quot;hao&quot;, 17730), (&quot;he&quot;, 17721), (&quot;hei&quot;, 17703), (&quot;hen&quot;, 17701), (&quot;heng&quot;, 17697), (&quot;hong&quot;, 17692), (&quot;hou&quot;, 17683), (&quot;hu&quot;, 17676), (&quot;hua&quot;, 17496), (&quot;huai&quot;, 17487), (&quot;huan&quot;, 17482), (&quot;huang&quot;, 17468), (&quot;hui&quot;, 17454), (&quot;hun&quot;, 17433), (&quot;huo&quot;, 17427), (&quot;ji&quot;, 17417), (&quot;jia&quot;, 17202), (&quot;jian&quot;, 17185), (&quot;jiang&quot;, 16983), (&quot;jiao&quot;, 16970), (&quot;jie&quot;, 16942), (&quot;jin&quot;, 16915), (&quot;jing&quot;, 16733), (&quot;jiong&quot;, 16708), (&quot;jiu&quot;, 16706), (&quot;ju&quot;, 16689), (&quot;juan&quot;, 16664), (&quot;jue&quot;, 16657), (&quot;jun&quot;, 16647), (&quot;ka&quot;, 16474), (&quot;kai&quot;, 16470), (&quot;kan&quot;, 16465), (&quot;kang&quot;, 16459), (&quot;kao&quot;, 16452), (&quot;ke&quot;, 16448), (&quot;ken&quot;, 16433), (&quot;keng&quot;, 16429), (&quot;kong&quot;, 16427), (&quot;kou&quot;, 16423), (&quot;ku&quot;, 16419), (&quot;kua&quot;, 16412), (&quot;kuai&quot;, 16407), (&quot;kuan&quot;, 16403), (&quot;kuang&quot;, 16401), (&quot;kui&quot;, 16393), (&quot;kun&quot;, 16220), (&quot;kuo&quot;, 16216), (&quot;la&quot;, 16212), (&quot;lai&quot;, 16205), (&quot;lan&quot;, 16202), (&quot;lang&quot;, 16187), (&quot;lao&quot;, 16180), (&quot;le&quot;, 16171), (&quot;lei&quot;, 16169), (&quot;leng&quot;, 16158), (&quot;li&quot;, 16155), (&quot;lia&quot;, 15959), (&quot;lian&quot;, 15958), (&quot;liang&quot;, 15944), (&quot;liao&quot;, 15933), (&quot;lie&quot;, 15920), (&quot;lin&quot;, 15915), (&quot;ling&quot;, 15903), (&quot;liu&quot;, 15889), (&quot;long&quot;, 15878), (&quot;lou&quot;, 15707), (&quot;lu&quot;, 15701), (&quot;lv&quot;, 15681), (&quot;luan&quot;, 15667), (&quot;lue&quot;, 15661), (&quot;lun&quot;, 15659), (&quot;luo&quot;, 15652), (&quot;ma&quot;, 15640), (&quot;mai&quot;, 15631), (&quot;man&quot;, 15625), (&quot;mang&quot;, 15454), (&quot;mao&quot;, 15448), (&quot;me&quot;, 15436), (&quot;mei&quot;, 15435), (&quot;men&quot;, 15419), (&quot;meng&quot;, 15416), (&quot;mi&quot;, 15408), (&quot;mian&quot;, 15394), (&quot;miao&quot;, 15385), (&quot;mie&quot;, 15377), (&quot;min&quot;, 15375), (&quot;ming&quot;, 15369), (&quot;miu&quot;, 15363), (&quot;mo&quot;, 15362), (&quot;mou&quot;, 15183), (&quot;mu&quot;, 15180), (&quot;na&quot;, 15165), (&quot;nai&quot;, 15158), (&quot;nan&quot;, 15153), (&quot;nang&quot;, 15150), (&quot;nao&quot;, 15149), (&quot;ne&quot;, 15144), (&quot;nei&quot;, 15143), (&quot;nen&quot;, 15141), (&quot;neng&quot;, 15140), (&quot;ni&quot;, 15139), (&quot;nian&quot;, 15128), (&quot;niang&quot;, 15121), (&quot;niao&quot;, 15119), (&quot;nie&quot;, 15117), (&quot;nin&quot;, 15110), (&quot;ning&quot;, 15109), (&quot;niu&quot;, 14941), (&quot;nong&quot;, 14937), (&quot;nu&quot;, 14933), (&quot;nv&quot;, 14930), (&quot;nuan&quot;, 14929), (&quot;nue&quot;, 14928), (&quot;nuo&quot;, 14926), (&quot;o&quot;, 14922), (&quot;ou&quot;, 14921), (&quot;pa&quot;, 14914), (&quot;pai&quot;, 14908), (&quot;pan&quot;, 14902), (&quot;pang&quot;, 14894), (&quot;pao&quot;, 14889), (&quot;pei&quot;, 14882), (&quot;pen&quot;, 14873), (&quot;peng&quot;, 14871), (&quot;pi&quot;, 14857), (&quot;pian&quot;, 14678), (&quot;piao&quot;, 14674), (&quot;pie&quot;, 14670), (&quot;pin&quot;, 14668), (&quot;ping&quot;, 14663), (&quot;po&quot;, 14654), (&quot;pu&quot;, 14645), (&quot;qi&quot;, 14630), (&quot;qia&quot;, 14594), (&quot;qian&quot;, 14429), (&quot;qiang&quot;, 14407), (&quot;qiao&quot;, 14399), (&quot;qie&quot;, 14384), (&quot;qin&quot;, 14379), (&quot;qing&quot;, 14368), (&quot;qiong&quot;, 14355), (&quot;qiu&quot;, 14353), (&quot;qu&quot;, 14345), (&quot;quan&quot;, 14170), (&quot;que&quot;, 14159), (&quot;qun&quot;, 14151), (&quot;ran&quot;, 14149), (&quot;rang&quot;, 14145), (&quot;rao&quot;, 14140), (&quot;re&quot;, 14137), (&quot;ren&quot;, 14135), (&quot;reng&quot;, 14125), (&quot;ri&quot;, 14123), (&quot;rong&quot;, 14122), (&quot;rou&quot;, 14112), (&quot;ru&quot;, 14109), (&quot;ruan&quot;, 14099), (&quot;rui&quot;, 14097), (&quot;run&quot;, 14094), (&quot;ruo&quot;, 14092), (&quot;sa&quot;, 14090), (&quot;sai&quot;, 14087), (&quot;san&quot;, 14083), (&quot;sang&quot;, 13917), (&quot;sao&quot;, 13914), (&quot;se&quot;, 13910), (&quot;sen&quot;, 13907), (&quot;seng&quot;, 13906), (&quot;sha&quot;, 13905), (&quot;shai&quot;, 13896), (&quot;shan&quot;, 13894), (&quot;shang&quot;, 13878), (&quot;shao&quot;, 13870), (&quot;she&quot;, 13859), (&quot;shen&quot;, 13847), (&quot;sheng&quot;, 13831), (&quot;shi&quot;, 13658), (&quot;shou&quot;, 13611), (&quot;shu&quot;, 13601), (&quot;shua&quot;, 13406), (&quot;shuai&quot;, 13404), (&quot;shuan&quot;, 13400), (&quot;shuang&quot;, 13398), (&quot;shui&quot;, 13395), (&quot;shun&quot;, 13391), (&quot;shuo&quot;, 13387), (&quot;si&quot;, 13383), (&quot;song&quot;, 13367), (&quot;sou&quot;, 13359), (&quot;su&quot;, 13356), (&quot;suan&quot;, 13343), (&quot;sui&quot;, 13340), (&quot;sun&quot;, 13329), (&quot;suo&quot;, 13326), (&quot;ta&quot;, 13318), (&quot;tai&quot;, 13147), (&quot;tan&quot;, 13138), (&quot;tang&quot;, 13120), (&quot;tao&quot;, 13107), (&quot;te&quot;, 13096), (&quot;teng&quot;, 13095), (&quot;ti&quot;, 13091), (&quot;tian&quot;, 13076), (&quot;tiao&quot;, 13068), (&quot;tie&quot;, 13063), (&quot;ting&quot;, 13060), (&quot;tong&quot;, 12888), (&quot;tou&quot;, 12875), (&quot;tu&quot;, 12871), (&quot;tuan&quot;, 12860), (&quot;tui&quot;, 12858), (&quot;tun&quot;, 12852), (&quot;tuo&quot;, 12849), (&quot;wa&quot;, 12838), (&quot;wai&quot;, 12831), (&quot;wan&quot;, 12829), (&quot;wang&quot;, 12812), (&quot;wei&quot;, 12802), (&quot;wen&quot;, 12607), (&quot;weng&quot;, 12597), (&quot;wo&quot;, 12594), (&quot;wu&quot;, 12585), (&quot;xi&quot;, 12556), (&quot;xia&quot;, 12359), (&quot;xian&quot;, 12346), (&quot;xiang&quot;, 12320), (&quot;xiao&quot;, 12300), (&quot;xie&quot;, 12120), (&quot;xin&quot;, 12099), (&quot;xing&quot;, 12089), (&quot;xiong&quot;, 12074), (&quot;xiu&quot;, 12067), (&quot;xu&quot;, 12058), (&quot;xuan&quot;, 12039), (&quot;xue&quot;, 11867), (&quot;xun&quot;, 11861), (&quot;ya&quot;, 11847), (&quot;yan&quot;, 11831), (&quot;yang&quot;, 11798), (&quot;yao&quot;, 11781), (&quot;ye&quot;, 11604), (&quot;yi&quot;, 11589), (&quot;yin&quot;, 11536), (&quot;ying&quot;, 11358), (&quot;yo&quot;, 11340), (&quot;yong&quot;, 11339), (&quot;you&quot;, 11324), (&quot;yu&quot;, 11303), (&quot;yuan&quot;, 11097), (&quot;yue&quot;, 11077), (&quot;yun&quot;, 11067), (&quot;za&quot;, 11055), (&quot;zai&quot;, 11052), (&quot;zan&quot;, 11045), (&quot;zang&quot;, 11041), (&quot;zao&quot;, 11038), (&quot;ze&quot;, 11024), (&quot;zei&quot;, 11020), (&quot;zen&quot;, 11019), (&quot;zeng&quot;, 11018), (&quot;zha&quot;, 11014), (&quot;zhai&quot;, 10838), (&quot;zhan&quot;, 10832), (&quot;zhang&quot;, 10815), (&quot;zhao&quot;, 10800), (&quot;zhe&quot;, 10790), (&quot;zhen&quot;, 10780), (&quot;zheng&quot;, 10764), (&quot;zhi&quot;, 10587), (&quot;zhong&quot;, 10544), (&quot;zhou&quot;, 10533), (&quot;zhu&quot;, 10519), (&quot;zhua&quot;, 10331), (&quot;zhuai&quot;, 10329), (&quot;zhuan&quot;, 10328), (&quot;zhuang&quot;, 10322), (&quot;zhui&quot;, 10315), (&quot;zhun&quot;, 10309), (&quot;zhuo&quot;, 10307), (&quot;zi&quot;, 10296), (&quot;zong&quot;, 10281), (&quot;zou&quot;, 10274), (&quot;zu&quot;, 10270), (&quot;zuan&quot;, 10262), (&quot;zui&quot;, 10260), (&quot;zun&quot;, 10256), (&quot;zuo&quot;, 10254); DROP FUNCTION IF EXISTS to_pinyin; DELIMITER $ CREATE FUNCTION to_pinyin(NAME text CHARSET gbk) RETURNS text CHARSET gbk BEGIN DECLARE mycode INT; DECLARE tmp_lcode VARCHAR(2) CHARSET gbk; DECLARE lcode INT; DECLARE tmp_rcode VARCHAR(2) CHARSET gbk; DECLARE rcode INT; DECLARE mypy text CHARSET gbk DEFAULT &#39;&#39;; DECLARE lp INT; SET mycode = 0; SET lp = 1; SET NAME = HEX(NAME); WHILE lp &lt; LENGTH(NAME) DO SET tmp_lcode = SUBSTRING(NAME, lp, 2); SET lcode = CAST(ASCII(UNHEX(tmp_lcode)) AS UNSIGNED); SET tmp_rcode = SUBSTRING(NAME, lp + 2, 2); SET rcode = CAST(ASCII(UNHEX(tmp_rcode)) AS UNSIGNED); IF lcode &gt; 128 THEN SET mycode =65536 - lcode * 256 - rcode ; SELECT CONCAT(mypy,pin_yin_) INTO mypy FROM t_base_pinyin WHERE CODE_ &gt;= ABS(mycode) ORDER BY CODE_ ASC LIMIT 1; SET lp = lp + 4; ELSE SET mypy = CONCAT(mypy,CHAR(CAST(ASCII(UNHEX(SUBSTRING(NAME, lp, 2))) AS UNSIGNED))); SET lp = lp + 2; END IF; END WHILE; RETURN LOWER(mypy); END; $ DELIMITER ; CREATE VIEW v_pinyin AS SELECT u.jjbh,u.CJLB,u.SFDD,u.CJXXDD,u.BJNR,u.CLJGNR,u.CJDWMC,u.JJRQSJ ，to_pinyin (u.CLJGNR) AS CLJGNR_PINYIN,to_pinyin (u.BJNR) AS BJNR_PINYIN，u.JJRQSJ FROM p_answer_handle_alarm u 抽取文本中数字/字母/中文DELIMITER $$ DROP FUNCTION IF EXISTS `Num_char_extract`$$ CREATE FUNCTION `Num_char_extract`(Varstring VARCHAR(100)CHARSET utf8, flag INT) RETURNS VARCHAR(50) CHARSET utf8 BEGIN DECLARE len INT DEFAULT 0; DECLARE Tmp VARCHAR(100) DEFAULT &#39;&#39;; SET len=CHAR_LENGTH(Varstring); IF flag = 0 THEN WHILE len &gt; 0 DO IF MID(Varstring,len,1)REGEXP&#39;[0-9]&#39; THEN SET Tmp=CONCAT(Tmp,MID(Varstring,len,1)); END IF; SET len = len - 1; END WHILE; ELSEIF flag=1 THEN WHILE len &gt; 0 DO IF (MID(Varstring,len,1)REGEXP &#39;[a-zA-Z]&#39;) THEN SET Tmp=CONCAT(Tmp,MID(Varstring,len,1)); END IF; SET len = len - 1; END WHILE; ELSEIF flag=2 THEN WHILE len &gt; 0 DO IF ( (MID(Varstring,len,1)REGEXP&#39;[0-9]&#39;) OR (MID(Varstring,len,1)REGEXP &#39;[a-zA-Z]&#39;) ) THEN SET Tmp=CONCAT(Tmp,MID(Varstring,len,1)); END IF; SET len = len - 1; END WHILE; ELSEIF flag=3 THEN WHILE len &gt; 0 DO IF NOT (MID(Varstring,len,1)REGEXP &#39;^[u0391-uFFE5]&#39;) THEN SET Tmp=CONCAT(Tmp,MID(Varstring,len,1)); END IF; SET len = len - 1; END WHILE; ELSE SET Tmp = &#39;Error: The second paramter should be in (0,1,2,3)&#39;; RETURN Tmp; END IF; RETURN REVERSE(Tmp); END$$ DELIMITER ; 中文转首字母查询CREATE FUNCTION `fristPinyin`(P_NAME VARCHAR(255)) RETURNS varchar(255) CHARSET utf8 BEGIN DECLARE V_RETURN VARCHAR(255); SET V_RETURN = ELT(INTERVAL(CONV(HEX(left(CONVERT(P_NAME USING gbk),1)),16,10), 0xB0A1,0xB0C5,0xB2C1,0xB4EE,0xB6EA,0xB7A2,0xB8C1,0xB9FE,0xBBF7, 0xBFA6,0xC0AC,0xC2E8,0xC4C3,0xC5B6,0xC5BE,0xC6DA,0xC8BB, 0xC8F6,0xCBFA,0xCDDA,0xCEF4,0xD1B9,0xD4D1), &#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;,&#39;E&#39;,&#39;F&#39;,&#39;G&#39;,&#39;H&#39;,&#39;J&#39;,&#39;K&#39;,&#39;L&#39;,&#39;M&#39;,&#39;N&#39;,&#39;O&#39;,&#39;P&#39;,&#39;Q&#39;,&#39;R&#39;,&#39;S&#39;,&#39;T&#39;,&#39;W&#39;,&#39;X&#39;,&#39;Y&#39;,&#39;Z&#39;); RETURN V_RETURN; END; CREATE FUNCTION `pinyin`(P_NAME VARCHAR(255)) RETURNS varchar(255) CHARSET utf8 BEGIN DECLARE V_COMPARE VARCHAR(255); DECLARE V_RETURN VARCHAR(255); DECLARE I INT; SET I = 1; SET V_RETURN = &#39;&#39;; while I &lt; LENGTH(P_NAME) do SET V_COMPARE = SUBSTR(P_NAME, I, 1); IF (V_COMPARE != &#39;&#39;) THEN #SET V_RETURN = CONCAT(V_RETURN, &#39;,&#39;, V_COMPARE); SET V_RETURN = CONCAT(V_RETURN, fristPinyin(V_COMPARE)); #SET V_RETURN = fristPinyin(V_COMPARE); END IF; SET I = I + 1; end while; IF (ISNULL(V_RETURN) or V_RETURN = &#39;&#39;) THEN SET V_RETURN = P_NAME; END IF; RETURN V_RETURN; END; select upper(pinyin(company_name)) as cn,company_name from job where upper(pinyin(company_name)) like &#39;%pa%&#39;; select pinyin(&quot;金石&quot;) 联表查询在A不在B表select * from (select DISTINCT(company_name) from job j1 where is_checked = ‘1’) j2 where j2.company_name not in(select DISTINCT(job_company_name) from standard_business s1)]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>function</tag>
        <tag>procedure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy集成selenium爬取boss直聘]]></title>
    <url>%2F2019%2F10%2F17%2Fscrapy%E9%9B%86%E6%88%90selenium%E7%88%AC%E5%8F%96boss%E7%9B%B4%E8%81%98%2F</url>
    <content type="text"><![CDATA[scrapy集成selenium爬取boss直聘 核心代码class BossSpider(scrapy.Spider): name = &#39;boss&#39; allowed_domains = [&#39;zhipin.com&#39;] start_urls = [&#39;https://www.zhipin.com/&#39;] nodes = [] def parse(self, response): driver = None chrome_options = webdriver.ChromeOptions() # proxy_url = get_random_proxy() # print(proxy_url + &quot;代理服务器正在爬取&quot;) # chrome_options.add_argument(&#39;--proxy-server=https://&#39; + proxy_url.strip()) prefs = { &#39;profile.default_content_setting_values&#39;: { &#39;images&#39;: 1, # 加载图片 &quot;User-Agent&quot;: UserAgent().random, # 更换UA } } chrome_options.add_experimental_option(&quot;prefs&quot;, prefs) chrome_options.add_experimental_option(&#39;excludeSwitches&#39;, [&#39;enable-automation&#39;]) if platform.system() == &quot;Windows&quot;: driver = webdriver.Chrome(&#39;chromedriver.exe&#39;, chrome_options=chrome_options) elif platform.system() == &quot;Linux&quot;: chrome_options.add_argument(&quot;--headless&quot;) chrome_options.add_argument(&#39;--disable-gpu&#39;) chrome_options.add_argument(&#39;--no-sandbox&#39;) driver = webdriver.Chrome( executable_path=&quot;/usr/bin/chromedriver&quot;, chrome_options=chrome_options) # driver.set_window_size(500, 200) data = [&quot;游戏&quot;, &quot;期货&quot;, &quot;贷款&quot;] for kw in data: url = &quot;https://www.zhipin.com/c101190400/?query={}&quot;.format(kw) driver.get(url) time.sleep(2) # 获取信息 last_url = driver.current_url source = etree.HTML(driver.page_source) links = source.xpath(&quot;//div[@class=&#39;job-primary&#39;]/div[@class=&#39;info-primary&#39;]//a/@href&quot;) global nodes nodes = list(map(lambda x: &quot;https://www.zhipin.com{}&quot;.format(x), links)) while len(source.xpath(&#39;//div[@class=&quot;page&quot;]/a[@class=&quot;next&quot; and @ka=&quot;page-next&quot;]&#39;)) == 1: next_page = driver.find_element_by_xpath( &#39;//div[@class=&quot;page&quot;]/a[@class=&quot;next&quot; and @ka=&quot;page-next&quot;]&#39;) WebDriverWait(driver, 10).until(expected_conditions.element_to_be_clickable( (By.XPATH, &#39;//div[@class=&quot;page&quot;]/a[@class=&quot;next&quot; and @ka=&quot;page-next&quot;]&#39;))) current_url = driver.current_url while last_url == current_url: self.loop_try(next_page) last_url = driver.current_url print(driver.current_url) source = etree.HTML(driver.page_source) new_links = source.xpath(&quot;//div[@class=&#39;job-primary&#39;]/div[@class=&#39;info-primary&#39;]//a/@href&quot;) new_nodes = list(map(lambda x: &quot;https://www.zhipin.com{}&quot;.format(x), new_links)) nodes.extend(new_nodes) yield Request(url=&quot;https://www.zhipin.com&quot;, callback=self.parse_detail, meta={&#39;params&#39;: (nodes, driver, kw)},dont_filter=True) def parse_detail(self,response): nodes, driver, kw = response.meta.get(&quot;params&quot;) for node in nodes: print(node) driver.execute_script(&quot;window.open(&#39;%s&#39;)&quot; % node) time.sleep(2) driver.switch_to.window(driver.window_handles[1]) WebDriverWait(driver, timeout=10).until( EC.presence_of_element_located((By.XPATH, &quot;//div[@class=&#39;detail-content&#39;]&quot;)) ) html = etree.HTML(driver.page_source) driver.close() driver.switch_to.window(driver.window_handles[0]) item = JobItem() item[&#39;recruitment_position&#39;] = html.xpath( &quot;//div[@class=&#39;job-primary detail-box&#39;]/div[@class=&#39;info-primary&#39;]/div[@class=&#39;name&#39;]/h1/text()&quot;)[0] item[&#39;salary&#39;] = html.xpath( &quot;//div[@class=&#39;job-primary detail-box&#39;]/div[@class=&#39;info-primary&#39;]/div[@class=&#39;name&#39;]/span/text()&quot;)[ 0] item[&#39;keyword&#39;] = kw item[&#39;url&#39;] = node item[&#39;source&#39;] = &quot;BOSS直聘&quot; item[&#39;update_date&#39;] = html.xpath(&#39;//div[@class=&quot;sider-company&quot;]/p[last()]/text()&#39;)[0] item[&#39;company_name&#39;] = html.xpath(&#39;//a[@ka=&quot;job-detail-company_custompage&quot;]&#39;)[0].attrib.get(&#39;title&#39;).strip().replace(&quot;\n招聘&quot;,&quot;&quot;) # item[&#39;company_name&#39;] = html.xpath(&#39;//div[@class=&quot;level-list&quot;]/preceding-sibling::div[1]/text()&#39;)[0] item[&#39;work_experience&#39;] = html.xpath(&#39;//*[@class=&quot;job-primary detail-box&quot;]/div[2]/p/text()&#39;)[1] item[&#39;education_background&#39;] = html.xpath(&#39;//*[@class=&quot;job-primary detail-box&quot;]/div[2]/p/text()&#39;)[2] item[&#39;job_requirements&#39;] = &quot;&quot;.join( html.xpath(&#39;//div[@class=&quot;detail-content&quot;]/div[@class=&quot;job-sec&quot;]/div[@class=&quot;text&quot;]/text()&#39;)) item[&#39;company_info&#39;] = &quot;&quot;.join( html.xpath(&#39;//div[@class=&quot;job-sec company-info&quot;]//div[@class=&quot;text&quot;]/text()&#39;)) item[&#39;company_address&#39;] = html.xpath(&#39;//*[@class=&quot;location-address&quot;]/text()&#39;)[0] item[&#39;company_welfare&#39;] = &quot;,&quot;.join(html.xpath( &#39;//div[@class=&quot;job-banner&quot;]/div[@class=&quot;job-primary detail-box&quot;]/div[@class=&quot;info-primary&quot;]/div[@class=&quot;tag-container&quot;]/div[@class=&quot;job-tags&quot;]/text()&#39;)) item[&#39;id&#39;] = get_md5(node) item[&#39;crawl_date&#39;] = datetime.now().strftime(&quot;%Y-%m-%d&quot;) yield item def loop_try(self,next_page): try: next_page.click() except: self.loop_try(next_page)]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
        <tag>selenium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python基础]]></title>
    <url>%2F2019%2F10%2F09%2Fpython%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[python基础 比心import time import os import math [([(time.sleep(a), print(&quot;\033[91m&quot;+i,end=&quot;&quot;,flush=True)) for i in (&#39;\n&#39;.join([&#39;&#39;.join([(&#39; I love U&#39;[(x-y)%9]if((x*0.05)**2+(y*0.1)**2-1)**3-(x*0.05)**2*(y*0.1)**3&lt;=0 else&#39; &#39;)for x in range(-30,30)])for y in range(15,-15,-1)]))] ,time.sleep(1/math.log(ai+3)), os.system(&#39;clear&#39;) ) for (ai,a) in enumerate([0.001,*[ 0.00001 ]*99])]![enter description here](https://www.github.com/OneJane/blog/raw/master/小书匠/1570610415940.png]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于selenium爬取智联招聘及国家企业信用信息公示系统]]></title>
    <url>%2F2019%2F09%2F18%2F%E5%9F%BA%E4%BA%8Eselenium%E7%88%AC%E5%8F%96%E6%99%BA%E8%81%94%E6%8B%9B%E8%81%98%E5%8F%8A%E5%9B%BD%E5%AE%B6%E4%BC%81%E4%B8%9A%E4%BF%A1%E7%94%A8%E4%BF%A1%E6%81%AF%E5%85%AC%E7%A4%BA%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[突破加密混淆的js文件，IP封锁，验证码识别（滑动和语序点击并存），useragent检查，多重url拼接cookie 智联招聘 通过获取链接返回的json数据拿到新的页面，selenium进行解析 class ZhilianSpider(scrapy.Spider): name = &#39;zhilian&#39; allowed_domains = [&#39;zhaopin.com&#39;] start_urls = [&#39;https://sou.zhaopin.com/&#39;] driver = None chrome_options = webdriver.ChromeOptions() # proxy_url = get_random_proxy() # print(proxy_url + &quot;代理服务器正在爬取&quot;) # chrome_options.add_argument(&#39;--proxy-server=https://&#39; + proxy_url.strip()) prefs = { &#39;profile.default_content_setting_values&#39;: { &#39;images&#39;: 1, # 不加载图片 &quot;User-Agent&quot;: UserAgent().random, # 更换UA } } chrome_options.add_experimental_option(&quot;prefs&quot;, prefs) if platform.system() == &quot;Windows&quot;: driver = webdriver.Chrome(&#39;chromedriver.exe&#39;, chrome_options=chrome_options) elif platform.system() == &quot;Linux&quot;: chrome_options.add_argument(&quot;--headless&quot;) chrome_options.add_argument(&#39;--disable-gpu&#39;) chrome_options.add_argument(&#39;--no-sandbox&#39;) driver = webdriver.Chrome( executable_path=&quot;/usr/bin/chromedriver&quot;, chrome_options=chrome_options) wait = WebDriverWait(driver, 15) def start_requests(self): data = [&quot;游戏&quot;, &quot;期货&quot;, &quot;贷款&quot;] for kw in data: yield Request( url=&quot;https://fe-api.zhaopin.com/c/i/sou?start=0&amp;pageSize=90&amp;cityId=639&amp;salary=0,0&amp;workExperience=-1&amp;education=-1&amp;companyType=-1&amp;employmentType=-1&amp;jobWelfareTag=-1&amp;kw=&quot; + kw + &quot;&amp;kt=3&quot;, meta={&quot;kw&quot;: kw}, callback=self.parse_pages) # response获取meta def parse_pages(self, response): numtotal = json.loads(response.text)[&quot;data&quot;][&quot;count&quot;] kw = response.meta.get(&quot;kw&quot;, &quot;游戏&quot;) for i in range(0, numtotal // 90 + 1): url = &quot;https://fe-api.zhaopin.com/c/i/sou?start=&quot; + str( 90 * i) + &quot;&amp;pageSize=90&amp;cityId=639&amp;salary=0,0&amp;workExperience=-1&amp;education=-1&amp;companyType=-1&amp;employmentType=-1&amp;jobWelfareTag=-1&amp;kw=&quot; + kw + &quot;&amp;kt=3&quot; yield Request( url=url, meta={&quot;kw&quot;: kw}, callback=self.parse) # response获取meta def parse(self, response): job_list = json.loads(response.text)[&quot;data&quot;][&quot;results&quot;] for job in job_list: yield Request(url=job[&quot;positionURL&quot;], callback=self.parse_detail, meta={&#39;cookiejar&#39;: &#39;chrome&#39;, &#39;kw&#39;: response.meta.get(&quot;kw&quot;, &quot;&quot;)}) def parse_detail(self, response): print(response.url) self.driver.get(response.url) self.driver.refresh() time.sleep(2) self.driver.implicitly_wait(20) dom = etree.HTML(self.driver.page_source) item = JobItem() item[&#39;recruitment_position&#39;] = null_if(dom.xpath(&#39;//*[@class=&quot;summary-plane__title&quot;]&#39;)) item[&#39;salary&#39;] = null_if(dom.xpath(&#39;//*[@class=&quot;summary-plane__salary&quot;]&#39;)) item[&#39;company_name&#39;] = dom.xpath(&#39;//*[@class=&quot;company__title&quot;]&#39;)[0].text item[&#39;work_experience&#39;] = dom.xpath(&#39;//ul[@class=&quot;summary-plane__info&quot;]/li[2]&#39;)[0].text item[&#39;education_background&#39;] = dom.xpath(&#39;//ul[@class=&quot;summary-plane__info&quot;]/li[3]&#39;)[0].text item[&#39;job_requirements&#39;] = remove_html( etree.tostring(dom.xpath(&#39;//div[@class=&quot;describtion__detail-content&quot;]&#39;)[0], encoding=&quot;utf-8&quot;).decode( &#39;utf-8&#39;)) item[&#39;company_info&#39;] = null_if(dom.xpath(&#39;//div[@class=&quot;company__description&quot;]&#39;)) item[&#39;company_address&#39;] = remove_html( etree.tostring(dom.xpath(&#39;//span[@class=&quot;job-address__content-text&quot;]&#39;)[0], encoding=&quot;utf-8&quot;).decode( &#39;utf-8&#39;)) if len(dom.xpath(&#39;//div[@class=&quot;highlights__content&quot;]&#39;)): item[&#39;company_welfare&#39;] = remove_html(etree.tostring(dom.xpath(&#39;//div[@class=&quot;highlights__content&quot;]&#39;)[0], encoding=&quot;utf-8&quot;).decode(&#39;utf-8&#39;)) else: item[&#39;company_welfare&#39;] = &#39;无&#39; item[&#39;id&#39;] = get_md5(self.driver.current_url) item[&#39;keyword&#39;] = response.meta.get(&quot;kw&quot;, &quot;&quot;) item[&#39;url&#39;] = response.url item[&#39;crawl_date&#39;] = datetime.now().strftime(&quot;%Y-%m-%d&quot;) yield item 国家企业信用信息系统获取cookiecrack.py class Crack(object): &quot;&quot;&quot; 同一ip频繁使用： 出现正常200但是没有结果 第一次解密出来是错误的 &quot;&quot;&quot; def __init__(self, url, test_url): path = os.getcwd() with open(os.path.join(path, &quot;wc_js.js&quot;), encoding=&#39;utf-8&#39;) as f: wc_js = f.read() self.wc_js = execjs.compile(wc_js) self.url = url self.test_url = test_url # 固定user_agent,后台使用user-agent验证cookies, 之后的访问也需要使用这个 self.headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.96 Safari/537.36&#39; } def acquire_js(self): &quot;&quot;&quot; 不带cookies请求首页，获得返回的js :return:页面中的js,和set_cookies中的jsluid &quot;&quot;&quot; response = requests.get(self.url, headers=self.headers) if response.status_code == 521: return response.text, response.headers[&#39;Set-Cookie&#39;].split(&#39;=&#39;)[1].split(&#39;;&#39;)[0] else: print(response.text) print(self.headers) return None, None def first_decryption(self, first_js): &quot;&quot;&quot; 解密js,获得第二层加密的js :param first_js: :return: &quot;&quot;&quot; x = re.findall(&#39;var x=&quot;(.*?)&quot;&#39;, first_js)[0] y = re.findall(&#39;,y=&quot;(.*?)&quot;&#39;, first_js)[0] second_js = self.wc_js.call(&#39;once_js&#39;, x, y) # second_js = self.wc_js.call(&#39;get_js&#39;, x, y, z) return second_js def regex(self, js): regex = &quot;!*window\[.*?\]&quot; find = re.findall(regex, js) if find: for f in find: if &#39;!&#39; in f: if len(re.findall(&#39;!&#39;, f)) % 2 == 0: js = js.replace(f, &#39;false&#39;) else: js = js.replace(f, &#39;true&#39;) else: js = js.replace(f, &#39;undefined&#39;) js = js.replace(&#39;window.headless&#39;, &#39;undefined&#39;) return js def replace_url(self, js): # 替换1 # 取出两个变量名 _3d = re.findall(&quot;(var .{0,5}=)document\.createElement\(&#39;div&#39;\);&quot;, js) _2b = re.findall(&quot;(var .{0,5}=).{0,5}\.match\(/https\?:\\\/\\\//\)\[0\];&quot;, js) # 替换成要访问的url js = re.sub(&quot;var .{0,5}=document\.createElement\(&#39;div&#39;\);&quot;, _3d[0] + f&#39;&quot;{self.url.replace(&quot;http://&quot;, &quot;&quot;)}&quot;;&#39;, js) js = re.sub(&quot;_.{0,5}\.innerHTML=&#39;&lt;a href=.{0,25}&lt;/a&gt;&#39;;&quot;, &quot;&quot;, js) js = re.sub(&quot;_.{0,5}=.{0,5}\.firstChild\.href;&quot;, &quot;&quot;, js) js = re.sub(&quot;var .{0,5}=.{0,5}\.match\(/https\?:\\\/\\\//\)\[0\];&quot;, _2b[0] + &#39;&quot;http://&quot;;&#39;, js) js = re.sub(&quot;_.{0,5}=.{0,5}\.substr\(.{0,5}\.length\)\.toLowerCase\(\);&quot;, &quot;&quot;, js) return js def second_decryption(self, second_js): &quot;&quot;&quot; 把第二层js准换成本地可以运行的js !!!此处可能会出错!!! :param second_js: 第一次解密的js :return: __jsl_clearance的值 &quot;&quot;&quot; # 转义字符 js = second_js.replace(&#39;\\\\&#39;, &#39;\\&#39;) # 切割 js = &#39;cookie&#39; + js.split(&#39;document.cookie&#39;)[1] js = js.split(&#39;GMT;Path=/;&#39;)[0] + &quot;&#39;&quot; if re.findall(&quot;(var .{0,5}=)document\.createElement\(&#39;div&#39;\);&quot;, js): js = self.replace_url(js) # 替换可能出现的window js = self.regex(js) s = &quot;&quot;&quot; function cook() { %s return cookie } &quot;&quot;&quot; new_js = s % js ctx = execjs.compile(new_js) # 切割获得的__jsl_clearance jsl = ctx.call(&#39;cook&#39;) jsl = jsl.split(&#39;;&#39;)[0] jsl_clearance = jsl.split(&#39;=&#39;)[1] return jsl_clearance def test_cookies(self, jsluid, jsl_clearance): &quot;&quot;&quot; 带cookies访问,测试拿到的是否正确 :param jsluid:cookies中的参数 :param jsl_clearance: cookies中的参数 :return: &quot;&quot;&quot; headers = self.headers.copy() headers[&#39;Cookie&#39;] = f&#39;__jsluid_h={jsluid}; __jsl_clearance={jsl_clearance};&#39; response = requests.get(self.test_url, headers=headers) print(response.text) return response.status_code def run(self): while True: first_js, jsluid = self.acquire_js() second_js = self.first_decryption(first_js) try: jsl_clearance = self.second_decryption(second_js) except: # print(second_js) continue else: code = self.test_cookies(jsluid, jsl_clearance) if code == 200: return jsluid, jsl_clearance else: print(code) # print(second_js) continue if __name__ == &#39;__main__&#39;: # # 企业信息公示系统 url = &quot;http://www.gsxt.gov.cn/index.html&quot; test_url = &quot;http://www.gsxt.gov.cn/index.html&quot; # # 66代理 # url = &quot;http://www.66ip.cn/2.html&quot; # test_url = &quot;http://www.66ip.cn/2.html&quot; # # 公安部网站 # url = &#39;http://www.mps.gov.cn/&#39; # test_url = &#39;http://www.mps.gov.cn/&#39; ck = Crack(url, test_url) jsluid, jsl_clearance = ck.run() print(&#39;jsluid:&#39;, jsluid) print(&#39;jsl_clearance:&#39;, jsl_clearance) 利用超级鹰破解验证码class SearchResultParse(object): &#39;&#39;&#39;查询结果页解析 &#39;&#39;&#39; def __init__(self, pagesource, base_url, parse_rule): self.selector = etree.HTML(pagesource) self.url_list = [] self.base_url = base_url self.parse_rule = parse_rule[&#39;search_result_url&#39;] def search_result_parse(self): self.url_list = [self.base_url + i for i in self.selector.xpath(self.parse_rule)] return self.url_list class PageDetailParse(object): &#39;&#39;&#39;详情页解析 &#39;&#39;&#39; def __init__(self, pagesource, parse_rule): self.selector = etree.HTML(pagesource) self.parse_rule = parse_rule self.info_list = {} def search_result_parse(self, primary_info=None): if primary_info is None: primary_info = [] for i in self.parse_rule[&#39;primaryinfo&#39;]: primary_info.append( self.selector.xpath(i).replace(&quot;\n&quot;, &quot;&quot;).replace(&quot;\t&quot;, &quot;&quot;).replace(&quot;\r&quot;, &quot;&quot;).replace(&quot; &quot;, &quot;&quot;)) self.info_list[&#39;primary_info&#39;] = primary_info return self.info_list class CookieRequest(object): &#39;&#39;&#39;带cookie访问查询结果 &#39;&#39;&#39; def __init__(self, url_list=None): &#39;&#39;&#39;设置requests中的session的cookie &#39;&#39;&#39; self.url_list = url_list self.session = requests.Session() self.result = [] self.headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.96 Safari/537.36&#39; } def cookie_requests(self): &#39;&#39;&#39;带cookie依次访问各个查询结果 &#39;&#39;&#39; url = &quot;http://www.gsxt.gov.cn/index.html&quot; test_url = &quot;http://www.gsxt.gov.cn/corp-query-entprise-info-hot-search-list.html?province=100000&quot; ck = Crack(url, test_url) jsluid, jsl_clearance, JSESSIONID = ck.run() self.headers[&#39;Cookie&#39;] = f&#39;__jsluid_h={jsluid}; __jsl_clearance={jsl_clearance};JSESSIONID={JSESSIONID}&#39; for url in self.url_list: response = self.session.get(url=url, headers=self.headers) self.result.append(response.text) time.sleep(5) return self.result class MaxEnterError(Exception): &#39;&#39;&#39;输入关键字最大尝试次数 &#39;&#39;&#39; def __init__(self, ErrorInfo): super().__init__(self) # 初始化父类 self.errorinfo = ErrorInfo def __str__(self): return self.errorinfo class GtClickShot(object): def __init__(self, username, password,soft_id): &#39;&#39;&#39;初始化超级鹰 softid已固化到程序 args: username(str):超级鹰普通用户名 password(str):超级鹰密码 &#39;&#39;&#39; self.username = username self.password = md5(password.encode(&quot;utf-8&quot;)).hexdigest() self.soft_id = soft_id self.base_params = { &#39;user&#39;: self.username, &#39;pass2&#39;: self.password, &#39;softid&#39;: self.soft_id, } self.headers = { &#39;Connection&#39;: &#39;Keep-Alive&#39;, &#39;User-Agent&#39;: &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)&#39;, } def PostPic(self, im, codetype): &quot;&quot;&quot;发送图片至打码平台 args： im(Byte): 图片字节 codetype(str): 题目类型 参考 http://www.chaojiying.com/price.html return(json):返回打码信息，包含坐标信息，坐标信息用“|”隔开 &quot;&quot;&quot; params = { &#39;codetype&#39;: codetype, } params.update(self.base_params) files = {&#39;userfile&#39;: (&#39;ccc.jpg&#39;, im)} r = requests.post(&#39;http://upload.chaojiying.net/Upload/Processing.php&#39;, data=params, files=files, headers=self.headers) return r.json() def ReportError(self, im_id): &quot;&quot;&quot;识别错误返回题分 args： im_id(str):报错题目的图片ID return(str):报错反馈 &quot;&quot;&quot; params = { &#39;id&#39;: im_id, } params.update(self.base_params) r = requests.post(&#39;http://upload.chaojiying.net/Upload/ReportError.php&#39;, data=params, headers=self.headers) return r.json() class CorpSearch(object): def __init__(self, init_url, index_url, headers, max_click): &#39;&#39;&#39;初始化 args: init_url:初始化url,加速乐反爬JS要求访问目标网站前需先访问初始化url获取gt和challenge index_url:目标网站首页url headers：请求头信息 max_click：最大循环点击次数为了应对点击不灵敏，设置循环检查点击。 self.wait:默认条件等待最大时间 self.click_valitimes:点击验证次数，大于0时需返回题分，等于0时不需要 &#39;&#39;&#39; chrome_options = webdriver.ChromeOptions() prefs = { &#39;profile.default_content_setting_values&#39;: { &#39;images&#39;: 1, # 加载图片 &quot;User-Agent&quot;: UserAgent().random, # 更换UA } } chrome_options.add_experimental_option(&quot;prefs&quot;, prefs) self.init_url = init_url self.index_url = index_url if platform.system() == &quot;Windows&quot;: self.driver = webdriver.Chrome(&#39;chromedriver.exe&#39;, chrome_options=chrome_options) elif platform.system() == &quot;Linux&quot;: chrome_options.add_argument(&quot;--headless&quot;) chrome_options.add_argument(&#39;--disable-gpu&#39;) chrome_options.add_argument(&#39;--no-sandbox&#39;) self.driver = webdriver.Chrome( executable_path=&quot;/usr/bin/chromedriver&quot;, chrome_options=chrome_options) self.wait = WebDriverWait(self.driver, 50) self.max_entertimes = max_click self.click_valitimes = 0 self.action = ActionChains(self.driver) self.gt_shot = GtClickShot(&quot;zhaoys&quot;, &quot;501314&quot;,&quot;901554&quot;) self.options = webdriver.ChromeOptions() self.headers = headers for option in self.headers: self.options.add_argument(option) # 初始化页面，绕过过加速乐反爬，获取gt和challenge,并加载进入首页 def init(self): &#39;&#39;&#39; 请求初始化网站，并进入首页 &#39;&#39;&#39; self.driver.get(self.init_url) self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, &quot;body &gt; pre:nth-child(1)&quot;))) self.driver.get(self.index_url) # 加载首页，输入查询关键词，点击查询按钮 # 如果点击按钮失效,自动重新回车，并设定最大回车次数，一旦超过设定值，抛出异常，结束程序 def input_query(self, keyword): &#39;&#39;&#39;输入关键词进行查询 args: keyword:查询关键词 return: 仅用于方法返回 &#39;&#39;&#39; enter_word = self.wait.until(EC.presence_of_element_located((By.ID, &quot;keyword&quot;))) self.wait.until(EC.presence_of_element_located((By.ID, &quot;btn_query&quot;))) time.sleep(random.randint(8, 15) / 10) enter_word.send_keys(keyword) time.sleep(random.randint(5, 10) / 10) enter_word.send_keys(Keys.ENTER) while True: if self.max_entertimes == 0: raise MaxEnterError(&#39;---Out of max times on the search enter---&#39;) gt_panel = self.driver.find_element_by_css_selector(&quot;body &gt; div.geetest_panel.geetest_wind&quot;) style_value = gt_panel.value_of_css_property(&quot;display&quot;) if style_value.strip() == &quot;block&quot;: break else: enter_word.send_keys(Keys.ENTER) time.sleep(random.randint(1, 5) / 10) self.max_entertimes -= 1 return # 判断页面中是否包含某个元素，注意是class_name def is_element_exist(self, class_name): &#39;&#39;&#39;判断某个元素是否存在 args: class_name:元素class属性名称 return: 存在(True),不存在(False) &#39;&#39;&#39; try: self.driver.find_element_by_class_name(class_name) return True except: return False # 屏幕截图，并将截图内容读入内存，加速计算操作 def get_screenshot(self): &#39;&#39;&#39;屏幕截图 return: 返回截图 &#39;&#39;&#39; screenshot = self.driver.get_screenshot_as_png() screenshot = Image.open(BytesIO(screenshot)) return screenshot # 获取验证验证码图片的位置，用于裁图 def get_position(self, pos_img): &#39;&#39;&#39;验证图片的坐标尺寸信息 args: pos_img:验证码定位点元素 return: 验证码定位点的坐标信息，注意依次为：左底，左高，右高，右底 &#39;&#39;&#39; location = pos_img.location size = pos_img.size top, bottom, left, right = location[&#39;y&#39;], location[&#39;y&#39;] + size[&#39;height&#39;], location[&#39;x&#39;], location[&#39;x&#39;] + size[ &#39;width&#39;] return (left, top, right, bottom) # 对于滑块验证码，获取完整的和缺块的验证码图片截图 def get_slide_images(self): &#39;&#39;&#39;获取有缺口和没缺口的图片 &#39;&#39;&#39; canvas_img = self.wait.until( EC.presence_of_element_located((By.CSS_SELECTOR, &quot;.geetest_canvas_img.geetest_absolute &gt; div&quot;))) position = self.get_position(canvas_img) befor_screenshot = self.get_screenshot() befor_img = befor_screenshot.crop(position) befor_img.save(&quot;befor_click.png&quot;) btn_slide = self.wait.until(EC.presence_of_element_located((By.CLASS_NAME, &quot;geetest_slider_button&quot;))) self.action.click_and_hold(btn_slide).perform() after_screenshot = self.get_screenshot() after_img = after_screenshot.crop(position) after_img.save(&quot;after_click.png&quot;) # 获取缺口位置，计算滑动距离（灰度化，求差值，阈值去燥，计算缺口位置，计算滑动距离） def get_slide_distance(self): &#39;&#39;&#39;获取滑动距离 return: 返回滑动距离 &#39;&#39;&#39; befor_click_img = &quot;F:\\Anaconda3\\Lib\\captcha\\gt_validate\\befor_click.png&quot; after_click_path = &quot;F:\\Anaconda3\\Lib\\captcha\\gt_validate\\after_click.png&quot; befor_img = cv2.imread(befor_click_img) after_img = cv2.imread(after_click_path) befor_gray = cv2.cvtColor(befor_img, cv2.COLOR_BGR2GRAY) after_gray = cv2.cvtColor(after_img, cv2.COLOR_BGR2GRAY) img_diff = np.array(befor_gray) - np.array(after_gray) height, width = img_diff.shape for i in range(height): for j in range(width): if img_diff[i][j] &gt; 245 or img_diff[i][j] &lt; 60: img_diff[i][j] = 0 start_position = random.choice([4, 5, 6]) reshape_img = img_diff.T sum_color = list(map(lambda x: sum(x), reshape_img)) for i in range(1, len(sum_color)): if sum_color[i] &gt; 1000 and i &gt; 60: end_position = i break slide_distance = end_position - start_position return slide_distance # 模拟鼠标轨迹，按照开始慢加速（2），中间快加速（5），后面慢加速（2），最后慢减速的方式（1） # 返回值是x值与Y值坐标以及sleep时间截点，起始中间最后都要sleep def get_track(self, distance, track_list=None): &#39;&#39;&#39;获取滑动轨迹 args: distance:滑动距离 kargs: Track_list:滑动轨迹，初始化为空 return: 滑动轨迹，断点位置(2处) &#39;&#39;&#39; if track_list is None: track_list = [] base = distance / 10 x1 = round(base * 2) x2 = round(base * 5) x3 = x1 x4 = distance - x1 - x2 - x3 ynoise_num = random.randint(5, 10) y1 = [random.randint(-2, 2) for _ in range(ynoise_num)] yrdm = list(set(random.choice(range(distance)) for _ in range(ynoise_num))) x = [1] * distance y = [0] * distance for i, j in enumerate(yrdm): y[j] = y1[i] t1 = sorted([random.randint(8, 13) / 1000 for _ in range(x1)], reverse=True) t2 = sorted([random.randint(1, 8) / 1000 for _ in range(x2)], reverse=True) t3 = sorted([random.randint(8, 13) / 1000 for _ in range(x3)], reverse=True) t4 = sorted([random.randint(12, 20) / 1000 for _ in range(x4)]) t = t1 + t2 + t3 + t4 for i in (zip(x, y, t)): track_list.append(i) return (track_list, x1 + x2, x1 + x2 + x3) # 对于点击验证码，获取验证码的校验文字和待点击图片截图,以及验证码弹框元素 def get_click_images(self): &#39;&#39;&#39;获取需点击的图片 return: 需点击坐标的图片， 提示图片(用于调试打码时的计算点击次数)， 验证码图片定位元素(用于定位鼠标位置并计算相对坐标) &#39;&#39;&#39; click_img_element = self.wait.until(EC.presence_of_element_located((By.CLASS_NAME, &quot;geetest_widget&quot;))) self.wait.until(EC.presence_of_element_located((By.CLASS_NAME, &quot;geetest_item_img&quot;))) time.sleep(random.randint(1, 5) / 10) click_position = self.get_position(click_img_element) all_screenshot = self.get_screenshot() click_img = all_screenshot.crop(click_position) click_img.save(&quot;click_img.png&quot;) tip_img = self.wait.until(EC.presence_of_element_located((By.CLASS_NAME, &quot;geetest_tip_img&quot;))) tip_position = self.get_position(tip_img) tip_img = all_screenshot.crop(tip_position) tip_img.save(&quot;tip_img.png&quot;) return (click_img, tip_img, click_img_element) # 计算要点击的字符数量，灰度化，反向二值化,转置，沿X坐标对Y求和，判断分割点数量，判断字符数量 def cal_char_num(self, char_img_path): &#39;&#39;&#39;计算需点击的字符数量 args: char_img_path:提示图片的存储路径 return: 点击次数 &#39;&#39;&#39; flag = 0 origin_img = cv2.imread(char_img_path) gray_img = cv2.cvtColor(origin_img, cv2.COLOR_BGR2GRAY) ret, thresh1 = cv2.threshold(gray_img, 127, 255, cv2.THRESH_BINARY_INV) transpos_img = np.array(thresh1).T result = list(map(lambda x: sum(x), transpos_img)) for i in range(len(result) - 3): if result[i] == 0 and result[i + 1] == 0 and result[i + 2] &gt; 0: flag += 1 return flag # 返回验证码字符的坐标，每个点击点的坐标,并转化为整数坐标 def char_absolute_coord(self, img, num, coord=None): &#39;&#39;&#39;调试用，点击验证码图片返回整数值坐标 args: img:验证码图片 num：点击次数 kargs: coord:验证码字符坐标 return: 字符坐标 &#39;&#39;&#39; if coord is None: coord = [] img = Image.open(img) plt.imshow(img) points = plt.ginput(num) plt.close() for i in points: x_co, y_co = i coord.append((round(x_co), round(y_co))) return coord # 返回从起点开始依次到每个点击文字的相对位置，形式为[(xoffset,yoffset),(),(),...] def get_offset_coord(self, absolute_coord, click_track=None): &#39;&#39;&#39;获取相邻点击字符的相对坐标，用于鼠标移动点击 args: absolute_coord：验证码字符的绝对坐标 kargs: click_track:每个需点击字符间的相对坐标或位移 return: 相对坐标或位移 &#39;&#39;&#39; if click_track is None: click_track = [] for i, j in enumerate(absolute_coord): if i == 0: click_track.append(j) else: click_track.append((j[0] - absolute_coord[i - 1][0], j[1] - absolute_coord[i - 1][1])) return click_track # 验证点击验证码,获取验证码数量，人工点击，按照计算的坐标相对偏移位置，依次点击文字进行验证 # 通过打码平台，将验证码图片发送后返回坐标信息，通过超级鹰打码平台 def click_captcha_validate(self): &#39;&#39;&#39;根据打码平台返回的坐标进行验证 return: 仅仅用于方法返回 &#39;&#39;&#39; click_img, tip_img, click_img_element = self.get_click_images() bytes_array = BytesIO() click_img.save(bytes_array, format=&quot;PNG&quot;) coord_result = self.gt_shot.PostPic(bytes_array.getvalue(), &quot;9005&quot;) print(coord_result) groups = coord_result.get(&quot;pic_str&quot;).split(&#39;|&#39;) if groups == &quot;&quot;: raise RuntimeError(&quot;打码超时&quot;) pic_id = coord_result.get(&quot;pic_id&quot;) points = [[int(num) for num in group.split(&#39;,&#39;)] for group in groups] # tip_img_path=&quot;D:\\Anaconda3\\Lib\\captcha\\gt_validate\\tip_img.png&quot; # click_img_path=&quot;D:\\Anaconda3\\Lib\\captcha\\gt_validate\\click_img.png&quot; # num=self.cal_char_num(tip_img_path) # points=self.char_absolute_coord(click_img_path,num) mouse_track = self.get_offset_coord(points) print(mouse_track) self.action.move_to_element_with_offset(click_img_element, 0, 0) for position in mouse_track: self.action.move_by_offset(position[0], position[1]) self.action.click() self.action.pause(random.randint(3, 7) / 10) self.action.perform() time.sleep(random.randint(4, 6) / 10) click_submit_btn = self.wait.until(EC.presence_of_element_located((By.CLASS_NAME, &#39;geetest_commit_tip&#39;))) click_submit_btn.click() self.action.reset_actions() self.valide_process(pic_id=pic_id) return # 验证滑动验证码，获取滑动距离和滑动轨迹，分别在起始，中间，结束时随机停顿 def slide_captcha_validate(self): &#39;&#39;&#39;滑动验证码验证 return: 仅仅用于方法返回 &#39;&#39;&#39; self.get_slide_images() distance = self.get_slide_distance() track, p1, p2 = self.get_track(distance) time.sleep(random.randint(3, 7) / 10) for i, j in enumerate(track): if i == p1 or i == p2: time.sleep(random.randint(3, 7) / 10) self.action.move_by_offset(j[0], j[1]) time.sleep(j[2]) time.sleep(random.randint(3, 7) / 10) self.action.release() self.valide_process() return # 验证是否成功破解，设置重启机制 # 超过最大验证次数需点击“点击此处重试” def valide_process(self, pic_id=None): &#39;&#39;&#39;验证过程 1&gt;判断极验弹框消失且查询结果框出现，验证成功，结束验证； 2&gt;第一步验证失败，超时； 3&gt;超时原因：极验验证框没消失(跳转至第4步)或查询结果框没出现(跳转至第6步)； 4&gt;极验验证框没消失，检验是否超过最大验证次数，如果是，需点击重试，跳至第7步，如果不是，跳至第5步； 5&gt;如果不是，判断验证类型，调用响应验证方法，跳至第1步； 6&gt;如果查询结果框没出现，直接退出关闭浏览器； 7&gt;点击重试时，如果是空白响应则退出浏览器，或者判断验证类型，调用响应验证方法，跳至第1步。 args: cap_type:验证码类型 pic_id:点击类验证码图片id return: 要么验证成功，要么退出浏览器 &#39;&#39;&#39; try: WebDriverWait(self.driver, 3).until_not( EC.visibility_of_element_located((By.CSS_SELECTOR, &quot;body &gt; div.geetest_panel&quot;))) WebDriverWait(self.driver, 10).until(EC.visibility_of_element_located((By.ID, &quot;advs&quot;))) print(&quot;Validate Successful&quot;) return except TimeoutException: try: gt_panel_error = self.driver.find_element_by_css_selector( &quot;body &gt; div.geetest_panel.geetest_wind &gt; div.geetest_panel_box &gt; div.geetest_panel_error&quot;) error_display = gt_panel_error.value_of_css_property(&quot;display&quot;) if error_display.strip() == &quot;block&quot;: gt_panel_error_content = self.driver.find_element_by_css_selector( &quot;.geetest_panel_error &gt; div.geetest_panel_error_content&quot;) self.action.move_to_element(gt_panel_error_content).click().perform() self.action.reset_actions() try: WebDriverWait(self.driver, 3).until_not( EC.visibility_of_element_located((By.CSS_SELECTOR, &quot;body &gt; div.geetest_panel&quot;))) WebDriverWait(self.driver, 10).until(lambda x: x.find_element_by_id(&#39;advs&#39;).is_displayed()) print(&quot;Validate Successful&quot;) return except TimeoutException: self.slide_orclick_validate(pic_id) else: self.slide_orclick_validate(pic_id) except: print(&#39;error occured&#39;) return # 判断是执行点击还是滑块 def slide_orclick_validate(self, pic_id=None): &#39;&#39;&#39;判断下一步是选择滑动验证还是点击验证还是退出浏览器 args: pic_id:点击类验证码图片id return: 要么滑动验证，要么点击验证，要么None &#39;&#39;&#39; try: WebDriverWait(self.driver, 3).until(EC.presence_of_element_located((By.CLASS_NAME, &quot;geetest_close&quot;))) print(&#39;Validate Failed,retry again&#39;) if self.is_element_exist(&quot;geetest_canvas_img&quot;): print(&#39;captcha type is slide&#39;) return self.slide_captcha_validate() else: print(&#39;captcha type is click&#39;) if self.click_valitimes &gt; 0: self.gt_shot.ReportError(pic_id) self.click_valitimes += 1 return self.click_captcha_validate() except: print(&quot;Directly no click or slide validate&quot;) return # 带cookie切换至首页继续检索 def switch_hmpg(self): &#39;&#39;&#39;由结果页切换至首页 return: 用于方法返回 &#39;&#39;&#39; self.wait.until(EC.presence_of_element_located((By.ID, &quot;advs&quot;))) hmpg_btn = self.driver.find_element_by_css_selector( &quot;body &gt; div.container &gt; div.header_box &gt; div &gt; div &gt; a:nth-child(1)&quot;) self.action.move_to_element(hmpg_btn).click().perform() self.action.reset_actions() self.wait.until(lambda x: x.find_element_by_id(&#39;btn_query&#39;).is_displayed()) return # 通过index界面或者点击首页继续检索时的爬取步骤 def main(self, keyword, start_pg=None): &#39;&#39;&#39;操作主程序 args: keyword:查询关键词 kargs: start_pg:是否需要初始化访问加速乐，默认要 &#39;&#39;&#39; if start_pg == &quot;homepage&quot;: self.switch_hmpg() else: self.init() self.input_query(keyword) self.slide_orclick_validate() # 保存cookie和检索结果，用于requests及详情解析 def to_dict(self): &#39;&#39;&#39;保存cookie（用于requests请求及详情解析）和查询结果 args: cookie_name:cookie文件名称 &#39;&#39;&#39; htmlpage = self.driver.page_source return { &#39;page&#39;: htmlpage } if __name__ == &#39;__main__&#39;: init_url = &quot;http://www.gsxt.gov.cn/SearchItemCaptcha&quot; index_url = &quot;http://www.gsxt.gov.cn/index.html&quot; base_url = &#39;http://www.gsxt.gov.cn&#39; result_parse_rule = {&#39;search_result_url&#39;: &#39;//*[@id=&quot;advs&quot;]/div/div[2]/a/@href&#39;} detail_parse_rule = { &#39;primaryinfo&#39;: [&#39;string(//*[@id=&quot;primaryInfo&quot;]/div/div[@class=&quot;overview&quot;]/dl[{}])&#39;.format(i) for i in range(15)], } max_click = 10 chm_headers = [&#39;Host=&quot;www.gsxt.gov.cn&quot;&#39;, &#39;Connection=&quot;keep-alive&quot;&#39;, &#39;User-Agent=&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36&quot;&#39;, &#39;Upgrade-Insecure-Requests=1&#39;, &#39;Accept=&quot;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&quot;&#39;, &#39;Accept-Encoding=&quot;gzip, deflate&quot;&#39;, &#39;Accept-Language=&quot;zh-CN,zh;q=0.9&quot;&#39;] search = CorpSearch(init_url, index_url, chm_headers, max_click) search.main(&quot;腾讯&quot;) cookie_html = search.to_dict() search_result = SearchResultParse(cookie_html[&#39;page&#39;], base_url, result_parse_rule) url_list = search_result.search_result_parse() detail_request = CookieRequest(url_list=url_list) detail_result = detail_request.cookie_requests() for pg in detail_result: pg_detail = PageDetailParse(pg, detail_parse_rule) detail = pg_detail.search_result_parse() m = re.findall(r&#39;\[(.*?)\]&#39;, str(detail)) info_list = m[0].replace(&#39;\&#39;&#39;, &#39;&#39;).split(&#39;, &#39;) sql = &quot;insert into company(code,name,type,start,end,) values(%s,%s,%s,%s.%s)&quot; count, rt_list = MysqlConnection.execute_sql(sql, (info_list[0],info_list[1],info_list[2],info_list[3]))爬虫实现class EnterPriseSpider(scrapy.Spider): name = &#39;enterprise&#39; allowed_domains = [&#39;gsxt.gov.cn&#39;] start_urls = [&#39;http://www.gsxt.gov.cn/index.html&#39;] def __init__(self, word=None, *args, **kwargs): super(eval(self.__class__.__name__), self).__init__(*args, **kwargs) self.word = word def start_requests(self): init_url = &quot;http://www.gsxt.gov.cn/SearchItemCaptcha&quot; index_url = &quot;http://www.gsxt.gov.cn/index.html&quot; base_url = &#39;http://www.gsxt.gov.cn&#39; result_parse_rule = {&#39;search_result_url&#39;: &#39;//*[@id=&quot;advs&quot;]/div/div[2]/a/@href&#39;} max_click = 10 chm_headers = [&#39;Host=&quot;www.gsxt.gov.cn&quot;&#39;, &#39;Connection=&quot;keep-alive&quot;&#39;, &#39;User-Agent=&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36&quot;&#39;, &#39;Upgrade-Insecure-Requests=1&#39;, &#39;Accept=&quot;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&quot;&#39;, &#39;Accept-Encoding=&quot;gzip, deflate&quot;&#39;, &#39;Accept-Language=&quot;zh-CN,zh;q=0.9&quot;&#39;] search = CorpSearch(init_url, index_url, chm_headers, max_click) search.main(self.word) cookie_html = search.to_dict() search_result = SearchResultParse(cookie_html[&#39;page&#39;], base_url, result_parse_rule) url_list = search_result.search_result_parse() yield Request(url=&quot;https://www.baidu.com/&quot;,callback=self.parse, meta={&#39;url_list&#39;: url_list}) def parse(self, response): detail_parse_rule = { &#39;primaryinfo&#39;: [&#39;string(//*[@id=&quot;primaryInfo&quot;]/div/div[@class=&quot;overview&quot;]/dl[{}])&#39;.format(i) for i in range(15)], } url_list = response.meta.get(&quot;url_list&quot;, &quot;&quot;) detail_request = CookieRequest(url_list=url_list) detail_result = detail_request.cookie_requests() for pg in detail_result: pg_detail = PageDetailParse(pg, detail_parse_rule) detail = pg_detail.search_result_parse() m = re.findall(r&#39;\[(.*?)\]&#39;, str(detail)) info_list = m[0].replace(&#39;\&#39;&#39;, &#39;&#39;).split(&#39;, &#39;) item = CompanyItem() item[&#39;name&#39;] = company_info(info_list, &quot;企业名称：&quot;) item[&#39;code&#39;] = company_info(info_list, &quot;统一社会信用代码：&quot;) item[&#39;type&#39;] = company_info(info_list, &quot;类型：&quot;) start = company_info(info_list, &quot;营业期限自：&quot;) partner_start = company_info(info_list, &quot;合伙期限自：&quot;) item[&#39;start&#39;] = start if &quot;无&quot; == partner_start else partner_start end = company_info(info_list, &quot;合伙期限自：&quot;) partner_end = company_info(info_list, &quot;合伙期限至：&quot;) item[&#39;end&#39;] = end if &quot;无&quot; == partner_end else partner_end item[&#39;capital&#39;] = company_info(info_list, &quot;注册资本：&quot;) item[&#39;owner&#39;] = company_info(info_list, &quot;法定代表人：&quot;) item[&#39;establish&#39;] = company_info(info_list, &quot;成立日期：&quot;) item[&#39;registration&#39;] = company_info(info_list, &quot;登记机关：&quot;) item[&#39;check&#39;] = company_info(info_list, &quot;核准日期：&quot;) item[&#39;status&#39;] = company_info(info_list, &quot;登记状态：&quot;) residence = company_info(info_list, &quot;住所：&quot;) premises = company_info(info_list, &quot;主要经营场所：&quot;) item[&#39;address&#39;] = residence if &quot;无&quot; == premises else premises item[&#39;scope&#39;] = company_info(info_list, &quot;经营范围：&quot;) item[&#39;partner&#39;] = company_info(info_list, &quot;执行事务合伙人:&quot;) yield item main.pyfrom scrapy import cmdline from scrapy.cmdline import execute import sys import os sys.path.append(os.path.dirname(os.path.abspath(__file__))) # execute([&quot;scrapy&quot;, &quot;crawl&quot;, &quot;enterprise&quot;,&quot;-a&quot;,&quot;word=百度&quot;]) execute([&quot;scrapy&quot;, &quot;crawl&quot;, &quot;zhilian&quot;]) 安装chromewget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl https://intoli.com/install-google-chrome.sh | bash ldd /opt/google/chrome/chrome | grep &quot;not found&quot;]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
        <tag>selenium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贪心nlp基础]]></title>
    <url>%2F2019%2F09%2F04%2F%E8%B4%AA%E5%BF%83nlp%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[贪心nlp基础 机器翻译 分词-&gt;翻译-&gt;排列组合-&gt;语言模型获取概率-&gt;获得结果 通过维特比算法将Translation Model和Language Model基于动态规划解决时间复杂度O(2^n)-&gt;O(n^p) 联合概率计算过程，马尔科夫假设N-gram 自然语言处理四个维度 声音(Phonetics): 语音识别 单词(Morphology): 分词，pos(part of speech词性标注)，NER(named entity recognition命名实体识别) 句子结构(Syntax):句法分析(Parsing依赖语言，CYK基于DP)，依存分析(dependency parsing词与词之间依赖关系) 语义分析(Sematic):NLU 时间复杂度和空间复杂度int a = 0, b = 0; for (i = 0; i &lt; N; i++) { # O(N)+O(N)=2*O(N)=O(N) a = a + rand();# N*1个操作 = O(N) b = b + rand();# N*1个操作 = O(N) } for (j = 0; j &lt; N/2; j++) { b = b + rand(); # N/2 *1个操作 = 1/2*O(N)=O(N) } 时间复杂度：O(N)空间复杂度： 2个单位的内存空间 = O(1) # constant space complexity int a = 0; i,j for (i = 0; i &lt; N; i++) { for (j = N; j &gt; i; j--) { a = a + i + j; } } i=0: j=N...1 N i=1: j=N...2 N-1 i=2: j=N...3 N-2 i=N-1: j=N 1 total = 1+2+3,...+N = N*(N+1)/2 = N*N/2 + N/2 = 1/2*O(N^2) + 1/2*O(N) = O(N^2) + O(N) = O(N^2) 时间复杂度：O(N^2);空间复杂度:3个单位的内存空间，不随程序变化而改变内存，O(1) int a = 0, i = N; while (i &gt; 0) { a += i; # 1个操作 i /= 2; #1个操作 } N = 40; i=40 i=20 2 i=10 2 i=5 2 i=2 2 i=1 2 i=0 2 terminate C* O(N) = O(N) if only if C跟N没有相关性 2*6=2*log(N) = 2* O(log N) = O(log N) 时间复杂度： O(log N) int i, j, k = 0; for (i = n / 2; i &lt;= n; i++) { for (j = 2; j &lt;= n; j = j * 2) { k = k + n / 2; } } 时间复杂度：O(n*log n) 当说算法X的效率要高于Y时指的是？ 假设存在一个足够大的数M，当n&gt;M时，我们可以保证X的实际效率要优于Y的实际效率n，比较时间复杂度O(1) O(log n) o(n) o(nlog n): quicksort, heapsort, mergesort o(n^2) o(n^3).. o(2^n) o(3^n)o(log n): 寻找一个element (从tree,heap), binary search 归并排序复杂度分析利用主定理公式 T(n) = T(n-2) + T(n-1) def fib(n): # base case if n &lt; 3: return 1 return fib(n-2)+fib(n-1) print (fib(50)) Fibonanci number (斐波那契数)计算时间复杂度和空间复杂度 import numpy as np def fib(n): tmp = np.zeros(n) tmp[0] = 1 tmp[1] = 1 for i in range(2,n): tmp[i] = tmp[i-2]+tmp[i-1] return tmp[n-1] # 时间复杂度O(N)不再是O(2^n) def fib(n): a,b=1,1 c =0 for i in range(2,n): c = a + b a = b b = c return c # 空间复杂度4，时间复杂度一样 通过DP动态规划改进时间复杂度和空间复杂度 搭建一个智能客服系统相似度匹配:正则(无数据),字符串相似度(训练数据)复杂度太高，通过倒排表作为过滤器 nlp系统流程 分词贪心算法缺点： 无法细分(细分更有解) 局部最优 效率低下(依赖于max_len) 歧义(不能考虑语义)语言模型原理：首先输入语句，根据词典生成所有可能的分词情况，通过语言模型计算每一个分词后的结果概率，选择概率最高分词语句。复杂度太高维特比算法(DP)根据词典生成所有可能的分词情况，通过语言模型计算每一个分词后的结果概率 两步合为一步。拼写纠错 错别字 输入语法有误过滤词通常会过滤掉停用词,出现频率很低的词汇。词形还原/词干提取 词形还原（lemmatization），是把一个任何形式的语言词汇还原为一般形式（能表达完整语义），而词干提取（stemming）是抽取词的词干或词根形式（不一定能够表达完整语义）。 文本表示Boolean Representation Count Based Representation 并非出现次数越多越重要，并非出现次数越少越不重要 Tf-idf Representation以上Representation均属于one-hot representation,无法表达单词之间的语义相似度。 Distributed Representation词向量 长度不依赖于词典，每个位置都有具体数值，通过模型训练得到分布式词向量，可以用来形容词之间的相似度，word2vec，某种意义上可以理解成单词的意思。分布式表示方法解决了one-hot的稀疏问题(量级大且稀疏)，而分布式表示方法可以自由定义向量位数表达句子或向量，并计算出单词之间的相似度，并可视化在空间。 句子向量 Sentence Similarity计算文本相似度 欧式距离 余弦相似度]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenCV与TensorFlow手写数字刷脸识别]]></title>
    <url>%2F2019%2F09%2F03%2FOpenCV%E4%B8%8ETensorFlow%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E5%88%B7%E8%84%B8%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[OpenCV与TensorFlow手写数字刷脸识别 http://yann.lecun.com/exdb/mnist/ 手写数字识别KNN最近领域# 1 重要 # 2 KNN最近领域 CNN卷积神经网络 2种 # 3 样本 # 4 旧瓶装新酒 ：数字识别的不同 # 4.1 网络 4。2 每一级 4.3 先原理 后代码 # 本质：knn test 样本 K个 max4 3个1 -》1 # 1 load Data 1.1 随机数 1.2 4组 训练 测试 （图片 和 标签） # 2 knn test train distance 5*500 = 2500 784=28*28 # 3 knn k个最近的图片5 500 1-》500train （4） # 4 k个最近的图片-&gt; parse centent label # 5 label -》 数字 p9 测试图片-》数据 # 6 检测概率统计 import tensorflow as tf import numpy as np import random from tensorflow.examples.tutorials.mnist import input_data # load data 2 one_hot : 1 0000 1 fileName mnist = input_data.read_data_sets(&#39;MNIST_data&#39;,one_hot=True) # 属性设置 trainNum = 55000 testNum = 10000 trainSize = 500 testSize = 5 k = 4 # data 分解 1 trainSize 2范围0-trainNum 3 replace=False trainIndex = np.random.choice(trainNum,trainSize,replace=False) testIndex = np.random.choice(testNum,testSize,replace=False) trainData = mnist.train.images[trainIndex]# 训练图片 trainLabel = mnist.train.labels[trainIndex]# 训练标签 testData = mnist.test.images[testIndex] testLabel = mnist.test.labels[testIndex] # 28*28 = 784 print(&#39;trainData.shape=&#39;,trainData.shape)#500*784 1 图片个数 2 784? print(&#39;trainLabel.shape=&#39;,trainLabel.shape)#500*10 print(&#39;testData.shape=&#39;,testData.shape)#5*784 print(&#39;testLabel.shape=&#39;,testLabel.shape)#5*10 print(&#39;testLabel=&#39;,testLabel)# 4 :testData [0] 3:testData[1] 6 # tf input 784-&gt;image trainDataInput = tf.placeholder(shape=[None,784],dtype=tf.float32) trainLabelInput = tf.placeholder(shape=[None,10],dtype=tf.float32) testDataInput = tf.placeholder(shape=[None,784],dtype=tf.float32) testLabelInput = tf.placeholder(shape=[None,10],dtype=tf.float32) #knn distance 5*785. 5*1*784 # 5测试图片 500训练图片 784 (3D) 2500*784 f1 = tf.expand_dims(testDataInput,1) # 维度扩展 f2 = tf.subtract(trainDataInput,f1)# 784 sum(784) f3 = tf.reduce_sum(tf.abs(f2),reduction_indices=2)# 完成数据累加 784 abs # 5*500 f4 = tf.negative(f3)# 取反 f5,f6 = tf.nn.top_k(f4,k=4) # 选取f4 最大的四个值 # f3 最小的四个值 # f6 index-&gt;trainLabelInput f7 = tf.gather(trainLabelInput,f6) # f8 num reduce_sum reduction_indices=1 &#39;竖直&#39; f8 = tf.reduce_sum(f7,reduction_indices=1) # tf.argmax 选取在某一个最大的值 index f9 = tf.argmax(f8,dimension=1) # f9 -&gt; test5 image -&gt; 5 num with tf.Session() as sess: # f1 &lt;- testData 5张图片 p1 = sess.run(f1,feed_dict={testDataInput:testData[0:5]}) print(&#39;p1=&#39;,p1.shape)# p1= (5, 1, 784) p2 = sess.run(f2,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) print(&#39;p2=&#39;,p2.shape)#p2= (5, 500, 784) (1,100) p3 = sess.run(f3,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) print(&#39;p3=&#39;,p3.shape)#p3= (5, 500) print(&#39;p3[0,0]=&#39;,p3[0,0]) #130.451 knn distance p3[0,0]= 155.812 p4 = sess.run(f4,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) print(&#39;p4=&#39;,p4.shape) print(&#39;p4[0,0]&#39;,p4[0,0]) p5,p6 = sess.run((f5,f6),feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) #p5= (5, 4) 每一张测试图片（5张）分别对应4张最近训练图片 #p6= (5, 4) print(&#39;p5=&#39;,p5.shape) print(&#39;p6=&#39;,p6.shape) print(&#39;p5[0,0]&#39;,p5[0]) print(&#39;p6[0,0]&#39;,p6[0])# p6 index p7 = sess.run(f7,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel}) print(&#39;p7=&#39;,p7.shape)#p7= (5, 4, 10) print(&#39;p7[]&#39;,p7) p8 = sess.run(f8,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel}) print(&#39;p8=&#39;,p8.shape) print(&#39;p8[]=&#39;,p8) p9 = sess.run(f9,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel}) print(&#39;p9=&#39;,p9.shape) print(&#39;p9[]=&#39;,p9) p10 = np.argmax(testLabel[0:5],axis=1) print(&#39;p10[]=&#39;,p10) j = 0 for i in range(0,5): if p10[i] == p9[i]: j = j+1 print(&#39;ac=&#39;,j*100/5) CNN卷积神经网络#cnn : 1 卷积 # ABC # A: 激励函数+矩阵 乘法加法 # A CNN : pool（激励函数+矩阵 卷积 加法） # C：激励函数+矩阵 乘法加法（A-》B） # C：激励函数+矩阵 乘法加法（A-》B） + softmax（矩阵 乘法加法） # loss：tf.reduce_mean(tf.square(y-layer2)) # loss：code #1 import import tensorflow as tf import numpy as np from tensorflow.examples.tutorials.mnist import input_data # 2 load data mnist = input_data.read_data_sets(&#39;MNIST_data&#39;,one_hot = True) # 3 input imageInput = tf.placeholder(tf.float32,[None,784]) # 28*28 labeInput = tf.placeholder(tf.float32,[None,10]) # 10 列数 # 4 data reshape # [None,784]-&gt;M*28*28*1 2D-&gt;4D 28*28 wh 1 channel imageInputReshape = tf.reshape(imageInput,[-1,28,28,1]) # 5 卷积 w0 : 卷积内核 5*5 out:32 in:1 w0 = tf.Variable(tf.truncated_normal([5,5,1,32],stddev = 0.1)) b0 = tf.Variable(tf.constant(0.1,shape=[32])) # 6 # layer1：激励函数+卷积运算 # imageInputReshape : M*28*28*1 w0:5,5,1,32 layer1 = tf.nn.relu(tf.nn.conv2d(imageInputReshape,w0,strides=[1,1,1,1],padding=&#39;SAME&#39;)+b0) # M*28*28*32 # pool 采样 数据量减少很多M*28*28*32 =&gt; M*7*7*32 layer1_pool = tf.nn.max_pool(layer1,ksize=[1,4,4,1],strides=[1,4,4,1],padding=&#39;SAME&#39;) # [1 2 3 4]-&gt;[4] # 7 layer2 out : 激励函数+乘加运算： softmax（激励函数 + 乘加运算） # [7*7*32,1024] w1 = tf.Variable(tf.truncated_normal([7*7*32,1024],stddev=0.1)) b1 = tf.Variable(tf.constant(0.1,shape=[1024])) h_reshape = tf.reshape(layer1_pool,[-1,7*7*32])# M*7*7*32 -&gt; N*N1 # [N*7*7*32] [7*7*32,1024] = N*1024 h1 = tf.nn.relu(tf.matmul(h_reshape,w1)+b1) # 7.1 softMax w2 = tf.Variable(tf.truncated_normal([1024,10],stddev=0.1)) b2 = tf.Variable(tf.constant(0.1,shape=[10])) pred = tf.nn.softmax(tf.matmul(h1,w2)+b2)# N*1024 1024*10 = N*10 # N*10( 概率 )N1【0.1 0.2 0.4 0.1 0.2 。。。】 # label。 【0 0 0 0 1 0 0 0.。。】 loss0 = labeInput*tf.log(pred) loss1 = 0 # 7.2 for m in range(0,500):# test 100 for n in range(0,10): loss1 = loss1 - loss0[m,n] loss = loss1/500 # 8 train train = tf.train.GradientDescentOptimizer(0.01).minimize(loss) # 9 run with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for i in range(100): images,labels = mnist.train.next_batch(500) sess.run(train,feed_dict={imageInput:images,labeInput:labels}) pred_test = sess.run(pred,feed_dict={imageInput:mnist.test.images,labeInput:labels}) acc = tf.equal(tf.arg_max(pred_test,1),tf.arg_max(mnist.test.labels,1)) acc_float = tf.reduce_mean(tf.cast(acc,tf.float32)) acc_result = sess.run(acc_float,feed_dict={imageInput:mnist.test.images,labeInput:mnist.test.labels}) print(acc_result) 刷脸识别爬虫获取样本import urllib from bs4 import BeautifulSoup html = urllib.request.urlopen( &#39;https://www.duitang.com/album/?id=69001447&#39;).read() # parse url data 1 html 2 &#39;html.parser&#39; 3 &#39;utf-8&#39; soup = BeautifulSoup(html, &#39;html.parser&#39;, from_encoding=&#39;utf-8&#39;) # img images = soup.findAll(&#39;img&#39;) print(images) imageName = 0 for image in images: link = image.get(&#39;src&#39;) print(&#39;link=&#39;, link) fileFormat = link[-3:] if fileFormat == &#39;png&#39; or fileFormat == &#39;jpg&#39;: fileSavePath = &#39;C:/Users/codewj/AnacondaProjects/5刷脸识别/images/&#39; + str(imageName) + &#39;.jpg&#39; imageName = imageName + 1 urllib.request.urlretrieve(link, fileSavePath) ffmpegffmpeg -i input.mp4 -r 1 -q:v 2 -f image2 pic-%03d.jpeg 视频提取帧 ffmpeg -i input.mp4 -ss 00:00:20 -t 10 -r 1 -q:v 2 -f image2 pic-%03d.jpeg ffmpeg会从input.mp4的第20s时间开始，往下10s，即20~30s这10秒钟之间，每隔1s就抓一帧，总共会抓10帧。 ffmpeg -i input.avi output.mp4 视频转格式 ffmpeg -i a.mp4 -acodec copy -vn a.aac 视频提取音频 ffmpeg -i input.mp4 -vcodec copy -an output.mp4 视频提取音频 ffmpeg -ss 00:00:15 -t 00:00:05 -i input.mp4 -vcodec copy -acodec copy output.mp4 视频剪切 ffmpeg -i input.mp4 -b:v 2000k -bufsize 2000k -maxrate 2500k output.mp4 码率控制 ffmpeg -i input.mp4 -vcodec mpeg4 output.mp4 视频编码格式转换mpeg4 ffmpeg -i input.mp4 -vf scale=960:540 output.mp4 将输入视频缩小到960x540输出 ffmpeg -i input.mp4 -i iQIYI_logo.png -filter_complex overlay output.mp4 视频添加logo opencv预处理# 1 load xml 2 load jpg 3 haar gray 4 detect 5 draw import cv2 import numpy as np # load xml 1 file name face_xml = cv2.CascadeClassifier(&#39;haarcascade_frontalface_default.xml&#39;) eye_xml = cv2.CascadeClassifier(&#39;haarcascade_eye.xml&#39;) # load jpg img = cv2.imread(&#39;face.jpg&#39;) cv2.imshow(&#39;src&#39;,img) # haar gray gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) # detect faces 1 data 2 scale 3 5 faces = face_xml.detectMultiScale(gray,1.3,5) print(&#39;face=&#39;,len(faces)) # draw index = 0 for (x,y,w,h) in faces: cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) roi_face = gray[y:y+h,x:x+w] roi_color = img[y:y+h,x:x+w] fileName = str(index)+&#39;.jpg&#39; cv2.imwrite(fileName,roi_color) index = index + 1 # 1 gray eyes = eye_xml.detectMultiScale(roi_face) print(&#39;eye=&#39;,len(eyes)) #for (e_x,e_y,e_w,e_h) in eyes: #cv2.rectangle(roi_color,(e_x,e_y),(e_x+e_w,e_y+e_h),(0,255,0),2) cv2.imshow(&#39;dst&#39;,img) cv2.waitKey(0) 某个人脸识别# 1 数据yale 2 准备train label-》train # 3 cnn 4 检测 import tensorflow as tf import numpy as np import scipy.io as sio f = open(&#39;Yale_64x64.mat&#39;,&#39;rb&#39;) mdict = sio.loadmat(f) # fea gnd train_data = mdict[&#39;fea&#39;] train_label = mdict[&#39;gnd&#39;] # 数据无序排列 train_data = np.random.permutation(train_data) train_label = np.random.permutation(train_label) test_data = train_data[0:64] test_label = train_label[0:64] np.random.seed(100) test_data = np.random.permutation(test_data) np.random.seed(100) test_label = np.random.permutation(test_label) # train [0-9] [10*N] [15*N] [0 0 1 0 0 0 0 0 0 0] -&gt; 2 train_data = train_data.reshape(train_data.shape[0],64,64,1).astype(np.float32)/255 train_labels_new = np.zeros((165,15))# 165 image 15 for i in range(0,165): j = int(train_label[i,0])-1 # 1-15 0-14 train_labels_new[i,j] = 1 test_data_input = test_data.reshape(test_data.shape[0],64,64,1).astype(np.float32)/255 test_labels_input = np.zeros((64,15))# 165 image 15 for i in range(0,64): j = int(test_label[i,0])-1 # 1-15 0-14 test_labels_input[i,j] = 1 # cnn acc tf.nn tf.layer data_input = tf.placeholder(tf.float32,[None,64,64,1]) label_input = tf.placeholder(tf.float32,[None,15]) layer1 = tf.layers.conv2d(inputs=data_input,filters=32,kernel_size=2,strides=1,padding=&#39;SAME&#39;,activation=tf.nn.relu) layer1_pool = tf.layers.max_pooling2d(layer1,pool_size=2,strides=2) layer2 = tf.reshape(layer1_pool,[-1,32*32*32]) layer2_relu = tf.layers.dense(layer2,1024,tf.nn.relu) output = tf.layers.dense(layer2_relu,15) loss = tf.losses.softmax_cross_entropy(onehot_labels=label_input,logits=output) train = tf.train.GradientDescentOptimizer(0.01).minimize(loss) accuracy = tf.metrics.accuracy(labels=tf.argmax(label_input,axis=1),predictions=tf.argmax(output,axis=1))[1] # run acc init = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer()) with tf.Session() as sess: sess.run(init) for i in range(0,200): train_data_input = np.array(train_data) train_label_input = np.array(train_labels_new) sess.run([train,loss],feed_dict={data_input:train_data_input,label_input:train_label_input}) acc = sess.run(accuracy,feed_dict={data_input:test_data_input,label_input:test_labels_input}) print(&#39;acc:%.2f&#39;,acc)]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenCV与TensorFlow 入门人工智能图像处理]]></title>
    <url>%2F2019%2F09%2F02%2FOpenCV%E4%B8%8ETensorFlow%20%E5%85%A5%E9%97%A8%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[OpenCV与TensorFlow 入门人工智能图像处理 AnacondaAnaconda Navigator新建Env环境,添加channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/tensorflow,opencv,numpy,matplotlib,beautifulsoup4,urllib3,scipy，并进入Home安装Jupyter notebook点击进入Jupyter Launch 基本命令yum install -y bzip2 sh Anaconda3-5.2.0-Linux-x86_64.sh 修改默认位置为/data1/anaconda3 vi /etc/profile PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin:$MAVEN_HOME/bin:/data1/anaconda3/bin source /etc/profile conda update conda conda info --envs conda create --name tensorflow36 activate tensorflow36 | source activate tensorflow36 | conda activate tensorflow36 source deactivate conda create -n spider python=3.6 创建虚拟环境 source activate spider pip install -i https://pypi.doubanio.com/simple/ --trusted-host pypi.doubanio.com fake_useragent scrapy browsercookie conda remove -n spider --all 删除虚拟环境 conda config --set show_channel_urls yes 设置搜索时显示通道地址 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --remove channels https://mirrors.tuna.tsinghua.edu.cn/tensorflow/linux/cpu/ conda list conda update anaconda-navigator conda update navigator-updater pip install opencv-pythonopencv入门opencv图片读取与展示 # 引入opencv2 import cv2 # 文件读取-封装格式解析-数据解码-数据加载 img = cv2.imread(&#39;a.jpg&#39;,1) cv2.imshow(&#39;image&#39;,img) # jpg png是文件封装格式，文件头（数据解码信息，附加信息，解码器根据附加信息将文件数据还原成最原始的数据）+文件数据（非文件原始数据，是压缩编码后的数据） # stop cv2.waitKey (0) opencv图片写入import cv2 img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imwrite(&#39;image1.jpg&#39;,img) # 1 name 2 data opencv图像质量jpg有损压缩import cv2 img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imwrite(&#39;imageTest.jpg&#39;,img,[cv2.IMWRITE_JPEG_QUALITY,50]) # 1M 100k 10k 0-100 # jpg RGB颜色分量组成 1.14M=720*547*3*8 bit/8 (b)=1.14M png无损压缩# 透明度属性 import cv2 img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imwrite(&#39;imageTest.png&#39;,img,[cv2.IMWRITE_PNG_COMPRESSION,0]) # jpg 0 压缩比高0-100 png 0 压缩比低0-9 # png RGB alpha opencv像素操作import cv2 img = cv2.imread(&#39;image0.jpg&#39;,1) (b,g,r) = img[100,100] print(b,g,r)# bgr #10 100 --- 110 100 for i in range(1,100): img[10+i,100] = (255,0,0) cv2.imshow(&#39;image&#39;,img) cv2.waitKey(0) #1000 ms tensorflow入门 tf_常量变量所有变量必须初始化完成 #opencv tensorflow #类比 语法 api 原理 #基础数据类型 运算符 流程 字典 数组 import tensorflow as tf data1 = tf.constant(2,dtype=tf.int32) data2 = tf.Variable(10,name=&#39;var&#39;) print(data1) print(data2) &#39;&#39;&#39; sess = tf.Session() print(sess.run(data1)) init = tf.global_variables_initializer() sess.run(init) print(sess.run(data2)) sess.close() # 本质 tf = tensor + 计算图 # tensor 数据 # op # graphs 数据操作 # session &#39;&#39;&#39; init = tf.global_variables_initializer() sess = tf.Session() with sess: sess.run(init) print(sess.run(data2)) tf_四则运算常量import tensorflow as tf data1 = tf.constant(6) data2 = tf.constant(2) dataAdd = tf.add(data1,data2) dataMul = tf.multiply(data1,data2) dataSub = tf.subtract(data1,data2) dataDiv = tf.divide(data1,data2) with tf.Session() as sess: print(sess.run(dataAdd)) print(sess.run(dataMul)) print(sess.run(dataSub)) print(sess.run(dataDiv)) print(&#39;end!&#39;) 变量import tensorflow as tf data1 = tf.constant(6) data2 = tf.Variable(2) dataAdd = tf.add(data1,data2) dataCopy = tf.assign(data2,dataAdd)# dataAdd -&gt;data2 dataMul = tf.multiply(data1,data2) dataSub = tf.subtract(data1,data2) dataDiv = tf.divide(data1,data2) init = tf.global_variables_initializer() with tf.Session() as sess: sess.run(init) # 所有变量必须完成初始化 print(sess.run(dataAdd)) print(sess.run(dataMul)) print(sess.run(dataSub)) print(sess.run(dataDiv)) print(&#39;sess.run(dataCopy)&#39;,sess.run(dataCopy))#8-&gt;data2 print(&#39;dataCopy.eval()&#39;,dataCopy.eval())#8+6-&gt;14-&gt;data2 = 14 print(&#39;tf.get_default_session()&#39;,tf.get_default_session().run(dataCopy)) print(&#39;end!&#39;) tf矩阵基础1占位#placehold import tensorflow as tf data1 = tf.placeholder(tf.float32) data2 = tf.placeholder(tf.float32) dataAdd = tf.add(data1,data2) with tf.Session() as sess: print(sess.run(dataAdd,feed_dict={data1:6,data2:2})) # 1 dataAdd 2 data (feed_dict = {1:6,2:2}) print(&#39;end!&#39;) 矩阵打印#类比 数组 M行N列 [] 内部[] [里面 列数据] [] 中括号整体 行数 #[[6,6]] [[6,6]] import tensorflow as tf data1 = tf.constant([[6,6]]) data2 = tf.constant([[2], [2]]) data3 = tf.constant([[3,3]]) data4 = tf.constant([[1,2], [3,4], [5,6]]) print(data4.shape)# 维度 with tf.Session() as sess: print(sess.run(data4)) #打印整体 print(sess.run(data4[0]))# 打印某一行 print(sess.run(data4[:,0]))#打印某列 print(sess.run(data4[0,1]))# 1 1 MN = 0 32 = M012 N01 矩阵计算import tensorflow as tf data1 = tf.constant([[6,6]]) data2 = tf.constant([[2], [2]]) data3 = tf.constant([[3,3]]) data4 = tf.constant([[1,2], [3,4], [5,6]]) matMul = tf.matmul(data1,data2) matMul2 = tf.multiply(data1,data2) matAdd = tf.add(data1,data3) with tf.Session() as sess: print(sess.run(matMul))#1 维 M=1 N2. 1X2(MK) 2X1(KN) = 1 print(sess.run(matAdd))#1行2列 print(sess.run(matMul2))# 1x2 2x1 = 2x2 print(sess.run([matMul,matAdd])) 矩阵定义import tensorflow as tf mat0 = tf.constant([[0,0,0],[0,0,0]]) mat1 = tf.zeros([2,3]) mat2 = tf.ones([3,2]) mat3 = tf.fill([2,3],15) with tf.Session() as sess: #print(sess.run(mat0)) #print(sess.run(mat1)) #print(sess.run(mat2)) print(sess.run(mat3)) mat4 = tf.constant([[2],[3],[4]]) mat5 = tf.zeros_like(mat1) mat6 = tf.linspace(0.0,2.0,11) mat7 = tf.random_uniform([2,3],-1,2) with tf.Session() as sess: print(sess.run(mat5)) print(sess.run(mat6)) print(sess.run(mat7)) tf模块Numpy的使用#CURD import numpy as np data1 = np.array([1,2,3,4,5]) print(data1) data2 = np.array([[1,2], [3,4]]) print(data2) #维度 print(data1.shape,data2.shape) # zero ones print(np.zeros([2,3]),np.ones([2,2])) # 改查 data2[1,0] = 5 print(data2) print(data2[1,1]) # 基本运算 data3 = np.ones([2,3]) print(data3*2)#对应相乘 print(data3/3) print(data3+2) # 矩阵+* data4 = np.array([[1,2,3],[4,5,6]]) print(data3+data4) print(data3*data4) tf模块matplotlib的使用import numpy as np import matplotlib.pyplot as plt # 折线 x = np.array([1,2,3,4,5,6,7,8]) y = np.array([3,5,7,6,2,6,10,15]) plt.plot(x,y,&#39;r&#39;)# 折线 1 x 2 y 3 color plt.plot(x,y,&#39;g&#39;,lw=10)# 4 line w # 柱状 x = np.array([1,2,3,4,5,6,7,8]) y = np.array([13,25,17,36,21,16,10,15]) plt.bar(x,y,0.2,alpha=1,color=&#39;b&#39;)# 5 color 4 透明度 3 0.9 plt.show() 绘制股票k线import tensorflow as tf import numpy as np import matplotlib.pyplot as plt date = np.linspace(1,15,15) endPrice = np.array([2511.90,2538.26,2510.68,2591.66,2732.98,2701.69,2701.29,2678.67,2726.50,2681.50,2739.17,2715.07,2823.58,2864.90,2919.08] ) beginPrice = np.array([2438.71,2500.88,2534.95,2512.52,2594.04,2743.26,2697.47,2695.24,2678.23,2722.13,2674.93,2744.13,2717.46,2832.73,2877.40]) print(date) # 定义绘图 plt.figure() # 数据装载 for i in range(0,15): # 1 柱状图 dateOne = np.zeros([2]) dateOne[0] = i; dateOne[1] = i; priceOne = np.zeros([2]) priceOne[0] = beginPrice[i] priceOne[1] = endPrice[i] if endPrice[i]&gt;beginPrice[i]: plt.plot(dateOne,priceOne,&#39;r&#39;,lw=8) else: plt.plot(dateOne,priceOne,&#39;g&#39;,lw=8) plt.show() 神经网络逼近股票收盘均价# layer1：激励函数+乘加运算 import tensorflow as tf import numpy as np import matplotlib.pyplot as plt date = np.linspace(1,15,15) endPrice = np.array([2511.90,2538.26,2510.68,2591.66,2732.98,2701.69,2701.29,2678.67,2726.50,2681.50,2739.17,2715.07,2823.58,2864.90,2919.08] ) beginPrice = np.array([2438.71,2500.88,2534.95,2512.52,2594.04,2743.26,2697.47,2695.24,2678.23,2722.13,2674.93,2744.13,2717.46,2832.73,2877.40]) print(date) plt.figure() for i in range(0,15): # 1 柱状图 dateOne = np.zeros([2]) dateOne[0] = i; dateOne[1] = i; priceOne = np.zeros([2]) priceOne[0] = beginPrice[i] priceOne[1] = endPrice[i] if endPrice[i]&gt;beginPrice[i]: plt.plot(dateOne,priceOne,&#39;r&#39;,lw=8) else: plt.plot(dateOne,priceOne,&#39;g&#39;,lw=8) #plt.show() # 输入矩阵A:15*1 隐藏层矩阵B:15*10 数据矩阵C:15*1 # A(15x1)*w1(1x10)+b1(1*10) = B(15x10) A-&gt;B # B(15x10)*w2(10x1)+b2(15x1) = C(15x1) B-&gt;C # 1次循环 A-|w1 w2 b1 b2|-&gt;C 与真实值相差，在2次循环时，梯度下降修改|w1 w2 b1 b2|减少2次误差..... # 1 A B C dateNormal = np.zeros([15,1]) priceNormal = np.zeros([15,1]) for i in range(0,15): dateNormal[i,0] = i/14.0; priceNormal[i,0] = endPrice[i]/3000.0; x = tf.placeholder(tf.float32,[None,1]) y = tf.placeholder(tf.float32,[None,1]) # B w1 = tf.Variable(tf.random_uniform([1,10],0,1)) b1 = tf.Variable(tf.zeros([1,10])) wb1 = tf.matmul(x,w1)+b1 layer1 = tf.nn.relu(wb1) # 激励函数 # C w2 = tf.Variable(tf.random_uniform([10,1],0,1)) b2 = tf.Variable(tf.zeros([15,1])) wb2 = tf.matmul(layer1,w2)+b2 layer2 = tf.nn.relu(wb2) loss = tf.reduce_mean(tf.square(y-layer2))#y 真实 layer2 计算 train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) # 梯度下降缩小loss with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for i in range(0,10000): sess.run(train_step,feed_dict={x:dateNormal,y:priceNormal}) # w1w2 b1b2 A + wb --&gt;layer2 pred = sess.run(layer2,feed_dict={x:dateNormal}) predPrice = np.zeros([15,1]) for i in range(0,15): predPrice[i,0]=(pred*3000)[i,0] plt.plot(date,predPrice,&#39;b&#39;,lw=1) plt.show() 计算机视觉加强之几何变换图片缩放API图片缩放# 1 load 2 info 3 resize 4 check import cv2 # 1表示彩色图片 img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape print(imgInfo) height = imgInfo[0] width = imgInfo[1] mode = imgInfo[2] # 1 放大 缩小 2 等比例 非 2:3 dstHeight = int(height*0.5) dstWidth = int(width*0.5) #最近临域插值 双线性插值 像素关系重采样 立方插值 dst = cv2.resize(img,(dstWidth,dstHeight)) cv2.imshow(&#39;image&#39;,dst) cv2.waitKey(0) 最近临域插值 原理 src 1020 dst 5*10 dst&lt;-src (1,2) &lt;- (2,4) dst x 1 -&gt; src x 2 newX newX = x(src 行/目标 行) newX = 1（10/5） = 2 newY = y(src 列/目标 列) newY = 2*（20/10）= 4 12.3 = 12 双线性插值 原理 A1 = 20% 上+80%下 A2 B1 = 30% 左+70%右 B2 1 最终点 = A1 30% + A2 70% 2 最终点 = B1 20% + B2 80% 实质：矩阵运算 源码图片缩放# 1 info 2 空白模版 3 xy import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] dstHeight = int(height/2) dstWidth = int(width/2) dstImage = np.zeros((dstHeight,dstWidth,3),np.uint8)#0-255 for i in range(0,dstHeight):#行 for j in range(0,dstWidth):#列 iNew = int(i*(height*1.0/dstHeight)) jNew = int(j*(width*1.0/dstWidth)) dstImage[i,j] = img[iNew,jNew] cv2.imshow(&#39;dst&#39;,dstImage) cv2.waitKey(0) # 1 opencv API resize 2 算法原理 3 源码 源码图片剪切#100 -》200 x #100-》300 y import cv2 img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape dst = img[100:200,100:300] cv2.imshow(&#39;image&#39;,dst) cv2.waitKey(0) 矩阵图片移位# 1 API 2 算法原理 3 源代码 import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] #### matShift = np.float32([[1,0,100],[0,1,200]])# 2*3 dst = cv2.warpAffine(img,matShift,(height,width))#1 data 2 mat 3 info # 移位 矩阵 cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) [1,0,100],[0,1,200] 22 21 组成[[1,0],[0,1]] 22 A[[100],[200]] 21 Bxy CAC+B = [[1x+0y],[0x+1*y]]+[[100],[200]] = [[x+100],[y+200]](10,20)-&gt;(110,120) 矩阵图片缩放#[[A1 A2 B1],[A3 A4 B2]] # [[A1 A2],[A3 A4]] [[B1],[B2]] # newX = A1*x + A2*y+B1 # newY = A3*x +A4*y+B2 # x-&gt;x*0.5 y-&gt;y*0.5 # newX = 0.5*x import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] matScale = np.float32([[0.5,0,0],[0,0.5,0]]) dst = cv2.warpAffine(img,matScale,(int(width/2),int(height/2))) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 源码图片移位import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape dst = np.zeros(img.shape,np.uint8) height = imgInfo[0] width = imgInfo[1] for i in range(0,height): for j in range(0,width-100): dst[i,j+100]=img[i,j] cv2.imshow(&#39;image&#39;,dst) cv2.waitKey(0) 源码图片镜像import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] deep = imgInfo[2] newImgInfo = (height*2,width,deep) dst = np.zeros(newImgInfo,np.uint8)#uint8 for i in range(0,height): for j in range(0,width): dst[i,j] = img[i,j] #x y = 2*h - y -1 dst[height*2-i-1,j] = img[i,j] for i in range(0,width): dst[height,i] = (0,0,255)#BGR cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 仿射变换import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] #src 3-&gt;dst 3 (左上角 左下角 右上角) matSrc = np.float32([[0,0],[0,height-1],[width-1,0]]) matDst = np.float32([[50,50],[300,height-200],[width-300,100]]) #组合 定义仿射变换矩阵 matAffine = cv2.getAffineTransform(matSrc,matDst)# mat 1 src 2 dst dst = cv2.warpAffine(img,matAffine,(width,height)) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) API图片旋转import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] # 2*3 matRotate = cv2.getRotationMatrix2D((height*0.5,width*0.5),45,1)# mat rotate 1 center 2 angle 3 scale #100*100 25 dst = cv2.warpAffine(img,matRotate,(height,width)) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 计算机视觉加强之图像特效API灰度处理#imread #方法1 imread import cv2 img0 = cv2.imread(&#39;image0.jpg&#39;,0) img1 = cv2.imread(&#39;image0.jpg&#39;,1) print(img0.shape) print(img1.shape) cv2.imshow(&#39;src&#39;,img0) cv2.waitKey(0) #方法2 cvtColor img = cv2.imread(&#39;image0.jpg&#39;,1) dst = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)# 颜色空间转换 1 data 2 BGR gray cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 源码灰度处理import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] # RGB R=G=B = gray (R+G+B)/3 dst = np.zeros((height,width,3),np.uint8) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] gray = (int(b)+int(g)+int(r))/3 dst[i,j] = np.uint8(gray) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) #方法4 gray = r*0.299+g*0.587+b*0.114 dst = np.zeros((height,width,3),np.uint8) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] b = int(b) g = int(g) r = int(r) gray = r*0.299+g*0.587+b*0.114 dst[i,j] = np.uint8(gray) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0)算法优化# 1 灰度 最重要 2 基础 3 实时性 # 定点-》浮点 +- */ &gt;&gt; # r*0.299+g*0.587+b*0.114 import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] # RGB R=G=B = gray (R+G+B)/3 dst = np.zeros((height,width,3),np.uint8) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] b = int(b) g = int(g) r = int(r) #gray = (r*1+g*2+b*1)/4 gray = (r+(g&lt;&lt;1)+b)&gt;&gt;2 dst[i,j] = np.uint8(gray) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 图片颜色反转#0-255 255-当前 灰度图片颜色反转 import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) # 1表示一个像素一种颜色 dst = np.zeros((height,width,1),np.uint8) for i in range(0,height): for j in range(0,width): grayPixel = gray[i,j] dst[i,j] = 255-grayPixel cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) #RGB 255-R=newR 彩色图片颜色反转 dst = np.zeros((height,width,3),np.uint8) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] dst[i,j] = (255-b,255-g,255-r) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 马赛克import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] for m in range(100,300): for n in range(100,200): # pixel -&gt;10*10 所有像素点用一个点替代 if m%10 == 0 and n%10==0: # for循环填充小矩形 for i in range(0,10): for j in range(0,10): (b,g,r) = img[m,n] img[i+m,j+n] = (b,g,r) cv2.imshow(&#39;dst&#39;,img) cv2.waitKey(0) 毛玻璃import cv2 import numpy as np import random img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] dst = np.zeros((height,width,3),np.uint8) mm = 8 ## -mm防止当前矩阵越界 for m in range(0,height-mm): for n in range(0,width-mm): index = int(random.random()*8)#0-8 (b,g,r) = img[m+index,n+index] dst[m,n] = (b,g,r) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 图片融合# dst = src1*a+src2*(1-a) 两张图片都需要大于第一张图片宽和高度的一半 import cv2 import numpy as np img0 = cv2.imread(&#39;image0.jpg&#39;,1) img1 = cv2.imread(&#39;image1.jpg&#39;,1) imgInfo = img0.shape height = imgInfo[0] width = imgInfo[1] # ROI roiH = int(height/2) roiW = int(width/2) img0ROI = img0[0:roiH,0:roiW] img1ROI = img1[0:roiH,0:roiW] # dst dst = np.zeros((roiH,roiW,3),np.uint8) dst = cv2.addWeighted(img0ROI,0.5,img1ROI,0.5,0)#add src1*a+src2*(1-a) # 1 src1 2 a 3 src2 4 1-a cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 图片边缘检测import cv2 import numpy as np import random img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] cv2.imshow(&#39;src&#39;,img) #canny 1 gray 2 高斯 3 canny gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) imgG = cv2.GaussianBlur(gray,(3,3),0) dst = cv2.Canny(img,50,50) #图片卷积——》th cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 源码边缘检测import cv2 import numpy as np import random import math img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] cv2.imshow(&#39;src&#39;,img) # sobel 1 算子模版 2 图片卷积 3 阈值判决 # [1 2 1 [ 1 0 -1 # 0 0 0 2 0 -2 # -1 -2 -1 ] 1 0 -1 ] # 四个点 [1 2 3 4] 计算模板 [a b c d] 卷积后 a*1+b*2+c*3+d*4 = dst # sqrt(a*a+b*b) = f&gt;th 则为边缘 gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) dst = np.zeros((height,width,1),np.uint8) for i in range(0,height-2): for j in range(0,width-2): # 竖直方向梯度 gy = gray[i,j]*1+gray[i,j+1]*2+gray[i,j+2]*1-gray[i+2,j]*1-gray[i+2,j+1]*2-gray[i+2,j+2]*1 # 水平方向梯度 gx = gray[i,j]+gray[i+1,j]*2+gray[i+2,j]-gray[i,j+2]-gray[i+1,j+2]*2-gray[i+2,j+2] # 计算梯度 grad = math.sqrt(gx*gx+gy*gy) if grad&gt;50: dst[i,j] = 255 else: dst[i,j] = 0 cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 浮雕效果import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) # newP = gray0-gray1+150 dst = np.zeros((height,width,1),np.uint8) for i in range(0,height): for j in range(0,width-1): grayP0 = int(gray[i,j]) grayP1 = int(gray[i,j+1]) newP = grayP0-grayP1+150 if newP &gt; 255: newP = 255 if newP &lt; 0: newP = 0 dst[i,j] = newP cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 颜色风格import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] #rgb -》RGB new “蓝色” # b=b*1.5 # g = g*1.3 dst = np.zeros((height,width,3),np.uint8) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] b = b*1.5 g = g*1.3 if b&gt;255: b = 255 if g&gt;255: g = 255 dst[i,j]=(b,g,r) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 油画特效# 1 gray # 2 7*7 10*10 3 0-255 256 4 64 0-63 64-127 # 3 10 0-63 99 64-127 # 4 count 5 dst = result import cv2 import numpy as np img = cv2.imread(&#39;image00.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) dst = np.zeros((height,width,3),np.uint8) for i in range(4,height-4): for j in range(4,width-4): array1 = np.zeros(8,np.uint8) # 定义8*8小方块 for m in range(-4,4): for n in range(-4,4): # 灰度等级划分8个段 每段32，/32知道p1投影在哪个灰度等级段 p1 = int(gray[i+m,j+n]/32) # 当前像素值完成累加 array1[p1] = array1[p1]+1 currentMax = array1[0] # l定义某一段 l = 0 for k in range(0,8): if currentMax&lt;array1[k]: currentMax = array1[k] l = k # 简化或者均值 for m in range(-4,4): for n in range(-4,4): if gray[i+m,j+n]&gt;=(l*32) and gray[i+m,j+n]&lt;=((l+1)*32): (b,g,r) = img[i+m,j+n] dst[i,j] = (b,g,r) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 线段绘制import cv2 import numpy as np newImageInfo = (500,500,3) dst = np.zeros(newImageInfo,np.uint8) # line # 绘制线段 1 dst 2 begin 3 end 4 color cv2.line(dst,(100,100),(400,400),(0,0,255)) # 5 line w cv2.line(dst,(100,200),(400,200),(0,255,255),20) # 6 line type cv2.line(dst,(100,300),(400,300),(0,255,0),20,cv2.LINE_AA) cv2.line(dst,(200,150),(50,250),(25,100,255)) cv2.line(dst,(50,250),(400,380),(25,100,255)) cv2.line(dst,(400,380),(200,150),(25,100,255)) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 矩形圆形绘制import cv2 import numpy as np newImageInfo = (500,500,3) dst = np.zeros(newImageInfo,np.uint8) # 1 2 左上角 3 右下角 4 5 fill -1 &gt;0 line w cv2.rectangle(dst,(50,100),(200,300),(255,0,0),5) # 2 center 3 r cv2.circle(dst,(250,250),(50),(0,255,0),2) # 2 center 3 轴 4 angle 5 begin 6 end 7 cv2.ellipse(dst,(256,256),(150,100),0,0,180,(255,255,0),-1) points = np.array([[150,50],[140,140],[200,170],[250,250],[150,50]],np.int32) print(points.shape) points = points.reshape((-1,1,2)) print(points.shape) cv2.polylines(dst,[points],True,(0,255,255)) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 文字图片绘制import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) font = cv2.FONT_HERSHEY_SIMPLEX cv2.rectangle(img,(200,100),(500,400),(0,255,0),3) # 1 dst 2 文字内容 3 坐标 4 5 字体大小 6 color 7 粗细 8 line type cv2.putText(img,&#39;this is flow&#39;,(100,300),font,1,(200,100,255),2,cv2.LINE_AA) cv2.imshow(&#39;src&#39;,img) cv2.waitKey(0) height = int(img.shape[0]*0.2) width = int(img.shape[1]*0.2) imgResize = cv2.resize(img,(width,height)) for i in range(0,height): for j in range(0,width): img[i+200,j+350] = imgResize[i,j] cv2.imshow(&#39;src&#39;,img) cv2.waitKey(0) 计算机视觉加强之图像美化灰度直方图源码# 1 0-255 2 概率 # 本质：统计每个像素灰度 出现的概率 0-255 p import cv2 import numpy as np import matplotlib.pyplot as plt img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) count = np.zeros(256,np.float) for i in range(0,height): for j in range(0,width): pixel = gray[i,j] index = int(pixel) count[index] = count[index]+1 for i in range(0,255): count[i] = count[i]/(height*width) x = np.linspace(0,255,256) y = count plt.bar(x,y,0.9,alpha=1,color=&#39;b&#39;) plt.show() cv2.waitKey(0)彩色直方图源码# 本质：统计每个像素灰度 出现的概率 0-255 p import cv2 import numpy as np import matplotlib.pyplot as plt img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] count_b = np.zeros(256,np.float) count_g = np.zeros(256,np.float) count_r = np.zeros(256,np.float) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] index_b = int(b) index_g = int(g) index_r = int(r) count_b[index_b] = count_b[index_b]+1 count_g[index_g] = count_g[index_g]+1 count_r[index_r] = count_r[index_r]+1 for i in range(0,256): count_b[i] = count_b[i]/(height*width) count_g[i] = count_g[i]/(height*width) count_r[i] = count_r[i]/(height*width) x = np.linspace(0,255,256) y1 = count_b plt.figure() plt.bar(x,y1,0.9,alpha=1,color=&#39;b&#39;) y2 = count_g plt.figure() plt.bar(x,y2,0.9,alpha=1,color=&#39;g&#39;) y3 = count_r plt.figure() plt.bar(x,y3,0.9,alpha=1,color=&#39;r&#39;) plt.show() cv2.waitKey(0) 灰度直方图均衡化# 本质：统计每个像素灰度 出现的概率 0-255 p # 累计概率 # 1 0.2 0.2 # 2 0.3 0.5 # 3 0.1 0.6 # 256 # 100 0.5 255*0.5 = new import cv2 import numpy as np import matplotlib.pyplot as plt img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) cv2.imshow(&#39;src&#39;,gray) count = np.zeros(256,np.float) for i in range(0,height): for j in range(0,width): pixel = gray[i,j] index = int(pixel) count[index] = count[index]+1 for i in range(0,255): count[i] = count[i]/(height*width) #计算累计概率 sum1 = float(0) for i in range(0,256): sum1 = sum1+count[i] count[i] = sum1 #print(count) # 计算映射表 map1 = np.zeros(256,np.uint16) for i in range(0,256): map1[i] = np.uint16(count[i]*255) # 映射 for i in range(0,height): for j in range(0,width): pixel = gray[i,j] gray[i,j] = map1[pixel] cv2.imshow(&#39;dst&#39;,gray) cv2.waitKey(0)彩色直方图均衡化# 本质：统计每个像素灰度 出现的概率 0-255 p # 累计概率 # 1 0.2 0.2 # 2 0.3 0.5 # 3 0.1 0.6 # 256 # 100 0.5 255*0.5 = new # 1 统计每个颜色出现的概率 2 累计概率 1 3 0-255 255*p # 4 pixel import cv2 import numpy as np import matplotlib.pyplot as plt img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] count_b = np.zeros(256,np.float) count_g = np.zeros(256,np.float) count_r = np.zeros(256,np.float) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] index_b = int(b) index_g = int(g) index_r = int(r) count_b[index_b] = count_b[index_b]+1 count_g[index_g] = count_g[index_g]+1 count_r[index_r] = count_r[index_r]+1 for i in range(0,255): count_b[i] = count_b[i]/(height*width) count_g[i] = count_g[i]/(height*width) count_r[i] = count_r[i]/(height*width) #计算累计概率 sum_b = float(0) sum_g = float(0) sum_r = float(0) for i in range(0,256): sum_b = sum_b+count_b[i] sum_g = sum_g+count_g[i] sum_r = sum_r+count_r[i] count_b[i] = sum_b count_g[i] = sum_g count_r[i] = sum_r #print(count) # 计算映射表 map_b = np.zeros(256,np.uint16) map_g = np.zeros(256,np.uint16) map_r = np.zeros(256,np.uint16) for i in range(0,256): map_b[i] = np.uint16(count_b[i]*255) map_g[i] = np.uint16(count_g[i]*255) map_r[i] = np.uint16(count_r[i]*255) # 映射 dst = np.zeros((height,width,3),np.uint8) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] b = map_b[b] g = map_g[g] r = map_r[r] dst[i,j] = (b,g,r) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 彩色直方图APIimport cv2 import numpy as np def ImageHist(image,type): color = (255,255,255) windowName = &#39;Gray&#39; if type == 31: color = (255,0,0) windowName = &#39;B Hist&#39; elif type == 32: color = (0,255,0) windowName = &#39;G Hist&#39; elif type == 33: color = (0,0,255) windowName = &#39;R Hist&#39; # 计算图片直方图1 image 2 [0]灰度直方图 3 mask None蒙版 4 256 5 0-255 hist = cv2.calcHist([image],[0],None,[256],[0.0,255.0]) # 获取像素值中最大最小值及各自下标 归一化处理 minV,maxV,minL,maxL = cv2.minMaxLoc(hist) histImg = np.zeros([256,256,3],np.uint8) for h in range(256): #处理完后绘制结果 intenNormal = int(hist[h]*256/maxV) cv2.line(histImg,(h,256),(h,256-intenNormal),color) cv2.imshow(windowName,histImg) return histImg img = cv2.imread(&#39;image0.jpg&#39;,1) channels = cv2.split(img)# RGB - R G B for i in range(0,3): ImageHist(channels[i],31+i) cv2.waitKey(0) 直方图均衡化#灰度 直方图均衡化 import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) cv2.imshow(&#39;src&#39;,gray) dst = cv2.equalizeHist(gray) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) #彩色 直方图均衡化 import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) (b,g,r) = cv2.split(img)#通道分解 bH = cv2.equalizeHist(b) gH = cv2.equalizeHist(g) rH = cv2.equalizeHist(r) result = cv2.merge((bH,gH,rH))# 通道合成 cv2.imshow(&#39;dst&#39;,result) cv2.waitKey(0) #YUV 直方图均衡化 import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgYUV = cv2.cvtColor(img,cv2.COLOR_BGR2YCrCb) cv2.imshow(&#39;src&#39;,img) channelYUV = cv2.split(imgYUV) channelYUV[0] = cv2.equalizeHist(channelYUV[0]) channels = cv2.merge(channelYUV) result = cv2.cvtColor(channels,cv2.COLOR_YCrCb2BGR) cv2.imshow(&#39;dst&#39;,result) cv2.waitKey(0) 图片修补生成坏图import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) for i in range(200,300): img[i,200] = (255,255,255) img[i,200+1] = (255,255,255) img[i,200-1] = (255,255,255) for i in range(150,250): img[250,i] = (255,255,255) img[250+1,i] = (255,255,255) img[250-1,i] = (255,255,255) cv2.imwrite(&#39;damaged.jpg&#39;,img) cv2.imshow(&#39;image&#39;,img) cv2.waitKey(0)修补#1 坏图 2 array 3 inpaint import cv2 import numpy as np img = cv2.imread(&#39;damaged.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] paint = np.zeros((height,width,1),np.uint8) # 描绘坏的部分的数组 for i in range(200,300): paint[i,200] = 255 paint[i,200+1] = 255 paint[i,200-1] = 255 for i in range(150,250): paint[250,i] = 255 paint[250+1,i] = 255 paint[250-1,i] = 255 cv2.imshow(&#39;paint&#39;,paint) #1 src 2 mask imgDst = cv2.inpaint(img,paint,3,cv2.INPAINT_TELEA) cv2.imshow(&#39;image&#39;,imgDst) cv2.waitKey(0) 亮度增强p = p+40import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] cv2.imshow(&#39;src&#39;,img) dst = np.zeros((height,width,3),np.uint8) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] bb = int(b)+40 gg = int(g)+40 rr = int(r)+40 if bb&gt;255: bb = 255 if gg&gt;255: gg = 255 if rr&gt;255: rr = 255 dst[i,j] = (bb,gg,rr) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) p = p*1.2+40import cv2 import numpy as np img = cv2.imread(&#39;image0.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] cv2.imshow(&#39;src&#39;,img) dst = np.zeros((height,width,3),np.uint8) for i in range(0,height): for j in range(0,width): (b,g,r) = img[i,j] bb = int(b*1.3)+10 gg = int(g*1.2)+15 if bb&gt;255: bb = 255 if gg&gt;255: gg = 255 dst[i,j] = (bb,gg,r) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 磨皮美白### 双边滤波 import cv2 img = cv2.imread(&#39;1.png&#39;,1) cv2.imshow(&#39;src&#39;,img) dst = cv2.bilateralFilter(img,15,35,35) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 高斯滤波import cv2 import numpy as np img = cv2.imread(&#39;image11.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) dst = cv2.GaussianBlur(img,(5,5),1.5) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 均值滤波#均值 6*6 1 。 * 【6*6】/36 = mean -》P import cv2 import numpy as np img = cv2.imread(&#39;image11.jpg&#39;,1) cv2.imshow(&#39;src&#39;,img) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] dst = np.zeros((height,width,3),np.uint8) for i in range(3,height-3): for j in range(3,width-3): sum_b = int(0) sum_g = int(0) sum_r = int(0) for m in range(-3,3):#-3 -2 -1 0 1 2 for n in range(-3,3): (b,g,r) = img[i+m,j+n] sum_b = sum_b+int(b) sum_g = sum_g+int(g) sum_r = sum_r+int(r) b = np.uint8(sum_b/36) g = np.uint8(sum_g/36) r = np.uint8(sum_r/36) dst[i,j] = (b,g,r) cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) 中值滤波# 中值滤波 3*3 import cv2 import numpy as np img = cv2.imread(&#39;image11.jpg&#39;,1) imgInfo = img.shape height = imgInfo[0] width = imgInfo[1] img = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY) cv2.imshow(&#39;src&#39;,img) dst = np.zeros((height,width,3),np.uint8) collect = np.zeros(9,np.uint8) for i in range(1,height-1): for j in range(1,width-1): k = 0 for m in range(-1,2): for n in range(-1,2): gray = img[i+m,j+n] collect[k] = gray k = k+1 # 0 1 2 3 4 5 6 7 8 # 1 for k in range(0,9): p1 = collect[k] for t in range(k+1,9): if p1&lt;collect[t]: mid = collect[t] collect[t] = p1 p1 = mid dst[i,j] = collect[4] cv2.imshow(&#39;dst&#39;,dst) cv2.waitKey(0) Q1:ImportError: libXext.so.6: cannot open shared object file: No such file or directory yum install libXext.x86_64 Q2: ImportError: libSM.so.6: cannot open shared object file: No such file or directory yum install libSM.x86_64 Q3:libXrender.so.1: cannot open shared object file: No such file or directory yum install libXrender.x86_64]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenCV与TensorFlow 机器学习]]></title>
    <url>%2F2019%2F09%2F02%2FOpenCV%E4%B8%8ETensorFlow%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[OpenCV与TensorFlow 机器学习 视频分解图片# 1 load 2 info 3 parse 4 imshow imwrite import cv2 cap = cv2.VideoCapture(&quot;1.mp4&quot;)# 获取一个视频打开cap 1 file name isOpened = cap.isOpened# 判断是否打开‘ print(isOpened) fps = cap.get(cv2.CAP_PROP_FPS)#帧率 width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))#w h height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) print(fps,width,height) i = 0 while(isOpened): if i == 10: break else: i = i+1 (flag,frame) = cap.read()# 读取每一张 flag frame fileName = &#39;image&#39;+str(i)+&#39;.jpg&#39; print(fileName) if flag == True: cv2.imwrite(fileName,frame,[cv2.IMWRITE_JPEG_QUALITY,100]) print(&#39;end!&#39;) 图片合成视频import cv2 img = cv2.imread(&#39;image1.jpg&#39;) imgInfo = img.shape size = (imgInfo[1],imgInfo[0]) print(size) videoWrite = cv2.VideoWriter(&#39;2.mp4&#39;,-1,5,size)# 写入对象 # 1 file name 2 编码器 3 帧率 4 size for i in range(1,11): fileName = &#39;image&#39;+str(i)+&#39;.jpg&#39; img = cv2.imread(fileName) videoWrite.write(img)# 写入方法 1 jpg data print(&#39;end!&#39;) 基于Haar+Adaboost人脸识别# 1 load xml 2 load jpg 3 haar gray 4 detect 5 draw import cv2 import numpy as np # load xml 1 file name face_xml = cv2.CascadeClassifier(&#39;haarcascade_frontalface_default.xml&#39;) eye_xml = cv2.CascadeClassifier(&#39;haarcascade_eye.xml&#39;) # load jpg img = cv2.imread(&#39;face.jpg&#39;) cv2.imshow(&#39;src&#39;,img) # haar gray gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) # detect faces 1 data 2 scale 3 5 faces = face_xml.detectMultiScale(gray,1.3,5) print(&#39;face=&#39;,len(faces)) # draw for (x,y,w,h) in faces: cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) roi_face = gray[y:y+h,x:x+w] roi_color = img[y:y+h,x:x+w] # 1 gray eyes = eye_xml.detectMultiScale(roi_face) print(&#39;eye=&#39;,len(eyes)) #for (e_x,e_y,e_w,e_h) in eyes: #cv2.rectangle(roi_color,(e_x,e_y),(e_x+e_w,e_y+e_h),(0,255,0),2) cv2.imshow(&#39;dst&#39;,img) cv2.waitKey(0) SVM身高体重预测# 1 思想 分类器 # 2 如何？ 寻求一个最优的超平面 分类 # 3 核：line # 4 数据：样本 # 5 训练 SVM_create train predict # svm本质 寻求一个最优的超平面 分类 # svm 核: line # 身高体重 训练 预测 import cv2 import numpy as np import matplotlib.pyplot as plt #1 准备data rand1 = np.array([[155,48],[159,50],[164,53],[168,56],[172,60]]) rand2 = np.array([[152,53],[156,55],[160,56],[172,64],[176,65]]) # 2 label label = np.array([[0],[0],[0],[0],[0],[1],[1],[1],[1],[1]]) # 3 data data = np.vstack((rand1,rand2)) data = np.array(data,dtype=&#39;float32&#39;) # svm 所有的数据都要有label # [155,48] -- 0 女生 [152,53] ---1 男生 # 监督学习 0 负样本 1 正样本 # 4 训练 svm = cv2.ml.SVM_create() # ml 机器学习模块 SVM_create() 创建 # 属性设置 svm.setType(cv2.ml.SVM_C_SVC) # svm type svm.setKernel(cv2.ml.SVM_LINEAR) # line svm.setC(0.01) # 训练 result = svm.train(data,cv2.ml.ROW_SAMPLE,label) # 预测 pt_data = np.vstack([[167,55],[162,57]]) #0 女生 1男生 pt_data = np.array(pt_data,dtype=&#39;float32&#39;) print(pt_data) (par1,par2) = svm.predict(pt_data) print(par2) Hog+SVM小狮子识别# 训练 # 1 参数 2hog 3 svm 4 computer hog 5 label 6 train 7 pred 8 draw import cv2 import numpy as np import matplotlib.pyplot as plt # 1 par PosNum = 820 # 正样本个数 NegNum = 1931 # 负样本个数 winSize = (64,128) blockSize = (16,16)# 105 blockStride = (8,8)#4 cell cellSize = (8,8) nBin = 9#9 bin 3780 # 2 hog create hog 1 win 2 block 3 blockStride 4 cell 5 bin hog = cv2.HOGDescriptor(winSize,blockSize,blockStride,cellSize,nBin) # 3 svm svm = cv2.ml.SVM_create() # 4 computer hog 特征提取存储 标签标识完成 featureNum = int(((128-16)/8+1)*((64-16)/8+1)*4*9) #3780 特征维度 featureArray = np.zeros(((PosNum+NegNum),featureNum),np.float32) # 用于装在特征，行正负样本个数，列就是特征维度 labelArray = np.zeros(((PosNum+NegNum),1),np.int32) # 标签 # svm 监督学习 样本 标签 svm -》image hog for i in range(0,PosNum): fileName = &#39;pos/&#39;+str(i+1)+&#39;.jpg&#39; img = cv2.imread(fileName) hist = hog.compute(img,(8,8))# 3780 for j in range(0,featureNum): featureArray[i,j] = hist[j] # featureArray hog [1,:] hog1 [2,:]hog2 labelArray[i,0] = 1 # 正样本 label 1 for i in range(0,NegNum): fileName = &#39;neg/&#39;+str(i+1)+&#39;.jpg&#39; img = cv2.imread(fileName) hist = hog.compute(img,(8,8))# 3780 for j in range(0,featureNum): featureArray[i+PosNum,j] = hist[j] labelArray[i+PosNum,0] = -1 # 负样本 label -1 svm.setType(cv2.ml.SVM_C_SVC) svm.setKernel(cv2.ml.SVM_LINEAR) svm.setC(0.01) # 6 train ret = svm.train(featureArray,cv2.ml.ROW_SAMPLE,labelArray) # 7 myHog ：《-myDetect # myDetect-《resultArray rho # myHog-》detectMultiScale # 7 检测 核心：create Hog -》 myDetect—》array-》 # resultArray-》resultArray = -1*alphaArray*supportVArray # rho-》svm-〉svm.train alpha = np.zeros((1),np.float32) rho = svm.getDecisionFunction(0,alpha) print(rho) print(alpha) alphaArray = np.zeros((1,1),np.float32) # 支持向量机数组 supportVArray = np.zeros((1,featureNum),np.float32) resultArray = np.zeros((1,featureNum),np.float32) alphaArray[0,0] = alpha resultArray = -1*alphaArray*supportVArray # detect检测创建 myDetect = np.zeros((3781),np.float32) for i in range(0,3780): myDetect[i] = resultArray[0,i] myDetect[3780] = rho[0] # rho svm （判决） myHog = cv2.HOGDescriptor() myHog.setSVMDetector(myDetect) # load 1表示彩色图片 imageSrc = cv2.imread(&#39;Test2.jpg&#39;,1) # (8,8) win 检测目标 objs = myHog.detectMultiScale(imageSrc,0,(8,8),(32,32),1.05,2) # xy wh 三维 最后一维 x = int(objs[0][0][0]) y = int(objs[0][0][1]) w = int(objs[0][0][2]) h = int(objs[0][0][3]) # 绘制展示 图片 起始位置 终止位置 颜色 线条宽度 cv2.rectangle(imageSrc,(x,y),(x+w,y+h),(255,0,0),2) cv2.imshow(&#39;dst&#39;,imageSrc) cv2.waitKey(0)]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow与Flask结合打造手写体数字识别]]></title>
    <url>%2F2019%2F08%2F30%2FTensorFlow%E4%B8%8EFlask%E7%BB%93%E5%90%88%E6%89%93%E9%80%A0%E6%89%8B%E5%86%99%E4%BD%93%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[TensorFlow与Flask结合打造手写体数字识别 AnacondaAnaconda Navigator新建Env环境,添加channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/tensorflow和opencv，并进入Home安装Jupyter notebook 定义模型modelmnist_testdemo/mnist/model.py 线性模型import tensorflow as tf # Y=W*x+b 线性模型 def regression(x): W = tf.Variable(tf.zeros([784, 10]), name=&quot;W&quot;) b = tf.Variable(tf.zeros([10]), name=&quot;b&quot;) y = tf.nn.softmax(tf.matmul(x, W) + b) return y, [W, b] 卷积模型# 卷积模型 def convolutional(x, keep_prob): # 卷积层 def conv2d(x, W): return tf.nn.conv2d(x, W, [1, 1, 1, 1], padding=&#39;SAME&#39;) # 池化层 def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&#39;SAME&#39;) # 定义权重 def weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial) # 边 def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) x_image = tf.reshape(x, [-1, 28, 28, 1]) W_conv1 = weight_variable([5, 5, 1, 32]) b_conv1 = bias_variable([32]) h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) h_pool1 = max_pool_2x2(h_conv1) W_conv2 = weight_variable([5, 5, 32, 64]) b_conv2 = bias_variable([64]) h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) h_pool2 = max_pool_2x2(h_conv2) # full connection W_fc1 = weight_variable([7 * 7 * 64, 1024]) b_fc1 = bias_variable([1024]) h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64]) h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) W_fc2 = weight_variable([1024, 10]) b_fc2 = bias_variable([10]) y = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) return y, [W_conv1, b_conv1, W_conv2, b_conv2, W_fc1, b_fc1, W_fc2, b_fc2] 定义数据mnist_testdemo/mnist/input_data.py from __future__ import absolute_import from __future__ import division from __future__ import print_function import gzip import os import tempfile import numpy from six.moves import urllib from six.moves import xrange import tensorflow as tf from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets 训练线性模型import os import input_data import model import tensorflow as tf # 从input_data中下载数据到MNIST_data data = input_data.read_data_sets(&#39;MNIST_data&#39;, one_hot=True) # create model with tf.variable_scope(&quot;regression&quot;): # 用户输入占位符 x = tf.placeholder(tf.float32, [None, 784]) y, variables = model.regression(x) # train y_ = tf.placeholder(&quot;float&quot;, [None, 10]) cross_entropy = -tf.reduce_sum(y_ * tf.log(y)) # 训练步骤 train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy) # 预测 correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) # 准确度 accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # 保存训练变量参数 saver = tf.train.Saver(variables) # 开始训练 with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for _ in range(20000): batch_xs, batch_ys = data.train.next_batch(100) sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys}) # 打印测试集和训练集的精准度 print((sess.run(accuracy, feed_dict={x:data.test.images, y_:data.test.labels}))) # 保存训练好的模型 path = saver.save( sess,os.path.join(os.path.dirname(__file__),&#39;data&#39;,&#39;regression.ckpt&#39;), write_meta_graph=False,write_state=False) print(&quot;Saved:&quot;, path) 生成mnist_testdemo/mnist/data/regression.ckpt.data-00000-of-00001和mnist_testdemo/mnist/data/regression.ckpt.index 训练卷积模型import os import model import tensorflow as tf import input_data data = input_data.read_data_sets(&#39;MNIST_data&#39;, one_hot=True) #model with tf.variable_scope(&quot;convolutional&quot;): x = tf.placeholder(tf.float32, [None, 784], name=&#39;x&#39;) keep_prob = tf.placeholder(tf.float32) y, variables = model.convolutional(x, keep_prob) #train y_ = tf.placeholder(tf.float32, [None, 10], name=&#39;y&#39;) cross_entropy = -tf.reduce_sum(y_ * tf.log(y)) # 随机梯度下降 train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) saver = tf.train.Saver(variables) with tf.Session() as sess: merged_summary_op = tf.summary.merge_all() summay_writer = tf.summary.FileWriter(&#39;./mnist_log/1&#39;, sess.graph) summay_writer.add_graph(sess.graph) sess.run(tf.global_variables_initializer()) for i in range(20000): batch = data.train.next_batch(50) if i % 100 == 0: train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0}) print(&quot;step %d, training accuracy %g&quot; % (i, train_accuracy)) sess.run(train_step, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5}) print(sess.run(accuracy, feed_dict={x: data.test.images, y_: data.test.labels, keep_prob: 1.0})) path = saver.save( sess, os.path.join(os.path.dirname(__file__), &#39;data&#39;, &#39;convalutional.ckpt&#39;), write_meta_graph=False, write_state=False) print(&quot;Saved:&quot;, path) 生成 mnist_testdemo/mnist/data/convalutional.ckpt.data-00000-of-00001和mnist_testdemo/mnist/data/convalutional.ckpt.index 集成flaskmnist_testdemo/main.py # -*- coding:utf-8 -*- import numpy as np import tensorflow as tf from flask import Flask, jsonify, render_template, request import pprint from mnist import model x = tf.placeholder(&quot;float&quot;, [None, 784]) sess = tf.Session() # 取出训练好的线性模型 with tf.variable_scope(&quot;regression&quot;): y1, variables = model.regression(x) saver = tf.train.Saver(variables) saver.restore(sess, &quot;mnist/data/regression.ckpt&quot;) # 取出训练好的卷积模型 with tf.variable_scope(&quot;convolutional&quot;): keep_prob = tf.placeholder(&quot;float&quot;) y2, variables = model.convolutional(x, keep_prob) saver = tf.train.Saver(variables) saver.restore(sess, &quot;mnist/data/convalutional.ckpt&quot;) # 根据输入调用线性模型并返回识别结果 def regression(input): return sess.run(y1, feed_dict={x: input}).flatten().tolist() # 根据输入调用卷积模型并返回识别结果 def convolutional(input): return sess.run(y2, feed_dict={x: input, keep_prob: 1.0}).flatten().tolist() app = Flask(__name__) @app.route(&#39;/api/mnist&#39;, methods=[&#39;POST&#39;]) def mnist(): # pprint.pprint(request.json) input = ((255 - np.array(request.json, dtype=np.uint8)) / 255.0).reshape(1, 784) output1 = regression(input) output2 = convolutional(input) pprint.pprint(output1) pprint.pprint(output2) return jsonify(results=[output1, output2]) @app.route(&#39;/&#39;) def main(): return render_template(&#39;index.html&#39;) if __name__ == &#39;__main__&#39;: app.debug = True app.run(host=&#39;0.0.0.0&#39;, port=8889) js核心代码drawInput() { var ctx = this.input.getContext(&#39;2d&#39;); var img = new Image(); img.onload = () =&gt; { var inputs = []; var small = document.createElement(&#39;canvas&#39;).getContext(&#39;2d&#39;); small.drawImage(img, 0, 0, img.width, img.height, 0, 0, 28, 28); var data = small.getImageData(0, 0, 28, 28).data; for (var i = 0; i &lt; 28; i++) { for (var j = 0; j &lt; 28; j++) { var n = 4 * (i * 28 + j); inputs[i * 28 + j] = (data[n + 0] + data[n + 1] + data[n + 2]) / 3; ctx.fillStyle = &#39;rgb(&#39; + [data[n + 0], data[n + 1], data[n + 2]].join(&#39;,&#39;) + &#39;)&#39;; ctx.fillRect(j * 5, i * 5, 5, 5); } } if (Math.min(...inputs) === 255) { return; } $.ajax({ url: &#39;/api/mnist&#39;, type: &#39;POST&#39;, contentType: &#39;application/json&#39;, data: JSON.stringify(inputs), success: (data) =&gt; { data = JSON.parse(data); for (let i = 0; i &lt; 2; i++) { var max = 0; var max_index = 0; for (let j = 0; j &lt; 10; j++) { var value = Math.round(data.results[i][j] * 1000); if (value &gt; max) { max = value; max_index = j; } var digits = String(value).length; for (var k = 0; k &lt; 3 - digits; k++) { value = &#39;0&#39; + value; } var text = &#39;0.&#39; + value; if (value &gt; 999) { text = &#39;1.000&#39;; } $(&#39;#output tr&#39;).eq(j + 1).find(&#39;td&#39;).eq(i).text(text); } for (let j = 0; j &lt; 10; j++) { if (j === max_index) { $(&#39;#output tr&#39;).eq(j + 1).find(&#39;td&#39;).eq(i).addClass(&#39;success&#39;); } else { $(&#39;#output tr&#39;).eq(j + 1).find(&#39;td&#39;).eq(i).removeClass(&#39;success&#39;); } } } } }); }; img.src = this.canvas.toDataURL(); } 前端将数据inputs以json传入/api/mnist regression(input)和convolutional(input)调用模型feed_dict喂参数返回结果]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nlp依存句法和语义依存分析]]></title>
    <url>%2F2019%2F08%2F27%2Fnlp%E4%BE%9D%E5%AD%98%E5%8F%A5%E6%B3%95%E5%92%8C%E8%AF%AD%E4%B9%89%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[nlp依存句法和语义依存分析 依存句法分析 依存语法 (Dependency Parsing, DP) 通过分析语言单位内成分之间的依存关系揭示其句法结构。 直观来讲,依存句法分析识别句子中的“主谓宾”、“定状补”这些语法成分,并分析各成分之间的关系。 依存句法分析标注关系 (共14种) 及含义如下:主谓关系 SBV subject-verb 我送她一束花 (我 &lt;– 送) 动宾关系 VOB 直接宾语,verb-object 我送她一束花 (送 –&gt; 花) 间宾关系 IOB 间接宾语,indirect-object 我送她一束花 (送 –&gt; 她) 前置宾语 FOB 前置宾语,fronting-object 他什么乢都读 (乢 &lt;– 读) 兼语 DBL double 他请我吃饭 (请 –&gt; 我) 定中关系 ATT attribute 红苹果 (红 &lt;– 苹果) 状中结构 ADV adverbial 非常美丽 (非常 &lt;– 美丽) 动补结构 CMP complement 做完了作业 (做 –&gt; 完) 并列关系 COO coordinate 大山和大海 (大山 –&gt; 大海) 介宾关系 POB preposition-object 在贸易区内 (在 –&gt; 内) 左附加关系 LAD left adjunct 大山和大海 (和 &lt;– 大海) 右附加关系 RAD right adjunct 孩子们 (孩子 –&gt; 们) 独立结构 IS independent structure 两个单句在结构上彼此独立 核心关系 HED head 指整个句子的核心 依存句法树解析recursionSearch.py # encoding=utf8 import re, os, json from stanfordParse import pos from stanfordParse import parse_sentence from recursionSearch import search def split_long_sentence_by_pos(text): del_flag = [&#39;DEC&#39;, &#39;AD&#39;, &#39;DEG&#39;, &#39;DER&#39;, &#39;DEV&#39;, &#39;SP&#39;, &#39;AS&#39;, &#39;ETC&#39;, &#39;SP&#39;, &#39;MSP&#39;, &#39;IJ&#39;, &#39;ON&#39;, &#39;JJ&#39;, &#39;FW&#39;, &#39;LB&#39;, &#39;SB&#39;, &#39;BA&#39;, &#39;AD&#39;, &#39;PN&#39;, &#39;RB&#39;] pos_tag = pos(text) new_str = &#39;&#39; for apos in pos_tag: if apos[1] not in del_flag: new_str += apos[0] return new_str def extract_parallel(text): parallel_text = [] pattern = re.compile(&#39;[，,][\u4e00-\u9fa5]{2,4}[，,]&#39;) search_obj = pattern.search(text) if search_obj: start_start, end = search_obj.span() rep = text[start_start:end - 2] rep1 = text[start_start:end - 1] if &#39;，&#39; in rep1: rep1.replace(&#39;，&#39;, &#39;、&#39;) if &#39;,&#39; in rep1: rep1.replace(&#39;,&#39;, &#39;、&#39;) text.replace(rep1, text) parallel_text.append(rep[1:]) text_leave = text.replace(rep, &#39;&#39;) while pattern.search(text_leave): start, end = pattern.search(text_leave).span() rep = text_leave[start:end - 2] rep1 = text[start_start:end - 1] if &#39;，&#39; in rep1: rep1.replace(&#39;，&#39;, &#39;、&#39;) if &#39;,&#39; in rep1: rep1.replace(&#39;,&#39;, &#39;、&#39;) text.replace(rep1, text) text_leave = text_leave.replace(rep, &#39;&#39;) parallel_text.append(rep[1:]) return parallel_text, text else: return None, text def split_long_sentence_by_sep(text): segment = [] if &#39;。&#39; or &#39;.&#39; or &#39;!&#39; or &#39;！&#39; or &#39;?&#39; or &#39;？&#39; or &#39;;&#39; or &#39;；&#39; in text: text = re.split(r&#39;[。.!！?？;；]&#39;, text) for seg in text: if seg == &#39;&#39; or seg == &#39; &#39;: continue para, seg = extract_parallel(seg) if len(seg) &gt; 19: seg = split_long_sentence_by_pos(seg) if len(seg) &gt; 19: seg = re.split(&#39;[，,]&#39;, seg) if isinstance(seg, list) and &#39;&#39; in seg: seg = seg.remove(&#39;&#39;) if isinstance(seg, list) and &#39; &#39; in seg: seg = seg.remove(&#39; &#39;) segment.append(seg) return segment def read_data(path): return open(path, &quot;r&quot;, encoding=&quot;utf8&quot;) def get_np_words(t): noun_phrase_list = [] for tree in t.subtrees(lambda t: t.height() == 3): if tree.label() == &#39;NP&#39; and len(tree.leaves()) &gt; 1: noun_phrase = &#39;&#39;.join(tree.leaves()) noun_phrase_list.append(noun_phrase) return noun_phrase_list def get_n_v_pair(t): for tree in t.subtrees(lambda t: t.height() == 3): if tree.label() == &#39;NP&#39; and len(tree.leaves()) &gt; 1: noun_phrase = &#39;&#39;.join(tree.leaves()) if __name__ == &quot;__main__&quot;: out = open(&quot;dependency.txt&quot;, &#39;w&#39;, encoding=&#39;utf8&#39;) itera = read_data(&#39;text.txt&#39;) for it in itera: s = parse_sentence(it) # 通过Stanfordnlp依存句法分析得到一个句法树 用nltk包装成树的结构 res = search(s) # 使用nltk遍历树，然后把短语合并 print(res) stanfordParse.py # encoding=utf8 from stanfordcorenlp import StanfordCoreNLP from nltk import Tree, ProbabilisticTree nlp = StanfordCoreNLP(&#39;E:/stanford-corenlp-full-2018-10-05&#39;, lang=&#39;zh&#39;) import nltk, re grammer = &quot;NP: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN&gt;}&quot; cp = nltk.RegexpParser(grammer) # 生成规则 pattern = re.compile(u&#39;[^a-zA-Z\u4E00-\u9FA5]&#39;) pattern_del = re.compile(&#39;(\a-zA-Z0-9+)&#39;) def _replace_c(text): &quot;&quot;&quot; 将英文标点符号替换成中文标点符号，并去除html语言的一些标志等噪音 :param text: :return: &quot;&quot;&quot; intab = &quot;,?!()&quot; outtab = &quot;，？！（）&quot; deltab = &quot; \n&lt;li&gt;&lt; li&gt;+_-.&gt;&lt;li \U0010fc01 _&quot; trantab = text.maketrans(intab, outtab, deltab) return text.translate(trantab) def parse_sentence(text): text = _replace_c(text) # 文本去噪 try: if len(text.strip()) &gt; 6: # 判断，文本是否大于6个字，小于6个字的我们认为不是句子 return Tree.fromstring( nlp.parse(text.strip())) # nlp.parse(text.strip())：是将句子变成依存句法树 Tree.fromstring是将str类型的树转换成nltk的结构的树 except: pass def pos(text): text = _replace_c(text) if len(text.strip()) &gt; 6: return nlp.pos_tag(text) else: return False def denpency_parse(text): return nlp.dependency_parse(text) from nltk.chunk.regexp import * sentenceSplit_host.py # encoding=utf8 import re, os, json from stanfordParse import pos from stanfordParse import parse_sentence from recursionSearch import search def split_long_sentence_by_pos(text): del_flag = [&#39;DEC&#39;, &#39;AD&#39;, &#39;DEG&#39;, &#39;DER&#39;, &#39;DEV&#39;, &#39;SP&#39;, &#39;AS&#39;, &#39;ETC&#39;, &#39;SP&#39;, &#39;MSP&#39;, &#39;IJ&#39;, &#39;ON&#39;, &#39;JJ&#39;, &#39;FW&#39;, &#39;LB&#39;, &#39;SB&#39;, &#39;BA&#39;, &#39;AD&#39;, &#39;PN&#39;, &#39;RB&#39;] pos_tag = pos(text) new_str = &#39;&#39; for apos in pos_tag: if apos[1] not in del_flag: new_str += apos[0] return new_str def extract_parallel(text): parallel_text = [] pattern = re.compile(&#39;[，,][\u4e00-\u9fa5]{2,4}[，,]&#39;) search_obj = pattern.search(text) if search_obj: start_start, end = search_obj.span() rep = text[start_start:end - 2] rep1 = text[start_start:end - 1] if &#39;，&#39; in rep1: rep1.replace(&#39;，&#39;, &#39;、&#39;) if &#39;,&#39; in rep1: rep1.replace(&#39;,&#39;, &#39;、&#39;) text.replace(rep1, text) parallel_text.append(rep[1:]) text_leave = text.replace(rep, &#39;&#39;) while pattern.search(text_leave): start, end = pattern.search(text_leave).span() rep = text_leave[start:end - 2] rep1 = text[start_start:end - 1] if &#39;，&#39; in rep1: rep1.replace(&#39;，&#39;, &#39;、&#39;) if &#39;,&#39; in rep1: rep1.replace(&#39;,&#39;, &#39;、&#39;) text.replace(rep1, text) text_leave = text_leave.replace(rep, &#39;&#39;) parallel_text.append(rep[1:]) return parallel_text, text else: return None, text def split_long_sentence_by_sep(text): segment = [] if &#39;。&#39; or &#39;.&#39; or &#39;!&#39; or &#39;！&#39; or &#39;?&#39; or &#39;？&#39; or &#39;;&#39; or &#39;；&#39; in text: text = re.split(r&#39;[。.!！?？;；]&#39;, text) for seg in text: if seg == &#39;&#39; or seg == &#39; &#39;: continue para, seg = extract_parallel(seg) if len(seg) &gt; 19: seg = split_long_sentence_by_pos(seg) if len(seg) &gt; 19: seg = re.split(&#39;[，,]&#39;, seg) if isinstance(seg, list) and &#39;&#39; in seg: seg = seg.remove(&#39;&#39;) if isinstance(seg, list) and &#39; &#39; in seg: seg = seg.remove(&#39; &#39;) segment.append(seg) return segment def read_data(path): return open(path, &quot;r&quot;, encoding=&quot;utf8&quot;) def get_np_words(t): noun_phrase_list = [] for tree in t.subtrees(lambda t: t.height() == 3): if tree.label() == &#39;NP&#39; and len(tree.leaves()) &gt; 1: noun_phrase = &#39;&#39;.join(tree.leaves()) noun_phrase_list.append(noun_phrase) return noun_phrase_list def get_n_v_pair(t): for tree in t.subtrees(lambda t: t.height() == 3): if tree.label() == &#39;NP&#39; and len(tree.leaves()) &gt; 1: noun_phrase = &#39;&#39;.join(tree.leaves()) if __name__ == &quot;__main__&quot;: out = open(&quot;dependency.txt&quot;, &#39;w&#39;, encoding=&#39;utf8&#39;) itera = read_data(&#39;text.txt&#39;) for it in itera: s = parse_sentence(it) # 通过Stanfordnlp依存句法分析得到一个句法树 用nltk包装成树的结构 res = search(s) # 使用nltk遍历树，然后把短语合并 print(res) 语义依存分析 语义依存分析：分析句子各个语言单位之间的语义关联,并将语义关联以依存结构呈现。使用语义依存刻画句子语义,好处在于丌需要去抽象词汇本身,而是通过词汇所承受的语义框架来描述该词汇,而论元的数目相对词汇来说数量总是少了很多的。语义依存分析目标是跨越句子表层句法结构的束缚,直接获取深层的语义信息。 例如以下三个句子,用不同的表达方式表达了同一个语义信息,即张三实施了一个吃的动作,吃的动作是对苹果实施的。 • 语义依存分析不受句法结构的影响,将具有直接语义关联的语言单元直接连接依存弧并标记上相应的语义关系。这也是语义依存分析不句法依存分析的重要区别。 • 语义依存关系分为三类,分别是主要语义角色,每一种语义角色对应存在一个嵌套关系和反关系;事件关系,描述两个事件间的关系;语义依附标记,标记说话者语气等依附性信息。 语义依存分析标注关系及含义如下:关系类型 Tag Description Example 施事关系 Agt Agent 我送她一束花 (我 &lt;-- 送) 当事关系 Exp Experiencer 我跑得快 (跑 --&gt; 我) 感事关系 Aft Affection 我思念家乡 (思念 --&gt; 我) 领事关系 Poss Possessor 他有一本好读 (他 &lt;-- 有) 受事关系 Pat Patient 他打了小明 (打 --&gt; 小明) 客事关系 Cont Content 他听到鞭炮声 (听 --&gt; 鞭炮声) 成事关系 Prod Product 他写了本小说 (写 --&gt; 小说) 源事关系 Orig Origin 我军缴获敌人四辆坦克 (缴获 --&gt; 坦克) 涉事关系 Datv Dative 他告诉我个秘密 ( 告诉 --&gt; 我 ) 比较角色 Comp Comitative 他成绩比我好 (他 --&gt; 我) 属事角色 Belg Belongings 老赵有俩女儿 (老赵 &lt;-- 有) 类事角色 Clas Classification 他是中学生 (是 --&gt; 中学生) 依据角色 Accd According 本庭依法宣判 (依法 &lt;-- 宣判) 缘故角色 Reas Reason 他在愁女儿婚事 (愁 --&gt; 婚事) 。。。。。。 名词短语块挖掘# encoding=utf8 import os, json, nltk, re from jpype import * from tokenizer import cut_hanlp huanhang = set([&#39;。&#39;, &#39;？&#39;, &#39;！&#39;, &#39;?&#39;]) keep_pos = &quot;q,qg,qt,qv,s,t,tg,g,gb,gbc,gc,gg,gm,gp,mg,Mg,n,an,ude1,nr,ns,nt,nz,nb,nba,nbc,nbp,nf,ng,nh,nhd,o,nz,nx,ntu,nts,nto,nth,ntch,ntcf,ntcb,ntc,nt,nsf,ns,nrj,nrf,nr2,nr1,nr,nnt,nnd,nn,nmc,nm,nl,nit,nis,nic,ni,nhm,nhd&quot; keep_pos_nouns = set(keep_pos.split(&quot;,&quot;)) keep_pos_v = &quot;v,vd,vg,vf,vl,vshi,vyou,vx,vi,vn&quot; keep_pos_v = set(keep_pos_v.split(&quot;,&quot;)) keep_pos_p = set([&#39;p&#39;, &#39;pbei&#39;, &#39;pba&#39;]) merge_pos = keep_pos_p | keep_pos_v keep_flag = set( [&#39;：&#39;, &#39;，&#39;, &#39;？&#39;, &#39;。&#39;, &#39;！&#39;, &#39;；&#39;, &#39;、&#39;, &#39;-&#39;, &#39;.&#39;, &#39;!&#39;, &#39;,&#39;, &#39;:&#39;, &#39;;&#39;, &#39;?&#39;, &#39;(&#39;, &#39;)&#39;, &#39;（&#39;, &#39;）&#39;, &#39;&lt;&#39;, &#39;&gt;&#39;, &#39;《&#39;, &#39;》&#39;]) drop_pos_set = set( [&#39;xu&#39;, &#39;xx&#39;, &#39;y&#39;, &#39;yg&#39;, &#39;wh&#39;, &#39;wky&#39;, &#39;wkz&#39;, &#39;wp&#39;, &#39;ws&#39;, &#39;wyy&#39;, &#39;wyz&#39;, &#39;wb&#39;, &#39;u&#39;, &#39;ud&#39;, &#39;ude1&#39;, &#39;ude2&#39;, &#39;ude3&#39;, &#39;udeng&#39;, &#39;udh&#39;]) def getNodes(parent, model_tagged_file): # 使用for循环遍历树 text = &#39;&#39; for node in parent: if type(node) is nltk.Tree: # 如果是NP或者VP的合并分词 if node.label() == &#39;NP&#39;: text += &#39;&#39;.join(node_child[0].strip() for node_child in node.leaves()) + &quot;/NP&quot; + 3 * &quot; &quot; if node.label() == &#39;VP&#39;: text += &#39;&#39;.join(node_child[0].strip() for node_child in node.leaves()) + &quot;/VP&quot; + 3 * &quot; &quot; else: # 不是树的，就是叶子节点，我们直接表解词PP或者其他O if node[1] in keep_pos_p: text += node[0].strip() + &quot;/PP&quot; + 3 * &quot; &quot; if node[0] in huanhang: text += node[0].strip() + &quot;/O&quot; + 3 * &quot; &quot; if node[1] not in merge_pos: text += node[0].strip() + &quot;/O&quot; + 3 * &quot; &quot; # print(&quot;hh&quot;) model_tagged_file.write(text + &quot;\n&quot;) def grammer(sentence, model_tagged_file): # {内/f 训/v 师/ng 单/b 柜/ng} &quot;&quot;&quot; input sentences shape like :[(&#39;工作&#39;, &#39;vn&#39;), (&#39;描述&#39;, &#39;v&#39;), (&#39;：&#39;, &#39;w&#39;), (&#39;我&#39;, &#39;rr&#39;), (&#39;曾&#39;, &#39;d&#39;), (&#39;在&#39;, &#39;p&#39;)] &quot;&quot;&quot; # 定义名词块 “&lt; &gt;”:一个单元 “*”：匹配零次或多次 “+”：匹配一次或多次 “&lt;ude1&gt;?”： “的”出现零次或一次 grammar1 = r&quot;&quot;&quot;NP: {&lt;m|mg|Mg|mq|q|qg|qt|qv|s|&gt;*&lt;a|an|ag&gt;*&lt;s|g|gb|gbc|gc|gg|gm|gp|n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|o|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd&gt;+&lt;f&gt;?&lt;ude1&gt;?&lt;g|gb|gbc|gc|gg|gm|gp|n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|o|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd&gt;+} {&lt;n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd&gt;+&lt;cc&gt;+&lt;n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd&gt;+} {&lt;m|mg|Mg|mq|q|qg|qt|qv|s|&gt;*&lt;q|qg|qt|qv&gt;*&lt;f|b&gt;*&lt;vi|v|vn|vg|vd&gt;+&lt;ude1&gt;+&lt;n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd&gt;+} {&lt;g|gb|gbc|gc|gg|gm|gp|n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd&gt;+&lt;vi&gt;?} VP:{&lt;v|vd|vg|vf|vl|vshi|vyou|vx|vi|vn&gt;+} &quot;&quot;&quot; # 动词短语块 cp = nltk.RegexpParser(grammar1) try: result = cp.parse(sentence) # nltk的依存语法分析，输出是以grammer设置的名词块为单位的树 except: pass else: getNodes(result, model_tagged_file) # 使用 getNodes 遍历树【这个是使用for循环，上一个是使用栈动态添加】 def data_read(): fout = open(&#39;nvp.txt&#39;, &#39;w&#39;, encoding=&#39;utf8&#39;) for line in open(&#39;text.txt&#39;, &#39;r&#39;, encoding=&#39;utf8&#39;): line = line.strip() grammer(cut_hanlp(line), fout) # 先进行hanlp进行分词，在使用grammer进行合并短语 fout.close() if __name__ == &#39;__main__&#39;: data_read() 自定义语法与CFG什么是语法解析? • 在自然语言学习过程中,每个人一定都学过语法,例如句子可以用主语、谓语、宾语来表示。在自然语言的处理过程中,有许多应用场景都需要考虑句子的语法,因此研究语法解析变得非常重要。 • 语法解析有两个主要的问题,其一是句子语法在计算机中的表达与存储方法,以及语料数据集;其二是语法解析的算法。 句子语法在计算机中的表达与存储方法• 对于第一个问题,我们可以用树状结构图来表示,如下图所示,S表示句子;NP、VP、PP是名词、动词、介词短语(短语级别);N、V、P分别是名词、动词、介词。 语法解析的算法上下文无关语法(Context-Free Grammer)• 为了生成句子的语法树,我们可以定义如下的一套上下文无关语法。 • 1)N表示一组非叶子节点的标注,例如{S、NP、VP、N...} • 2)Σ表示一组叶子结点的标注,例如{boeing、is...} • 3)R表示一组觃则,每条规则可以表示为 • 4)S表示语法树开始的标注 • 举例来说,语法的一个语法子集可以表示为下图所示。 当给定一个句子时,我们便可以按照从左到右的顺序来解析语法。 例如,句子the man sleeps就可以表示为(S (NP (DT the) (NN man)) (VP sleeps))。 概率分布的上下文无关语法(Probabilistic Context-Free Grammar)• 上下文无关的语法可以很容易的推导出一个句子的语法结构,但是缺点是推导出的结构可能存在二义性。 • 由于语法的解析存在二义性,我们就需要找到一种方法从多种可能的语法树中找出最可能的一棵树。 一种常见的方法既是PCFG (Probabilistic Context-Free Grammar)。 如下图所示,除了常见的语法规则以外,我们还对每一条规则赋予了一个概率。 对于每一棵生成的语法树,我们将其中所有规则的概率的乘积作为语法树的出现概率。 当我们获得多颗语法树时,我们可以分别计算每颗语法树的概率p(t),出现概率最大的那颗语法树就是我们希望得到的结果,即arg max p(t)。 训练算法• 我们已经定义了语法解析的算法,而这个算法依赖于CFG中对于N、Σ、 R、S的定义以及PCFG中的p(x)。上文中我们提到了Penn Treebank通 过手工的方法已经提供了一个非常大的语料数据集,我们的任务就是从 语料库中训练出PCFG所需要的参数。 • 1)统计出语料库中所有的N与Σ; • 2)利用语料库中的所有规则作为R; • 3)针对每个规则A -&gt; B,从语料库中估算p(x) = p(A -&gt; B) / p(A); • 在CFG的定义的基础上,我们重新定义一种叫Chomsky的语法格式。 这种格式要求每条规则只能是X -&gt; Y1 Y2或者X -&gt; Y的格式。实际上 Chomsky语法格式保证生产的语法树总是二叉树的格式,同时任意一 棵语法树总是能够转化成Chomsky语法格式。语法树预测算法• 假设我们已经有一个PCFG的模型,包含N、Σ、R、S、p(x)等参数,并 且语法树总是Chomsky语法格式。当输入一个句子x1, x2, ... , xn时, 我们要如何计算句子对应的语法树呢? • 第一种方法是暴力遍历的方法,每个单词x可能有m = len(N)种取值, 句子长度是n,每种情况至少存在n个规则,所以在时间复杂度O(m n n) 的情况下,我们可以判断出所有可能的语法树并计算出最佳的那个。 • 第二种方法当然是动态规划,我们定义w[i, j, X]是第i个单词至第j个单 词由标注X来表示的最大概率。直观来讲,例如xi, xi+1, ... , xj,当 X=PP时,子树可能是多种解释方式,如(P NP)或者(PP PP),但是w[i, j, PP]代表的是继续往上一层递归时,我们只选择当前概率最大的组合 方式。 语法解析按照上述的算法过程便完成了。虽说PCFG也有一些缺点,例如:1)缺乏词法信息;2)连续短语(如名词、介词)的处理等。但总体来讲它给语法解析提供了一种非常有效的实现方法。 # encoding=utf8 def exec_cmd(cmd): p = subprocess.Popen( cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, env=ENVIRON) out, err = p.communicate() return out, err import nltk, os, jieba from nltk.tree import Tree from nltk.draw import TreeWidget from nltk.draw.tree import TreeView from nltk.draw.util import CanvasFrame from nltk.parse import RecursiveDescentParser class Cfg(): &#39;&#39;&#39; &#39;&#39;&#39; def setUp(self): pass def tearDown(self): pass def test_sample(self): print(&quot;test_sample&quot;) # This is a CFG grammar, where: # Start Symbol : S # Nonterminal : NP,VP,DT,NN,VB # Terminal : &quot;I&quot;, &quot;a&quot; ,&quot;saw&quot; ,&quot;dog&quot; grammar = nltk.grammar.CFG.fromstring(&quot;&quot;&quot; S -&gt; NP VP NP -&gt; DT NN | NN VP -&gt; VB NP DT -&gt; &quot;a&quot; NN -&gt; &quot;I&quot; | &quot;dog&quot; VB -&gt; &quot;saw&quot; &quot;&quot;&quot;) sentence = &quot;I saw a dog&quot;.split() parser = RecursiveDescentParser(grammar) final_tree = parser.parse(sentence) for i in final_tree: print(i) def test_nltk_cfg_qtype(self): print(&quot;test_nltk_cfg_qtype&quot;) gfile = os.path.join( curdir, os.path.pardir, &quot;config&quot;, &quot;grammar.question-type.cfg&quot;) question_grammar = nltk.data.load(&#39;file:%s&#39; % gfile) def get_missing_words(grammar, tokens): &quot;&quot;&quot; Find list of missing tokens not covered by grammar &quot;&quot;&quot; missing = [tok for tok in tokens if not grammar._lexical_index.get(tok)] return missing sentence = &quot;what is your name&quot; sent = sentence.split() missing = get_missing_words(question_grammar, sent) target = [] for x in sent: if x in missing: continue target.append(x) rd_parser = RecursiveDescentParser(question_grammar) result = [] print(&quot;target: &quot;, target) for tree in rd_parser.parse(target): result.append(x) print(&quot;Question Type\n&quot;, tree) if len(result) == 0: print(&quot;Not Question Type&quot;) def cfg_en(self): print(&quot;test_nltk_cfg_en&quot;) # 定义英文语法规则 grammar = nltk.CFG.fromstring(&quot;&quot;&quot; S -&gt; NP VP VP -&gt; V NP | V NP PP V -&gt; &quot;saw&quot; | &quot;ate&quot; NP -&gt; &quot;John&quot; | &quot;Mary&quot; | &quot;Bob&quot; | Det N | Det N PP Det -&gt; &quot;a&quot; | &quot;an&quot; | &quot;the&quot; | &quot;my&quot; N -&gt; &quot;dog&quot; | &quot;cat&quot; | &quot;cookie&quot; | &quot;park&quot; PP -&gt; P NP P -&gt; &quot;in&quot; | &quot;on&quot; | &quot;by&quot; | &quot;with&quot; &quot;&quot;&quot;) sent = &quot;Mary saw Bob&quot;.split() rd_parser = RecursiveDescentParser(grammar) result = [] for i, tree in enumerate(rd_parser.parse(sent)): result.append(tree) assert len(result) &gt; 0, &quot; CFG tree parse fail.&quot; print(result) def cfg_zh(self): grammar = nltk.CFG.fromstring(&quot;&quot;&quot; S -&gt; N VP VP -&gt; V NP | V NP | V N V -&gt; &quot;尊敬&quot; N -&gt; &quot;我们&quot; | &quot;老师&quot; &quot;&quot;&quot;) sent = &quot;我们 尊敬 老师&quot;.split() rd_parser = RecursiveDescentParser(grammar) result = [] for i, tree in enumerate(rd_parser.parse(sent)): result.append(tree) print(&quot;Tree [%s]: %s&quot; % (i + 1, tree)) assert len(result) &gt; 0, &quot;Can not recognize CFG tree.&quot; if len(result) == 1: print(&quot;Draw tree with Display ...&quot;) result[0].draw() else: print(&quot;WARN: Get more then one trees.&quot;) print(result) if __name__ == &#39;__main__&#39;: cfg = Cfg() cfg.cfg_en() cfg.cfg_zh()]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nlp分词词性标注及命名实体]]></title>
    <url>%2F2019%2F08%2F27%2Fnlp%E5%88%86%E8%AF%8D%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8%E5%8F%8A%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%2F</url>
    <content type="text"><![CDATA[nlp分词词性标注及命名实体 分词==中文分词==(Chinese Word Segmentation) 指的是将一个汉字序列切分成一个一个单独的词。分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。 词性标注==词性标注==(Part-of-Speech tagging 或POS tagging) 又称词类标注或者简称标注,是指为分词结果中的每个单词标注一个正确的词性的程 序,也即确定每个词是名词、动词、形容词或其他词性的过程。在汉语中,词性标注比较简单,因为汉语词汇词性多变的情况比较少见,大多词语只有一个词性,或者出现频次最高的词性远远高于第二位的词性。据说,只需选取最高频词性,即可实现80%准确率的中文词性标注程序。 命名实体识别==命名实体识别==(Named Entity Recognition,简称NER) 又称作“专名识别”,是指识别文本中具有特定意义的实体,主要包括人名、地名、机构名、专有名词等。一般来说,命名实体识别的任务就是识别出待处理文本中三大类(实体类、时间类和数字类)、七小类(人名、机构名、地名、时间、日期、货币和百分比)命名实体。 在不同的顷目中,命名实体类别具有不同的定义。 准确分词之加载自定义字典分词当分词工具分词不准确时,该怎么办? 加载自定义字典?该如何加载?cut_data.py # -*- coding=utf8 -*- import jieba import re from tokenizer import cut_hanlp # 加载字典 jieba.load_userdict(&quot;dict.txt&quot;) # 交叉拼接list，把FLAG*替换成*期 def merge_two_list(a, b): c = [] len_a, len_b = len(a), len(b) minlen = min(len_a, len_b) for i in range(minlen): c.append(a[i]) c.append(b[i]) if len_a &gt; len_b: for i in range(minlen, len_a): c.append(a[i]) else: for i in range(minlen, len_b): c.append(b[i]) return c if __name__ == &quot;__main__&quot;: fp = open(&quot;text.txt&quot;, &quot;r&quot;, encoding=&quot;utf8&quot;) fout = open(&quot;result_cut.txt&quot;, &quot;w&quot;, encoding=&quot;utf8&quot;) # 特殊符号字典无法分离，正则区分 regex1 = u&#39;(?:[^\u4e00-\u9fa5（）*&amp;……%￥$，,。.@! ！]){1,5}期&#39; # 非汉字xxx期 regex2 = r&#39;(?:[0-9]{1,3}[.]?[0-9]{1,3})%&#39; # xx.xx% p1 = re.compile(regex1) p2 = re.compile(regex2) for line in fp.readlines(): # 逐行读取 result1 = p1.findall(line) # 返回匹配到的list if result1: regex_re1 = result1 line = p1.sub(&quot;FLAG1&quot;, line) # 将匹配到的替换成FLAG1 result2 = p2.findall(line) if result2: line = p2.sub(&quot;FLAG2&quot;, line) words = jieba.cut(line) # 结巴分词，type(word)返回一个generator object result = &quot; &quot;.join(words) # 结巴分词结果 本身是一个generator object，所以使用 “ ”.join() 拼接起来 # E:\hanlp\data\dictionary\custom\resume_nouns.txt在E:\hanlp\hanlp.properties配置 # CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; 现代汉语补充词库.txt; 全国地名大全.txt ns; 人名词典.txt; 机构名词典.txt; resume_nouns.txt; 上海地名.txt ns;data/dictionary/person/nrf.txt nrf; words1 = cut_hanlp(line) # hanlp分词结果，返回的是str if &quot;FLAG1&quot; in result: result = result.split(&quot;FLAG1&quot;) result = merge_two_list(result, result1) ss = result result = &quot;&quot;.join(result) # 本身是个list，我们需要的是str，所以使用 &quot;&quot;.join() 拼接起来 if &quot;FLAG2&quot; in result: result = result.split(&quot;FLAG2&quot;) result = merge_two_list(result, result2) result = &quot;&quot;.join(result) # print(result) fout.write(&quot;jieba:&quot; + result) fout.write(&quot;hanlp:&quot; + words1) fout.close() tokenizer.py # encoding=utf8 import os, gc, re, sys from jpype import * root_path = &quot;E:/hanlp&quot; djclass_path = &quot;-Djava.class.path=&quot; + root_path + os.sep + &quot;hanlp-1.7.4.jar;&quot; + root_path startJVM(getDefaultJVMPath(), djclass_path, &quot;-Xms1g&quot;, &quot;-Xmx1g&quot;) Tokenizer = JClass(&#39;com.hankcs.hanlp.tokenizer.StandardTokenizer&#39;) def to_string(sentence, return_generator=False): if return_generator: return (word_pos_item.toString().split(&#39;/&#39;) for word_pos_item in Tokenizer.segment(sentence)) else: return &quot; &quot;.join([word_pos_item.toString().split(&#39;/&#39;)[0] for word_pos_item in Tokenizer.segment(sentence)]) # 这里的“”.split(&#39;/&#39;)可以将string拆分成list 如：&#39;ssfa/fsss&#39;.split(&#39;/&#39;) =&gt; [&#39;ssfa&#39;, &#39;fsss&#39;] def seg_sentences(sentence, with_filter=True, return_generator=False): segs = to_string(sentence, return_generator=return_generator) if with_filter: g = [word_pos_pair[0] for word_pos_pair in segs if len(word_pos_pair) == 2 and word_pos_pair[0] != &#39; &#39; and word_pos_pair[1] not in drop_pos_set] else: g = [word_pos_pair[0] for word_pos_pair in segs if len(word_pos_pair) == 2 and word_pos_pair[0] != &#39; &#39;] return iter(g) if return_generator else g def cut_hanlp(raw_sentence, return_list=True): if len(raw_sentence.strip()) &gt; 0: return to_string(raw_sentence) if return_list else iter(to_string(raw_sentence)) 准确分词之动态调整词频和字典顺序当分词字典的词冲突,相互影响该怎么办? 调整词频和字典顺序。cut_data.py # -*- coding=utf8 -*- import jieba import re from tokenizer import cut_hanlp jieba.load_userdict(&quot;dict.txt&quot;) # # 设置高词频：一个 # jieba.suggest_freq(&#39;台中&#39;,tune=True) # 设置高词频：dict.txt中的每一行都设置一下 # fp=open(&quot;dict.txt&quot;, &#39;r&#39;, encoding=&#39;utf8&#39;) # for line in fp: # line = line.strip() # jieba.suggest_freq(line, tune=True) # # 设置高词频：dict.txt中的每一行都设置一下快速方法 [jieba.suggest_freq(line.strip(), tune=True) for line in open(&quot;dict.txt&quot;, &#39;r&#39;, encoding=&#39;utf8&#39;)] if __name__ == &quot;__main__&quot;: string = &quot;台中正确应该不会被切开。&quot; # 通过调整词频 suggest_freq(line, tune=True) words_jieba = &quot; &quot;.join(jieba.cut(string, HMM=False)) # 通过排序sort_dict_by_lenth，优先按照长的字典项匹配 words_hanlp = cut_hanlp(string) print(&quot;words_jieba:&quot; + words_jieba, &#39;\n&#39;, &quot;words_hanlp:&quot; + words_hanlp) sort_dict_by_lenth.py # encoding=utf8 import os dict_file = open( &quot;E:&quot; + os.sep + &quot;hanlp&quot; + os.sep + &quot;data&quot; + os.sep + &quot;dictionary&quot; + os.sep + &quot;custom&quot; + os.sep + &quot;resume_nouns.txt&quot;, &#39;r&#39;, encoding=&#39;utf8&#39;) d = {} [d.update({line: len(line.split(&quot; &quot;)[0])}) for line in dict_file] # 读取源字典文件并从长到短排序 优先匹配长字典项 f = sorted(d.items(), key=lambda x: x[1], reverse=True) dict_file = open( &quot;E:&quot; + os.sep + &quot;hanlp&quot; + os.sep + &quot;data&quot; + os.sep + &quot;dictionary&quot; + os.sep + &quot;custom&quot; + os.sep + &quot;resume_nouns1.txt&quot;, &#39;w&#39;, encoding=&#39;utf8&#39;) [dict_file.write(item[0]) for item in f] dict_file.close() 词性标注代码实现及信息提取extract_data.py # -*- coding=utf8 -*- import jieba import re from tokenizer import seg_sentences fp = open(&quot;text.txt&quot;, &#39;r&#39;, encoding=&#39;utf8&#39;) fout = open(&quot;out.txt&quot;, &#39;w&#39;, encoding=&#39;utf8&#39;) for line in fp: line = line.strip() if len(line) &gt; 0: fout.write(&#39; &#39;.join(seg_sentences(line)) + &quot;\n&quot;) fout.close() if __name__ == &quot;__main__&quot;: pass tokenizer.py # encoding=utf8 import os, gc, re, sys from jpype import * root_path = &quot;E:/hanlp&quot; djclass_path = &quot;-Djava.class.path=&quot; + root_path + os.sep + &quot;hanlp-1.7.4.jar;&quot; + root_path startJVM(getDefaultJVMPath(), djclass_path, &quot;-Xms1g&quot;, &quot;-Xmx1g&quot;) Tokenizer = JClass(&#39;com.hankcs.hanlp.tokenizer.StandardTokenizer&#39;) keep_pos = &quot;q,qg,qt,qv,s,t,tg,g,gb,gbc,gc,gg,gm,gp,m,mg,Mg,mq,n,an,vn,ude1,nr,ns,nt,nz,nb,nba,nbc,nbp,nf,ng,nh,nhd,o,nz,nx,ntu,nts,nto,nth,ntch,ntcf,ntcb,ntc,nt,nsf,ns,nrj,nrf,nr2,nr1,nr,nnt,nnd,nn,nmc,nm,nl,nit,nis,nic,ni,nhm,nhd&quot; keep_pos_nouns = set(keep_pos.split(&quot;,&quot;)) keep_pos_v = &quot;v,vd,vg,vf,vl,vshi,vyou,vx,vi&quot; keep_pos_v = set(keep_pos_v.split(&quot;,&quot;)) keep_pos_p = set([&#39;p&#39;, &#39;pbei&#39;, &#39;pba&#39;]) drop_pos_set = set( [&#39;xu&#39;, &#39;xx&#39;, &#39;y&#39;, &#39;yg&#39;, &#39;wh&#39;, &#39;wky&#39;, &#39;wkz&#39;, &#39;wp&#39;, &#39;ws&#39;, &#39;wyy&#39;, &#39;wyz&#39;, &#39;wb&#39;, &#39;u&#39;, &#39;ud&#39;, &#39;ude1&#39;, &#39;ude2&#39;, &#39;ude3&#39;, &#39;udeng&#39;, &#39;udh&#39;, &#39;p&#39;, &#39;rr&#39;, &#39;w&#39;]) han_pattern = re.compile(r&#39;[^\dA-Za-z\u3007\u4E00-\u9FCB\uE815-\uE864]+&#39;) HanLP = JClass(&#39;com.hankcs.hanlp.HanLP&#39;) def to_string(sentence, return_generator=False): if return_generator: return (word_pos_item.toString().split(&#39;/&#39;) for word_pos_item in Tokenizer.segment(sentence)) else: return [(word_pos_item.toString().split(&#39;/&#39;)[0], word_pos_item.toString().split(&#39;/&#39;)[1]) for word_pos_item in Tokenizer.segment(sentence)] def seg_sentences(sentence, with_filter=True, return_generator=False): segs = to_string(sentence, return_generator=return_generator) if with_filter: g = [word_pos_pair[0] for word_pos_pair in segs if len(word_pos_pair) == 2 and word_pos_pair[0] != &#39; &#39; and word_pos_pair[1] not in drop_pos_set] else: g = [word_pos_pair[0] for word_pos_pair in segs if len(word_pos_pair) == 2 and word_pos_pair[0] != &#39; &#39;] return iter(g) if return_generator else g TextRank算法原理介绍tex_rank.py # -*- coding=utf8 -*- from jieba import analyse # 引入TextRank关键词抽取接口 textrank = analyse.textrank # 原始文本 text = &quot;非常线程是程序执行时的最小单位，它是进程的一个执行流，\ 是CPU调度和分派的基本单位，一个进程可以由很多个线程组成，\ 线程间共享进程的所有资源，每个线程有自己的堆栈和局部变量。\ 线程由CPU独立调度执行，在多CPU环境下就允许多个线程同时运行。\ 同样多线程也可以实现并发操作，每个请求分配一个线程来处理。&quot; print(&quot;\nkeywords by textrank:&quot;) # 基于TextRank算法进行关键词抽取 keywords = textrank(text, topK=10, withWeight=True, allowPOS=(&#39;ns&#39;, &#39;n&#39;)) # 输出抽取出的关键词 f words = [keyword for keyword, w in keywords if w &gt; 0.2] print(&#39; &#39;.join(words) + &quot;\n&quot;) jieba 词性标注 标注 含义 来源 Ag 形语素 形容词性语素形容词代码为 a,语素代码g前面置以A a 形容词 取英语形容词 adjective的第1个字母 ad 副形词 直接作状语的形容词形容词代码 a和副词代码d并在一起 an 名形词 具有名词功能的形容词形容词代码 a和名词代码n并在一起 b 区别词 取汉字“别”的声母 c 连词 取英语连词 conjunction的第1个字母 dg 副语素 副词性语素副词代码为 d,语素代码g前面置以D d 副词 取 adverb的第2个字母,因其第1个字母已用于形容词 e 叹词 取英语叹词 exclamation的第1个字母 f 方位词 取汉字“方” g 语素 绝大多数语素都能作为合成词的“词根”,取汉字“根”的声母 h 前接成分 取英语 head的第1个字母 i 成语 取英语成语 idiom的第1个字母 j 简称略语 取汉字“简”的声母 k 后接成分 l 习用语 习用语尚未成为成语,有点“临时性”,取“临”的声母 m 数词 取英语 numeral的第3个字母,n,u已有他用 Ng 名语素 名词性语素名词代码为 n,语素代码g前面置以N n 名词 取英语名词 noun的第1个字母 nr 人名 名词代码 n和“人(ren)”的声母并在一起 ns 地名 名词代码 n和处所词代码s并在一起 nt 机构团体 “团”的声母为 t,名词代码n和t并在一起 nz 其他丏名 “丏”的声母的第 1个字母为z,名词代码n和z并在一起 o 拟声词 取英语拟声词 onomatopoeia的第1个字母 p 介词 取英语介词 prepositional的第1个字母 q 量词 取英语 quantity的第1个字母 r 代词 取英语代词 pronoun的第2个字母,因p已用于介词 s 处所词 取英语 space的第1个字母 tg 时语素 时间词性语素时间词代码为 t,在语素的代码g前面置以T t 时间词 取英语 time的第1个字母 u 助词 取英语助词 auxiliary vg 动语素 动词性语素动词代码为 v在语素的代码g前面置以V v 动词 取英语动词 verb的第一个字母 vd 副动词 直接作状语的动词动词和副词的代码并在一起 vn 名动词 指具有名词功能的动词动词和名词的代码并在一起 w 标点符号 x 非语素字 非语素字只是一个符号,字母 x通常用于代表未知数、符号 y 语气词 取汉字“语”的声母 z 状态词 取汉字“状”的声母的前一个字母 un 未知词 不可识别词及用户自定义词组取英文Unkonwn首两个字母(非北大标准,CSW分词中定义) hanlp词性标注 标注 含义 a 形容词 ad 副形词 ag 形容词性语素 al 形容词性惯用语 an 名形词 b 区别词 begin 仅用于始##始 bg 区别语素 bl 区别词性惯用语 c 连词 cc 并列连词 d 副词 dg 辄,俱,复之类的副词 dl 连语 e 叹词 end 仅用于终##终 f 方位词 g 学术词汇 gb 生物相关词汇 gbc 生物类别 gc 化学相关词汇 gg 地理地质相关词汇 gi 计算机相关词汇 gm 数学相关词汇 gp 物理相关词汇 h 前缀 i 成语 j 简称略语 k 后缀 l 习用语 m 数词 mg 数语素 Mg 甲乙丙丁之类的数词 mq 数量词 n 名词 nb 生物名 nba 动物名 nbc 动物纲目 nbp 植物名 nf 食品，比如“薯片” ng 名词性语素 nh 医药疾病等健康相关名词 nhd 疾病 nhm 药品 ni 机构相关（不是独立机构名） nic 下属机构 nis 机构后缀 nit 教育相关机构 nl 名词性惯用语 nm 物品名 nmc 化学品名 nn 工作相关名词 nnd 职业 nnt 职务职称 nr 人名 nr1 复姓 nr2 蒙古姓名 nrf 音译人名 nrj 日语人名 ns 地名 nsf 音译地名 nt 机构团体名 ntc 公司名 ntcb 银行 ntcf 工厂 ntch 酒店宾馆 nth 医院 nto 政府机构 nts 中小学 ntu 大学 nx 字母专名 nz 其他专名 o 拟声词 p 介词 pba 介词“把” pbei 介词“被” q 量词 qg 量词语素 qt 时量词 qv 动量词 r 代词 rg 代词性语素 Rg 古汉语代词性语素 rr 人称代词 ry 疑问代词 rys 处所疑问代词 ryt 时间疑问代词 ryv 谓词性疑问代词 rz 指示代词 rzs 处所指示代词 rzt 时间指示代词 rzv 谓词性指示代词 s 处所词 t 时间词 tg 时间词性语素 u 助词 ud 助词 ude1 的 底 ude2 地 ude3 得 udeng 等 等等 云云 udh 的话 ug 过 uguo 过 uj 助词 ul 连词 ule 了 喽 ulian 连 （“连小学生都会”） uls 来讲 来说 而言 说来 usuo 所 uv 连词 uyy 一样 一般 似的 般 uz 着 uzhe 着 uzhi 之 v 动词 vd 副动词 vf 趋向动词 vg 动词性语素 vi 不及物动词（内动词） vl 动词性惯用语 vn 名动词 vshi 动词“是” vx 形式动词 vyou 动词“有” w 标点符号 wb 百分号千分号，全角：％ ‰ 半角：% wd 逗号，全角：， 半角：, wf 分号，全角：； 半角： ; wh 单位符号，全角：￥ ＄ ￡ ° ℃ 半角：$ wj 句号，全角：。 wky 右括号，全角：） 〕 ］ ｝ 》 】 〗 〉 半角： ) ] { &gt; wkz 左括号，全角：（ 〔 ［ ｛ 《 【 〖 〈 半角：( [ { &lt; wm 冒号，全角：： 半角： : wn 顿号，全角：、 wp 破折号，全角：—— －－ ——－ 半角：— —- ws 省略号，全角：…… … wt 叹号，全角：！ ww 问号，全角：？ wyy 右引号，全角：” ’ 』 wyz 左引号，全角：“ ‘ 『 x 字符串 xu 网址URL xx 非语素字 y 语气词(delete yg) yg 语气语素 z 状态词 zg 状态词]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nlp基础]]></title>
    <url>%2F2019%2F08%2F26%2Fnlp%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[nlp基础 简介 NLP( Natural Language Processing ) 是 自然 语言 处理 的 简称,是研究人与计算机交互的语言问题的一门学科。机器理解并解释人类写作与说话方式的能力。近年来,深度学习技术在自然语言处理方面的研究和应用也取得了显著的成果。 提问和回答、知识工程、语言生成、语音识别,语音合成,自动分词,句法分析,语法纠错,关键词提取,文本分类/聚类,文本自动摘要,信息检索(ES,Solr),信息抽取,知识图谱,机器翻译,人机对话,机器写作,情感分析,文字识别,阅读理解,推荐系统,高考机器人等。 环境搭建Anaconda3-5.1.0-Windows-x86_64.exe将Anaconda加入系统环境变量 常用开发包numpy numpy系统是Python的一种开源的数值计算包。 包括：1、一个强大的N维数组对象Array；2、比较成熟的（广播）函数库；3、用于整合C/C++和Fortran代码的工具包；4、实用的线性代数、傅里叶变换和随机数生成函数。numpy和稀疏矩阵运算包scipy配合使用更加方便。 conda install numpy NLTK Natural Language Toolkit，自然语言处理工具包，在NLP领域中， 最常使用的一个Python库。 conda install nltk Gensim Gensim是一个占内存低，接口简单，免费的Python库，它可以用来从文档中自动提取语义主题。它包含了很多非监督学习算法如：TF/IDF，潜在语义分析（Latent Semantic Analysis，LSA）、隐含狄利克雷分配（Latent Dirichlet Allocation，LDA），层次狄利克雷过程 （Hierarchical Dirichlet Processes，HDP）等。 Gensim支持Word2Vec,Doc2Vec等模型。 conda install gensimpip install gensim如不可安装python库gensim‑3.8.0‑cp36‑cp36m‑win_amd64.whl下载后pip install Tensorflow TensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统。TensorFlow可被用于语音识别或图像识别等多项机器学习和深度学习领域。TensorFlow是一个采用数据流图（data flow graphs），用于数值计算的开源软件库。节点（Nodes）在图中表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor）。它灵活的架构让你可以在多种平台上展开计算，例如台式计算机中的一个或多个CPU（戒GPU），服务器，移动设备等等。TensorFlow 最初由Google大脑小组（隶属于Google机器智能研究机构）的研究员和工程师们开发出来，用于机器学习和深度神经网络方面的研究，但这个系统的通用 性使其也可广泛用于其他计算领域。 conda install tensorflowpip install tensorflow python库pip install tensorflow-1.9.0-cp36-cp36m-win_amd64.whl下载后pip install jieba “结巴”中文分词：是广泛使用的中文分词工具，具有以下特点： 1）三种分词模式：精确模式，全模式和搜索引擎模式 2）词性标注和返回词语在原文的起止位置（ Tokenize） 3）可加入自定义字典 4）代码对 Python 2/3 均兼容 5）支持多种语言，支持简体繁体 项目地址 pip install jieba demo# encoding=utf-8 import jieba import jieba.posseg as pseg print(&quot;\njieba分词全模式：&quot;) seg_list = jieba.cut(&quot;我来到北京清华大学&quot;, cut_all=True) print(&quot;Full Mode: &quot; + &quot;/ &quot;.join(seg_list)) # 全模式 print(&quot;\njieba分词精确模式：&quot;) seg_list = jieba.cut(&quot;我来到北京清华大学&quot;, cut_all=False) print(&quot;Default Mode: &quot; + &quot;/ &quot;.join(seg_list)) # 精确模式 print(&quot;\njieba默认分词是精确模式：&quot;) seg_list = jieba.cut(&quot;他来到了网易杭研大厦&quot;) # 默认是精确模式 print(&quot;, &quot;.join(seg_list)) print(&quot;\njiba搜索引擎模式：&quot;) seg_list = jieba.cut_for_search(&quot;小明硕士毕业于中国科学院计算所，后在日本京都大学深造&quot;) # 搜索引擎模式 print(&quot;, &quot;.join(seg_list)) strings=&quot;是广泛使用的中文分词工具，具有以下特点：&quot; words = pseg.cut(strings) print(&quot;\njieba词性标注：&quot;) for word, flag in words: print(&#39;%s %s&#39; % (word, flag)) Stanford NLP Stanford NLP提供了一系列自然语言分析工具。它能够给出基本的 词形，词性，不管是公司名还是人名等，格式化的日期，时间，量词， 并且能够标记句子的结构，语法形式和字词依赖，指明那些名字指向同 样的实体，指明情绪，提取发言中的开放关系等。 1.一个集成的语言分析工具集； 2.进行快速，可靠的任意文本分析； 3.整体的高质量的文本分析; 4.支持多种主流语言; 5.多种编程语言的易用接口; 6.方便的简单的部署web服务。 Python 版本stanford nlp 安装 • 1)安装stanford nlp自然语言处理包: pip install stanfordcorenlp • 2)下载Stanford CoreNLP文件 https://stanfordnlp.github.io/CoreNLP/download.html • 3)下载中文模型jar包,https://nlp.stanford.edu/software/stanford-chinese-corenlp-2018-10-05-models.jar • 4)把下载的stanford-chinese-corenlp-2018-10-05-models.jar放在解压后的Stanford CoreNLP文件夹中，改Stanford CoreNLP文件夹名为stanfordnlp（可选） • 5)在Python中引用模型: • from stanfordcorenlp import StanfordCoreNLP • nlp = StanfordCoreNLP(r‘path&#39;, lang=&#39;zh&#39;) 例如： nlp = StanfordCoreNLP(r&#39;/home/kuo/NLP/module/stanfordnlp/&#39;, lang=&#39;zh&#39;) demo# -*-encoding=utf8-*- from stanfordcorenlp import StanfordCoreNLP nlp = StanfordCoreNLP(r&#39;E:\stanford-corenlp-full-2018-10-05&#39;, lang=&#39;zh&#39;) fin = open(&#39;news.txt&#39;, &#39;r&#39;, encoding=&#39;utf8&#39;) fner = open(&#39;ner.txt&#39;, &#39;w&#39;, encoding=&#39;utf8&#39;) ftag = open(&#39;pos_tag.txt&#39;, &#39;w&#39;, encoding=&#39;utf8&#39;) for line in fin: line = line.strip() # 去掉空行 if len(line) &lt; 1: continue # 命名实体识别 fner.write(&quot; &quot;.join([each[0] + &quot;/&quot; + each[1] for each in nlp.ner(line) if len(each) == 2]) + &quot;\n&quot;) # 词性识别 ftag.write(&quot; &quot;.join([each[0] + &quot;/&quot; + each[1] for each in nlp.pos_tag(line) if len(each) == 2]) + &quot;\n&quot;) fner.close() ftag.close() print(&quot;okkkkk&quot;) sentence = &#39;清华大学位于北京。&#39; print(nlp.word_tokenize(sentence)) print(nlp.pos_tag(sentence)) print(nlp.ner(sentence)) print(nlp.parse(sentence)) print(nlp.dependency_parse(sentence)) Hanlp HanLP是由一系列模型与算法组成的Java工具包，目标是普及自然 语言处理在生产环境中的应用。HanLP具备功能完善、性能高效、架构 清晰、语料时新、可自定义的特点。 功能：中文分词 词性标注 命名实体识别 依存句法分析 关键词提取 新词发现 短语提取 自动摘要 文本分类 拼音简繁 • 1、安装Java:我装的是Java 1.8 • 2、安裝Jpype, conda install -c conda-forge jpype1=0.7 [或者]pip install jpype1 • 3、测试是否按照成功: from jpype import * startJVM(getDefaultJVMPath(), &quot;-ea&quot;) java.lang.System.out.println(&quot;Hello World&quot;) shutdownJVM() • 比如data目录是root=E:/hanlp/data,那么root=root=E:/hanlp • 1、https://github.com/hankcs/HanLP/releases 下载hanlp-1.7.4-release.zip包，data-for-1.7.4.zip包,解压后重命名为hanlp• 2、配置文件• 示例配置文件:hanlp.properties• 配置文件的作用是告诉HanLP数据包的位置,只需修改第一行:root=E:/hanlp demo#-*- coding:utf-8 -*- from jpype import * startJVM(getDefaultJVMPath(), &quot;-Djava.class.path=E:\hanlp\hanlp-1.7.4.jar;E:\hanlp&quot;, &quot;-Xms1g&quot;, &quot;-Xmx1g&quot;) # 启动JVM，Linux需替换分号;为冒号: print(&quot;=&quot; * 30 + &quot;HanLP分词&quot; + &quot;=&quot; * 30) HanLP = JClass(&#39;com.hankcs.hanlp.HanLP&#39;) # 中文分词 print(HanLP.segment(&#39;你好，欢迎在Python中调用HanLP的API&#39;)) print(&quot;-&quot; * 70) print(&quot;=&quot; * 30 + &quot;标准分词&quot; + &quot;=&quot; * 30) StandardTokenizer = JClass(&#39;com.hankcs.hanlp.tokenizer.StandardTokenizer&#39;) print(StandardTokenizer.segment(&#39;你好，欢迎在Python中调用HanLP的API&#39;)) print(&quot;-&quot; * 70) # NLP分词NLPTokenizer会执行全部命名实体识别和词性标注 print(&quot;=&quot; * 30 + &quot;NLP分词&quot; + &quot;=&quot; * 30) NLPTokenizer = JClass(&#39;com.hankcs.hanlp.tokenizer.NLPTokenizer&#39;) print(NLPTokenizer.segment(&#39;中国科学院计算技术研究所的宗成庆教授正在教授自然语言处理课程&#39;)) print(&quot;-&quot; * 70) print(&quot;=&quot; * 30 + &quot;索引分词&quot; + &quot;=&quot; * 30) IndexTokenizer = JClass(&#39;com.hankcs.hanlp.tokenizer.IndexTokenizer&#39;) termList = IndexTokenizer.segment(&quot;主副食品&quot;); for term in termList: print(str(term) + &quot; [&quot; + str(term.offset) + &quot;:&quot; + str(term.offset + len(term.word)) + &quot;]&quot;) print(&quot;-&quot; * 70) print(&quot;=&quot; * 30 + &quot; CRF分词&quot; + &quot;=&quot; * 30) print(&quot;-&quot; * 70) print(&quot;=&quot; * 30 + &quot; 极速词典分词&quot; + &quot;=&quot; * 30) SpeedTokenizer = JClass(&#39;com.hankcs.hanlp.tokenizer.SpeedTokenizer&#39;) print(NLPTokenizer.segment(&#39;江西鄱阳湖干枯，中国最大淡水湖变成大草原&#39;)) print(&quot;-&quot; * 70) print(&quot;=&quot; * 30 + &quot; 自定义分词&quot; + &quot;=&quot; * 30) CustomDictionary = JClass(&#39;com.hankcs.hanlp.dictionary.CustomDictionary&#39;) CustomDictionary.add(&#39;攻城狮&#39;) CustomDictionary.add(&#39;单身狗&#39;) HanLP = JClass(&#39;com.hankcs.hanlp.HanLP&#39;) print(HanLP.segment(&#39;攻城狮逆袭单身狗，迎娶白富美，走上人生巅峰&#39;)) print(&quot;-&quot; * 70) print(&quot;=&quot; * 20 + &quot;命名实体识别与词性标注&quot; + &quot;=&quot; * 30) NLPTokenizer = JClass(&#39;com.hankcs.hanlp.tokenizer.NLPTokenizer&#39;) print(NLPTokenizer.segment(&#39;中国科学院计算技术研究所的宗成庆教授正在教授自然语言处理课程&#39;)) print(&quot;-&quot; * 70) document = &quot;水利部水资源司司长陈明忠9月29日在国务院新闻办举行的新闻发布会上透露，&quot; \ &quot;根据刚刚完成了水资源管理制度的考核，有部分省接近了红线的指标，&quot; \ &quot;有部分省超过红线的指标。对一些超过红线的地方，陈明忠表示，对一些取用水项目进行区域的限批，&quot; \ &quot;严格地进行水资源论证和取水许可的批准。&quot; print(&quot;=&quot; * 30 + &quot;关键词提取&quot; + &quot;=&quot; * 30) print(HanLP.extractKeyword(document, 8)) print(&quot;-&quot; * 70) print(&quot;=&quot; * 30 + &quot;自动摘要&quot; + &quot;=&quot; * 30) print(HanLP.extractSummary(document, 3)) print(&quot;-&quot; * 70) text = r&quot;算法工程师\n 算法（Algorithm）是一系列解决问题的清晰指令，也就是说，能够对一定规范的输入，在有限时间内获得所要求的输出。如果一个算法有缺陷，或不适合于某个问题，执行这个算法将不会解决这个问题。不同的算法可能用不同的时间、空间或效率来完成同样的任务。一个算法的优劣可以用空间复杂度与时间复杂度来衡量。算法工程师就是利用算法处理事物的人。\n \n 1职位简介\n 算法工程师是一个非常高端的职位；\n 专业要求：计算机、电子、通信、数学等相关专业；\n 学历要求：本科及其以上的学历，大多数是硕士学历及其以上；\n 语言要求：英语要求是熟练，基本上能阅读国外专业书刊；\n 必须掌握计算机相关知识，熟练使用仿真工具MATLAB等，必须会一门编程语言。\n\n2研究方向\n 视频算法工程师、图像处理算法工程师、音频算法工程师 通信基带算法工程师\n \n 3目前国内外状况\n 目前国内从事算法研究的工程师不少，但是高级算法工程师却很少，是一个非常紧缺的专业工程师。算法工程师根据研究领域来分主要有音频/视频算法处理、图像技术方面的二维信息算法处理和通信物理层、雷达信号处理、生物医学信号处理等领域的一维信息算法处理。\n 在计算机音视频和图形图像技术等二维信息算法处理方面目前比较先进的视频处理算法：机器视觉成为此类算法研究的核心；另外还有2D转3D算法(2D-to-3D conversion)，去隔行算法(de-interlacing)，运动估计运动补偿算法(Motion estimation/Motion Compensation)，去噪算法(Noise Reduction)，缩放算法(scaling)，锐化处理算法(Sharpness)，超分辨率算法(Super Resolution),手势识别(gesture recognition),人脸识别(face recognition)。\n 在通信物理层等一维信息领域目前常用的算法：无线领域的RRM、RTT，传送领域的调制解调、信道均衡、信号检测、网络优化、信号分解等。\n 另外数据挖掘、互联网搜索算法也成为当今的热门方向。\n&quot; print(&quot;=&quot; * 30 + &quot;短语提取&quot; + &quot;=&quot; * 30) print(HanLP.extractPhrase(text, 10)) print(&quot;-&quot; * 70) shutdownJVM()]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
        <tag>nlp</tag>
        <tag>numpy</tag>
        <tag>NLTK</tag>
        <tag>Gensim</tag>
        <tag>Stanford NLP</tag>
        <tag>Hanlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jmeter基础]]></title>
    <url>%2F2019%2F08%2F26%2Fjmeter%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[jmeter基本环境搭建及基础操作 压力测试工具对比 loadrunner 性能稳定，压测结果及细粒度大，可以自定义脚本进行压测，但是太过于重大，功能比较繁多 apache ab(单接口压测最方便) 模拟多线程并发请求,ab命令对发出负载的计算机要求很低，既不会占用很多CPU，也不会占用太多的内存，但却会给目标服务器造成巨大的负载, 简单DDOS攻击等 webbench webbench首先fork出多个子进程，每个子进程都循环做web访问测试。子进程把访问的结果通过pipe告诉父进程，父进程做最终的统计结果。 jmeter 压测不同的协议和应用 1) Web - HTTP, HTTPS (Java, NodeJS, PHP, ASP.NET, …) 2) SOAP / REST Webservices 3) FTP 4) Database via JDBC 5) LDAP 轻量目录访问协议 6) Message-oriented middleware (MOM) via JMS 7) Mail - SMTP(S), POP3(S) and IMAP(S) 8) TCP等等 使用场景及优点 1）功能测试 2）压力测试 3）分布式压力测试 4）纯java开发 5）上手容易，高性能 4）提供测试数据分析 5）各种报表数据图形展示环境搭建 需要安装JDK8。或者JDK9,JDK10 快速下载 windows： https://archive.apache.org/dist/jmeter/binaries/apache-jmeter-4.0.zip mac或者linux：https://archive.apache.org/dist/jmeter/binaries/apache-jmeter-4.0.tgz 目录 bin:核心可执行文件，包含配置 jmeter.bat: windows启动文件： jmeter: mac或者linux启动文件： jmeter-server：mac或者Liunx分布式压测使用的启动文件 jmeter-server.bat：mac或者Liunx分布式压测使用的启动文件 jmeter.properties: 核心配置文件 extras：插件拓展的包 lib:核心的依赖包 ext:核心包 junit:单元测试包 修改界面语言 1、控制台修改 menu -&gt; options -&gt; choose language 2、配置文件修改 bin目录 -&gt; jmeter.properties 默认 #language=en 改为 language=zh_CN 基本测试 java -jar gs-spring-boot-0.1.0.jar 添加-&gt;threads-&gt;线程组（控制总体并发） 线程数：虚拟用户数。一个虚拟用户占用一个进程或线程准备时长（Ramp-Up Period(in seconds)）：全部线程启动的时长，比如100个线程，20秒，则表示20秒内100个线程都要启动完成，每秒启动5个线程循环次数：每个线程发送的次数，假如值为5，100个线程，则会发送500次请求，可以勾选永远循环 线程组-&gt;添加-&gt; Sampler(采样器) -&gt; Http （一个线程组下面可以增加几个Sampler） 名称：采样器名称 注释：对这个采样器的描述web服务器： 默认协议是http 默认端口是80 服务器名称或IP ：请求的目标服务器名称或IP地址路径：服务器URLUse multipart/from-data for HTTP POST ：当发送POST请求时，使用Use multipart/from-data方法发送，默认不选中。 线程组-&gt;添加-&gt;监听器-&gt;察看结果树 该结果树属于全局，可已针对每一个请求设置结果树 线程组 -&gt; 添加 -&gt; 断言 -&gt; 响应断言 apply to(应用范围): Main sample only: 仅当前父取样器 进行断言，一般一个请求，如果发一个请求会触发多个，则就有sub sample（比较少用）要测试的响应字段： 响应文本：即响应的数据，比如json等文本 响应代码：http的响应状态码，比如200，302，404这些 响应信息：http响应代码对应的响应信息，例如：OK, Found Response Header: 响应头模式匹配规则： 包括：包含在里面就成功 匹配：响应内容完全匹配，不区分大小写 equals：完全匹配，区分大小写 线程组-&gt; 添加 -&gt; 监听器 -&gt; 断言结果 里面的内容是sampler采样器的名称断言失败，查看结果树任务结果颜色标红(通过结果数里面双击不通过的记录，可以看到错误信息)每个sample下面可以加单独的结果树，然后同时加多个断言，最外层可以加个结果树进行汇总 线程组-&gt;添加-&gt;监听器-&gt;聚合报告（Aggregate Report） lable: sampler的名称 Samples: 一共发出去多少请求,例如10个用户，循环10次，则是 100 Average: 平均响应时间 Median: 中位数，也就是 50％ 用户的响应时间 90% Line : 90％ 用户的响应不会超过该时间 （90% of the samples took no more than this time. The remaining samples at least as long as this） 95% Line : 95％ 用户的响应不会超过该时间 99% Line : 99％ 用户的响应不会超过该时间 min : 最小响应时间 max : 最大响应时间 Error%：错误的请求的数量/请求的总数 Throughput： 吞吐量——默认情况下表示每秒完成的请求数（Request per Second) 可类比为qps,并发数提高，qps不涨则瓶颈 KB/Sec: 每秒接收数据量 启动测试http请求 变量实操很多变量在全局中都有使用，或者测试数据更改，可以在一处定义，四处使用，比如服务器地址 线程组-&gt;add -&gt; Config Element(配置原件)-&gt; User Definde Variable（用户定义的变量） 通过${xx}调用 线程组-&gt;add -&gt; Config Element(配置原件)-&gt; CSV data set config (CSV数据文件设置) csv变量使用csv_name txt多变量使用csv_name csv_pwd 数据库test1 Add directory or jar to classpath添加mysql-connector-java-5.1.30.jarThread Group -&gt; add -&gt; sampler -&gt; jdbc request 无参查询 有参查询 预编译 更新 预编译 JDBC request-&gt;add -&gt; config element -&gt; JDBC connection configuration Variable Name for created pool同JDBC request 的Variable Name for created pool declared in JDBC connection configuration Thread Group -&gt; add -&gt; sampler -&gt; debug sampler variable name of pool declared in JDBC connection configuration（和配置文件同名）Query Type 查询类型parameter values 参数值parameter types 参数类型variable names sql执行结果变量名result variable names 所有结果当做一个对象存储query timeouts 查询超时时间handle results 处理结果集 分布式压测]]></content>
      <categories>
        <category>测试</category>
      </categories>
      <tags>
        <tag>jmeter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy爬取ssr链接]]></title>
    <url>%2F2019%2F08%2F23%2Fscrapy%E7%88%AC%E5%8F%96ssr%E9%93%BE%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[基于scrapy爬取ssr链接 环境搭建python3.5 虚拟环境virtualenvpip install virtualenv 提示pip版本太低 python -m pip install --upgrade pip pip install -i https://pypi.doubanio.com/simple/ --trusted-host pypi.doubanio.com django 使用豆瓣源加速 pip uninstall django 卸载django virtualenv scrapytest 默认环境创建虚拟环境 cd scrapytest/Scripts &amp;&amp; activate.bat &amp;&amp; python 进入3.5虚拟环境 virtualenv -p D:\Python27\python.exe scrapytest cd scrapytest/Scripts &amp;&amp; activate.bat &amp;&amp; python 进入2.7虚拟环境 deactivate.bat 退出虚拟环境 apt-get install python-virtualenv 安装虚拟环境 virtualenv py2 &amp;&amp; cd py2 &amp;&amp; cd bin &amp;&amp; source activate &amp;&amp; python 进入2.7虚拟环境 virtualenv -p /usr/bin/python3 py3 &amp;&amp; &amp;&amp; cd py3 &amp;&amp; cd bin &amp;&amp; source activate &amp;&amp; python 进入3.5虚拟环境虚拟环境virtualenvwrapperpip install virtualenvwrapper pip install virtualenvwrapper-win 解决workon不是内部指令 workon 列出所有虚拟环境 新建环境变量 WORKON_HOME=E:\envs mkvirtualenv py3scrapy 新建并进入虚拟环境 deactivate 退出虚拟环境 workon py3scrapy 进入指定虚拟环境 pip install -i https://pypi.douban.com/simple scrapy 安装scrapy源 若缺少lxml出错https://www.lfd.uci.edu/~gohlke/pythonlibs/寻找对应版本的lxml的whl源 python -m pip install --upgrade pip 更新pip pip install lxml-4.1.1-cp35-cp35m-win_amd64.whl 若缺少Twisted出错http://www.lfd.uci.edu/~gohlke/pythonlibs/#lxml搜对应版本Twisted pip install Twisted‑17.9.0‑cp35‑cp35m‑win_amd64.whl mkvirtualenv --python=D:\Python27\python.exe py2scrapy 一般不会出问题 pip install -i https://pypi.douban.com/simple scrapy pip install virtualenvwrapper find / -name virualenvwrapper.sh vim ~/.bashrc export WORKON_HOME=$HOME/.virtualenvs source /home/wj/.local/bin/virtualenvwrapper.sh source ~/.bashrc mkvirtualenv py2scrapy 指向生成~/.virtualenv deactivate 退出虚拟环境 mkdirtualenv --python=/usr/bin/python3 py3scrapy项目实战项目搭建pip install virtualenvwrapper-win mkvirtualenv --python=F:\Python\Python35\python.exe ssr pip install Twisted-17.9.0-cp35-cp35m-win_amd64.whl pip install -i https://pypi.douban.com/simple/ scrapy scrapy startproject ssr cd ssr scrapy genspider ssr https://freevpn-ss.tk/category/technology/ scrapy genspider --list scrapy genspider -t crawl lagou www.lagou.com 使用crawl模板 pycharm--新建项目---Pure Python---Interpreter为E:\envs\ssr\Scripts\python.exe pycharm--打开---ssr,修改settings--project Interpreter为D:\Envs\ss pip list pip install -i https://pypi.douban.com/simple pypiwin32 pillow requests redis fake-useragent pip install mysqlclient-1.4.4-cp35-cp35m-win_amd64.whl 或者pip install -i https://pypi.douban.com/simple mysqlclient 出错apt-get install libmysqlclient-dev 或者yum install python-devel mysql-devel scrapy crawl jobbole 修改settings.py ROBOTSTXT_OBEY = False scrapy shell http://blog.jobbole.com/ 可以在脚本中调试xpath或者chrome浏览器右键copy xpath,chrome浏览器右键copy selector scrapy shell -s USER_AGENT=&quot;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:51.0) Gecko/20100101 Firefox/51.0&quot; https://www.zhihu.com/question/56320032 pip freeze &gt; requirements.txt 生成依赖到文件 pip install -r requirements.txt 一键安装依赖 job_list = json.loads(response.text)[&quot;data&quot;][&quot;results返回response爬虫开发1scrapy shell https://freevpn-ss.tk/category/technology/ shell中查看节点 response.css(&quot;.posts-list .panel a::attr(href)&quot;).extract_first() response.css(&quot;.posts-list .panel a img::attr(src)&quot;).extract_first() response.xpath(&quot;//*[@id=&#39;container&#39;]/div/ul/li/article/a/img/@src&quot;).extract_first() view(response) 启动类main.py from scrapy.cmdline import execute import sys import os sys.path.append(os.path.dirname(os.path.abspath(__file__))) execute([&quot;scrapy&quot;, &quot;crawl&quot;, &quot;freevpn-ss.tk&quot;]) 基础配置ssr/settings.py import os BOT_NAME = &#39;ssr&#39; SPIDER_MODULES = [&#39;ssr.spiders&#39;] NEWSPIDER_MODULE = &#39;ssr.spiders&#39; # Crawl responsibly by identifying yourself (and your website) on the user-agent #USER_AGENT = &#39;ssr (+http://www.yourdomain.com)&#39; # Obey robots.txt rules ROBOTSTXT_OBEY = False import sys BASE_DIR = os.path.dirname(os.path.abspath(os.path.dirname(__file__))) sys.path.insert(0, os.path.join(BASE_DIR, &#39;ssr&#39;)) MYSQL_HOST = &quot;127.0.0.1&quot; MYSQL_DBNAME = &quot;scrapy&quot; MYSQL_USER = &quot;root&quot; MYSQL_PASSWORD = &quot;&quot; ITEM_PIPELINES = { &#39;ssr.pipelines.MysqlTwistedPipline&#39;: 2,#连接池异步插入 &#39;ssr.pipelines.JsonExporterPipleline&#39;: 1,#连接池异步插入 } ssr/pipelines.py from scrapy.exporters import JsonItemExporter from scrapy.pipelines.images import ImagesPipeline import codecs import json import MySQLdb import MySQLdb.cursors from twisted.enterprise import adbapi from ssr.utils.common import DateEncoder class SsrPipeline(object): def process_item(self, item, spider): return item class SsrImagePipeline(ImagesPipeline): def item_completed(self, results, item, info): if &quot;front_image_url&quot; in item: for ok, value in results: image_file_path = value[&quot;path&quot;] # 填充自定义路径 item[&quot;front_image_path&quot;] = image_file_path return item class JsonWithEncodingPipeline(object): # 自定义json文件的导出 def __init__(self): self.file = codecs.open(&#39;article.json&#39;, &#39;w&#39;, encoding=&quot;utf-8&quot;) def process_item(self, item, spider): # 序列化，ensure_ascii利于中文,json没法序列化date格式，需要新写函数 lines = json.dumps(dict(item), ensure_ascii=False, cls=DateEncoder) + &quot;\n&quot; self.file.write(lines) return item def spider_closed(self, spider): self.file.close() class JsonExporterPipleline(object): # 调用scrapy提供的json export导出json文件 def __init__(self): self.file = open(&#39;ssr.json&#39;, &#39;wb&#39;) self.exporter = JsonItemExporter(self.file, encoding=&quot;utf-8&quot;, ensure_ascii=False) self.exporter.start_exporting() def close_spider(self, spider): self.exporter.finish_exporting() self.file.close() def process_item(self, item, spider): self.exporter.export_item(item) return item class MysqlPipeline(object): # 采用同步的机制写入mysql def __init__(self): self.conn = MySQLdb.connect(&#39;127.0.0.1&#39;, &#39;root&#39;, &#39;123456&#39;, &#39;scrapy&#39;, charset=&quot;utf8&quot;, use_unicode=True) self.cursor = self.conn.cursor() def process_item(self, item, spider): insert_sql = &quot;&quot;&quot; insert into ssr(url, ip,ssr, port,password,secret) VALUES (%s, %s, %s, %s, %s) &quot;&quot;&quot; self.cursor.execute(insert_sql, (item[&quot;url&quot;],item[&quot;ssr&quot;], item[&quot;ip&quot;], item[&quot;port&quot;], item[&quot;password&quot;], item[&quot;secret&quot;])) self.conn.commit() class MysqlTwistedPipline(object): # 异步连接池插入数据库，不会阻塞 def __init__(self, dbpool): self.dbpool = dbpool @classmethod def from_settings(cls, settings):# 初始化时即被调用静态方法 dbparms = dict( host = settings[&quot;MYSQL_HOST&quot;],#setttings中定义 db = settings[&quot;MYSQL_DBNAME&quot;], user = settings[&quot;MYSQL_USER&quot;], passwd = settings[&quot;MYSQL_PASSWORD&quot;], charset=&#39;utf8&#39;, cursorclass=MySQLdb.cursors.DictCursor, use_unicode=True, ) dbpool = adbapi.ConnectionPool(&quot;MySQLdb&quot;, **dbparms) return cls(dbpool) def process_item(self, item, spider): #使用twisted将mysql插入变成异步执行 query = self.dbpool.runInteraction(self.do_insert, item) query.addErrback(self.handle_error, item, spider) #处理异常 def handle_error(self, failure, item, spider): #处理异步插入的异常 print (failure) def do_insert(self, cursor, item): #执行具体的插入，不具体的如MysqlPipeline.process_item() #根据不同的item 构建不同的sql语句并插入到mysql中 insert_sql, params = item.get_insert_sql() cursor.execute(insert_sql, params) 实体类ssr/items.py import scrapy from scrapy.loader import ItemLoader from scrapy.loader.processors import MapCompose, TakeFirst, Join import re import datetime from w3lib.html import remove_tags def date_convert(value): try: create_date = datetime.datetime.strptime(value, &quot;%Y/%m/%d&quot;).date() except Exception as e: create_date = datetime.datetime.now().date() return create_date def get_nums(value): match_re = re.match(&quot;.*?(\d+).*&quot;, value) if match_re: nums = int(match_re.group(1)) else: nums = 0 return nums def return_value(value): return value class SsrItemLoader(ItemLoader): # 自定义itemloader default_output_processor = TakeFirst() class SsrItem(scrapy.Item): url = scrapy.Field() ip = scrapy.Field( input_processor=MapCompose(return_value),#传递进来可以预处理 ) port = scrapy.Field() ssr = scrapy.Field() front_image_url = scrapy.Field() password = scrapy.Field() secret = scrapy.Field() def get_insert_sql(self): insert_sql = &quot;&quot;&quot; insert into ssr(url,ssr, ip, port, password,secret) VALUES (%s, %s,%s, %s, %s,%s) ON DUPLICATE KEY UPDATE ssr=VALUES(ssr) &quot;&quot;&quot; params = (self[&quot;url&quot;],self[&quot;ssr&quot;], self[&quot;ip&quot;],self[&quot;port&quot;], self[&quot;password&quot;],self[&quot;secret&quot;]) return insert_sql, params 核心代码ssr/spiders/freevpn_ss_tk.py # -*- coding: utf-8 -*- import time from datetime import datetime from urllib import parse import scrapy from scrapy.http import Request from ssr.items import SsrItemLoader, SsrItem class FreevpnSsTkSpider(scrapy.Spider): name = &#39;freevpn-ss.tk&#39; # 必须一级域名 allowed_domains = [&#39;freevpn-ss.tk&#39;] start_urls = [&#39;https://freevpn-ss.tk/category/technology/&#39;] custom_settings = { # 优先并覆盖项目，避免被重定向 &quot;COOKIES_ENABLED&quot;: False, # 关闭cookies &quot;DOWNLOAD_DELAY&quot;: 1, &#39;DEFAULT_REQUEST_HEADERS&#39;: { &#39;Accept&#39;: &#39;application/json, text/javascript, */*; q=0.01&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate, br&#39;, &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.8&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;, &#39;Cookie&#39;: &#39;&#39;, &#39;Host&#39;: &#39;freevpn-ss.tk&#39;, &#39;Origin&#39;: &#39;https://freevpn-ss.tk/&#39;, &#39;Referer&#39;: &#39;https://freevpn-ss.tk/&#39;, &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36&#39;, } } def parse(self, response): # post_nodes = response.css(&quot;.posts-list .panel&gt;a&quot;) # for post_node in post_nodes: # image_url = post_node.css(&quot;img::attr(src)&quot;).extract_first(&quot;&quot;) # post_url = post_node.css(&quot;::attr(href)&quot;).extract_first(&quot;&quot;) # yield Request(url=parse.urljoin(response.url, post_url), meta={&quot;front_image_url&quot;: image_url},callback=self.parse_detail) # response获取meta # # next_url = response.css(&quot;.next-page a::attr(href)&quot;).extract_first(&quot;&quot;) # if next_url: # print(next_url) # yield Request(url=parse.urljoin(response.url, next_url), callback=self.parse) post_node = response.css(&quot;.posts-list .panel&gt;a&quot;)[0] image_url = post_node.css(&quot;img::attr(src)&quot;).extract_first(&quot;&quot;) post_url = post_node.css(&quot;::attr(href)&quot;).extract_first(&quot;&quot;) yield Request(url=parse.urljoin(response.url, post_url), meta={&quot;front_image_url&quot;: image_url}, callback=self.parse_detail) # response获取meta def parse_detail(self, response): # 通过item loader加载item front_image_url = response.meta.get(&quot;front_image_url&quot;, &quot;&quot;) # 文章封面图 ssr_nodes = response.css(&quot;table tbody tr&quot;) with open(datetime.now().strftime(&#39;%Y-%m-%d&#39;), &#39;a&#39;) as file_object: for ssr in ssr_nodes: item_loader = SsrItemLoader(item=SsrItem(), response=response) # 默认ItemLoader是一个list，自定义TakeFirst() print(ssr.xpath(&quot;td[4]/text()&quot;).extract_first(&quot;&quot;)) item_loader.add_value(&quot;url&quot;, response.url) item_loader.add_value(&quot;ssr&quot;, ssr.css(&quot;td:nth-child(1)&gt;a::attr(href)&quot;).extract_first(&quot;&quot;)) item_loader.add_value(&quot;ip&quot;, ssr.css(&quot;td:nth-child(2)::text&quot;).extract_first(&quot;&quot;)) item_loader.add_value(&quot;front_image_url&quot;, front_image_url) item_loader.add_value(&quot;port&quot;, ssr.xpath(&quot;td[3]/text()&quot;).extract_first(&quot;&quot;)) item_loader.add_value(&quot;password&quot;, ssr.xpath(&quot;td[4]/text()&quot;).extract_first(&quot;&quot;)) item_loader.add_value(&quot;secret&quot;, ssr.xpath(&quot;td[5]/text()&quot;).extract_first(&quot;&quot;)) ssr_item = item_loader.load_item() file_object.write(ssr.css(&quot;td:nth-child(1)&gt;a::attr(href)&quot;).extract_first(&quot;&quot;)+&quot;\n&quot;) yield ssr_item # 将传到piplines中 爬虫开发2# -*- coding: utf-8 -*- import time from datetime import datetime from urllib import parse import scrapy from scrapy.http import Request from ssr.items import SsrItemLoader, SsrItem class FanQiangSpider(scrapy.Spider): name = &#39;fanqiang.network&#39; # 必须一级域名 allowed_domains = [&#39;fanqiang.network&#39;] start_urls = [&#39;https://fanqiang.network/免费ssr&#39;] def parse(self, response): post_nodes = response.css(&quot;.post-content table tbody tr&quot;) item_loader = SsrItemLoader(item=SsrItem(), response=response) with open(datetime.now().strftime(&#39;%Y-%m-%d&#39;), &#39;a&#39;) as file_object: for post_node in post_nodes: item_loader.add_value(&quot;url&quot;, response.url) item_loader.add_value(&quot;ssr&quot;, post_node.css(&quot;td:nth-child(1)&gt;a::attr(href)&quot;).extract_first(&quot;&quot;).replace(&quot;http://freevpn-ss.tk/&quot;, &quot;&quot;)) item_loader.add_value(&quot;ip&quot;, post_node.css(&quot;td:nth-child(2)::text&quot;).extract_first(&quot;&quot;)) item_loader.add_value(&quot;port&quot;, post_node.xpath(&quot;td[3]/text()&quot;).extract_first(&quot;&quot;)) item_loader.add_value(&quot;password&quot;, post_node.xpath(&quot;td[4]/text()&quot;).extract_first(&quot;&quot;)) item_loader.add_value(&quot;secret&quot;, post_node.xpath(&quot;td[5]/text()&quot;).extract_first(&quot;&quot;)) ssr_item = item_loader.load_item() file_object.write(post_node.css(&quot;td:nth-child(1)&gt;a::attr(href)&quot;).extract_first(&quot;&quot;).replace(&quot;http://freevpn-ss.tk/&quot;, &quot;&quot;) + &quot;\n&quot;) yield ssr_item # 将传到piplines中 多爬虫同时运行settings.py COMMANDS_MODULE = &#39;ssr&#39; ssr/crawlall.py from scrapy.commands import ScrapyCommand class Command(ScrapyCommand): requires_project = True def syntax(self): return &#39;[options]&#39; def short_desc(self): return &#39;Runs all of the spiders&#39; def run(self, args, opts): spider_list = self.crawler_process.spiders.list() for name in spider_list: self.crawler_process.crawl(name, **opts.__dict__) self.crawler_process.start()main.py from scrapy import cmdline from scrapy.cmdline import execute import sys import os sys.path.append(os.path.dirname(os.path.abspath(__file__))) # execute([&quot;scrapy&quot;, &quot;crawl&quot;, &quot;freevpn-ss.tk&quot;]) # execute([&quot;scrapy&quot;, &quot;crawl&quot;, &quot;fanqiang.network&quot;]) cmdline.execute(&quot;scrapy crawlall&quot;.split()) 防反爬随机uapip install -i https://pypi.doubanio.com/simple/ –trusted-host pypi.doubanio.com scrapy-fake-useragent DOWNLOADER_MIDDLEWARES = { &#39;scrapy_fake_useragent.middleware.RandomUserAgentMiddleware&#39;: 1, } 报错socket.timeout: timed out，查看F:/Anaconda3/Lib/site-packages/fake_useragent/settings.py __version__ = &#39;0.1.11&#39; DB = os.path.join( tempfile.gettempdir(), &#39;fake_useragent_{version}.json&#39;.format( version=__version__, ), ) CACHE_SERVER = &#39;https://fake-useragent.herokuapp.com/browsers/{version}&#39;.format( version=__version__, ) BROWSERS_STATS_PAGE = &#39;https://www.w3schools.com/browsers/default.asp&#39; BROWSER_BASE_PAGE = &#39;http://useragentstring.com/pages/useragentstring.php?name={browser}&#39; # noqa BROWSERS_COUNT_LIMIT = 50 REPLACEMENTS = { &#39; &#39;: &#39;&#39;, &#39;_&#39;: &#39;&#39;, } SHORTCUTS = { &#39;internet explorer&#39;: &#39;internetexplorer&#39;, &#39;ie&#39;: &#39;internetexplorer&#39;, &#39;msie&#39;: &#39;internetexplorer&#39;, &#39;edge&#39;: &#39;internetexplorer&#39;, &#39;google&#39;: &#39;chrome&#39;, &#39;googlechrome&#39;: &#39;chrome&#39;, &#39;ff&#39;: &#39;firefox&#39;, } OVERRIDES = { &#39;Edge/IE&#39;: &#39;Internet Explorer&#39;, &#39;IE/Edge&#39;: &#39;Internet Explorer&#39;, } HTTP_TIMEOUT = 5 HTTP_RETRIES = 2 HTTP_DELAY = 0.1 http://useragentstring.com/pages/useragentstring.php?name=Chrome 打开超时报错，其中CACHE_SERVER是存储了所有UserAgent的json数据，再次观察其中DB这个变量，结合fake_useragent\fake.py中的逻辑，判断这个变量应该是存储json数据的，所以大体逻辑应该是，首次初始化时，会自动爬取CACHE_SERVER中的json数据，然后将其存储到本地，所以我们直接将json存到指定路径下，再次初始化时，应该就不会报错 &gt;&gt;&gt; import tempfile &gt;&gt;&gt; print(tempfile.gettempdir()) C:\Users\codewj\AppData\Local\Temp 将CACHE_SERVER的json数据保存为fake_useragent_0.1.11.json,并放到目录C:\Users\codewj\AppData\Local\Temp中 &gt;&gt;&gt; import fake_useragent &gt;&gt;&gt; ua = fake_useragent.UserAgent() &gt;&gt;&gt; ua.data_browsers[&#39;chrome&#39;][0] &#39;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36&#39; 注：如果CACHE_SERVER不是https://fake-useragent.herokuapp.com/browsers/0.1.11，请更新一下库pip install –upgrade fake_useragent]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
        <tag>selenium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[好客租房mybatisplus]]></title>
    <url>%2F2019%2F08%2F22%2F%E5%A5%BD%E5%AE%A2%E7%A7%9F%E6%88%BFmybatisplus%2F</url>
    <content type="text"><![CDATA[基于dubbo react mybatisplus elk实战整合开发 mysqldocker run -di --name=mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 docker.io/mysql mysql -u root -p ALTER USER &#39;root&#39;@&#39;%&#39; IDENTIFIED WITH mysql_native_password BY &#39;123456&#39;; docker logs -fn 500 mysql MybatisPlus入门执行建表 haoke.sql mybatis-plus/pom.xml &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--mybatis-plus的springboot支持--&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.0.5&lt;/version&gt; &lt;/dependency&gt; &lt;!--mysql驱动--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; application.properties spring.application.name = itcast-mybatis-plus spring.datasource.driver-class-name=com.mysql.jdbc.Driver spring.datasource.url=jdbc:mysql://192.168.3.237:3306/haoke?useUnicode=true&amp;characterEncoding=utf8&amp;autoReconnect=true&amp;allowMultiQueries=true&amp;useSSL=false spring.datasource.username=root spring.datasource.password=123456 com/onejane/mybatisplus/pojo/User.java @Data public class User { @TableId(value = &quot;ID&quot;, type = IdType.AUTO) private Long id; private String name; private Integer age; private String email; } com/onejane/mybatisplus/mapper/UserMapper.java public interface UserMapper extends BaseMapper&lt;User&gt; { } com/onejane/mybatisplus/MyApplication.java @MapperScan(&quot;com.onejane.mybatisplus.mapper&quot;) //设置mapper接口的扫描包 @SpringBootApplication public class MyApplication { /** * 分页插件 */ @Bean public PaginationInterceptor paginationInterceptor() { return new PaginationInterceptor(); } public static void main(String[] args) { SpringApplication.run(MyApplication.class, args); } } com/onejane/mybatisplus/mapper/UserMaperTest.java @RunWith(SpringRunner.class) @SpringBootTest public class UserMaperTest { @Autowired private UserMapper userMapper; @Test public void testSelect(){ List&lt;User&gt; users = this.userMapper.selectList(null); for (User user : users) { System.out.println(user); } } @Test public void testSelectById(){ User user = this.userMapper.selectById(3L); System.out.println(user); } @Test public void testSelectByLike(){ QueryWrapper&lt;User&gt; wrapper = new QueryWrapper&lt;User&gt;(); wrapper.like(&quot;name&quot;, &quot;o&quot;); List&lt;User&gt; list = this.userMapper.selectList(wrapper); for (User user : list) { System.out.println(user); } } @Test public void testSelectByLe(){ QueryWrapper&lt;User&gt; wrapper = new QueryWrapper&lt;User&gt;(); wrapper.le(&quot;age&quot;, 20); List&lt;User&gt; list = this.userMapper.selectList(wrapper); for (User user : list) { System.out.println(user); } } @Test public void testSave(){ User user = new User(); user.setAge(25); user.setEmail(&quot;zhangsan@qq.com&quot;); user.setName(&quot;zhangsan&quot;); int count = this.userMapper.insert(user); System.out.println(&quot;新增数据成功! count =&gt; &quot; + count); } @Test public void testDelete(){ this.userMapper.deleteById(7L); System.out.println(&quot;删除成功!&quot;); } @Test public void testUpdate(){ User user = new User(); user.setId(6L); user.setName(&quot;lisi&quot;); this.userMapper.updateById(user); System.out.println(&quot;修改成功!&quot;); } @Test public void testSelectPage() { Page&lt;User&gt; page = new Page&lt;&gt;(2, 2); IPage&lt;User&gt; userIPage = this.userMapper.selectPage(page, null); System.out.println(&quot;总条数 ------&gt; &quot; + userIPage.getTotal()); System.out.println(&quot;当前页数 ------&gt; &quot; + userIPage.getCurrent()); System.out.println(&quot;当前每页显示数 ------&gt; &quot; + userIPage.getSize()); List&lt;User&gt; records = userIPage.getRecords(); for (User user : records) { System.out.println(user); } } } 兼容配置application.properties ## 指定全局配置文件 mybatis-plus.config-location = classpath:mybatis-config.xml # 指定mapper.xml文件 mybatis-plus.mapper-locations = classpath*:mybatis/*.xml Lombok&lt;!--简化代码的工具包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;version&gt;1.18.4&lt;/version&gt; &lt;/dependency&gt; 安装idea Lombok插件 @Data：注解在类上；提供类所有属性的 getting 和 setting 方法，此外还提供了equals、canEqual、hashCode、toString 方法 @Setter setting 方法 @Getter：注解在属性上；为属性提供 getting 方法 @Slf4j：注解在类上；为类提供一个 属性名为log 的 slf4j日志对象 @NoArgsConstructor：注解在类上；为类提供一个无参的构造方法 @AllArgsConstructor：注解在类上；为类提供一个全参的构造方法 @Builder：使用Builder模式构建对象 @Slf4j @Data @AllArgsConstructor @Builder public class Item { private Long id; private String title; private Long price; public Item() { log.info(&quot;写日志。。。。。&quot;); } public static void main(String[] args) { Item item1 = new Item(1L,&quot;哈哈哈&quot;,10L); Item item2 = Item.builder().price(100L)title(&quot;hello&quot;).id(1L).build(); System.out.println(item1.getId()); } } 搭建后台服务系统haoke-manage├─haoke-manage-api-server├─haoke-manage-dubbo-server│ ├─haoke-manage-dubbo-server-ad│ ├─haoke-manage-dubbo-server-common│ ├─haoke-manage-dubbo-server-generator MybatisPlus的AutoGenerator插件生成代码文件│ ├─haoke-manage-dubbo-server-house-resources│ │ ├─haoke-manage-dubbo-server-house-resources-dubbo-interface 对外提供的sdk包 只提供pojo实体以及接口，不提供实现类│ │ ├─haoke-manage-dubbo-server-house-resources-dubbo-service 具体实现 haoke-manage&lt;!--spring boot的支持放在groupId上面--&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.0.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;!--springboot 测试支持--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--dubbo的springboot支持--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;0.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--dubbo框架--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;dubbo&lt;/artifactId&gt; &lt;version&gt;2.6.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;3.7&lt;/version&gt; &lt;/dependency&gt; &lt;!--zk依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.13&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.sgroschupf&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; haoke-manage-api-serverpom.xml &lt;dependencies&gt; &lt;!--springboot的web支持--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.onejane&lt;/groupId&gt; &lt;artifactId&gt;haoke-manage-dubbo-server-house-resources-dubbo-interface&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; application.properties # Spring boot application spring.application.name = haoke-manage-api-server server.port = 18080 #logging.level.root=DEBUG # dubbo的应用名称 dubbo.application.name = dubbo-consumer-haoke-manage # zk注册中心 dubbo.registry.address = zookeeper://192.168.3.237:2181 dubbo.registry.client = zkclient Pagination @Data @AllArgsConstructor public class Pagination { private Integer current; private Integer pageSize; private Integer total; }TableResult @Data @AllArgsConstructor public class TableResult&lt;T&gt; { private List&lt;T&gt; list; private Pagination pagination; } HouseResourcesService @Service public class HouseResourcesService { @Reference(version = &quot;1.0.0&quot;) private ApiHouseResourcesService apiHouseResourcesService; public boolean save(HouseResources houseResources) { int result = this.apiHouseResourcesService.saveHouseResources(houseResources); return result == 1; } public TableResult&lt;HouseResources&gt; queryList(HouseResources houseResources, Integer currentPage, Integer pageSize) { PageInfo&lt;HouseResources&gt; pageInfo = this.apiHouseResourcesService. queryHouseResourcesList(currentPage, pageSize, houseResources); return new TableResult&lt;&gt;(pageInfo.getRecords(), new Pagination(currentPage, pageSize, pageInfo.getTotal())); } /** * 根据id查询房源数据 * * @param id * @return */ public HouseResources queryHouseResourcesById(Long id){ // 调用dubbo中的服务进行查询数据 return this.apiHouseResourcesService.queryHouseResourcesById(id); } public boolean update(HouseResources houseResources) { return this.apiHouseResourcesService.updateHouseResources(houseResources); } }HouseResourcesController @Controller @RequestMapping(&quot;house/resources&quot;) public class HouseResourcesController { @Autowired private HouseResourcesService houseResourcesService; /** * 新增房源 * * @param houseResources json数据 * @return */ @PostMapping @ResponseBody public ResponseEntity&lt;Void&gt; save(@RequestBody HouseResources houseResources) { try { boolean bool = this.houseResourcesService.save(houseResources); if (bool) { return ResponseEntity.status(HttpStatus.CREATED).build(); } } catch (Exception e) { e.printStackTrace(); } return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).build(); } /** * 查询房源列表 * * @param houseResources * @param currentPage * @param pageSize * @return */ @GetMapping @ResponseBody public ResponseEntity&lt;TableResult&gt; list(HouseResources houseResources, @RequestParam(name = &quot;currentPage&quot;, defaultValue = &quot;1&quot;) Integer currentPage, @RequestParam(name = &quot;pageSize&quot;, defaultValue = &quot;10&quot;) Integer pageSize) { return ResponseEntity.ok(this.houseResourcesService.queryList(houseResources, currentPage, pageSize)); } /** * 修改房源 * * @param houseResources json数据 * @return */ @PutMapping @ResponseBody public ResponseEntity&lt;Void&gt; update(@RequestBody HouseResources houseResources) { try { boolean bool = this.houseResourcesService.update(houseResources); if (bool) { return ResponseEntity.status(HttpStatus.NO_CONTENT).build(); } } catch (Exception e) { e.printStackTrace(); } return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).build(); } } DubboApiApplication @SpringBootApplication public class DubboApiApplication { public static void main(String[] args) { SpringApplication.run(DubboApiApplication.class, args); } }haoke-manage-dubbo-serverpom.xml &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; haoke-manage-dubbo-server-generatorpom.xml &lt;dependencies&gt; &lt;!-- freemarker 模板引擎 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.freemarker&lt;/groupId&gt; &lt;artifactId&gt;freemarker&lt;/artifactId&gt; &lt;version&gt;2.3.28&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.0.5&lt;/version&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt; CodeGenerator 配置数据源及账户密码，运行即可生成对应文件，并将pojo类拷贝到工程中备用 public class CodeGenerator { /** * &lt;p&gt; * 读取控制台内容 * &lt;/p&gt; */ public static String scanner(String tip) { Scanner scanner = new Scanner(System.in); StringBuilder help = new StringBuilder(); help.append(&quot;请输入&quot; + tip + &quot;：&quot;); System.out.println(help.toString()); if (scanner.hasNext()) { String ipt = scanner.next(); if (StringUtils.isNotEmpty(ipt)) { return ipt; } } throw new MybatisPlusException(&quot;请输入正确的&quot; + tip + &quot;！&quot;); } public static void main(String[] args) { // 代码生成器 AutoGenerator mpg = new AutoGenerator(); // 全局配置 GlobalConfig gc = new GlobalConfig(); String projectPath = System.getProperty(&quot;user.dir&quot;); gc.setOutputDir(projectPath + &quot;/src/main/java&quot;); gc.setAuthor(&quot;onejane&quot;); gc.setOpen(false); mpg.setGlobalConfig(gc); // 数据源配置 DataSourceConfig dsc = new DataSourceConfig(); dsc.setUrl(&quot;jdbc:mysql://192.168.3.237:3306/haoke?useUnicode=true&amp;characterEncoding=utf8&amp;autoReconnect=true&amp;allowMultiQueries=true&quot;); // dsc.setSchemaName(&quot;public&quot;); dsc.setDriverName(&quot;com.mysql.jdbc.Driver&quot;); dsc.setUsername(&quot;root&quot;); dsc.setPassword(&quot;123456&quot;); mpg.setDataSource(dsc); // 包配置 PackageConfig pc = new PackageConfig(); pc.setModuleName(scanner(&quot;模块名&quot;)); pc.setParent(&quot;com.onejane.haoke.dubbo.server&quot;); mpg.setPackageInfo(pc); // 自定义配置 InjectionConfig cfg = new InjectionConfig() { @Override public void initMap() { // to do nothing } }; List&lt;FileOutConfig&gt; focList = new ArrayList&lt;&gt;(); focList.add(new FileOutConfig(&quot;/templates/mapper.xml.ftl&quot;) { @Override public String outputFile(TableInfo tableInfo) { // 自定义输入文件名称 return projectPath + &quot;/src/main/resources/mapper/&quot; + pc.getModuleName() + &quot;/&quot; + tableInfo.getEntityName() + &quot;Mapper&quot; + StringPool.DOT_XML; } }); cfg.setFileOutConfigList(focList); mpg.setCfg(cfg); mpg.setTemplate(new TemplateConfig().setXml(null)); // 策略配置 StrategyConfig strategy = new StrategyConfig(); strategy.setNaming(NamingStrategy.underline_to_camel); strategy.setColumnNaming(NamingStrategy.underline_to_camel); strategy.setSuperEntityClass(&quot;com.onejane.haoke.dubbo.server.pojo.BasePojo&quot;); strategy.setEntityLombokModel(true); strategy.setRestControllerStyle(true); strategy.setSuperControllerClass(&quot;com.baomidou.ant.common.BaseController&quot;); strategy.setInclude(scanner(&quot;表名&quot;)); strategy.setSuperEntityColumns(&quot;id&quot;); strategy.setControllerMappingHyphenStyle(true); strategy.setTablePrefix(pc.getModuleName() + &quot;_&quot;); mpg.setStrategy(strategy); mpg.setTemplateEngine(new FreemarkerTemplateEngine()); mpg.execute(); } } 测试 haoke-manage-dubbo-server-house-resourcespom.xml &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.onejane&lt;/groupId&gt; &lt;artifactId&gt;haoke-manage-dubbo-server-common&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;!--需要注意：传递依赖中，如果需要使用，请显示引入--&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.0.5&lt;/version&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt; haoke-manage-dubbo-server-house-resources-dubbo-interfaceApiHouseResourcesService // dubbo service public interface ApiHouseResourcesService { /** * 新增房源 * * @param houseResources * @return -1:输入的参数不符合要求，0：数据插入数据库失败，1：成功 */ int saveHouseResources(HouseResources houseResources); /** * 分页查询房源列表 * * @param page 当前页 * @param pageSize 页面大小 * @param queryCondition 查询条件 * @return */ PageInfo&lt;HouseResources&gt; queryHouseResourcesList(int page, int pageSize, HouseResources queryCondition); /** * 根据id查询房源数据 * * @param id * @return */ HouseResources queryHouseResourcesById(Long id); boolean updateHouseResources(HouseResources houseResources); } HouseResources @Data @Accessors(chain = true) @TableName(&quot;tb_house_resources&quot;) public class HouseResources extends BasePojo { private static final long serialVersionUID = 779152022777511825L; @TableId(value = &quot;id&quot;, type = IdType.AUTO) private Long id; /** * 房源标题 */ private String title; /** * 楼盘id */ private Long estateId; /** * 楼号（栋） */ private String buildingNum; /** * 单元号 */ private String buildingUnit; /** * 门牌号 */ private String buildingFloorNum; /** * 租金 */ private Integer rent; /** * 租赁方式，1-整租，2-合租 */ private Integer rentMethod; /** * 支付方式，1-付一押一，2-付三押一，3-付六押一，4-年付押一，5-其它 */ private Integer paymentMethod; /** * 户型，如：2室1厅1卫 */ private String houseType; /** * 建筑面积 */ private String coveredArea; /** * 使用面积 */ private String useArea; /** * 楼层，如：8/26 */ private String floor; /** * 朝向：东、南、西、北 */ private String orientation; /** * 装修，1-精装，2-简装，3-毛坯 */ private Integer decoration; /** * 配套设施， 如：1,2,3 */ private String facilities; /** * 图片，最多5张 */ private String pic; /** * 描述 */ private String houseDesc; /** * 联系人 */ private String contact; /** * 手机号 */ private String mobile; /** * 看房时间，1-上午，2-中午，3-下午，4-晚上，5-全天 */ private Integer time; /** * 物业费 */ private String propertyCost; } haoke-manage-dubbo-server-house-resources-dubbo-servicepom.xml &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.onejane&lt;/groupId&gt; &lt;artifactId&gt;haoke-manage-dubbo-server-house-resources-dubbo-interface&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; application.properties # Spring boot application spring.application.name = haoke-manage-dubbo-server-house-resources # 数据库 spring.datasource.driver-class-name=com.mysql.jdbc.Driver spring.datasource.url=jdbc:mysql://192.168.3.237:3306/haoke?useUnicode=true&amp;characterEncoding=utf8&amp;autoReconnect=true&amp;allowMultiQueries=true&amp;useSSL=false spring.datasource.username=root spring.datasource.password=123456 # 服务的扫描包 dubbo.scan.basePackages = com.onejane.haoke.dubbo.server.api # 应用名称 dubbo.application.name = dubbo-provider-house-resources # 协议以及端口 dubbo.protocol.name = dubbo dubbo.protocol.port = 20880 # zk注册中心 dubbo.registry.address = zookeeper://192.168.3.237:2181 dubbo.registry.client = zkclient MybatisConfig @MapperScan(&quot;com.onejane.haoke.dubbo.server.mapper&quot;) @Configuration public class MybatisConfig { } HouseResourcesMapper public interface HouseResourcesMapper extends BaseMapper&lt;HouseResources&gt; { } HouseResourcesService // spring service public interface HouseResourcesService { /** * @param houseResources * * @return -1:输入的参数不符合要求，0：数据插入数据库失败，1：成功 */ int saveHouseResources(HouseResources houseResources); PageInfo&lt;HouseResources&gt; queryHouseResourcesList(int page, int pageSize, HouseResources queryCondition); /** * 根据房源id查询房源数据 * * @param id * @return */ HouseResources queryHouseResourcesById(Long id); /** * 更新房源数据 * * @param houseResources * @return */ boolean updateHouseResources(HouseResources houseResources); }HouseResourcesServiceImpl @Transactional @Service public class HouseResourcesServiceImpl extends BaseServiceImpl&lt;HouseResources&gt; implements HouseResourcesService { /** * @param houseResources * @return -1:输入的参数不符合要求，0：数据插入数据库失败，1：成功 */ @Override public int saveHouseResources(HouseResources houseResources) { // 添加校验或者是其他的一些逻辑 if (StringUtils.isBlank(houseResources.getTitle())) { // 不符合要求 return -1; } return super.save(houseResources); } @Override public PageInfo&lt;HouseResources&gt; queryHouseResourcesList(int page, int pageSize, HouseResources queryCondition) { QueryWrapper queryWrapper = new QueryWrapper(); // 根据数据的更新时间做倒序排序 queryWrapper.orderByDesc(&quot;updated&quot;); IPage iPage = super.queryPageList(queryWrapper, page, pageSize); return new PageInfo&lt;HouseResources&gt;(Long.valueOf(iPage.getTotal()).intValue(), page, pageSize, iPage.getRecords()); } public HouseResources queryHouseResourcesById(Long id) { return super.queryById(id); } @Override public boolean updateHouseResources(HouseResources houseResources) { return super.update(houseResources) == 1; } } DubboProvider @SpringBootApplication public class DubboProvider { /** * 分页插件 */ @Bean public PaginationInterceptor paginationInterceptor() { return new PaginationInterceptor(); } public static void main(String[] args) { new SpringApplicationBuilder(DubboProvider.class) .web(WebApplicationType.NONE) // 非 Web 应用 .run(args); } } ApiHouseResourcesServiceImpl @Service(version = &quot;1.0.0&quot;) // 分离dubbo服务和spring服务，易于扩展 public class ApiHouseResourcesServiceImpl implements ApiHouseResourcesService { @Autowired private HouseResourcesService houseResourcesService; @Override public int saveHouseResources(HouseResources houseResources) { return this.houseResourcesService.saveHouseResources(houseResources); } @Override public PageInfo&lt;HouseResources&gt; queryHouseResourcesList(int page, int pageSize, HouseResources queryCondition) { return this.houseResourcesService.queryHouseResourcesList(page, pageSize, queryCondition); } public HouseResources queryHouseResourcesById(Long id) { return this.houseResourcesService.queryHouseResourcesById(id); } @Override public boolean updateHouseResources(HouseResources houseResources) { return this.houseResourcesService.updateHouseResources(houseResources); } } 启动DubboAdmin中显示 测试 整合前端src/pages/haoke/House/AddResource.js添加标题及修改表单提交地址 &lt;FormItem {...formItemLayout} label=&quot;房源标题&quot;&gt; {getFieldDecorator(&#39;title&#39;,{rules:[{ required: true, message:&quot;此项为必填项&quot; }]})(&lt;Input style={{ width: '100%' }} /&gt;)} &lt;/FormItem&gt; dispatch({ type: &#39;house/submitHouseForm&#39;, payload: values, }); src/pages/haoke/House/models/form.js增加model import { routerRedux } from &#39;dva/router&#39;; import { message } from &#39;antd&#39;; import { addHouseResource } from &#39;@/services/haoke&#39;; export default { namespace: &#39;house&#39;, state: { }, effects: { *submitHouseForm({ payload }, { call }) { yield call(addHouseResource, payload); message.success(&#39;提交成功&#39;); } }, reducers: { saveStepFormData(state, { payload }) { return { ...state }; }, }, }; src/services/haoke.js增加服务，请求服务并且处理业务逻辑 import request from &#39;@/utils/request&#39;; export async function addHouseResource(params) { return request(&#39;/haoke/house/resources&#39;, { method: &#39;POST&#39;, body: params }); } 由于我们前端系统8000和后台服务系统18080的端口不同，会导致跨域问题，我们通过umi提供的反向代理功能解决这个问题。haoke-manage-web/config/config.js proxy: { &#39;/haoke/&#39;: { target: &#39;http://127.0.0.1:18080/&#39;, changeOrigin: true, pathRewrite: { &#39;^/haoke/&#39;: &#39;&#39; } } }, 代理效果是这样的：以haoke开头的请求都会被代理请求：http://localhost:8000/haoke/house/resources实际：http://127.0.0.1:18080/house/resources 测试启动前端：itcast-haoke-manage-web&gt;tyarn start启动提供者：itcast-haoke-manage-dubbo-server-house-resources-dubbo-service/DubboProvider启动消费者：tcast-haoke-manage/itcast-haoke-manage-api-server/DubboApiApplication]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>mybatisplus</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[好客租房dubbo]]></title>
    <url>%2F2019%2F08%2F21%2F%E5%A5%BD%E5%AE%A2%E7%A7%9F%E6%88%BFdubbo%2F</url>
    <content type="text"><![CDATA[基于dubbo react elk实战整合开发 整体架构 后端架构：SpringBoot+StringMVC+Dubbo+Mybatis+ELK+区块链 前端架构：React.js+html5+百度地图+微信小程序前端初始化haoke-manage-web tyarn install tyarn start 修改logo及版权信息src/layouts/BasicLayout.js 全局的布局文件 &lt;SiderMenu![itcast-haoke-manage-web](./attachments/itcast-haoke-manage-web.zip) logo={logo} Authorized={Authorized} theme={navTheme} onCollapse={this.handleMenuCollapse} menuData={menuData} isMobile={isMobile} {...this.props} /&gt; src/components/SiderMenu/index.js &lt;SiderMenu {...props} flatMenuKeys={getFlatMenuKeys(menuData)} collapsed={isMobile ? false : collapsed} /&gt; src/components/SiderMenu/SiderMenu.js &lt;div className={styles.logo} id=&quot;logo&quot;&gt; &lt;Link to=&quot;/&quot;&gt; {/* &lt;img src={logo} alt=&quot;logo&quot; /&gt; */} &lt;h1&gt;好客租房 · 管理系统&lt;/h1&gt; &lt;/Link&gt; &lt;/div&gt; src/layouts/Footer.js &lt;Footer style={{ padding: 0 }}&gt; &lt;GlobalFooter copyright={ &lt;Fragment&gt; Copyright &lt;Icon type=&quot;copyright&quot; /&gt; 2018 黑马程序员 博学谷 出品 &lt;/Fragment&gt; } /&gt; &lt;/Footer&gt; 新增房源config/router.config.js { path: &#39;/&#39;, redirect: &#39;/house/resource&#39; }, // 进入系统默认打开房源管理 { //房源管理 path: &#39;/house&#39;, name: &#39;house&#39;, icon: &#39;home&#39;, routes: [ { path: &#39;/house/resource&#39;, name: &#39;resource&#39;, component: &#39;./haoke/House/Resource&#39; }, { path: &#39;/house/addResource&#39;, name: &#39;addResource&#39;, component: &#39;./haoke/House/AddResource&#39; }, { path: &#39;/house/kanfang&#39;, name: &#39;kanfang&#39;, component: &#39;./haoke/House/KanFang&#39; }, { path: &#39;/house/zufang&#39;, name: &#39;zufang&#39;, component: &#39;./haoke/House/ZuFang&#39; } ] }, src/pages/haoke/House/AddResource.js @Form.create() 对页面进行了包装，包装之后，会在this.props中增加form对象,将拥有getFieldDecorator 双向绑定等功能,经过 getFieldDecorator 包装的控件，表单控件会自动添加 value （或 valuePropName 指定的其他属性） onChange （或 trigger 指定的其他属性），数据同步将被 Form 接管 你不再需要也不应该用 onChange 来做同步，但还是可以继续监听 onChange 等事件。 你不能用控件的 value defaultValue 等属性来设置表单域的值，默认值可以用getFieldDecorator 里的 initialValue,利用rule进行参数规则校验 你不应该用 setState ，可以使用 this.props.form.setFieldsValue 来动态改变表单值。 表单提交&lt;Button type=&quot;primary&quot; htmlType=&quot;submit&quot; loading={submitting}&gt; &lt;Form onSubmit={this.handleSubmit} hideRequiredMark style={{ marginTop: 8 }}&gt; 进行提交拦截 handleSubmit = e =&gt; { 通过form.validateFieldsAndScroll()对表单进行校验，通过values获取表单中输入的值。通过dispatch()调用model中定义的方法。 const { dispatch, form } = this.props; e.preventDefault(); console.log(this.state.fileList); form.validateFieldsAndScroll((err, values) =&gt; { if (!err) { if(values.facilities){ values.facilities = values.facilities.join(&quot;,&quot;); } if(values.floor_1 &amp;&amp; values.floor_2){ values.floor = values.floor_1 + &quot;/&quot; + values.floor_2; } values.houseType = values.houseType_1 + &quot;室&quot; + values.houseType_2 + &quot;厅&quot; + values.houseType_3 + &quot;卫&quot; + values.houseType_4 + &quot;厨&quot; + values.houseType_2 + &quot;阳台&quot;; delete values.floor_1; delete values.floor_2; delete values.houseType_1; delete values.houseType_2; delete values.houseType_3; delete values.houseType_4; delete values.houseType_5; dispatch({ type: &#39;form/submitRegularForm&#39;, payload: values, }); } }); }; 自动完成const estateMap = new Map([ [&#39;中远两湾城&#39;,&#39;1001|上海市,上海市,普陀区,远景路97弄&#39;], [&#39;上海康城&#39;,&#39;1002|上海市,上海市,闵行区,莘松路958弄&#39;], [&#39;保利西子湾&#39;,&#39;1003|上海市,上海市,松江区,广富林路1188弄&#39;], [&#39;万科城市花园&#39;,&#39;1004|上海市,上海市,闵行区,七莘路3333弄2区-15区&#39;], [&#39;上海阳城&#39;,&#39;1005|上海市,上海市,闵行区,罗锦路888弄&#39;] ]); &lt;AutoComplete style={{ width: '100%' }} dataSource={this.state.estateDataSource} placeholder=&quot;搜索楼盘&quot; onSelect={(value, option)=&gt;{ let v = estateMap.get(value); this.setState({ estateAddress: v.substring(v.indexOf(&#39;|&#39;)+1), estateId : v.substring(0,v.indexOf(&#39;|&#39;)) }); }} onSearch={this.handleSearch} filterOption={(inputValue, option) =&gt; option.props.children.toUpperCase().indexOf(inputValue.toUpperCase()) !== -1} /&gt; 通过onSearch进行动态设置数据源，这里使用的数据是静态数据 handleSearch = (value)=&gt;{ let arr = new Array(); if(value.length &gt; 0 ){ estateMap.forEach((v, k) =&gt; { if(k.startsWith(value)){ arr.push(k); } }); } this.setState({ estateDataSource: arr }); } ; 通过onSelect设置 选中楼盘后，在楼盘地址中填写地址数据 onSelect={(value, option)=&gt;{ let v = estateMap.get(value); this.setState({ estateAddress: v.substring(v.indexOf(&#39;|&#39;)+1), estateId : v.substring(0,v.indexOf(&#39;|&#39;)) }); }} 图片上传父组件通过属性的方式进行引用子组件，自组件在bind方法中改变this的引用为父组件 &lt;FormItem {...formItemLayout} label=&quot;上传室内图&quot;&gt; &lt;PicturesWall handleFileList={this.handleFileList.bind(this)}/&gt; &lt;/FormItem&gt; 父组件中获取数据 handleFileList = (obj)=&gt;{ console.log(obj, &quot;图片列表&quot;); } src/pages/haoke/Utils/PicturesWall.js 在子组件中通过this.props获取父组件方法传入的函数，进行调用，即可把数据传递到父组件中 handleChange = ({ fileList }) =&gt; { this.setState({ fileList }); this.props.handleFileList(this.state.fileList); } &lt;Upload action=&quot;1111111&quot; listType=&quot;picture-card&quot; fileList={fileList} onPreview={this.handlePreview} onChange={this.handleChange} &gt; {fileList.length &gt;= 5 ? null : uploadButton} &lt;/Upload&gt; 后端后台系统服务采用RPC+微服务的架构思想，RPC采用dubbo架构作为服务治理框架，对外接口采用RESTFul+GraphQL接口方式。 单一应用架构当网站流量很小时，只需一个应用，将所有功能都部署在一起，以减少部署节点和成本。此时，用于简化增删改查工作量的数据访问框架(ORM)是关键。 垂直应用架构当访问量逐渐增大，单一应用增加机器带来的加速度越来越小，将应用拆成互不相干的几个应用，以提升效率。此时，用于加速前端页面开发的Web框架(MVC)是关键。 分布式服务架构当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。此时，用于提高业务复用及整合的分布式服务框架(RPC)是关键。 流动计算架构当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。此时，用于提高机器利用率的资源调度和治理中心(SOA)是关键。调用关系说明 服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。流程说明： 服务提供者启动时: 向 /dubbo/com.foo.BarService/providers 目录下写入自己的 URL 地址 服务消费者启动时: 订阅 /dubbo/com.foo.BarService/providers 目录下的提供者 URL 地址。并向/dubbo/com.foo.BarService/consumers 目录下写入自己的 URL 地址 监控中心启动时: 订阅 /dubbo/com.foo.BarService 目录下的所有提供者和消费者 URL 地址。支持以下功能： 当提供者出现断电等异常停机时，注册中心能自动删除提供者信息 当注册中心重启时，能自动恢复注册数据，以及订阅请求 当会话过期时，能自动恢复注册数据，以及订阅请求 当设置 &lt;dubbo:registry check=”false” /&gt; 时，记录失败注册和订阅请求，后台定时重试 可通过 &lt;dubbo:registry username=”admin” password=”1234” /&gt; 设置 zookeeper 登录信息 可通过 &lt;dubbo:registry group=”dubbo” /&gt; 设置 zookeeper 的根节点，不设置将使用无根树 支持 * 号通配符 &lt;dubbo:reference group=”*” version=”*” /&gt; ，可订阅服务的所有分组和所有版本的提供者 zk安装apt-get install --reinstall systemd -y apt-get install -y docker.io systemctl start docker docker create --name zk -p 2181:2181 zookeeper:3.5 docker start zk 可以通过ZooInspector连接查看 yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum makecache fast yum list docker-ce --showduplicates | sort -r yum install -y docker-ce-18.09.5 systemctl restart docker docker create --name zk -p 2181:2181 zookeeper:3.5 docker start zk ZooInspector执行ZooInspector\build\start.bat查看zk信息 服务提供方dubbo/pom.xml &lt;!--添加SpringBoot parent支持--&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.0.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;!--添加SpringBoot测试--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--添加dubbo的springboot依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;0.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--添加dubbo依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;dubbo&lt;/artifactId&gt; &lt;version&gt;2.6.4&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;!--添加springboot的maven插件--&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; dubbo/dubbo-service/pom.xml &lt;dependencies&gt; &lt;!--添加springboot依赖，非web项目--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.13&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.sgroschupf&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; com/onejane/dubbo/pojo/User.java // 使用dubbo要求传输的对象必须实现序列化接口 public class User implements java.io.Serializable { private static final long serialVersionUID = -7341603933521593227L; private Long id; private String username; private String password; private Integer age; } com/onejane/dubbo/service/UserService.java public interface UserService { List&lt;User&gt; queryAll(); } com/onejane/dubbo/service/impl/UserServiceImpl.java @Service(version = &quot;${dubbo.service.version}&quot;) //声明这是一个dubbo服务 public class UserServiceImpl implements UserService { public List&lt;User&gt; queryAll() { List&lt;User&gt; list = new ArrayList&lt;User&gt;(); for (int i = 0; i &lt; 10; i++) { User user = new User(); user.setAge(10 + i); user.setId(Long.valueOf(i + 1)); user.setPassword(&quot;123456&quot;); user.setUsername(&quot;username_&quot; + i); list.add(user); } System.out.println(&quot;---------Service 3------------&quot;); return list; } } application.properties # Spring boot application spring.application.name = dubbo-service server.port = 9090 # Service version dubbo.service.version = 1.0.0 # 服务的扫描包 dubbo.scan.basePackages =com.onejane.dubbo.service # 应用名称 dubbo.application.name = dubbo-provider-demo # 协议以及端口 dubbo.protocol.name = dubbo dubbo.protocol.port = 20882 # zk注册中心 dubbo.registry.address = zookeeper://192.168.3.237:2181 dubbo.registry.client = zkclient com/onejane/dubbo/DubboProvider.java @SpringBootApplication public class DubboProvider { public static void main(String[] args) { new SpringApplicationBuilder(DubboProvider.class) .web(WebApplicationType.NONE) // 非 Web 应用 .run(args); } } 服务消费方dubbo/dubbo-comsumer/pom.xml &lt;dependencies&gt; &lt;!--添加springboot依赖，非web项目--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.13&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.sgroschupf&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.1&lt;/version&gt; &lt;/dependency&gt; &lt;!--引入service的依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;com.onejane&lt;/groupId&gt; &lt;artifactId&gt;dubbo-service&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; application.properties # Spring boot application spring.application.name = dubbo-consumer server.port = 9091 # 应用名称 dubbo.application.name = dubbo-consumer-demo # zk注册中心 dubbo.registry.address = zookeeper://192.168.3.237:2181 dubbo.registry.client = zkclient com/onejane/dubbo/UserServiceTest.java @RunWith(SpringRunner.class) @SpringBootTest public class UserServiceTest { // 负载均衡策略 默认随机 测试时启动dubbo.protocol.port多个不同端口的userService服务，并修改打印值进行区分 // loadbalance = &quot;roundrobin&quot;设置负载均衡策略 @Reference(version = &quot;1.0.0&quot;, loadbalance = &quot;roundrobin&quot;) private UserService userService; @Test public void testQueryAll() { for (int i = 0; i &lt; 100; i++) { System.out.println(&quot;开始调用远程服务 &gt;&gt;&gt;&gt;&gt;&quot; + i); List&lt;User&gt; users = this.userService.queryAll(); for (User user : users) { System.out.println(user); } try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } } } }启动服务后，即可测试 Dubbo Admintar zxf incubator-dubbo-ops.tar.gz -C /usr/local/ tar zxf apache-maven-3.6.0-bin.tar.gz -C /usr/local/ vim incubator-dubbo-ops/dubbo-admin-backend/src/main/resources/application.properties dubbo.registry.address=zookeeper://192.168.3.237:2181 vim /etc/profile 如误操作导致基础命令丢失，export PATH=/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin export MAVEN_HOME=/usr/local/apache-maven-3.6.0 export JAVA_HOME=/usr/local/jdk export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$MAVEN_HOME/bin:$PATH source /etc/profile cd /usr/local/incubator-dubbo-ops &amp;&amp; mvn clean install vim dubbo-admin-backend/src/main/resources/application.properties server.port=8888 mvn --projects dubbo-admin-backend spring-boot:run http://192.168.3.237:8888Dubbo 缺省协议不适合传送大数据量的服务，比如传文件，传视频等，除非请求量很低。Dubbo 缺省协议dubbo:// 协议采用单一长连接和 NIO 异步通讯，适合于小数据量大并发的服务调用，以及服务消费者机器数远大于服务提供者机器数的情况。 Transporter （传输）: mina, netty, grizzySerialization（序列化）: dubbo, hessian2, java, jsonDispatcher（分发调度）: all, direct, message, execution, connectionThreadPool（线程池）: fixed, cached 连接个数：单连接 连接方式：长连接 传输协议：TCP 传输方式：NIO 异步传输 序列化：Hessian 二进制序列化 适用范围：传入传出参数数据包较小（建议小于100K），消费者比提供者个数多，单一消费者无法压满提供 者，尽量不要用 dubbo 协议传输大文件或超大字符串。 适用场景：常规远程服务方法调用]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>ant design pro</tag>
        <tag>dubbo</tag>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[好客租房react]]></title>
    <url>%2F2019%2F08%2F21%2F%E5%A5%BD%E5%AE%A2%E7%A7%9F%E6%88%BFreact%2F</url>
    <content type="text"><![CDATA[基于spring cloud alibaba实战整合开发 React入门mock dvareact/config/config.js export default { plugins:[ [&#39;umi-plugin-react&#39;,{ dva: true, //开启dva进行数据分层管理 }] ] }; react/mock/MockListData.js export default { &#39;get /ds/list&#39;: function (req, res) { //模拟请求返回数据 res.json({ data: [1, 2, 3, 4, 5], maxNum: 5 }); } } react/src/util/request.js // import fetch from &#39;dva/fetch&#39;; function checkStatus(response) { if (response.status &gt;= 200 &amp;&amp; response.status &lt; 300) { return response; } const error = new Error(response.statusText); error.response = response; throw error; } /** * Requests a URL, returning a promise. * * @param {string} url The URL we want to request * @param {object} [options] The options we want to pass to &quot;fetch&quot; * @return {object} An object containing either &quot;data&quot; or &quot;err&quot; */ export default async function request(url, options) { const response = await fetch(url, options); checkStatus(response); return await response.json(); } react/src/models/ListData.js import request from &#39;../util/request&#39;; export default { namespace: &#39;list&#39;, state: { data: [], maxNum: 1 }, reducers : { // 定义的一些函数 addNewData : function (state, result) { // state：指的是更新之前的状态数据, result: 请求到的数据 if(result.data){ //如果state中存在data数据，直接返回，在做初始化的操作 return result.data; } let maxNum = state.maxNum + 1; let newArr = [...state.data, maxNum]; return { data : newArr, maxNum : maxNum } //通过return 返回更新后的数据 } }, effects: { //新增effects配置，用于异步加载数据 *initData(params, sagaEffects) { //定义异步方法 const {call, put} = sagaEffects; //获取到call、put方法 const url = &quot;/ds/list&quot;; // 定义请求的url let data = yield call(request, url); //执行请求 yield put({ // 调用reducers中的方法 type : &quot;addNewData&quot;, //指定方法名 data : data //传递ajax回来的数据 }); } } } react/src/pages/List.js import React from &#39;react&#39;; import { connect } from &#39;dva&#39;; const namespace = &quot;list&quot;; // 说明：第一个回调函数，作用：将page层和model层进行链接，返回modle中的数据,并且将返回的数据，绑定到this.props // 接收第二个函数，这个函数的作用：将定义的函数绑定到this.props中，调用model层中定义的函数 @connect((state) =&gt; { return { dataList : state[namespace].data, maxNum : state[namespace].maxNum } }, (dispatch) =&gt; { // dispatch的作用：可以调用model层定义的函数 return { // 将返回的函数，绑定到this.props中 add : function () { dispatch({ //通过dispatch调用modle中定义的函数,通过type属性，指定函数命名，格式：namespace/函数名 type : namespace + &quot;/addNewData&quot; }); }, init : () =&gt; { dispatch({ //通过dispatch调用modle中定义的函数,通过type属性，指定函数命名，格式：namespace/函数名 type : namespace + &quot;/initData&quot; }); } } }) class List extends React.Component{ componentDidMount(){ //初始化的操作 this.props.init(); } render(){ return ( &lt;div&gt; &lt;ul&gt; { this.props.dataList.map((value,index)=&gt;{ return &lt;li key={index}&gt;{value}&lt;/li&gt; }) } &lt;/ul&gt; &lt;button onClick={() =&gt; { this.props.add(); }}&gt;点我&lt;/button&gt; &lt;/div&gt; ); } } export default List; umi dev Ant Design 入门react/config/config.js export default { plugins:[ [&#39;umi-plugin-react&#39;,{ dva: true, //开启dva进行数据分层管理 antd: true // 开启Ant Design功能 }] ], routes: [{ path: &#39;/&#39;, component: &#39;../layouts&#39;, //配置布局路由 routes: [ { path: &#39;/&#39;, component: &#39;./index&#39; }, { path: &#39;/myTabs&#39;, component: &#39;./myTabs&#39; }, { path: &#39;/user&#39;, routes: [ { path: &#39;/user/list&#39;, component: &#39;./user/UserList&#39; }, { path: &#39;/user/add&#39;, component: &#39;./user/UserAdd&#39; } ] } ] }] }; react/mock/MockListData.js &#39;get /ds/user/list&#39;: function (req, res) { res.json([{ key: &#39;1&#39;, name: &#39;张三1&#39;, age: 32, address: &#39;上海市&#39;, tags: [&#39;程序员&#39;, &#39;帅气&#39;], }, { key: &#39;2&#39;, name: &#39;李四2&#39;, age: 42, address: &#39;北京市&#39;, tags: [&#39;屌丝&#39;], }, { key: &#39;3&#39;, name: &#39;王五3&#39;, age: 32, address: &#39;杭州市&#39;, tags: [&#39;高富帅&#39;, &#39;富二代&#39;], }]); react/src/models/UserListData.js import request from &quot;../util/request&quot;; export default { namespace: &#39;userList&#39;, state: { list: [] }, effects: { *initData(params, sagaEffects) { const {call, put} = sagaEffects; const url = &quot;/ds/user/list&quot;; let data = yield call(request, url); yield put({ type : &quot;queryList&quot;, data : data }); } }, reducers: { queryList(state, result) { let data = [...result.data]; return { //更新状态值 list: data } } } } react/src/layouts/index.js import React from &#39;react&#39;; import { Layout, Menu, Icon } from &#39;antd&#39;; import Link from &#39;umi/link&#39;; const { Header, Footer, Sider, Content } = Layout; const SubMenu = Menu.SubMenu; // layouts/index.js文件将被作为全 局的布局文件。 class BasicLayout extends React.Component{ constructor(props){ super(props); this.state = { collapsed: true, } } render(){ return ( &lt;Layout&gt; &lt;Sider width={256} style={{minHeight: '100vh', color: 'white'}}&gt; &lt;div style={{ height: '32px', background: 'rgba(255,255,255,.2)', margin: '16px'}}/&gt; &lt;Menu defaultSelectedKeys={[&#39;1&#39;]} defaultOpenKeys={[&#39;sub1&#39;]} mode=&quot;inline&quot; theme=&quot;dark&quot; inlineCollapsed={this.state.collapsed} &gt; &lt;SubMenu key=&quot;sub1&quot; title={&lt;span&gt;&lt;Icon type=&quot;user&quot;/&gt;&lt;span&gt;用户管理&lt;/span&gt;&lt;/span&gt;}&gt; &lt;Menu.Item key=&quot;1&quot;&gt;&lt;Link to=&quot;/user/add&quot;&gt;新增用户&lt;/Link&gt;&lt;/Menu.Item&gt; &lt;Menu.Item key=&quot;2&quot;&gt;&lt;Link to=&quot;/user/list&quot;&gt;新增列表&lt;/Link&gt;&lt;/Menu.Item&gt; &lt;/SubMenu&gt; &lt;/Menu&gt; &lt;/Sider&gt; &lt;Layout&gt; &lt;Header style={{ background: '#fff', textAlign: 'center', padding: 0 }}&gt;Header&lt;/Header&gt; &lt;Content style={{ margin: '24px 16px 0' }}&gt; &lt;div style={{ padding: 24, background: '#fff', minHeight: 360 }}&gt; { this.props.children } &lt;/div&gt; &lt;/Content&gt; &lt;Footer style={{ textAlign: 'center' }}&gt;后台系统&lt;/Footer&gt; &lt;/Layout&gt; &lt;/Layout&gt; ) } } export default BasicLayout; react/src/pages/MyTabs.js import React from &#39;react&#39;; import { Tabs } from &#39;antd&#39;; // 第一步，导入需要使用的组件 const TabPane = Tabs.TabPane; function callback(key) { console.log(key); } class MyTabs extends React.Component{ render(){ return ( &lt;Tabs defaultActiveKey=&quot;1&quot; onChange={callback}&gt; &lt;TabPane tab=&quot;Tab 1&quot; key=&quot;1&quot;&gt;hello antd wo de 第一个 tabs&lt;/TabPane&gt; &lt;TabPane tab=&quot;Tab 2&quot; key=&quot;2&quot;&gt;Content of Tab Pane 2&lt;/TabPane&gt; &lt;TabPane tab=&quot;Tab 3&quot; key=&quot;3&quot;&gt;Content of Tab Pane 3&lt;/TabPane&gt; &lt;/Tabs&gt; ) } } export default MyTabs;react/src/pages/user/UserList.js import React from &#39;react&#39;; import { connect } from &#39;dva&#39;; import {Table, Divider, Tag, Pagination } from &#39;antd&#39;; const {Column} = Table; const namespace = &#39;userList&#39;; @connect((state)=&gt;{ return { data : state[namespace].list } }, (dispatch) =&gt; { return { initData : () =&gt; { dispatch({ type: namespace + &quot;/initData&quot; }); } } }) class UserList extends React.Component { componentDidMount(){ this.props.initData(); } render() { return ( &lt;div&gt; &lt;Table dataSource={this.props.data} pagination={{position:"bottom",total:500,pageSize:10, defaultCurrent:3}}&gt; &lt;Column title=&quot;姓名&quot; dataIndex=&quot;name&quot; key=&quot;name&quot; /&gt; &lt;Column title=&quot;年龄&quot; dataIndex=&quot;age&quot; key=&quot;age&quot; /&gt; &lt;Column title=&quot;地址&quot; dataIndex=&quot;address&quot; key=&quot;address&quot; /&gt; &lt;Column title=&quot;标签&quot; dataIndex=&quot;tags&quot; key=&quot;tags&quot; render={tags =&gt; ( &lt;span&gt; {tags.map(tag =&gt; &lt;Tag color=&quot;blue&quot; key={tag}&gt;{tag}&lt;/Tag&gt;)} &lt;/span&gt; )} /&gt; &lt;Column title=&quot;操作&quot; key=&quot;action&quot; render={(text, record) =&gt; ( &lt;span&gt; &lt;a href=&quot;javascript:;&quot;&gt;编辑&lt;/a&gt; &lt;Divider type=&quot;vertical&quot;/&gt; &lt;a href=&quot;javascript:;&quot;&gt;删除&lt;/a&gt; &lt;/span&gt; )} /&gt; &lt;/Table&gt; &lt;/div&gt; ); } } export default UserList; react/src/pages/user/UserAdd.js import React from &#39;react&#39; class UserAdd extends React.Component{ render(){ return ( &lt;div&gt;新增用户&lt;/div&gt; ); } } export default UserAdd; react/src/pages/index.js import React from &#39;react&#39; class Index extends React.Component { render(){ return &lt;div&gt;首页&lt;/div&gt; } } export default Index; // http://localhost:8000/Ant Design Pro 入门https://github.com/ant-design/ant-design-pro├── config # umi 配置，包含路由，构建等配置├── mock # 本地模拟数据├── public│ └── favicon.png # Favicon├── src│ ├── assets # 本地静态资源│ ├── components # 业务通用组件│ ├── e2e # 集成测试用例│ ├── layouts # 通用布局│ ├── models # 全局 dva model│ ├── pages # 业务页面入口和常用模板│ ├── services # 后台接口服务│ ├── utils # 工具库│ ├── locales # 国际化资源│ ├── global.less # 全局样式│ └── global.js # 全局 JS├── tests # 测试工具├── README.md└── package.json tyarn install #安装相关依赖 tyarn start #启动服务 http://localhost:8000/dashboard/analysis 测试新增路由config/router.config.js 默认配置两套路由 { path: &#39;/new&#39;, name: &#39;new&#39;, icon: &#39;user&#39;, routes: [ { path: &#39;/new/analysis&#39;, name: &#39;analysis&#39;, component: &#39;./New/NewAnalysis&#39;, }, { path: &#39;/new/monitor&#39;, name: &#39;monitor&#39;, component: &#39;./Dashboard/Monitor&#39;, }, { path: &#39;/new/workplace&#39;, name: &#39;workplace&#39;, component: &#39;./Dashboard/Workplace&#39;, }, ], }, src/pages/New/NewAnalysis.js import React from &#39;react&#39; class NewAnalysis extends React.Component { render() { return (&lt;div&gt;NewAnalysis&lt;/div&gt;); } } export default NewAnalysis; src/locales/zh-CN.js &#39;menu.new&#39;: &#39;New Dashboard&#39;, &#39;menu.new.analysis&#39;: &#39;New 分析页&#39;, &#39;menu.new.monitor&#39;: &#39;New 监控页&#39;, &#39;menu.new.workplace&#39;: &#39;New 工作台&#39;, model执行流程http://localhost:8000/list/table-listsrc/pages/List/TableList.js Table组件生成表格，数据源是data &lt;StandardTable selectedRows={selectedRows} loading={loading} data={data} columns={this.columns} onSelectRow={this.handleSelectRows} onChange={this.handleStandardTableChange} /&gt; data数据从构造方法的props中获取 const { rule: { data }, loading, } = this.props; rule数据由@connect装饰器获取，{ rule, loading }是解构表达式，props从connect中获取数据 @connect(({ rule, loading }) =&gt; ({ rule, loading: loading.models.rule, })) src/pages/List/models/rule.js 生成数据rule reducers: { save(state, action) { return { ...state, data: action.payload, }; }, src/pages/List/TableList.js 组件加载完成后加载数据 componentDidMount() { const { dispatch } = this.props; dispatch({ type: &#39;rule/fetch&#39;, }); } src/pages/List/models/rule.js 从rule.js中reducers加载save方法数据 *fetch({ payload }, { call, put }) { const response = yield call(queryRule, payload); yield put({ type: &#39;save&#39;, payload: response, }); }, queryRule是在/services/api中进行了定义 export async function queryRule(params) { return request(`/api/rule?${stringify(params)}`); } 数据的mock在mock/rule.js中完成 export default { &#39;GET /api/rule&#39;: getRule, &#39;POST /api/rule&#39;: postRule, }; git commit -m “ant-design-pro 下载” –no-verify]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>react</tag>
        <tag>ant design</tag>
        <tag>ant design pro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[好客租房es6语法]]></title>
    <url>%2F2019%2F08%2F21%2F%E5%A5%BD%E5%AE%A2%E7%A7%9F%E6%88%BFes6%2F</url>
    <content type="text"><![CDATA[基于dubbo react elk实战整合开发 基础语法&lt;!DOCTYPE html&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;script&gt; // var 定义全局变量 for (var i = 0; i &lt; 5; i++) { console.log(i); } console.log(&quot;循环外：&quot; + i); // let 控制变量作用域 for (let j = 0; j &lt; 5; j++) { console.log(j); } console.log(&quot;循环外：&quot; + j); // const声明常量 不可修改 const a = 1; console.log(&quot;a=&quot;, a); a = 2; console.log(&quot;a=&quot;, a) // 字符串拓展函数 console.log(&quot;hello world&quot;.includes(&quot;hello&quot;)); console.log(&quot;hello world&quot;.startsWith(&quot;hello&quot;)); console.log(&quot;hello world&quot;.endsWith(&quot;world&quot;)); // 字符串模板保留换行源格式 let str = ` hello world `; console.log(str); // 新定义数组解构获取顺序获取值 let arr = [1, 2, 3] const [x, y, z] = arr; console.log(x, y, z) const [m] = arr; console.log(m) // 新定义对象结构顺序获取属性值 const person = { name: &#39;wj&#39;, age: 21, language: [&#39;java&#39;, &#39;js&#39;, &#39;php&#39;] } const {name, age, language} = person console.log(name, age, language) const {name: n, age: a, language: l} = person; console.log(n) // 函数默认值 function add(a, b = 1) { // b = b || 1; return a + b; } console.log(add(10)) // 箭头函数 var print = obj =&gt; (console.log(obj)) //一个参数 var sum = (a, b) =&gt; a + b // 多个参数 var sayHello1 = () =&gt; console.log(&quot;hello&quot;) // 没有参数 var sayHello2 = (a, b) =&gt; { // 多个函数 console.log(&quot;hello&quot;) console.log(&quot;world&quot;) return a + b; } // 对象函数属性简写 let persons = { name: &quot;jack&quot;, eat: food =&gt; console.log(persons.name + &quot;再吃&quot; + food), // 这里拿不到this eat1(food) { console.log(this.name + &quot;再吃&quot; + food) }, eat2(food) { console.log(this.name + &quot;再吃&quot; + food) } }; persons.eat(&quot;西瓜&quot;) // 箭头函数结合解构表达式 var hi = ({name}) =&gt; console.log(&quot;hello,&quot; + name) hi(persons) // map reduce let array = [&#39;1&#39;, &#39;2&#39;].map(s =&gt; parseInt(s)); //将原数组所有元素用函数处理哇年后放入新数组返回 let array1 = arr.map(function (s) { return parseInt(s); }) console.log(array) let num = [1, 20, 6, 5]; console.log(num.reduce((a, b) =&gt; a + b)) // 从左到友依次用reduce处理，并把结果作为下次reduce的第一个参数 let result = arr.reduce((a, b) =&gt; { return a + b; }, 1); //接受函数必须，初始值可选 // 扩展运算符 console.log(...[2, 3], ...[1, 20, 6, 5], 0) // 数组合并 let add1 = (x, y) =&gt; x + y; console.log(add1(...[1, 2])) const [f, ...l] = [1, 2, 3, 4, 5] //结合结构表达式 console.log(f, l) console.log(...&#39;heool&#39;) //字符串转数组 // promise 异步执行 const p = new Promise((resolve, reject) =&gt; { setTimeout(() =&gt; { const num = Math.random(); // 随机返回成功或失败 if (num &lt; 0.5) { resolve(&quot;成功 num=&quot; + num) } else { reject(&quot;失败 num=&quot; + num) } }, 300) }) p.then(function (msg) { console.log(msg) }).catch(function (msg) { console.log(msg) }) // set map let set = new Set() set.add(1) // clear delete has forEach(function(){}) size set.forEach(value =&gt; console.log(value)) let set2 = new Set([1, 2, 2, 1, 1]) // map为&lt;object,object&gt; const map = new Map([ [&#39;key1&#39;, &#39;value1&#39;], [&#39;value2&#39;, &#39;value2&#39;] ]) const set3 = new Set([ [&#39;t&#39;, &#39;t&#39;], [&#39;h&#39;, &#39;h&#39;] ]) const map2 = new Map(set3) const map3 = new Map(map) map.set(&#39;z&#39;, &#39;z&#39;) // clear delete(key) has(key) forEach(function(value,key){}) size values keys entries for (let key of map.keys()) { console.log(key) } console.log(...map.values()) // 类的基本用法 class User { constructor(name, age = 20) { this.name = name; this.age = age; } sayHi() { return &quot;hi&quot; } static isAdult(age) { if (age &gt;= 18) { return &quot;成年人&quot; } return &quot;未成年人&quot; } } class zhangsan extends User { // 类的继承 constructor() { super(&quot;张三&quot;, 10) this.address = &quot;上海&quot; } test(){ return &quot;name=&quot;+this.name; } } let user = new User(&quot;张三&quot;) let zs = new zhangsan() console.log(user) console.log(user.sayHi()) console.log(User.isAdult(20)) console.log(zs.name, zs.address) console.log(zs.sayHi()) // Generator函数 function* hello() { yield &quot;h&quot; yield &quot;e&quot; return &quot;a&quot; } let h = hello(); for (let obj of h) { //循环遍历或next遍历 console.log(&quot;===&quot; + obj) } console.log(h.next()) console.log(h.next()) console.log(h.next()) // 修饰器 修改类的行为 @T class Animal { constructor(name,age=20){ this.name=name; this.age=age; } } function T(target){ console.log(target); target.contry = &quot;china&quot; //通过修饰器添加的属性是静态属性 } console.log(Animal.contry) // 无法运行，需要转码：将ES6活ES2017转为ES5使用（将箭头函数转为普通函数） &lt;/script&gt; &lt;/html&gt;umi转码 node -v v8.12.0 npm i yarn tyarn -g tyarn使用淘宝源 tyarn -v 1.16.0 若报错通过yarn global bin获取路径加入Path tyarn global add umi umi tyarn init -y 多一个package.json umi g page index 生成page文件夹 编辑index.js index.js // 修饰器 function T(target) { console.log(target); target.country=&quot;中国&quot; } @T class People{ constructor(name,age=20){ this.name=name; this.age=age; } } console.log(People.country); import Util from &#39;./Util&#39;; console.log(Util.sum(10, 5)); Util.js class Util { static sum = (a,b) =&gt; { return a + b; } } export default Util; umi dev,通过http://localhost:8000/ 查看控制台 reactjs tyarn init -y tyarn add umi –dev tyarn add umi-plugin-react –dev umi jsxreact/config/config.js export default {}; react/src/pages/HelloWorld.js export default ()=&gt;{ const t=()=&gt;&quot;pig&quot; return ( &lt;div&gt;hello world {t()}&lt;/div&gt; ); } umi build 转码生成文件 dist\umi.jsumi dev 访问http://localhost:8000 todolistreact/src/pages/HelloWorld.js import React from &#39;react&#39; class HelloWorld extends React.Component{ render() { // this.props.name接受属性，this.props.children接受标签内容 return &lt;div&gt;hello name={this.props.name},say={this.props.children}&lt;/div&gt; } } export default HelloWorld; react/src/pages/Show.js import React from &#39;react&#39; import HelloWorld from &#39;./HelloWorld&#39; class Show extends React.Component{ render() { return &lt;HelloWorld name=&quot;zhansan&quot;&gt;haha&lt;/HelloWorld&gt; } } export default Show; react/src/pages/List.js import React from &#39;react&#39;; class List extends React.Component{ constructor(props){ super(props); this.state = { dataList : [1,2,3], maxNum : 3 }; } /*this.state值在构造参数中完成，要修改this.state的值，需要调用this.setState()完成*/ render(){ return ( &lt;div&gt; &lt;ul&gt; { this.state.dataList.map((value,index)=&gt;{ return &lt;li key={index}&gt;{value}&lt;/li&gt; }) } &lt;/ul&gt; &lt;button onClick={() =&gt; { let maxNum = this.state.maxNum + 1; let list = [...this.state.dataList,maxNum]; this.setState({ dataList: list, maxNum: maxNum }); }}&gt;点我&lt;/button&gt; &lt;/div&gt; ); } } export default List; LifeCycleimport React from &#39;react&#39;; //第一步，导入React class LifeCycle extends React.Component { constructor(props) { super(props); //构造方法 console.log(&quot;constructor()&quot;); } componentDidMount() { //组件挂载后调用 console.log(&quot;componentDidMount()&quot;); } componentWillUnmount() { //在组件从 DOM 中移除之前立刻被调用。 console.log(&quot;componentWillUnmount()&quot;); } componentDidUpdate() { //在组件完成更新后立即调用。在初始化时不会被调用。 console.log(&quot;componentDidUpdate()&quot;); } shouldComponentUpdate(nextProps, nextState){ // 每当this.props或this.state有变化，在render方法执行之前，就会调用这个方法。 // 该方法返回一个布尔值，表示是否应该继续执行render方法，即如果返回false，UI 就不会更新，默认返回true。 // 组件挂载时，render方法的第一次执行，不会调用这个方法。 console.log(&quot;shouldComponentUpdate()&quot;); } render() { return ( &lt;div&gt; &lt;h1&gt;React Life Cycle!&lt;/h1&gt; &lt;/div&gt; ); } } export default LifeCycle;]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>react</tag>
        <tag>umi</tag>
        <tag>es6</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Alibaba网关服务]]></title>
    <url>%2F2019%2F08%2F21%2FAlibaba%E7%BD%91%E5%85%B3%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[基于spring cloud alibaba实战整合开发 ht-micro-record-service-gateway&lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-service-gateway&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-service-gateway&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt; &lt;artifactId&gt;jsr305&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt; &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;!-- Commons Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Commons Begin --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;!-- docker的maven插件，官网 https://github.com/spotify/docker-maven-plugin --&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt; &lt;version&gt;0.4.13&lt;/version&gt; &lt;configuration&gt; &lt;imageName&gt;192.168.2.7:5000/${project.artifactId}:${project.version}&lt;/imageName&gt; &lt;baseImage&gt;onejane-jdk1.8 &lt;/baseImage&gt; &lt;entryPoint&gt;[&quot;java&quot;, &quot;-jar&quot;,&quot;/${project.build.finalName}.jar&quot;]&lt;/entryPoint&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/&lt;/targetPath&gt; &lt;directory&gt;${project.build.directory}&lt;/directory&gt; &lt;include&gt;${project.build.finalName}.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;dockerHost&gt;http://192.168.2.7:2375&lt;/dockerHost&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;com.ht.micro.record.service.gateway.GatewayServiceApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; bootstrap.properties spring.application.name=ht-micro-record-service-gateway-config spring.main.allow-bean-definition-overriding=true spring.cloud.nacos.config.file-extension=yaml spring.cloud.nacos.config.server-addr=192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 nacos配置 spring: application: name: ht-micro-record-service-gateway jackson: date-format: yyyy-MM-dd HH:mm:ss time-zone: GMT+8 default-property-inclusion: non_null cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8721 dashboard: 192.168.2.7:190 gateway: discovery: locator: enabled: true routes: http://localhost:9000/api/user/user/20 访问user服务 - id: HT-MICRO-RECORD-SERVICE-USER uri: lb://ht-micro-record-service-user predicates: - Path=/api/user/** filters: - StripPrefix=2 server: port: 9000 feign: sentinel: enabled: true management: endpoints: web: exposure: include: &quot;*&quot; logging: level: org.springframework.cloud.gateway: debug GatewayServiceApplication.java @SpringBootApplication @EnableDiscoveryClient @EnableFeignClients public class GatewayServiceApplication { // ----------------------------- 解决跨域 Begin ----------------------------- private static final String ALL = &quot;*&quot;; private static final String MAX_AGE = &quot;18000L&quot;; @Bean public RouteDefinitionLocator discoveryClientRouteDefinitionLocator(DiscoveryClient discoveryClient, DiscoveryLocatorProperties properties) { return new DiscoveryClientRouteDefinitionLocator(discoveryClient, properties); } @Bean public ServerCodecConfigurer serverCodecConfigurer() { return new DefaultServerCodecConfigurer(); } @Bean public WebFilter corsFilter() { return (ServerWebExchange ctx, WebFilterChain chain) -&gt; { ServerHttpRequest request = ctx.getRequest(); if (!CorsUtils.isCorsRequest(request)) { return chain.filter(ctx); } HttpHeaders requestHeaders = request.getHeaders(); ServerHttpResponse response = ctx.getResponse(); HttpMethod requestMethod = requestHeaders.getAccessControlRequestMethod(); HttpHeaders headers = response.getHeaders(); headers.add(HttpHeaders.ACCESS_CONTROL_ALLOW_ORIGIN, requestHeaders.getOrigin()); headers.addAll(HttpHeaders.ACCESS_CONTROL_ALLOW_HEADERS, requestHeaders.getAccessControlRequestHeaders()); if (requestMethod != null) { headers.add(HttpHeaders.ACCESS_CONTROL_ALLOW_METHODS, requestMethod.name()); } headers.add(HttpHeaders.ACCESS_CONTROL_ALLOW_CREDENTIALS, &quot;true&quot;); headers.add(HttpHeaders.ACCESS_CONTROL_EXPOSE_HEADERS, ALL); headers.add(HttpHeaders.ACCESS_CONTROL_MAX_AGE, MAX_AGE); if (request.getMethod() == HttpMethod.OPTIONS) { response.setStatusCode(HttpStatus.OK); return Mono.empty(); } return chain.filter(ctx); }; } // ----------------------------- 解决跨域 End ----------------------------- public static void main(String[] args) { SpringApplication.run(GatewayServiceApplication.class, args); } }]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>spring cloud alibaba</tag>
        <tag>gateway</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Alibaba分布式session]]></title>
    <url>%2F2019%2F08%2F21%2FAlibaba%E5%88%86%E5%B8%83%E5%BC%8Fsession%2F</url>
    <content type="text"><![CDATA[基于spring cloud alibaba实战整合开发 com.ht.micro.record.commons.domain.User @Data public class User implements Serializable { private long userId; private String username; private String password; } ht-micro-record-service-smspom.xml &lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-freemarker&lt;/artifactId&gt; &lt;/dependency&gt;SmsServiceApplication.java @EnableRedisHttpSession(maxInactiveIntervalInSeconds= 1800) //开启redis session支持,并配置session过期时间UserController.java @Controller @RequestMapping(value = &quot;user&quot;) public class UserController extends AbstractBaseController&lt;TbUser&gt; { @RequestMapping public String index() { return &quot;index&quot;; } @RequestMapping(&quot;home&quot;) public String home() { return &quot;home&quot;; } @PostMapping(&quot;login&quot;) public String login(User user, HttpSession session) { // 随机生成用户id user.setUserId(Math.round(Math.floor(Math.random() * 10 * 1000))); // 将用户信息保存到id中 session.setAttribute(&quot;USER&quot;, user); return &quot;home&quot;; } @PostMapping(&quot;logout&quot;) public String logout(HttpSession session) { session.removeAttribute(&quot;USER&quot;); session.invalidate(); return &quot;home&quot;; } }templates/home.ftl &lt;!doctype html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;title&gt;主页面&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h5&gt;登录用户: ${Session[&quot;USER&quot;].username} &lt;/h5&gt; &lt;h5&gt;用户编号: ${Session[&quot;USER&quot;].userId} &lt;/h5&gt; &lt;/body&gt; &lt;/html&gt;templates/index.ftl &lt;!doctype html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;title&gt;登录页面&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;form action=&quot;/user/login&quot; method=&quot;post&quot;&gt; 用户：&lt;input type=&quot;text&quot; name=&quot;username&quot;&gt;&lt;br/&gt; 密码：&lt;input type=&quot;password&quot; name=&quot;password&quot;&gt;&lt;br/&gt; &lt;button type=&quot;submit&quot;&gt;登录&lt;/button&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt;ht-micro-record-service-user pom.xml &lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-freemarker&lt;/artifactId&gt; &lt;/dependency&gt; application.yml所有模块加入redis配置 redis: #host: 127.0.0.1 #port: 6379 jedis: pool: # 连接池最大连接数,使用负值表示无限制。 max-active: 8 # 连接池最大阻塞等待时间,使用负值表示无限制。 max-wait: -1s # 连接池最大空闲数,使用负值表示无限制。 max-idle: 8 # 连接池最小空闲连接，只有设置为正值时候才有效 min-idle: 1 timeout: 300ms session: # session 存储方式 支持redis、mongo、jdbc、hazelcast store-type: redis cluster: nodes: 192.168.2.5:8001,192.168.2.5:8002,192.168.2.5:8003,192.168.2.7:8004,192.168.2.7:8005,192.168.2.7:8006 # 如果是集群节点 采用如下配置指定节点 #spring.redis.cluster.nodes templates/index.ftl &lt;!doctype html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;title&gt;登录页面&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;form action=&quot;/user/login&quot; method=&quot;post&quot;&gt; 用户：&lt;input type=&quot;text&quot; name=&quot;username&quot;&gt;&lt;br/&gt; 密码：&lt;input type=&quot;password&quot; name=&quot;password&quot;&gt;&lt;br/&gt; &lt;button type=&quot;submit&quot;&gt;登录&lt;/button&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; templates/home.ftl &lt;!doctype html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;title&gt;主页面&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h5&gt;登录用户: ${Session[&quot;USER&quot;].username} &lt;/h5&gt; &lt;h5&gt;用户编号: ${Session[&quot;USER&quot;].userId} &lt;/h5&gt; &lt;/body&gt; &lt;/html&gt;UserServiceApplication.java @EnableRedisHttpSession(maxInactiveIntervalInSeconds= 1800) //开启redis session支持,并配置session过期时间 UserController.java @Controller @RequestMapping(value = &quot;user&quot;) public class UserController extends AbstractBaseController&lt;TbUser&gt; { @Autowired private TbUserService tbUserService; @Autowired private UserService userService; @RequestMapping public String index() { return &quot;index&quot;; } @RequestMapping(&quot;home&quot;) public String home() { return &quot;home&quot;; } @PostMapping(&quot;login&quot;) public String login(User user, HttpSession session) { // 随机生成用户id user.setUserId(Math.round(Math.floor(Math.random() * 10 * 1000))); // 将用户信息保存到id中 session.setAttribute(&quot;USER&quot;, user); return &quot;home&quot;; } @PostMapping(&quot;logout&quot;) public String logout(HttpSession session) { session.removeAttribute(&quot;USER&quot;); session.invalidate(); return &quot;home&quot;; } } http://localhost:9507/user 登陆后，进入http://localhost:9507/user/login 页面查看用户信息手动进入http://localhost:9506/user/home 查看相同用户信息，User实体位置在两个服务中保持一致。如果项目中有es配置，需要es优先配置Netty @PostConstruct public void init() { /*由于netty的冲突，需要在ElasticConfig中显示指定早于RedisConfig装配，并且指定初始化时再一次添加忽略es中netty的一些配置*/ System.setProperty(&quot;es.set.netty.runtime.available.processors&quot;, &quot;false&quot;); bulkRequest = elasticClientSingleton.getBulkRequest(esConfig); }]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>session</tag>
        <tag>redis</tag>
        <tag>freemarker</tag>
        <tag>spring cloud alibaba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Alibaba整合xxl-job]]></title>
    <url>%2F2019%2F08%2F21%2FAlibaba%E5%AE%9A%E6%97%B6%E5%99%A8%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[基于spring cloud alibaba实战整合开发 ht-micro-record-service-job &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-service-job&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-service-job&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.xuxueli&lt;/groupId&gt; &lt;artifactId&gt;xxl-job-core&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt; &lt;artifactId&gt;jsr305&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt; &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rocketmq&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;!-- Projects Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-commons-service&lt;/artifactId&gt; &lt;version&gt;${project.parent.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Projects End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;ht.micro.record.service.job.JobServiceApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; bootstrap.properties spring.application.name=ht-micro-record-service-xxl-job-config spring.cloud.nacos.config.file-extension=yaml spring.cloud.nacos.config.server-addr=192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 application.yaml spring: datasource: druid: url: jdbc:mysql://192.168.2.7:185/ht_micro_record?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false username: root password: 123456 initial-size: 1 min-idle: 1 max-active: 20 test-on-borrow: true driver-class-name: com.mysql.jdbc.Driver cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 config: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 server: port: 9701 xxl: job: executor: logpath: logs/xxl-job/jobhandler appname: xxl-job-executor port: 1234 logretentiondays: -1 ip: 192.168.3.233 admin: addresses: http://192.168.2.7:183/xxl-job-admin accessToken:nacos配置 spring: application: name: ht-micro-record-service-xxl-job cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8719 dashboard: 192.168.2.7:190 server: port: 9701 management: endpoints: web: exposure: include: &quot;*&quot; com/ht/micro/record/service/job/JobServiceApplication.java @SpringBootApplication(scanBasePackages = &quot;com.ht.micro.record&quot;) @EnableDiscoveryClient @MapperScan(basePackages = &quot;com.ht.micro.record.commons.mapper&quot;) public class JobServiceApplication { public static void main(String[] args) { SpringApplication.run(JobServiceApplication.class, args); } } com/ht/micro/record/service/job/config/XxlJobConfig.java @Configuration @ComponentScan(basePackages = &quot;com.ht.micro.record.service.job.handler&quot;) public class XxlJobConfig { private Logger logger = LoggerFactory.getLogger(XxlJobConfig.class); @Value(&quot;${xxl.job.admin.addresses}&quot;) private String adminAddresses; @Value(&quot;${xxl.job.executor.appname}&quot;) private String appName; @Value(&quot;${xxl.job.executor.ip}&quot;) private String ip; @Value(&quot;${xxl.job.executor.port}&quot;) private int port; @Value(&quot;${xxl.job.accessToken}&quot;) private String accessToken; @Value(&quot;${xxl.job.executor.logpath}&quot;) private String logPath; @Value(&quot;${xxl.job.executor.logretentiondays}&quot;) private int logRetentionDays; @Bean(initMethod = &quot;start&quot;, destroyMethod = &quot;destroy&quot;) public XxlJobSpringExecutor xxlJobExecutor() { logger.info(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; xxl-job config init.&quot;); XxlJobSpringExecutor xxlJobSpringExecutor = new XxlJobSpringExecutor(); xxlJobSpringExecutor.setAdminAddresses(adminAddresses); xxlJobSpringExecutor.setAppName(appName); xxlJobSpringExecutor.setIp(ip); xxlJobSpringExecutor.setPort(port); xxlJobSpringExecutor.setAccessToken(accessToken); xxlJobSpringExecutor.setLogPath(logPath); xxlJobSpringExecutor.setLogRetentionDays(logRetentionDays); return xxlJobSpringExecutor; } } com/ht/micro/record/service/job/handler/TestJobHandler.java @JobHandler(value=&quot;testJobHandler&quot;) @Component public class TestJobHandler extends IJobHandler { @Override public ReturnT&lt;String&gt; execute(String param) throws Exception { XxlJobLogger.log(&quot;XXL-JOB, Hello World.&quot;); for (int i = 0; i &lt; 5; i++) { XxlJobLogger.log(&quot;beat at:&quot; + i); TimeUnit.SECONDS.sleep(2); } return SUCCESS; } } http://192.168.2.7:183/xxl-job-admin]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>spring cloud alibaba</tag>
        <tag>xxl-job</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Alibaba服务间调用的多种方式]]></title>
    <url>%2F2019%2F08%2F21%2FAlibaba%E6%9C%8D%E5%8A%A1%E9%97%B4%E8%B0%83%E7%94%A8%E7%9A%84%E5%A4%9A%E7%A7%8D%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[基于spring cloud alibaba实战整合开发 @[TOC] ht-micro-record-service-dubbopom.xml &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;modules&gt; &lt;module&gt;ht-micro-record-service-dubbo-provider&lt;/module&gt; &lt;module&gt;ht-micro-record-service-dubbo-consumer&lt;/module&gt; &lt;/modules&gt; &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo&lt;/artifactId&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;name&gt;ht-micro-record-service-dubbo&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; ht-micro-record-service-dubbo-apipom.xml &lt;parent&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo&lt;/artifactId&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo-api&lt;/artifactId&gt; PortApi.java public interface PortApi { String showPort(); } ht-micro-record-service-dubbo-providerpom.xml &lt;parent&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo&lt;/artifactId&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo-provider&lt;/artifactId&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo-api&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-dubbo&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--elasticsearch--&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.java.dev.jna&lt;/groupId&gt; &lt;artifactId&gt;jna&lt;/artifactId&gt; &lt;!-- &lt;version&gt;3.0.9&lt;/version&gt; --&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt; &lt;artifactId&gt;gson&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;com.ht.micro.record.service.dubbo.provider.DubboProviderApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; logback.xml &lt;configuration&gt; &lt;!-- 控制台输出 --&gt; &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;layout class=&quot;ch.qos.logback.classic.PatternLayout&quot;&gt; &lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符 --&gt; &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n&lt;/pattern&gt; &lt;/layout&gt; &lt;/appender&gt; &lt;!--定义日志文件的存储地址 勿在 LogBack 的配置中使用相对路径 --&gt; &lt;property name=&quot;LOG_HOME&quot; value=&quot;E:/log&quot; /&gt; &lt;!-- 按照每天生成日志文件 --&gt; &lt;appender name=&quot;FILE&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;!--日志文件输出的文件名 --&gt; &lt;FileNamePattern&gt;${LOG_HOME}/log.%d{yyyy-MM-dd}.log&lt;/FileNamePattern&gt; &lt;MaxHistory&gt;30&lt;/MaxHistory&gt; &lt;/rollingPolicy&gt; &lt;layout class=&quot;ch.qos.logback.classic.PatternLayout&quot;&gt; &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n&lt;/pattern&gt; &lt;/layout&gt; &lt;!--日志文件最大的大小 --&gt; &lt;triggeringPolicy class=&quot;ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy&quot;&gt; &lt;MaxFileSize&gt;10MB&lt;/MaxFileSize&gt; &lt;/triggeringPolicy&gt; &lt;/appender&gt; &lt;!-- 日志输出级别 --&gt; &lt;root level=&quot;INFO&quot;&gt; &lt;appender-ref ref=&quot;STDOUT&quot; /&gt; &lt;appender-ref ref=&quot;FILE&quot; /&gt; &lt;/root&gt; &lt;!--mybatis 日志配置--&gt; &lt;logger name=&quot;com.mapper&quot; level=&quot;DEBUG&quot; /&gt; &lt;/configuration&gt; bootstrap.yml spring: application: name: ht-micro-record-service-dubbo-provider cloud: nacos: config: file-extension: yaml server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 main: allow-bean-definition-overriding: true dubbo: scan: base-packages: com.ht.micro.record.service.dubbo.provider.dubbo protocol: name: dubbo port: -1 registry: address: spring-cloud://192.168.2.7:8848?backup=192.168.2.7:8849,192.168.2.7:8850 es: nodes: 192.168.2.5:1800,192.168.2.7:1800 host: 192.168.2.5,192.168.2.7 port: 1801,1801 types: doc clusterName: elasticsearch-cluster #笔录分析库 blDbName: t_record_analyze #接警信息库 jjDbName: p_answer_alarm #处警信息库 cjDbName: p_handle_alarm #警情分析结果信息库 jqfxDbName: t_alarm_analysis_result #标准化分析 cjjxqDbName: t_cjjxq #接处警整合库 jcjDbName: p_answer_handle_alarm #警情分析 daDbName: t_data_analysis #警情结果表(neo4j使用) neo4jData: t_neo4j_data #市局接处警表 cityDbName: city_answer_handle_alarm nacos配置ht-micro-record-service-dubbo-provider.yaml spring: application: name: ht-micro-record-service-dubbo-provider cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8719 dashboard: 192.168.2.7:190 server: port: 9502com.ht.micro.record.service.dubbo.provider.DubboProviderApplication @SpringBootApplication @EnableDiscoveryClient public class DubboProviderApplication { public static void main(String[] args) { SpringApplication.run(DubboProviderApplication.class, args); } } com.ht.micro.record.service.dubbo.provider.utils.ElasticClient @Service public class ElasticClient { @Autowired private EsConfig esConfig; private static final Logger logger = LoggerFactory.getLogger(ElasticClient.class); private static BulkRequestBuilder bulkRequest; @Autowired private ElasticClientSingleton elasticClientSingleton; @PostConstruct public void init() { bulkRequest = elasticClientSingleton.getBulkRequest(esConfig); } /** * @param url * @param query * @return * @Description 发送请求 * @Author 裴健(peij@htdatacloud.com) * @Date 2016年6月13日 * @History * @his1 */ public static String postRequest(String url, String query) { RestTemplate restTemplate = new RestTemplate(); MediaType type = MediaType.parseMediaType(&quot;application/json; charset=UTF-8&quot;); HttpHeaders headers = new HttpHeaders(); headers.setContentType(type); headers.add(&quot;Accept&quot;, MediaType.APPLICATION_JSON.toString()); HttpEntity&lt;String&gt; formEntity = new HttpEntity&lt;String&gt;(query, headers); String result = restTemplate.postForObject(url, formEntity, String.class); return result; } /** * action 提交操作 */ // public void action() { // int reqSize = bulkRequest.numberOfActions(); // //读不到数据了，默认已经全部读取 // if (reqSize == 0) { // bulkRequest.request().requests().clear(); // } // bulkRequest.setTimeout(new TimeValue(1000 * 60 * 5)); //超时30秒 // BulkResponse bulkResponse = bulkRequest.execute().actionGet(); // //持久化异常 // if (bulkResponse.hasFailures()) { // logger.error(bulkResponse.buildFailureMessage()); // bulkRequest.request().requests().clear(); // } // logger.info(&quot;import over....&quot; + bulkResponse.getItems().length); // } }com.ht.micro.record.service.dubbo.provider.utils.ElasticClientSingleton @Service public class ElasticClientSingleton { protected final Logger logger = LoggerFactory.getLogger(ElasticClientSingleton.class); private AtomicInteger atomicPass = new AtomicInteger(); // 0 未初始化, 1 已初始化 private TransportClient transportClient; private BulkRequestBuilder bulkRequest; public synchronized void init(EsConfig esConfig) { try { String ipArray = esConfig.getHost(); String portArray = esConfig.getPort(); String cluster = esConfig.getClusterName(); Settings settings = Settings.builder() .put(&quot;cluster.name&quot;, cluster) //连接的集群名 .put(&quot;client.transport.ignore_cluster_name&quot;, true) .put(&quot;client.transport.sniff&quot;, false)//如果集群名不对，也能连接 .build(); transportClient = new PreBuiltTransportClient(settings); String[] ips = ipArray.split(&quot;,&quot;); String[] ports = portArray.split(&quot;,&quot;); for (int i = 0; i &lt; ips.length; i++) { transportClient.addTransportAddress(new TransportAddress(InetAddress.getByName(ips[i]), Integer .parseInt(ports[i]))); } atomicPass.set(1); } catch (Exception e) { e.printStackTrace(); logger.error(e.getMessage()); atomicPass.set(0); destroy(); } } public void destroy() { if (transportClient != null) { transportClient.close(); transportClient = null; } } public BulkRequestBuilder getBulkRequest(EsConfig esConfig) { if (atomicPass.get() == 0) { // 初始化 init(esConfig); } bulkRequest = transportClient.prepareBulk(); return bulkRequest; } public TransportClient getTransportClient(EsConfig esConfig) { if (atomicPass.get() == 0) { // 初始化 init(esConfig); } return transportClient; } }com.ht.micro.record.service.dubbo.provider.utils.EsConfig @Service public class EsConfig { @Value(&quot;${es.nodes}&quot;) private String nodes; @Value(&quot;${es.host}&quot;) private String host; @Value(&quot;${es.port}&quot;) private String port; @Value(&quot;${es.blDbName}&quot;) private String blDbName; @Value(&quot;${es.jjDbName}&quot;) private String jjDbName; @Value(&quot;${es.cjDbName}&quot;) private String cjDbName; @Value(&quot;${es.jqfxDbName}&quot;) private String jqfxDbName; @Value(&quot;${es.clusterName}&quot;) private String clusterName; @Value(&quot;${es.jjDbName}&quot;) private String answerDbName; @Value(&quot;${es.cjDbName}&quot;) private String handleDbName; @Value(&quot;${es.cjjxqDbName}&quot;) private String cjjxqDbName; @Value(&quot;${es.jcjDbName}&quot;) private String jcjDbName; @Value(&quot;${es.daDbName}&quot;) private String daDbName; @Value(&quot;${es.daDbName}&quot;) private String fxDbName; @Value(&quot;${es.types}&quot;) private String types; @Value(&quot;${es.neo4jData}&quot;) private String neo4jData; @Value(&quot;${es.cityDbName}&quot;) private String cityDbName; public String getCityDbName() { return cityDbName; } public String getTypes() { return types; } public String getFxDbName() { return fxDbName; } public String getDaDbName() { return daDbName; } public String getJcjDbName() { return jcjDbName; } public String getCjjxqDbName() { return cjjxqDbName; } public String getNodes() { return nodes; } public String getHost() { return host; } public String getPort() { return port; } public String getClusterName() { return clusterName; } public String getBlDbName() { return blDbName; } public String getJjDbName() { return jjDbName; } public String getCjDbName() { return cjDbName; } public String getJqfxDbName() { return jqfxDbName; } public String getNeo4jData() { return neo4jData; } } com.ht.micro.record.service.dubbo.provider.dubbo.PortApiImpl @Service public class PortApiImpl implements PortApi { @Value(&quot;${server.port}&quot;) private Integer port; @Override public String showPort() { return &quot;port= &quot;+ port; } }com.ht.micro.record.service.dubbo.provider.controller.ProviderController @RestController @RequestMapping(&quot;/provider&quot;) public class ProviderController { @Autowired private ConfigurableApplicationContext applicationContext; @Value(&quot;${server.port}&quot;) private Integer port; @GetMapping(&quot;/port&quot;) public Object port() { return &quot;port= &quot;+ port + &quot;, name=&quot; + applicationContext.getEnvironment().getProperty(&quot;user.name&quot;); } }ht-micro-record-service-dubbo-consumerpom.xml &lt;parent&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo&lt;/artifactId&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo-consumer&lt;/artifactId&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-service-dubbo-api&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-dubbo&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt; &lt;artifactId&gt;jsr305&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt; &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;com.ht.micro.record.service.dubbo.consumer.DubboConsumerApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;bootstrap.yml spring: application: name: ht-micro-record-service-dubbo-consumer main: allow-bean-definition-overriding: true cloud: nacos: config: file-extension: yaml server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 # 与nacos兼容性不好，配置到项目中 dubbo: scan: base-packages: com.ht.micro.record.service.dubbo.consumer protocol: name: dubbo port: -1 registry: address: spring-cloud://192.168.2.7:8848?backup=192.168.2.7:8849,192.168.2.7:8850com.ht.micro.record.service.dubbo.consumer.DubboConsumerApplication @SpringBootApplication(scanBasePackages = &quot;com.ht.micro.record&quot;, exclude = {DataSourceAutoConfiguration.class}) @EnableFeignClients @EnableDiscoveryClient public class DubboConsumerApplication { public static void main(String[] args) { SpringApplication.run(DubboConsumerApplication.class, args); } } com.ht.micro.record.service.dubbo.consumer.service.PortService @FeignClient(value = &quot;ht-micro-record-service-dubbo-provider&quot;) public interface PortService { @GetMapping(value = &quot;/provider/port&quot;) String showPort(); }com.ht.micro.record.service.dubbo.consumer.controller.ConsumerController @RestController @RequestMapping(&quot;/consumer&quot;) public class ConsumerController { @Autowired private LoadBalancerClient loadBalancerClient; @Autowired private PortService portService; private RestTemplate restTemplate = new RestTemplate(); @Reference(check = false) private PortApi portApi; @GetMapping(&quot;/rest&quot;) public Object rest() { ServiceInstance serviceInstance = loadBalancerClient.choose(&quot;ht-micro-record-service-dubbo-provider&quot;); String url = String.format(&quot;http://%s:%s/provider/port&quot;, serviceInstance.getHost(), serviceInstance.getPort()); System.out.println(&quot;request url:&quot; + url); return restTemplate.getForObject(url, String.class); } @GetMapping(&quot;/rpc&quot;) public Object rpc() { return portApi.showPort(); } @GetMapping(&quot;/feign&quot;) public Object feign(){ return portService.showPort(); } }]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>spring cloud alibaba</tag>
        <tag>dubbo</tag>
        <tag>nacos</tag>
        <tag>feign</tag>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Alibaba服务搭建]]></title>
    <url>%2F2019%2F08%2F21%2FAlibaba%E6%9C%8D%E5%8A%A1%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[基于spring cloud alibaba实战整合开发 基础服务通用类com.ht.micro.record.commonsdto.AbstractBaseDomain @Data public abstract class AbstractBaseDomain implements Serializable { /** * 该注解需要保留，用于 tk.mybatis 回显 ID */ @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; /** * 格式化日期，由于是北京时间（我们是在东八区），所以时区 +8 */ @JsonFormat(pattern = &quot;yyyy-MM-dd HH:mm:ss&quot;, timezone = &quot;GMT+8&quot;) private Date created; @JsonFormat(pattern = &quot;yyyy-MM-dd HH:mm:ss&quot;, timezone = &quot;GMT+8&quot;) private Date updated; } dto.AbstractBaseResult @Data public abstract class AbstractBaseResult implements Serializable { /** * 此为内部类,JsonInclude.Include.NON_NULL去掉返回值中为null的属性 */ @Data @JsonInclude(JsonInclude.Include.NON_NULL) protected static class Links { private String self; private String next; private String last; } @Data @JsonInclude(JsonInclude.Include.NON_NULL) protected static class DataBean&lt;T extends AbstractBaseDomain&gt; { private String type; private Long id; private T attributes; private T relationships; private Links links; } } dto.BaseResultFactory public class BaseResultFactory&lt;T extends AbstractBaseDomain&gt; { /** * 设置日志级别，用于限制发生错误时，是否显示调试信息(detail) * * @see ErrorResult#detail */ public static final String LOGGER_LEVEL_DEBUG = &quot;DEBUG&quot;; private static BaseResultFactory baseResultFactory; private BaseResultFactory() { } // 设置通用的响应 private static HttpServletResponse response; public static BaseResultFactory getInstance(HttpServletResponse response) { if (baseResultFactory == null) { synchronized (BaseResultFactory.class) { if (baseResultFactory == null) { baseResultFactory = new BaseResultFactory(); } } } BaseResultFactory.response = response; // 设置通用响应 baseResultFactory.initResponse(); return baseResultFactory; } public static BaseResultFactory getInstance() { if (baseResultFactory == null) { synchronized (BaseResultFactory.class) { if (baseResultFactory == null) { baseResultFactory = new BaseResultFactory(); } } } return baseResultFactory; } /** * 构建单笔数据结果集 * * @param self 当前请求路径 * @return */ public AbstractBaseResult build(String self, T attributes) { return new SuccessResult(self, attributes); } /** * 构建多笔数据结果集 * * @param self 当前请求路径 * @param next 下一页的页码 * @param last 最后一页的页码 * @return */ public AbstractBaseResult build(String self, int next, int last, List&lt;T&gt; attributes) { return new SuccessResult(self, next, last, attributes); } /** * 构建请求错误的响应结构，调试显示detail，上线不显示，通过配置日志级别 * * @param code HTTP 状态码 * @param title 错误信息 * @param detail 调试信息 * @param level 日志级别，只有 DEBUG 时才显示详情 * @return */ public AbstractBaseResult build(int code, String title, String detail, String level) { // 设置请求失败的响应码 response.setStatus(code); if (LOGGER_LEVEL_DEBUG.equals(level)) { return new ErrorResult(code, title, detail); } else { return new ErrorResult(code, title, null); } } /** * 初始化 HttpServletResponse */ private void initResponse() { // 需要符合 JSON API 规范 response.setHeader(&quot;Content-Type&quot;, &quot;application/vnd.api+json&quot;); } } dto.ErrorResult @Data @AllArgsConstructor @EqualsAndHashCode(callSuper = false) // JSON 不显示为 null 的属性 @JsonInclude(JsonInclude.Include.NON_NULL) public class ErrorResult extends AbstractBaseResult { private int code; private String title; /** * 调试信息 */ private String detail; } dto.SuccessResult @Data @EqualsAndHashCode(callSuper = false) public class SuccessResult&lt;T extends AbstractBaseDomain&gt; extends AbstractBaseResult { private Links links; private List&lt;DataBean&gt; data; /** * 请求的结果（单笔） * @param self 当前请求路径 * @param attributes 领域模型 */ public SuccessResult(String self, T attributes) { links = new Links(); links.setSelf(self); createDataBean(null, attributes); } /** * 请求的结果（分页） * @param self 当前请求路径 * @param next 下一页的页码 * @param last 最后一页的页码 * @param attributes 领域模型集合 */ public SuccessResult(String self, int next, int last, List&lt;T&gt; attributes) { links = new Links(); links.setSelf(self); links.setNext(self + &quot;?page=&quot; + next); links.setLast(self + &quot;?page=&quot; + last); attributes.forEach(attribute -&gt; createDataBean(self, attribute)); } /** * 创建 DataBean * @param self 当前请求路径 * @param attributes 领域模型 */ private void createDataBean(String self, T attributes) { if (data == null) { data = new ArrayList&lt;&gt;(); } DataBean dataBean = new DataBean(); dataBean.setId(attributes.getId()); dataBean.setType(attributes.getClass().getSimpleName()); dataBean.setAttributes(attributes); if (StringUtils.isNotBlank(self)) { Links links = new Links(); links.setSelf(self + &quot;/&quot; + attributes.getId()); dataBean.setLinks(links); } data.add(dataBean); } } utils.MapperUtils public class MapperUtils { private final static ObjectMapper objectMapper = new ObjectMapper(); public static ObjectMapper getInstance() { return objectMapper; } /** * 转换为 JSON 字符串 * * @param obj * @return * @throws Exception */ public static String obj2json(Object obj) throws Exception { return objectMapper.writeValueAsString(obj); } /** * 转换为 JSON 字符串，忽略空值 * * @param obj * @return * @throws Exception */ public static String obj2jsonIgnoreNull(Object obj) throws Exception { ObjectMapper mapper = new ObjectMapper(); mapper.setSerializationInclusion(JsonInclude.Include.NON_NULL); return mapper.writeValueAsString(obj); } /** * 转换为 JavaBean * * @param jsonString * @param clazz * @return * @throws Exception */ public static &lt;T&gt; T json2pojo(String jsonString, Class&lt;T&gt; clazz) throws Exception { objectMapper.configure(DeserializationFeature.ACCEPT_SINGLE_VALUE_AS_ARRAY, true); return objectMapper.readValue(jsonString, clazz); } /** * 字符串转换为 Map&lt;String, Object&gt; * * @param jsonString * @return * @throws Exception */ public static &lt;T&gt; Map&lt;String, Object&gt; json2map(String jsonString) throws Exception { ObjectMapper mapper = new ObjectMapper(); mapper.setSerializationInclusion(JsonInclude.Include.NON_NULL); return mapper.readValue(jsonString, Map.class); } /** * 字符串转换为 Map&lt;String, T&gt; */ public static &lt;T&gt; Map&lt;String, T&gt; json2map(String jsonString, Class&lt;T&gt; clazz) throws Exception { Map&lt;String, Map&lt;String, Object&gt;&gt; map = objectMapper.readValue(jsonString, new TypeReference&lt;Map&lt;String, T&gt;&gt;() { }); Map&lt;String, T&gt; result = new HashMap&lt;String, T&gt;(); for (Map.Entry&lt;String, Map&lt;String, Object&gt;&gt; entry : map.entrySet()) { result.put(entry.getKey(), map2pojo(entry.getValue(), clazz)); } return result; } /** * 深度转换 JSON 成 Map * * @param json * @return */ public static Map&lt;String, Object&gt; json2mapDeeply(String json) throws Exception { return json2MapRecursion(json, objectMapper); } /** * 把 JSON 解析成 List，如果 List 内部的元素存在 jsonString，继续解析 * * @param json * @param mapper 解析工具 * @return * @throws Exception */ private static List&lt;Object&gt; json2ListRecursion(String json, ObjectMapper mapper) throws Exception { if (json == null) { return null; } List&lt;Object&gt; list = mapper.readValue(json, List.class); for (Object obj : list) { if (obj != null &amp;&amp; obj instanceof String) { String str = (String) obj; if (str.startsWith(&quot;[&quot;)) { obj = json2ListRecursion(str, mapper); } else if (obj.toString().startsWith(&quot;{&quot;)) { obj = json2MapRecursion(str, mapper); } } } return list; } /** * 把 JSON 解析成 Map，如果 Map 内部的 Value 存在 jsonString，继续解析 * * @param json * @param mapper * @return * @throws Exception */ private static Map&lt;String, Object&gt; json2MapRecursion(String json, ObjectMapper mapper) throws Exception { if (json == null) { return null; } Map&lt;String, Object&gt; map = mapper.readValue(json, Map.class); for (Map.Entry&lt;String, Object&gt; entry : map.entrySet()) { Object obj = entry.getValue(); if (obj != null &amp;&amp; obj instanceof String) { String str = ((String) obj); if (str.startsWith(&quot;[&quot;)) { List&lt;?&gt; list = json2ListRecursion(str, mapper); map.put(entry.getKey(), list); } else if (str.startsWith(&quot;{&quot;)) { Map&lt;String, Object&gt; mapRecursion = json2MapRecursion(str, mapper); map.put(entry.getKey(), mapRecursion); } } } return map; } /** * 将 JSON 数组转换为集合 * * @param jsonArrayStr * @param clazz * @return * @throws Exception */ public static &lt;T&gt; List&lt;T&gt; json2list(String jsonArrayStr, Class&lt;T&gt; clazz) throws Exception { JavaType javaType = getCollectionType(ArrayList.class, clazz); List&lt;T&gt; list = (List&lt;T&gt;) objectMapper.readValue(jsonArrayStr, javaType); return list; } /** * 获取泛型的 Collection Type * * @param collectionClass 泛型的Collection * @param elementClasses 元素类 * @return JavaType Java类型 * @since 1.0 */ public static JavaType getCollectionType(Class&lt;?&gt; collectionClass, Class&lt;?&gt;... elementClasses) { return objectMapper.getTypeFactory().constructParametricType(collectionClass, elementClasses); } /** * 将 Map 转换为 JavaBean * * @param map * @param clazz * @return */ public static &lt;T&gt; T map2pojo(Map map, Class&lt;T&gt; clazz) { return objectMapper.convertValue(map, clazz); } /** * 将 Map 转换为 JSON * * @param map * @return */ public static String mapToJson(Map map) { try { return objectMapper.writeValueAsString(map); } catch (Exception e) { e.printStackTrace(); } return &quot;&quot;; } /** * 将 JSON 对象转换为 JavaBean * * @param obj * @param clazz * @return */ public static &lt;T&gt; T obj2pojo(Object obj, Class&lt;T&gt; clazz) { return objectMapper.convertValue(obj, clazz); } } utils.RegexpUtils public class RegexpUtils { /** * 验证手机号 */ public static final String PHONE = &quot;^((13[0-9])|(15[^4,\\D])|(18[0,5-9]))\\d{8}$&quot;; /** * 验证邮箱地址 */ public static final String EMAIL = &quot;\\w+(\\.\\w)*@\\w+(\\.\\w{2,3}){1,3}&quot;; /** * 验证手机号 * @param phone * @return */ public static boolean checkPhone(String phone) { return phone.matches(PHONE); } /** * 验证邮箱 * @param email * @return */ public static boolean checkEmail(String email) { return email.matches(EMAIL); } } web.AbstractBaseController public abstract class AbstractBaseController&lt;T extends AbstractBaseDomain&gt; { // 用于动态获取配置文件的属性值 private static final String ENVIRONMENT_LOGGING_LEVEL_MY_SHOP = &quot;logging.level.com.ht.micro.record&quot;; @Resource protected HttpServletRequest request; @Resource protected HttpServletResponse response; @Autowired private ConfigurableApplicationContext applicationContext; @ModelAttribute public void initReqAndRes(HttpServletRequest request, HttpServletResponse response) { this.request = request; this.response = response; } /** * 请求成功 * @param self * @param attribute * @return */ protected AbstractBaseResult success(String self, T attribute) { return BaseResultFactory.getInstance(response).build(self, attribute); } /** * 请求成功 * @param self * @param next * @param last * @param attributes * @return */ protected AbstractBaseResult success(String self, int next, int last, List&lt;T&gt; attributes) { return BaseResultFactory.getInstance(response).build(self, next, last, attributes); } /** * 请求失败 * @param title * @param detail * @return */ protected AbstractBaseResult error(String title, String detail) { return error(HttpStatus.UNAUTHORIZED.value(), title, detail); } /** * 请求失败 * @param code * @param title * @param detail * @return */ protected AbstractBaseResult error(int code, String title, String detail) { return BaseResultFactory.getInstance(response).build(code, title, detail, applicationContext.getEnvironment().getProperty(ENVIRONMENT_LOGGING_LEVEL_MY_SHOP)); } } ht-micro-record-commons-domaindomain.TbUser @Table(name = &quot;tb_user&quot;) @JsonInclude(JsonInclude.Include.NON_NULL) public class TbUser extends AbstractBaseDomain { /** * 用户名 */ @NotNull(message = &quot;用户名不可为空&quot;) @Length(min = 5, max = 20, message = &quot;用户名长度必须介于 5 和 20 之间&quot;) private String username; /** * 密码，加密存储 */ @JsonIgnore private String password; /** * 注册手机号 */ private String phone; /** * 注册邮箱 */ @NotNull(message = &quot;邮箱不可为空&quot;) @Pattern(regexp = RegexpUtils.EMAIL, message = &quot;邮箱格式不正确&quot;) private String email; /** * 获取用户名 * * @return username - 用户名 */ public String getUsername() { return username; } /** * 设置用户名 * * @param username 用户名 */ public void setUsername(String username) { this.username = username; } /** * 获取密码，加密存储 * * @return password - 密码，加密存储 */ public String getPassword() { return password; } /** * 设置密码，加密存储 * * @param password 密码，加密存储 */ public void setPassword(String password) { this.password = password; } /** * 获取注册手机号 * * @return phone - 注册手机号 */ public String getPhone() { return phone; } /** * 设置注册手机号 * * @param phone 注册手机号 */ public void setPhone(String phone) { this.phone = phone; } /** * 获取注册邮箱 * * @return email - 注册邮箱 */ public String getEmail() { return email; } /** * 设置注册邮箱 * * @param email 注册邮箱 */ public void setEmail(String email) { this.email = email; } } validator.BeanValidator @Component public class BeanValidator { @Autowired private Validator validatorInstance; private static Validator validator; // 系统启动时将静态对象注入容器，不可用Autowire直接注入 @PostConstruct public void init() { BeanValidator.validator = validatorInstance; } /** * 调用 JSR303 的 validate 方法, 验证失败时抛出 ConstraintViolationException. */ private static void validateWithException(Validator validator, Object object, Class&lt;?&gt;... groups) throws ConstraintViolationException { Set constraintViolations = validator.validate(object, groups); if (!constraintViolations.isEmpty()) { throw new ConstraintViolationException(constraintViolations); } } /** * 辅助方法, 转换 ConstraintViolationException 中的 Set&lt;ConstraintViolations&gt; 中为 List&lt;message&gt;. */ private static List&lt;String&gt; extractMessage(ConstraintViolationException e) { return extractMessage(e.getConstraintViolations()); } /** * 辅助方法, 转换 Set&lt;ConstraintViolation&gt; 为 List&lt;message&gt; */ private static List&lt;String&gt; extractMessage(Set&lt;? extends ConstraintViolation&gt; constraintViolations) { List&lt;String&gt; errorMessages = new ArrayList&lt;&gt;(); for (ConstraintViolation violation : constraintViolations) { errorMessages.add(violation.getMessage()); } return errorMessages; } /** * 辅助方法, 转换 ConstraintViolationException 中的 Set&lt;ConstraintViolations&gt; 为 Map&lt;property, message&gt;. */ private static Map&lt;String, String&gt; extractPropertyAndMessage(ConstraintViolationException e) { return extractPropertyAndMessage(e.getConstraintViolations()); } /** * 辅助方法, 转换 Set&lt;ConstraintViolation&gt; 为 Map&lt;property, message&gt;. */ private static Map&lt;String, String&gt; extractPropertyAndMessage(Set&lt;? extends ConstraintViolation&gt; constraintViolations) { Map&lt;String, String&gt; errorMessages = new HashMap&lt;&gt;(); for (ConstraintViolation violation : constraintViolations) { errorMessages.put(violation.getPropertyPath().toString(), violation.getMessage()); } return errorMessages; } /** * 辅助方法, 转换 ConstraintViolationException 中的 Set&lt;ConstraintViolations&gt; 为 List&lt;propertyPath message&gt;. */ private static List&lt;String&gt; extractPropertyAndMessageAsList(ConstraintViolationException e) { return extractPropertyAndMessageAsList(e.getConstraintViolations(), &quot; &quot;); } /** * 辅助方法, 转换 Set&lt;ConstraintViolations&gt; 为 List&lt;propertyPath message&gt;. */ private static List&lt;String&gt; extractPropertyAndMessageAsList(Set&lt;? extends ConstraintViolation&gt; constraintViolations) { return extractPropertyAndMessageAsList(constraintViolations, &quot; &quot;); } /** * 辅助方法, 转换 ConstraintViolationException 中的 Set&lt;ConstraintViolations&gt; 为 List&lt;propertyPath + separator + message&gt;. */ private static List&lt;String&gt; extractPropertyAndMessageAsList(ConstraintViolationException e, String separator) { return extractPropertyAndMessageAsList(e.getConstraintViolations(), separator); } /** * 辅助方法, 转换 Set&lt;ConstraintViolation&gt; 为 List&lt;propertyPath + separator + message&gt;. */ private static List&lt;String&gt; extractPropertyAndMessageAsList(Set&lt;? extends ConstraintViolation&gt; constraintViolations, String separator) { List&lt;String&gt; errorMessages = new ArrayList&lt;&gt;(); for (ConstraintViolation violation : constraintViolations) { errorMessages.add(violation.getPropertyPath() + separator + violation.getMessage()); } return errorMessages; } /** * 服务端参数有效性验证 * * @param object 验证的实体对象 * @param groups 验证组 * @return 验证成功：返回 null；验证失败：返回错误信息 */ public static String validator(Object object, Class&lt;?&gt;... groups) { try { validateWithException(validator, object, groups); } catch (ConstraintViolationException ex) { List&lt;String&gt; list = extractMessage(ex); list.add(0, &quot;数据验证失败：&quot;); // 封装错误消息为字符串 StringBuilder sb = new StringBuilder(); for (int i = 0; i &lt; list.size(); i++) { String exMsg = list.get(i); if (i != 0) { sb.append(String.format(&quot;%s. %s&quot;, i, exMsg)).append(list.size() &gt; 1 ? &quot;&lt;br/&gt;&quot; : &quot;&quot;); } else { sb.append(exMsg).append(list.size() &gt; 1 ? &quot;&lt;br/&gt;&quot; : &quot;&quot;); } } return sb.toString(); } return null; } } ht-micro-record-commons-mapper├─src│ └─main│ ├─java│ │ ├─com│ │ │ └─ht│ │ │ └─micro│ │ │ └─record│ │ │ └─commons│ │ │ └─mapper│ │ │ ├─baseMapper│ │ │ └─dicMapper│ │ └─tk│ │ └─mybatis│ │ └─mapper│ └─resources│ ├─baseMapper│ └─dicMappermapper分开存储用来识别多数据源tk.mybatis.mapper.MyMapper public interface MyMapper&lt;T&gt; extends Mapper&lt;T&gt;, MySqlMapper&lt;T&gt; { } ht-micro-record-commons-serviceBaseCrudService public interface BaseCrudService&lt;T extends AbstractBaseDomain&gt; { /** * 查询属性值是否唯一 * * @param property * @param value * @return true/唯一，false/不唯一 */ default boolean unique(String property, String value) { return false; } /** * 保存 * * @param domain * @return */ default T save(T domain) { return null; } /** * 分页查询 * @param domain * @param pageNum * @param pageSize * @return */ default PageInfo&lt;T&gt; page(T domain, int pageNum, int pageSize) { return null; } } impl.BaseCrudServiceImpl public class BaseCrudServiceImpl&lt;T extends AbstractBaseDomain, M extends MyMapper&lt;T&gt;&gt; implements BaseCrudService&lt;T&gt; { @Autowired protected M mapper; private Class&lt;T&gt; entityClass = (Class&lt;T&gt;) ((ParameterizedType) getClass().getGenericSuperclass()).getActualTypeArguments()[0]; @Override public boolean unique(String property, String value) { Example example = new Example(entityClass); example.createCriteria().andEqualTo(property, value); int result = mapper.selectCountByExample(example); if (result &gt; 0) { return false; } return true; } @Override public T save(T domain) { int result = 0; Date currentDate = new Date(); // 创建 if (domain.getId() == null) { /** * 用于自动回显 ID，领域模型中需要 @ID 注解的支持 * {@link AbstractBaseDomain} */ result = mapper.insertUseGeneratedKeys(domain); } // 更新 else { result = mapper.updateByPrimaryKey(domain); } // 保存数据成功 if (result &gt; 0) { return domain; } // 保存数据失败 return null; } @Override public PageInfo&lt;T&gt; page(T domain, int pageNum, int pageSize) { Example example = new Example(entityClass); example.createCriteria().andEqualTo(domain); PageHelper.startPage(pageNum, pageSize); PageInfo&lt;T&gt; pageInfo = new PageInfo&lt;&gt;(mapper.selectByExample(example)); return pageInfo; } } TbUserService public interface TbUserService extends BaseCrudService&lt;TbUser&gt; { TbUser getById(long id); } impl.TbUserServiceImpl @Service public class TbUserServiceImpl extends BaseCrudServiceImpl&lt;TbUser, TbUserMapper&gt; implements TbUserService { @Autowired private TbUserMapper tbUserMapper; public TbUser getById(long id){ return tbUserMapper.selectByPrimaryKey(id); } } ht-micro-record-service-user &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath/&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-service-user&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-service-user&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt; &lt;artifactId&gt;jsr305&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt; &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt; &lt;artifactId&gt;jsr305&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt; &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rocketmq&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;!-- Projects Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-commons-service&lt;/artifactId&gt; &lt;version&gt;${project.parent.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Projects End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;!-- docker的maven插件，官网 https://github.com/spotify/docker-maven-plugin --&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt; &lt;version&gt;0.4.13&lt;/version&gt; &lt;configuration&gt; &lt;imageName&gt;192.168.2.7:5000/${project.artifactId}:${project.version}&lt;/imageName&gt; &lt;baseImage&gt;onejane-jdk1.8 &lt;/baseImage&gt; &lt;entryPoint&gt;[&quot;java&quot;, &quot;-jar&quot;,&quot;/${project.build.finalName}.jar&quot;]&lt;/entryPoint&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/&lt;/targetPath&gt; &lt;directory&gt;${project.build.directory}&lt;/directory&gt; &lt;include&gt;${project.build.finalName}.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;dockerHost&gt;http://192.168.2.7:2375&lt;/dockerHost&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;com.ht.micro.record.UserServiceApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; application.yml配置多数据源及RocketMQ spring: cloud: stream: rocketmq: binder: name-server: 192.168.2.7:9876 bindings: output: content-type: application/json destination: topic-email producer: group: group-email datasource: base: #监控统计拦截的filters filters: stat type: com.alibaba.druid.pool.DruidDataSource jdbc-url: jdbc:mysql://192.168.2.7:185/ht_micro_record?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false username: root password: 123456 driver-class-name: com.mysql.jdbc.Driver max-idle: 10 max-wait: 10000 min-idle: 5 initial-size: 5 validation-query: SELECT 1 test-on-borrow: false test-while-idle: true time-between-eviction-runs-millis: 18800 dic: #监控统计拦截的filters filters: stat type: com.alibaba.druid.pool.DruidDataSource jdbc-url: jdbc:mysql://192.168.1.48:3306/ht_nlp?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false username: root password: Sonar@1234 driver-class-name: com.mysql.jdbc.Driver max-idle: 10 max-wait: 10000 min-idle: 5 initial-size: 5 validation-query: SELECT 1 test-on-borrow: false test-while-idle: true time-between-eviction-runs-millis: 18800 bootstrap.properties spring.application.name=ht-micro-record-service-user-config spring.cloud.nacos.config.file-extension=yaml spring.cloud.nacos.config.server-addr=192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 logging.level.com.ht.micro.record=DEBUG nacos配置 spring: application: name: ht-micro-record-service-user cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8719 dashboard: 192.168.2.7:190 server: port: 9506 management: endpoints: web: exposure: include: &quot;*&quot; com.ht.micro.record.UserServiceApplication @SpringBootApplication(scanBasePackages = &quot;com.ht.micro.record&quot;,exclude = {DataSourceAutoConfiguration.class, DataSourceTransactionManagerAutoConfiguration.class, MybatisAutoConfiguration.class}) @EnableDiscoveryClient @EnableBinding({Source.class}) @MapperScan(basePackages = &quot;com.ht.micro.record.commons.mapper&quot;) @EnableAsync @EnableSwagger2 public class UserServiceApplication { public static void main(String[] args) { SpringApplication.run(UserServiceApplication.class, args); } } com.ht.micro.record.user.config.BaseMybatisConfig @Configuration @MapperScan(basePackages = {&quot;com.ht.micro.record.commons.mapper.baseMapper&quot;}, sqlSessionTemplateRef = &quot;baseSqlSessionTemplate&quot;) public class BaseMybatisConfig { @Value(&quot;${spring.datasource.base.filters}&quot;) String filters; @Value(&quot;${spring.datasource.base.driver-class-name}&quot;) String driverClassName; @Value(&quot;${spring.datasource.base.username}&quot;) String username; @Value(&quot;${spring.datasource.base.password}&quot;) String password; @Value(&quot;${spring.datasource.base.jdbc-url}&quot;) String url; @Bean(name=&quot;baseDataSource&quot;) @Primary//必须加此注解，不然报错，下一个类则不需要添加 spring.datasource @ConfigurationProperties(prefix=&quot;spring.datasource.base&quot;)//prefix值必须是application.properteis中对应属性的前缀 public DataSource baseDataSource() throws SQLException { DruidDataSource druid = new DruidDataSource(); // 监控统计拦截的filters druid.setFilters(filters); // 配置基本属性 druid.setDriverClassName(driverClassName); druid.setUsername(username); druid.setPassword(password); druid.setUrl(url); return druid; } @Bean(name=&quot;baseSqlSessionFactory&quot;) @Primary public SqlSessionFactory baseSqlSessionFactory(@Qualifier(&quot;baseDataSource&quot;)DataSource dataSource)throws Exception{ // 创建Mybatis的连接会话工厂实例 SqlSessionFactoryBean bean=new SqlSessionFactoryBean(); bean.setDataSource(dataSource);//// 设置数据源bean //添加XML目录 ResourcePatternResolver resolver=new PathMatchingResourcePatternResolver(); try{ bean.setMapperLocations(resolver.getResources(&quot;classpath:baseMapper/*Mapper.xml&quot;));//// 设置mapper文件路径 return bean.getObject(); }catch(Exception e){ e.printStackTrace(); throw new RuntimeException(e); } } @Bean(name=&quot;baseSqlSessionTemplate&quot;) @Primary public SqlSessionTemplate baseSqlSessionTemplate(@Qualifier(&quot;baseSqlSessionFactory&quot;)SqlSessionFactory sqlSessionFactory)throws Exception{ SqlSessionTemplate template=new SqlSessionTemplate(sqlSessionFactory);//使用上面配置的Factory return template; } } com.ht.micro.record.user.config.DicMybatisConfig @Configuration @MapperScan(basePackages = {&quot;com.ht.micro.record.commons.mapper.dicMapper&quot;}, sqlSessionTemplateRef = &quot;dicSqlSessionTemplate&quot;) public class DicMybatisConfig { @Value(&quot;${spring.datasource.dic.filters}&quot;) String filters; @Value(&quot;${spring.datasource.dic.driver-class-name}&quot;) String driverClassName; @Value(&quot;${spring.datasource.dic.username}&quot;) String username; @Value(&quot;${spring.datasource.dic.password}&quot;) String password; @Value(&quot;${spring.datasource.dic.jdbc-url}&quot;) String url; @Bean(name = &quot;dicDataSource&quot;) @ConfigurationProperties(prefix=&quot;spring.datasource.dic&quot;) public DataSource dicDataSource() throws SQLException { DruidDataSource druid = new DruidDataSource(); // 监控统计拦截的filters // druid.setFilters(filters); // 配置基本属性 druid.setDriverClassName(driverClassName); druid.setUsername(username); druid.setPassword(password); druid.setUrl(url); /* //初始化时建立物理连接的个数 druid.setInitialSize(initialSize); //最大连接池数量 druid.setMaxActive(maxActive); //最小连接池数量 druid.setMinIdle(minIdle); //获取连接时最大等待时间，单位毫秒。 druid.setMaxWait(maxWait); //间隔多久进行一次检测，检测需要关闭的空闲连接 druid.setTimeBetweenEvictionRunsMillis(timeBetweenEvictionRunsMillis); //一个连接在池中最小生存的时间 druid.setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis); //用来检测连接是否有效的sql druid.setValidationQuery(validationQuery); //建议配置为true，不影响性能，并且保证安全性。 druid.setTestWhileIdle(testWhileIdle); //申请连接时执行validationQuery检测连接是否有效 druid.setTestOnBorrow(testOnBorrow); druid.setTestOnReturn(testOnReturn); //是否缓存preparedStatement，也就是PSCache，oracle设为true，mysql设为false。分库分表较多推荐设置为false druid.setPoolPreparedStatements(poolPreparedStatements); // 打开PSCache时，指定每个连接上PSCache的大小 druid.setMaxPoolPreparedStatementPerConnectionSize(maxPoolPreparedStatementPerConnectionSize); */ return druid; } @Bean(name = &quot;dicSqlSessionFactory&quot;) public SqlSessionFactory dicSqlSessionFactory(@Qualifier(&quot;dicDataSource&quot;)DataSource dataSource)throws Exception{ SqlSessionFactoryBean bean=new SqlSessionFactoryBean(); bean.setDataSource(dataSource); //添加XML目录 ResourcePatternResolver resolver=new PathMatchingResourcePatternResolver(); try{ bean.setMapperLocations(resolver.getResources(&quot;classpath:dicMapper/*Mapper.xml&quot;)); return bean.getObject(); }catch(Exception e){ e.printStackTrace(); throw new RuntimeException(e); } } @Bean(name=&quot;dicSqlSessionTemplate&quot;) public SqlSessionTemplate dicSqlSessionTemplate(@Qualifier(&quot;dicSqlSessionFactory&quot;)SqlSessionFactory sqlSessionFactory)throws Exception{ SqlSessionTemplate template=new SqlSessionTemplate(sqlSessionFactory);//使用上面配置的Factory return template; } } com.ht.micro.record.user.service.UserService @Service public class UserService { @Autowired private MessageChannel output; // @EnableAsync在Application中开启异步 @Async public void sendEmail(TbUser tbUser) throws Exception { output.send(MessageBuilder.withPayload(MapperUtils.obj2json(tbUser)).build()); } } com.ht.micro.record.user.controller.UserController @RestController @RequestMapping(value = &quot;user&quot;) public class UserController extends AbstractBaseController&lt;TbUser&gt; { @Autowired private TbUserService tbUserService; @Autowired private UserService userService; // http://localhost:9506/user/2 // @ApiOperation(value = &quot;查询用户&quot;, notes = &quot;根据id获取用户名&quot;) @GetMapping(value = {&quot;{id}&quot;}) public String getName(@PathVariable long id){ return tbUserService.getById(id).getUsername(); } @ApiOperation(value = &quot;用户注册&quot;, notes = &quot;参数为实体类，注意用户名和邮箱不要重复&quot;) @PostMapping(value = &quot;reg&quot;) public AbstractBaseResult reg(@ApiParam(name = &quot;tbUser&quot;, value = &quot;用户模型&quot;) TbUser tbUser) { // 数据校验 String message = BeanValidator.validator(tbUser); if (StringUtils.isNotBlank(message)) { return error(message, null); } // 验证密码是否为空 if (StringUtils.isBlank(tbUser.getPassword())) { return error(&quot;密码不可为空&quot;, null); } // 验证用户名是否重复 if (!tbUserService.unique(&quot;username&quot;, tbUser.getUsername())) { return error(&quot;用户名已存在&quot;, null); } // 验证邮箱是否重复 if (!tbUserService.unique(&quot;email&quot;, tbUser.getEmail())) { return error(&quot;邮箱重复，请重试&quot;, null); } // 注册用户 try { tbUser.setPassword(DigestUtils.md5DigestAsHex(tbUser.getPassword().getBytes())); TbUser user = tbUserService.save(tbUser); if (user != null) { userService.sendEmail(user); response.setStatus(HttpStatus.CREATED.value()); return success(request.getRequestURI(), user); } } catch (Exception e) { // 这里补一句，将 RegService 中的异常抛到 Controller 中，这样可以打印出调试信息 return error(HttpStatus.INTERNAL_SERVER_ERROR.value(), &quot;注册邮件发送失败&quot;, e.getMessage()); } // 注册失败 return error(&quot;注册失败，请重试&quot;, null); } } ht-micro-record-service-sms &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-service-sms&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-service-sms&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-mail&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.sourceforge.nekohtml&lt;/groupId&gt; &lt;artifactId&gt;nekohtml&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt; &lt;artifactId&gt;jsr305&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt; &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rocketmq&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;!-- Projects Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-commons-domain&lt;/artifactId&gt; &lt;version&gt;${project.parent.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Projects End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;ht-micro-record-service-sms&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;com.ht.micro.record.service.email.SmsServiceApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- docker的maven插件，官网 https://github.com/spotify/docker-maven-plugin --&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt; &lt;version&gt;0.4.13&lt;/version&gt; &lt;configuration&gt; &lt;imageName&gt;192.168.2.7:5000/${project.artifactId}:${project.version}&lt;/imageName&gt; &lt;baseImage&gt;jdk1.8&lt;/baseImage&gt; &lt;entryPoint&gt;[&quot;java&quot;, &quot;-jar&quot;,&quot;/${project.build.finalName}.jar&quot;]&lt;/entryPoint&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/&lt;/targetPath&gt; &lt;directory&gt;${project.build.directory}&lt;/directory&gt; &lt;include&gt;${project.build.finalName}.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;dockerHost&gt;http://192.168.2.7:2375&lt;/dockerHost&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; application.yml spring: cloud: stream: rocketmq: binder: name-server: 192.168.2.7:9876 bindings: input: consumer: orderly: true bindings: input: destination: topic-email content-type: application/json group: group-email consumer: maxAttempts: 1 thymeleaf: cache: false mode: HTML encoding: UTF-8 servlet: content-type: text/html bootstrap.properties spring.application.name=ht-micro-record-service-sms-config spring.cloud.nacos.config.file-extension=yaml spring.cloud.nacos.config.server-addr=192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 nacos配置 spring: application: name: ht-micro-record-service-sms mail: host: smtp.163.com port: 25 # 你的邮箱授权码 password: codewj123456 properties: mail: smtp: auth: true starttls: enable: true required: true # 发送邮件的邮箱地址 username: m15806204096@163.com cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8719 dashboard: 192.168.2.7:190 server: port: 9507 management: endpoints: web: exposure: include: &quot;*&quot; com.ht.micro.record.service.email.SmsServiceApplication @SpringBootApplication(scanBasePackages = &quot;com.ht.micro.record&quot;) @EnableDiscoveryClient @EnableBinding({Sink.class}) @EnableAsync public class SmsServiceApplication { public static void main(String[] args) { SpringApplication.run(SmsServiceApplication.class, args); } } com.ht.micro.record.service.email.service.EmailService @Service public class EmailService { @Autowired private ConfigurableApplicationContext applicationContext; @Autowired private JavaMailSender javaMailSender; @Autowired private TemplateEngine templateEngine; @StreamListener(&quot;input&quot;) public void receive(String json) { try { // 发送普通邮件 TbUser tbUser = MapperUtils.json2pojo(json, TbUser.class); sendEmail(&quot;欢迎注册&quot;, &quot;欢迎 &quot; + tbUser.getUsername() + &quot; 加入华通晟云！&quot;, tbUser.getEmail()); // 发送 HTML 模板邮件 Context context = new Context(); context.setVariable(&quot;username&quot;, tbUser.getUsername()); String emailTemplate = templateEngine.process(&quot;reg&quot;, context); sendTemplateEmail(&quot;欢迎注册&quot;, emailTemplate, tbUser.getEmail()); } catch (Exception e) { e.printStackTrace(); } } /** * 发送普通邮件 * @param subject * @param body * @param to */ @Async public void sendEmail(String subject, String body, String to) { SimpleMailMessage message = new SimpleMailMessage(); message.setFrom(applicationContext.getEnvironment().getProperty(&quot;spring.mail.username&quot;)); message.setTo(to); message.setSubject(subject); message.setText(body); javaMailSender.send(message); } /** * 发送 HTML 模板邮件 * @param subject * @param body * @param to */ @Async public void sendTemplateEmail(String subject, String body, String to) { MimeMessage message = javaMailSender.createMimeMessage(); try { MimeMessageHelper helper = new MimeMessageHelper(message, true); helper.setFrom(applicationContext.getEnvironment().getProperty(&quot;spring.mail.username&quot;)); helper.setTo(to); helper.setSubject(subject); helper.setText(body, true); javaMailSender.send(message); } catch (Exception e) { } } } templates/reg.html &lt;!DOCTYPE html SYSTEM &quot;http://www.thymeleaf.org/dtd/xhtml1-strict-thymeleaf-spring4-4.dtd&quot;&gt; &lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot; xmlns:th=&quot;http://www.thymeleaf.org&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot;&gt; &lt;title&gt;注册通知&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div&gt; 欢迎 &lt;span th:text=&quot;${username}&quot;&gt;&lt;/span&gt; 加入 华通晟云！！ &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; post http://localhost:9506/user/reg?password=123456&amp;username=codewj&amp;email=1051103813@qq.com 实现注册 集成Swagger2ht-micro-record-commons/pom.xml &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;/dependency&gt;com.ht.micro.record.commons.config.Swagger2Configuration @Configuration public class Swagger2Configuration { @Bean public Docket createRestApi() { return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) .select() .apis(RequestHandlerSelectors.basePackage(&quot;com.ht.micro.record&quot;)) // 配置controller地址 .paths(PathSelectors.any()) .build(); } private ApiInfo apiInfo() { return new ApiInfoBuilder() .title(&quot;微服务 API 文档&quot;) .description(&quot;微服务 API 网关接口，http://www.htdatacloud.com/&quot;) .termsOfServiceUrl(&quot;http://www.htdatacloud.com&quot;) .version(&quot;1.0.0&quot;) .build(); } }com.ht.micro.record.UserServiceApplication @EnableSwagger2 com.ht.micro.record.user.controller.UserController @ApiOperation(value = &quot;用户注册&quot;, notes = &quot;参数为实体类，注意用户名和邮箱不要重复&quot;) @PostMapping 服务提供消费配置项目skywalking链路追踪并启动 -javaagent:E:\Project\ht-micro-record\ht-micro-record-external-skywalking\agent\skywalking-agent.jar -Dskywalking.agent.service_name=ht-micro-record-service-user -Dskywalking.collector.backend_service=192.168.2.7:11800 -javaagent:E:\Project\hello-spring-cloud-alibaba\hello-spring-cloud-external-skywalking\agent\skywalking-agent.jar -Dskywalking.agent.service_name=nacos-consumer-feign -Dskywalking.collector.backend_service=192.168.3.229:11800http://192.168.3.233:9501/user/10 调用服务http://192.168.2.7:193/#/monitor/dashboard 查看skywalking的链路追踪]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>spring cloud alibaba</tag>
        <tag>skywalking</tag>
        <tag>rocketmq</tag>
        <tag>validate</tag>
        <tag>multi druid</tag>
        <tag>swagger</tag>
        <tag>email</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Alibaba环境搭建]]></title>
    <url>%2F2019%2F08%2F21%2FAlibaba%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[基于spring cloud alibaba实战整合开发 创建统一的依赖管理ht-micro-record-dependencies &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.2.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;name&gt;ht-micro-record-dependencies&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;properties&gt; &lt;!-- Environment Settings --&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;!-- Spring Cloud Settings --&gt; &lt;spring-cloud.version&gt;Greenwich.RELEASE&lt;/spring-cloud.version&gt; &lt;spring-cloud-alibaba.version&gt;0.9.0.RELEASE&lt;/spring-cloud-alibaba.version&gt; &lt;!-- Spring Boot Settings --&gt; &lt;spring-boot-alibaba-druid.version&gt;1.1.18&lt;/spring-boot-alibaba-druid.version&gt; &lt;spring-boot-tk-mybatis.version&gt;2.1.4&lt;/spring-boot-tk-mybatis.version&gt; &lt;spring-boot-pagehelper.version&gt;1.2.12&lt;/spring-boot-pagehelper.version&gt; &lt;!-- Commons Settings --&gt; &lt;mysql.version&gt;5.1.38&lt;/mysql.version&gt; &lt;swagger2.version&gt;2.9.2&lt;/swagger2.version&gt; &lt;/properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;${spring-cloud.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;${spring-cloud-alibaba.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;${spring-boot-alibaba-druid.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;${spring-boot-tk-mybatis.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;${spring-boot-pagehelper.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Commons Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;${mysql.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;${swagger2.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;${swagger2.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!--防止版本冲突，统一版本在父pom中--&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;24.0-jre&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Commons End --&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;build&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;!-- Compiler 插件, 设定 JDK 版本，所有子服务统一打包jdk版本 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.8.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- 打包 jar 文件时，配置 manifest 文件，加入 lib 包的 jar 依赖 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;addMavenDescriptor&gt;false&lt;/addMavenDescriptor&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;!-- Add directory entries --&gt; &lt;addDefaultImplementationEntries&gt;true&lt;/addDefaultImplementationEntries&gt; &lt;addDefaultSpecificationEntries&gt;true&lt;/addDefaultSpecificationEntries&gt; &lt;addClasspath&gt;true&lt;/addClasspath&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- resource --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!-- install --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-install-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!-- clean --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-clean-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!-- ant --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!-- dependency --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;!-- 资源文件配置 --&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;excludes&gt; &lt;exclude&gt;**/*.java&lt;/exclude&gt; &lt;/excludes&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/build&gt; &lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;name&gt;Nexus Release Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;name&gt;Nexus Snapshot Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;Nexus Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-public/&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;aliyun-repos&lt;/id&gt; &lt;name&gt;Aliyun Repository&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;sonatype-repos&lt;/id&gt; &lt;name&gt;Sonatype Repository&lt;/name&gt; &lt;url&gt;https://oss.sonatype.org/content/groups/public&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;sonatype-repos-s&lt;/id&gt; &lt;name&gt;Sonatype Repository&lt;/name&gt; &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;spring-snapshots&lt;/id&gt; &lt;name&gt;Spring Snapshots&lt;/name&gt; &lt;url&gt;https://repo.spring.io/snapshot&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;spring-milestones&lt;/id&gt; &lt;name&gt;Spring Milestones&lt;/name&gt; &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;Nexus Plugin Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-public/&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;/pluginRepository&gt; &lt;pluginRepository&gt; &lt;id&gt;aliyun-repos&lt;/id&gt; &lt;name&gt;Aliyun Repository&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt;创建通用的工具类库ht-micro-record-commons &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath/&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-commons&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-commons&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;!-- Spring Begin ，每个RequestMapping之上都执行@ModelAttribute注解的方法，请求时都会带入request和response--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; &lt;artifactId&gt;tomcat-embed-core&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring End HttpServletRequest等实体，provided 其他服务依赖commons时不会自动引入tomcat依赖--&gt; &lt;!-- Apache Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Apache End --&gt; &lt;!-- Commons Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.ben-manes.caffeine&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.persistence&lt;/groupId&gt; &lt;artifactId&gt;javax.persistence-api&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Commons End --&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;ht-micro-record-commons&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;classifier&gt;exec&lt;/classifier&gt; &lt;mainClass&gt;com.ht.micro.record.commons.CommonsApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;创建通用的领域模型ht-micro-record-commons-domain &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath/&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-commons-domain&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-commons-domain&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;!-- Spring Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring End --&gt; &lt;!-- Commons Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate.validator&lt;/groupId&gt; &lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Commons End --&gt; &lt;!-- Projects Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-commons&lt;/artifactId&gt; &lt;version&gt;${project.parent.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Projects End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;ht-micro-record-commons-domain&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;classifier&gt;exec&lt;/classifier&gt; &lt;mainClass&gt;com.ht.micro.record.commons.CommonsDomainApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;创建通用的数据访问ht-micro-record-commons-mapper &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-commons-mapper&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-commons-mapper&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Commons Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Commons End --&gt; &lt;!-- Projects Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-commons-domain&lt;/artifactId&gt; &lt;version&gt;${project.parent.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Projects End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;ht-micro-record-commons-domain&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;classifier&gt;exec&lt;/classifier&gt; &lt;mainClass&gt;com.ht.micro.record.commons.CommonsMapperApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;创建通用的业务逻辑ht-micro-record-commons-service &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-commons-service&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-commons-service&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-commons-mapper&lt;/artifactId&gt; &lt;version&gt;${project.parent.version}&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;ht-micro-record-commons-domain&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;classifier&gt;exec&lt;/classifier&gt; &lt;mainClass&gt;com.ht.micro.record.commons.CommonsMapperApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;创建通用的代码生成ht-micro-record-database &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-database&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-database&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2018-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper&lt;/artifactId&gt; &lt;version&gt;4.1.4&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.3.5&lt;/version&gt; &lt;configuration&gt; &lt;configurationFile&gt;${basedir}/src/main/resources/generator/generatorConfig.xml&lt;/configurationFile&gt; &lt;overwrite&gt;true&lt;/overwrite&gt; &lt;verbose&gt;true&lt;/verbose&gt; &lt;/configuration&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.38&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper&lt;/artifactId&gt; &lt;version&gt;4.1.4&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;tk.mybatis.mapper.MyMapper public interface MyMapper&lt;T&gt; extends Mapper&lt;T&gt;, MySqlMapper&lt;T&gt; { }generator/generatorConfig.xml &lt;generatorConfiguration&gt; &lt;!-- 引入数据库连接配置 --&gt; &lt;properties resource=&quot;jdbc.properties&quot;/&gt; &lt;context id=&quot;Mysql&quot; targetRuntime=&quot;MyBatis3Simple&quot; defaultModelType=&quot;flat&quot;&gt; &lt;property name=&quot;beginningDelimiter&quot; value=&quot;`&quot;/&gt; &lt;property name=&quot;endingDelimiter&quot; value=&quot;`&quot;/&gt; &lt;!-- 配置 tk.mybatis 插件 --&gt; &lt;plugin type=&quot;tk.mybatis.mapper.generator.MapperPlugin&quot;&gt; &lt;property name=&quot;mappers&quot; value=&quot;tk.mybatis.mapper.MyMapper&quot;/&gt; &lt;/plugin&gt; &lt;!-- 配置数据库连接 --&gt; &lt;jdbcConnection driverClass=&quot;${jdbc.driverClass}&quot; connectionURL=&quot;${jdbc.connectionURL}&quot; userId=&quot;${jdbc.username}&quot; password=&quot;${jdbc.password}&quot;&gt; &lt;/jdbcConnection&gt; &lt;!-- 配置实体类存放路径 --&gt; &lt;javaModelGenerator targetPackage=&quot;com.ht.micro.record.commons.domain&quot; targetProject=&quot;src/main/java&quot;/&gt; &lt;!-- 配置 XML 存放路径 --&gt; &lt;sqlMapGenerator targetPackage=&quot;mapper&quot; targetProject=&quot;src/main/resources/baseMapper&quot;/&gt; &lt;!-- 配置 DAO 存放路径 --&gt; &lt;javaClientGenerator targetPackage=&quot;com.ht.micro.record.commons.mapper.baseMapper&quot; targetProject=&quot;src/main/java&quot; type=&quot;XMLMAPPER&quot;/&gt; &lt;!-- 配置需要指定生成的数据库和表，% 代表所有表 生成@Table中删除ht-micro-record.. --&gt; &lt;table catalog=&quot;ht_micro_record&quot; tableName=&quot;%&quot;&gt; &lt;!-- mysql 配置 --&gt; &lt;generatedKey column=&quot;id&quot; sqlStatement=&quot;Mysql&quot; identity=&quot;true&quot;/&gt; &lt;/table&gt; &lt;/context&gt; &lt;/generatorConfiguration&gt;jdbc.properties jdbc.driverClass=com.mysql.jdbc.Driver jdbc.connectionURL=jdbc:mysql://192.168.2.5:185/ht_micro_record?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false jdbc.username=root jdbc.password=123456mvn mybatis-generator:generate 自动生成表实体和mapper接口 创建外部链路追踪ht-micro-record-external-skywalking &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-external-skywalking&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;name&gt;ht-micro-record-external-skywalking&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;!-- 配置执行器 --&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;!-- 绑定到 package 生命周期阶段上 --&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;!-- 只运行一次 --&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;finalName&gt;skywalking&lt;/finalName&gt; &lt;descriptors&gt; &lt;!-- 配置描述文件路径 --&gt; &lt;descriptor&gt;src/main/assembly/assembly.xml&lt;/descriptor&gt; &lt;/descriptors&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;ht-micro-record-external-skywalking/src/main/assembly/assembly.xml &lt;assembly&gt; &lt;id&gt;6.0.0-Beta&lt;/id&gt; &lt;formats&gt; &lt;!-- 打包的文件格式，支持 zip、tar.gz、tar.bz2、jar、dir、war --&gt; &lt;format&gt;tar.gz&lt;/format&gt; &lt;/formats&gt; &lt;!-- tar.gz 压缩包下是否生成和项目名相同的根目录，有需要请设置成 true --&gt; &lt;includeBaseDirectory&gt;false&lt;/includeBaseDirectory&gt; &lt;dependencySets&gt; &lt;dependencySet&gt; &lt;!-- 是否把本项目添加到依赖文件夹下，有需要请设置成 true --&gt; &lt;useProjectArtifact&gt;false&lt;/useProjectArtifact&gt; &lt;outputDirectory&gt;lib&lt;/outputDirectory&gt; &lt;!-- 将 scope 为 runtime 的依赖包打包 --&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependencySet&gt; &lt;/dependencySets&gt; &lt;fileSets&gt; &lt;fileSet&gt; &lt;!-- 设置需要打包的文件路径 --&gt; &lt;directory&gt;agent&lt;/directory&gt; &lt;!-- 打包后的输出路径 --&gt; &lt;outputDirectory&gt;&lt;/outputDirectory&gt; &lt;/fileSet&gt; &lt;/fileSets&gt; &lt;/assembly&gt;apache-skywalking-apm-incubating-6.0.0-beta.tar.gz解压获取apache-skywalking-apm-bin/agent到ht-micro-record-external-skywalking下mvn clean package 会在 target 目录下创建名为 skywalking-6.0.0-Beta.tar.gz 的压缩包mvn clean install 会在本地仓库目录下创建名为 hello-spring-cloud-external-skywalking-1.0.0-SNAPSHOT-6.0.0-Beta.tar.gz 的压缩包 mkdir /data2/skywalking vim docker-compose.yml version: &#39;3.3&#39; services: elasticsearch: image: wutang/elasticsearch-shanghai-zone:6.3.2 container_name: elasticsearch restart: always ports: - 191:9200 - 192:9300 environment: cluster.name: elasticsearch docker-compose up -d https://mirrors.huaweicloud.com/apache/incubator/skywalking/6.0.0-beta/apache-skywalking-apm-incubating-6.0.0-beta.tar.gz tar zxf apache-skywalking-apm-incubating-6.0.0-beta.tar.gz cd apache-skywalking-apm-incubating vim apache-skywalking-apm-bin/config/application.yml 注释h2,打开es并修改clusterNodes地址 # h2: # driver: ${SW_STORAGE_H2_DRIVER:org.h2.jdbcx.JdbcDataSource} # url: ${SW_STORAGE_H2_URL:jdbc:h2:mem:skywalking-oap-db} # user: ${SW_STORAGE_H2_USER:sa} elasticsearch: nameSpace: ${SW_NAMESPACE:&quot;&quot;} clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:192.168.2.7:191} indexShardsNumber: ${SW_STORAGE_ES_INDEX_SHARDS_NUMBER:2} indexReplicasNumber: ${SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:0} # Batch process setting, refer to https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.5/java-docs-bulk-processor.html bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:2000} # Execute the bulk every 2000 requests bulkSize: ${SW_STORAGE_ES_BULK_SIZE:20} # flush the bulk every 20mb flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10} # flush the bulk every 10 seconds whatever the number of requests concurrentRequests: ${SW_STORAGE_ES_CONCURRENT_REQUESTS:2} # the number of concurrent requests apache-skywalking-apm-incubating/webapp/webapp.yml port 193 ./apache-skywalking-apm-incubating/bin/startup.sh]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>spring cloud alibaba</tag>
        <tag>tk mybatis</tag>
        <tag>skywalking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Alibaba限流熔断降级]]></title>
    <url>%2F2019%2F08%2F21%2FAlibaba%E9%99%90%E6%B5%81%E7%86%94%E6%96%AD%E9%99%8D%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[基于spring cloud alibaba实战整合开发 限流sentinel存储 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; application.yml spring: application: name: ht-micro-record-service-user cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8719 dashboard: 192.168.2.7:19 访问ht-micro-record-service-user服务，http://localhost:9506/apply/page/1/2快速的调用两次http://localhost:9506/apply/page/1/2 接口之后，第三次调用被限流了 nacos存储 &lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-datasource-nacos&lt;/artifactId&gt; &lt;/dependency&gt; application.yml spring: cloud: sentinel: datasource: ds: nacos: dataId: ${spring.application.name}-sentinel groupId: DEFAULT_GROUP rule-type: flow server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850nacos中配置ht-micro-record-service-user-sentinel [ { &quot;resource&quot;: &quot;/apply/page/1/2&quot;, &quot;limitApp&quot;: &quot;default&quot;, &quot;grade&quot;: 1, &quot;count&quot;: 5, &quot;strategy&quot;: 0, &quot;controlBehavior&quot;: 0, &quot;clusterMode&quot;: false } ] 进入http://192.168.2.7:190/#/dashboard/identity/ht-micro-record-service-user 访问 Sentinel控制台中修改规则：仅存在于服务的内存中，不会修改Nacos中的配置值，重启后恢复原来的值。 Nacos控制台中修改规则：服务的内存中规则会更新，Nacos中持久化规则也会更新，重启后依然保持。限流捕获处理异常ht-micro-record-service-dubbo-provider &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-datasource-nacos&lt;/artifactId&gt; &lt;/dependency&gt; com.ht.micro.record.service.dubbo.provider.DubboProviderApplication // 注解支持的配置Bean @Bean public SentinelResourceAspect sentinelResourceAspect() { return new SentinelResourceAspect(); } bootstrap.yml spring: application: name: ht-micro-record-service-dubbo-provider cloud: nacos: config: file-extension: yaml server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8720 dashboard: 192.168.2.7:190 datasource: ds: nacos: dataId: ${spring.application.name}-sentinel groupId: DEFAULT_GROUP rule-type: flow data-type: json server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850nacos配置ht-micro-record-service-dubbo-provider-sentinel [ { &quot;resource&quot;: &quot;protected-resource&quot;, &quot;controlBehavior&quot;: 2, &quot;count&quot;: 1, &quot;grade&quot;: 1, &quot;limitApp&quot;: &quot;default&quot;, &quot;strategy&quot;: 0 } ] nacos配置ht-micro-record-service-dubbo-provider.yaml spring: application: name: ht-micro-record-service-dubbo-provider cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 server: port: 9502 com.ht.micro.record.service.dubbo.provider.controller.ProviderController @GetMapping(&quot;/port&quot;) @SentinelResource(value = &quot;protected-resource&quot;, blockHandler = &quot;handleBlock&quot;) public Object port() { return &quot;port= &quot;+ port + &quot;, name=&quot; + applicationContext.getEnvironment().getProperty(&quot;user.name&quot;); } public String handleBlock(BlockException ex) { return &quot;限流了&quot;; }启动服务后http://192.168.2.7:190/#/dashboard/flow/ht-micro-record-service-dubbo-provider快速访问http://localhost:9502/provider/port 每秒超过1个请求将会显示限流了 熔断 @GetMapping(&quot;name&quot;) @SentinelResource(value = &quot;getName&quot;, fallback = &quot;getNameFallback&quot;) public String userName(String name){ for (int i = 0; i &lt; 100000000L; i++) { } return &quot;getName &quot; + name; } // 该方法降级处理函数，参数要与原函数getName相同，并且返回值类型也要与原函数相同，此外，该方法必须与原函数在同一个类中 public String getNameFallback(String name){ return &quot;getNameFallback&quot;; } 访问http://localhost:9502/provider/name查看http://192.168.2.7:190/#/dashboard/identity/ht-micro-record-service-dubbo-provider根据响应时间，大于10毫秒，则熔断降级（要连续超过5个请求超过10毫秒才会熔断）快速访问http://localhost:9502/provider/name @GetMapping(&quot;name&quot;) @SentinelResource(value = &quot;getName&quot;, fallback = &quot;getNameFallback&quot;) public String userName(String name){ for (int i = 0; i &lt; 100000000L; i++) { throw new RuntimeException(); } return &quot;getName &quot; + name; } // 该方法降级处理函数，参数要与原函数getName相同，并且返回值类型也要与原函数相同，此外，该方法必须与原函数在同一个类中 public String getNameFallback(String name){ return &quot;getNameFallback&quot;; } 显示抛出是10个异常，然后返回结果是熔断处理方法返回结果 流控规则 降级规则timeWindow为熔断恢复时间熔断模式，当熔断触发后，需要等待timewindow时间，再关闭熔断器。 0 根据rt时间，当超过指定规则的时间连续超过5笔，则触发熔断。 1 根据异常比例熔断 DEGRADE_GRADE_EXCEPTION_RATIO 2 根据单位时间内异常总数做熔断热点规则系统规则授权规则 Sentinel整合Nacos动态发布git clone https://github.com/alibaba/Sentinel.git cd Sentinel/sentinel-dashboard vim pom.xml &lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-datasource-nacos&lt;/artifactId&gt; &lt;!--&lt;scope&gt;test&lt;/scope&gt;--&gt; &lt;/dependency&gt; 修改resources/app/scripts/directives/sidebar/sidebar.html &lt;li ui-sref-active=&quot;active&quot;&gt; &lt;a ui-sref=&quot;dashboard.flowV1({app: entry.app})&quot;&gt; &lt;i class=&quot;glyphicon glyphicon-filter&quot;&gt;&lt;/i&gt;&amp;nbsp;&amp;nbsp;流控规则 &lt;/a&gt; &lt;/li&gt; 为 &lt;li ui-sref-active=&quot;active&quot;&gt; &lt;a ui-sref=&quot;dashboard.flow({app: entry.app})&quot;&gt; &lt;i class=&quot;glyphicon glyphicon-filter&quot;&gt;&lt;/i&gt;&amp;nbsp;&amp;nbsp;流控规则 &lt;/a&gt; &lt;/li&gt; com.alibaba.csp.sentinel.dashboard.rule.nacos.NacosConfigConstant public final class NacosConfigConstant { public static final String GROUP_ID = &quot;DEFAULT_GROUP&quot;; public static final String FLOW_DATA_ID_POSTFIX = &quot;-flow-rules&quot;; public static final String PARAM_FLOW_DATA_ID_POSTFIX = &quot;-param-flow-rules&quot;; public static final String DEGRADE_DATA_ID_POSTFIX = &quot;-degrade-rules&quot;; public static final String SYSTEM_DATA_ID_POSTFIX = &quot;-system-rules&quot;; public static final String AUTHORITY_DATA_ID_POSTFIX = &quot;-authority-rules&quot;; } com.alibaba.csp.sentinel.dashboard.rule.nacos.NacosConfigProperties @Component public class NacosConfigProperties { @Value(&quot;${houyi.nacos.server.ip}&quot;) private String ip; @Value(&quot;${houyi.nacos.server.port}&quot;) private String port; @Value(&quot;${houyi.nacos.server.namespace}&quot;) private String namespace; @Value(&quot;${houyi.nacos.server.group-id}&quot;) private String groupId; public String getIp() { return ip; } public void setIp(String ip) { this.ip = ip; } public String getPort() { return port; } public void setPort(String port) { this.port = port; } public String getNamespace() { return namespace; } public void setNamespace(String namespace) { this.namespace = namespace; } public String getGroupId() { return groupId; } public void setGroupId(String groupId) { this.groupId = groupId; } public String getServerAddr() { return this.getIp()+&quot;:&quot;+this.getPort(); } @Override public String toString() { return &quot;NacosConfigProperties [ip=&quot; + ip + &quot;, port=&quot; + port + &quot;, namespace=&quot; + namespace + &quot;, groupId=&quot; + groupId + &quot;]&quot;; } } com.alibaba.csp.sentinel.dashboard.rule.nacos.NacosConfig @Configuration public class NacosConfig { @Autowired private NacosConfigProperties nacosConfigProperties; @Bean public ConfigService nacosConfigService() throws Exception { Properties properties = new Properties(); properties.put(PropertyKeyConst.SERVER_ADDR, nacosConfigProperties.getServerAddr()); if(nacosConfigProperties.getNamespace() != null &amp;&amp; !&quot;&quot;.equals(nacosConfigProperties.getNamespace())) properties.put(PropertyKeyConst.NAMESPACE, nacosConfigProperties.getNamespace()); return ConfigFactory.createConfigService(properties); } } application.properties houyi.nacos.server.ip=192.168.2.7 houyi.nacos.server.port=8848 houyi.nacos.server.namespace= houyi.nacos.server.group-id=DEFAULT_GROUP FlowRulecom.alibaba.csp.sentinel.dashboard.rule.nacos.FlowRuleNacosPublisher @Component(&quot;flowRuleNacosPublisher&quot;) public class FlowRuleNacosPublisher implements DynamicRulePublisher&lt;List&lt;FlowRuleEntity&gt;&gt; { @Autowired private ConfigService configService; @Autowired private NacosConfigProperties nacosConfigProperties; @Override public void publish(String app, List&lt;FlowRuleEntity&gt; rules) throws Exception { AssertUtil.notEmpty(app, &quot;app name cannot be empty&quot;); if (rules == null) { return; } configService.publishConfig(app + NacosConfigConstant.FLOW_DATA_ID_POSTFIX, nacosConfigProperties.getGroupId(), JSON.toJSONString(rules.stream().map(FlowRuleEntity::toRule).collect(Collectors.toList()))); } } com.alibaba.csp.sentinel.dashboard.rule.nacos.FlowRuleNacosProvider @Component(&quot;flowRuleNacosProvider&quot;) public class FlowRuleNacosProvider implements DynamicRuleProvider&lt;List&lt;FlowRuleEntity&gt;&gt; { private static Logger logger = LoggerFactory.getLogger(FlowRuleNacosProvider.class); @Autowired private ConfigService configService; @Autowired private NacosConfigProperties nacosConfigProperties; @Override public List&lt;FlowRuleEntity&gt; getRules(String appName) throws Exception { String rulesStr = configService.getConfig(appName + NacosConfigConstant.FLOW_DATA_ID_POSTFIX, nacosConfigProperties.getGroupId(), 3000); logger.info(&quot;nacosConfigProperties{}:&quot;, nacosConfigProperties); logger.info(&quot;从Nacos中获取到限流规则信息{}&quot;, rulesStr); if (StringUtil.isEmpty(rulesStr)) { return new ArrayList&lt;&gt;(); } List&lt;FlowRule&gt; rules = RuleUtils.parseFlowRule(rulesStr); if (rules != null) { return rules.stream().map(rule -&gt; FlowRuleEntity.fromFlowRule(appName, nacosConfigProperties.getIp(), Integer.valueOf(nacosConfigProperties.getPort()), rule)) .collect(Collectors.toList()); } else { return new ArrayList&lt;&gt;(); } } } com.alibaba.csp.sentinel.dashboard.controller.FlowControllerV1将原始HTTP调用改为对应的Provider及Publisher去除 @Autowired private SentinelApiClient sentinelApiClient; 新增Qualifier和Component值保持一致 @Autowired @Qualifier(&quot;flowRuleNacosProvider&quot;) private DynamicRuleProvider&lt;List&lt;FlowRuleEntity&gt;&gt; provider; @Autowired @Qualifier(&quot;flowRuleNacosPublisher&quot;) private DynamicRulePublisher&lt;List&lt;FlowRuleEntity&gt;&gt; publisher; @GetMapping(&quot;/rules&quot;) public Result&lt;List&lt;FlowRuleEntity&gt;&gt; apiQueryMachineRules(HttpServletRequest request, @RequestParam String app, @RequestParam String ip, @RequestParam Integer port) { AuthUser authUser = authService.getAuthUser(request); authUser.authTarget(app, PrivilegeType.READ_RULE); if (StringUtil.isEmpty(app)) { return Result.ofFail(-1, &quot;app can&#39;t be null or empty&quot;); } if (StringUtil.isEmpty(ip)) { return Result.ofFail(-1, &quot;ip can&#39;t be null or empty&quot;); } if (port == null) { return Result.ofFail(-1, &quot;port can&#39;t be null&quot;); } try { List&lt;FlowRuleEntity&gt; rules = provider.getRules(app); rules = repository.saveAll(rules); return Result.ofSuccess(rules); } catch (Throwable throwable) { logger.error(&quot;Error when querying flow rules&quot;, throwable); return Result.ofThrowable(-1, throwable); } } private boolean publishRules(String app, String ip, Integer port) { List&lt;FlowRuleEntity&gt; rules = repository.findAllByMachine(MachineInfo.of(app, ip, port)); try { publisher.publish(app, rules); logger.info(&quot;添加限流规则成功{}&quot;, JSON.toJSONString(rules.stream().map(FlowRuleEntity::toRule).collect(Collectors.toList()))); return true; } catch (Exception e) { logger.info(&quot;添加限流规则失败{}&quot;,JSON.toJSONString(rules.stream().map(FlowRuleEntity::toRule).collect(Collectors.toList()))); e.printStackTrace(); return false; } } DegradeRulecom.alibaba.csp.sentinel.dashboard.rule.nacos.DegradeRuleNacosPublisher @Component(&quot;degradeRuleNacosPublisher&quot;) public class DegradeRuleNacosPublisher implements DynamicRulePublisher&lt;List&lt;DegradeRuleEntity&gt;&gt; { @Autowired private ConfigService configService; @Autowired private NacosConfigProperties nacosConfigProperties; @Override public void publish(String app, List&lt;DegradeRuleEntity&gt; rules) throws Exception { AssertUtil.notEmpty(app, &quot;app name cannot be empty&quot;); if (rules == null) { return; } configService.publishConfig(app + NacosConfigConstant.DEGRADE_DATA_ID_POSTFIX, nacosConfigProperties.getGroupId(), JSON.toJSONString(rules.stream().map(DegradeRuleEntity::toRule).collect(Collectors.toList()))); } } com.alibaba.csp.sentinel.dashboard.rule.nacos.DegradeRuleNacosProvider @Component(&quot;degradeRuleNacosProvider&quot;) public class DegradeRuleNacosProvider implements DynamicRuleProvider&lt;List&lt;DegradeRuleEntity&gt;&gt; { private static Logger logger = LoggerFactory.getLogger(DegradeRuleNacosProvider.class); @Autowired private ConfigService configService; @Autowired private NacosConfigProperties nacosConfigProperties; @Override public List&lt;DegradeRuleEntity&gt; getRules(String appName) throws Exception { String rulesStr = configService.getConfig(appName + NacosConfigConstant.DEGRADE_DATA_ID_POSTFIX, nacosConfigProperties.getGroupId(), 3000); logger.info(&quot;nacosConfigProperties{}:&quot;, nacosConfigProperties); logger.info(&quot;从Nacos中获取到熔断降级规则信息{}&quot;, rulesStr); if (StringUtil.isEmpty(rulesStr)) { return new ArrayList&lt;&gt;(); } List&lt;DegradeRule&gt; rules = RuleUtils.parseDegradeRule(rulesStr); if (rules != null) { return rules.stream().map(rule -&gt; DegradeRuleEntity.fromDegradeRule(appName, nacosConfigProperties.getIp(), Integer.valueOf(nacosConfigProperties.getPort()), rule)) .collect(Collectors.toList()); } else { return new ArrayList&lt;&gt;(); } } } com.alibaba.csp.sentinel.dashboard.controller.DegradeController去除 @Autowired private SentinelApiClient sentinelApiClient;新增 @Autowired @Qualifier(&quot;degradeRuleNacosProvider&quot;) private DynamicRuleProvider&lt;List&lt;DegradeRuleEntity&gt;&gt; provider; @Autowired @Qualifier(&quot;degradeRuleNacosPublisher&quot;) private DynamicRulePublisher&lt;List&lt;DegradeRuleEntity&gt;&gt; publisher; @ResponseBody @RequestMapping(&quot;/rules.json&quot;) public Result&lt;List&lt;DegradeRuleEntity&gt;&gt; queryMachineRules(HttpServletRequest request, String app, String ip, Integer port) { AuthUser authUser = authService.getAuthUser(request); authUser.authTarget(app, PrivilegeType.READ_RULE); if (StringUtil.isEmpty(app)) { return Result.ofFail(-1, &quot;app can&#39;t be null or empty&quot;); } if (StringUtil.isEmpty(ip)) { return Result.ofFail(-1, &quot;ip can&#39;t be null or empty&quot;); } if (port == null) { return Result.ofFail(-1, &quot;port can&#39;t be null&quot;); } try { List&lt;DegradeRuleEntity&gt; rules = provider.getRules(app); rules = repository.saveAll(rules); return Result.ofSuccess(rules); } catch (Throwable throwable) { logger.error(&quot;queryApps error:&quot;, throwable); return Result.ofThrowable(-1, throwable); } } private boolean publishRules(String app, String ip, Integer port) { List&lt;DegradeRuleEntity&gt; rules = repository.findAllByMachine(MachineInfo.of(app, ip, port)); try { publisher.publish(app, rules); logger.info(&quot;添加熔断降级规则成功{}&quot;, JSON.toJSONString(rules.stream().map(DegradeRuleEntity::toRule).collect(Collectors.toList()))); return true; } catch (Exception e) { logger.info(&quot;添加熔断降级规则失败{}&quot;,JSON.toJSONString(rules.stream().map(DegradeRuleEntity::toRule).collect(Collectors.toList()))); e.printStackTrace(); return false; } }重新打包启动 mvn clean package -DskipTests cd sentinel-dashboard/target/ nohup java -Dserver.port=190 -Dcsp.sentinel.dashboard.server=localhost:190 -Dproject.name=sentinel-dashboard -jar sentinel-dashboard.jar &amp;项目中bootstrap.yml spring: application: name: ht-micro-record-service-dubbo-provider cloud: nacos: config: file-extension: yaml server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8720 dashboard: localhost:8080 datasource: ds: nacos: dataId: ${spring.application.name}-flow-rules groupId: DEFAULT_GROUP rule-type: flow # 流控 # rule-type: degrade # 熔断 data-type: json server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 ds1: nacos: dataId: ${spring.application.name}-degrade-rules groupId: DEFAULT_GROUP rule-type: degrade # 熔断 data-type: json server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 nacos配置ht-micro-record-service-dubbo-provider.yaml spring: application: name: ht-micro-record-service-dubbo-provider cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 server: port: 9502http://localhost:8080/#/dashboard 新增规则在http://192.168.2.7:8848/nacos 中自动同步暂时实现flow，degrade 基于Feign的熔断controller @GetMapping(&quot;name&quot;) @SentinelResource(value = &quot;getName&quot;, fallback = &quot;getNameFallback&quot;) public String userName(String name){ microServiceUserInf.getUserInfoById(1); return &quot;getName &quot; + name; } service@FeignClient(value = &quot;ht-micro-record-service-user&quot;,fallback = MicroServiceUserInfFallBack.class) public interface MicroServiceUserInf { /** * 通过用户id 获取用户信息 * @param id * @return */ @GetMapping(value = &quot;/user/getUserInfoById/{id}&quot;) TUser getUserInfoById(@PathVariable(value=&quot;id&quot; ) Integer id); } serviceFallBack@Component public class MicroServiceUserInfFallBack implements MicroServiceUserInf { @Autowired private TUserMapper userMapper; @Override public TUser getUserInfoById(Integer id) { log.info(&quot;开启熔断&quot;); return userMapper.selectByPrimaryKey(id); } } nacosspring: application: name: ht-micro-record-service-caserecord cloud: nacos: discovery: server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 sentinel: transport: port: 8723 dashboard: 192.168.2.7:190 datasource: ds: nacos: dataId: ${spring.application.name}-degrade-rules groupId: DEFAULT_GROUP rule-type: degrade data-type: json server-addr: 192.168.2.7:8848,192.168.2.7:8849,192.168.2.7:8850 feign: sentinel: enabled: true pom&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-datasource-nacos&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt; &lt;artifactId&gt;jsr305&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt; &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt;]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>spring cloud alibaba</tag>
        <tag>sentinel</tag>
        <tag>nacos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux]]></title>
    <url>%2F2019%2F08%2F20%2Flinux%2F</url>
    <content type="text"><![CDATA[linux使用技巧及常规软件安装 网卡防火墙vi /etc/sysconfig/network-scripts/ifcfg-eth0 ONBOOT=yes IPADDR=192.168.56.101 BOOTPROTO=static IPADDR与当前网卡在同一个网段 service network restart service iptables stop firewall-cmd --permanent --add-port=8080-8085/tcp firewall-cmd --permanent --remove-port=8080-8085/tcp firewall-cmd --permanent --list-ports firewall-cmd --permanent --list-services 查看使用互联网的程序 firewall-cmd --reload chkconfig --del iptables setenforce 0 systemctl stop firewalld.service systemctl disable firewalld.service vim /etc/sysconfig/selinux SELINUX=disabled 基本命令du -h --max-depth=1 查看各文件夹大小 nohup sh inotify3.sh &gt;&gt;333.out &amp; 后台执行脚本并把输出都指定文件 jobs -l 查看运行的后台进程 fg 1 通过jobid将后台进程提取到前台运行 ctrl + z 将暂停当前正在运行到进程，fg放入后台运行 yum -c /etc/yum.conf --installroot=/usr/local --releasever=/ install lszrz 安装文件到其他目录ekillvim /usr/local/bin/ekill ps aux | grep -e $* | grep -v grep | awk &#39;{print $2}&#39; | xargs -i kill {}chmod a+x /usr/local/bin/ekill 通过ekill删除进程 免密登陆192.168.2.7： ssh-keygen -t rsa ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.2.5 ssh root@192.168.2.5 jdk+maventar zxf jdk-8u60-linux-x64.tar.gz -C /data2/ &amp;&amp; mv jdk1.8.0_60 jdk tar zxf apache-maven-3.6.1-bin.tar.gz -C /data2 &amp;&amp; mv apache-maven-3.6.1 maven vim /etc/profile JAVA_HOME=/data2/jdk JRE_HOME=$JAVA_HOME/jre MAVEN_HOME=/data2/maven PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin:$MAVEN_HOME/bin CLASSPATH=:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib/dt.jar export JAVA_HOME JRE_HOME PATH CLASSPATH MAVEN_HOME source /etc/profile vim setting.xml &lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt; &lt;!-- localRepository | The path to the local repository maven will use to store artifacts. | | Default: ${user.home}/.m2/repository &lt;localRepository&gt;/path/to/local/repo&lt;/localRepository&gt; --&gt; &lt;localRepository&gt;E:\apache-maven-3.6.1\respository&lt;/localRepository&gt; &lt;servers&gt; &lt;server&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;123456&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;123456&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;3rdParty&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;123456&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt; &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;ui&lt;/id&gt; &lt;name&gt;Mirror from UK&lt;/name&gt; &lt;url&gt;http://uk.maven.org/maven2/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;mirrorOf&gt;maven-releases&lt;/mirrorOf&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-releases/&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;mirrorOf&gt;maven-snapshots&lt;/mirrorOf&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-snapshots/&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-releases/&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-snapshots/&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;3rdParty&lt;/id&gt; &lt;url&gt;http://192.168.2.7:182/repository/3rdParty/&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/repository&gt; &lt;!-- 私有库地址--&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;url&gt;http://192.168.2.4:8081/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;ui&lt;/id&gt; &lt;name&gt;ui&lt;/name&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;url&gt;http://uk.maven.org/maven2/&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;!--插件库地址--&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;url&gt;http://192.168.2.4:8081/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-releases/&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-snapshots/&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;pluginRepository&gt; &lt;id&gt;aliyun&lt;/id&gt; &lt;name&gt;aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;activeProfiles&gt; &lt;activeProfile&gt;nexus&lt;/activeProfile&gt; &lt;/activeProfiles&gt; &lt;/settings&gt; Gitgitignore无效git rm -r --cached . git add . git commit -m &#39;.gitignore&#39; git push origin master git config –system core.longpaths true 提交长文件名或者idea取消run git hooks Mongowindows 管理员身份启动cmd https://fastdl.mongodb.org/win32/mongodb-win32-x86_64-2008plus-ssl-3.6.2-signed.msi mkdir data\db 进入bin执行mongod --dbpath D:\MongoDB\Server\3.6\data\db 进入bin执行mongo进入客户端，db.test.insert({&#39;a&#39;:&#39;b&#39;}) , db.test.find() 管理员cmd：touch data\logs\mongo.log ,进入bin: mongod --bind_ip 0.0.0.0 --logpath D:\MongoDB\Server\3.6\data\logs\mongo.log --logappend --dbpath D:\MongoDB\Server\3.6\data\db --port 27017 --serviceName &quot;MongoDB&quot; --serviceDisplayName &quot;MongoDB&quot; --install linux https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-ubuntu1404-3.6.2.tgz uname -a 查看系统描述 lsb_release -a 查看系统版本 tar -zxvf mongodb-linux-i686-2.0.1.tgz &amp;&amp; mv mongodb-linux-i686-2.0.1 mongodb cd mongodb/ &amp;&amp; mkdir log &amp;&amp; mkdir data touch log/mongodb ./mongod -port 10001 --dbpath ~/mongodb/data/ --logpath ~/mongodb/log/mongodb.log 新开窗口./bin/mongo 127.0.0.1:10001 mongo --port 27017 配置密码 show dbs 查看所有库 use yapi 使用yapi库 db 查看当前所在库 db.qa.insert({&#39;username&#39;:&#39;aaa&#39;,&#39;age&#39;:&#39;18&#39;}) db.qa.find() 查看所有集合 db.createUser( { user: &quot;yapi&quot;, pwd: &quot;df123456&quot;, roles: [ { role: &quot;dbOwner&quot;, db: &quot;yapi&quot; } ] } ) db.system.users.find() db.system.users.remove({}) systemctl restart mongod /etc/mongodb.conf 远程访问 net: bind_ip = 0.0.0.0 port = 27017 security: authorization: enabled javascriptEnabled: false db.auth(&#39;yapi&#39;,&#39;df123456&#39;)GitLab方案1docker run \ --publish 1443:443 --publish 180:80 --publish 122:22 \ --name gitlab \ --volume /usr/local/docker/gitlab/config:/etc/gitlab \ --volume /usr/local/docker/gitlab/logs:/var/log/gitlab \ --volume /usr/local/docker/gitlab/data:/var/opt/gitlab \ gitlab/gitlab-ce方案2vim /usr/local/docker/gitlab/docker-compose.yml version: &#39;3&#39; services: gitlab: image: &#39;twang2218/gitlab-ce-zh:10.5&#39; restart: always hostname: &#39;192.168.2.5&#39; container_name: gitlab environment: TZ: &#39;Asia/Shanghai&#39; GITLAB_OMNIBUS_CONFIG: | external_url &#39;http://192.168.2.5:180&#39; gitlab_rails[&#39;gitlab_shell_ssh_port&#39;] = 2222 unicorn[&#39;port&#39;] = 8888 nginx[&#39;listen_port&#39;] = 8080 ports: - &#39;180:8080&#39; - &#39;8443:443&#39; - &#39;2222:22&#39; volumes: - /usr/local/docker/gitlab/config:/etc/gitlab - /usr/local/docker/gitlab/data:/var/opt/gitlab - /usr/local/docker/gitlab/logs:/var/log/gitlab ERROR: error while removing network: network gitlab_default id e3f084651bcc6b6ca5d5b7fb122d0ef3aba108292989441abc82f14343fea827 has active endpoints docker network inspect gitlab_default docker network disconnect -f gitlab_default gitlab docker-compose down --remove-orphans docker-compose logs -ft gitlab 配置邮箱 vim /usr/local/docker/gitlab/config/gitlab.rb gitlab_rails[&#39;smtp_enable&#39;] = true gitlab_rails[&#39;smtp_address&#39;] = &quot;smtp.163.com&quot; gitlab_rails[&#39;smtp_port&#39;] = 25 gitlab_rails[&#39;smtp_user_name&#39;] = &quot;m15806204096@163.com&quot; gitlab_rails[&#39;smtp_password&#39;] = &quot;codewj123456&quot; gitlab_rails[&#39;smtp_domain&#39;] = &quot;163.com&quot; gitlab_rails[&#39;smtp_authentication&#39;] = &quot;login&quot; gitlab_rails[&#39;smtp_enable_starttls_auto&#39;] = true gitlab_rails[&#39;smtp_tls&#39;] = false gitlab_rails[&#39;gitlab_email_from&#39;] = &quot;m15806204096@163.com&quot; user[&quot;git_user_email&quot;] = &quot;m15806204096@163.com&quot;root/123456 管理区域-&gt;设置-&gt;开启注册 注册时发送确认邮件docker-compose restart新增ssh密钥 ssh-keygen -t rsa -C “15806204096@163.com“,将.ssh的公钥加入Gitlab的SSH密钥访问 http://192.168.2.5:180/ root 12345678 方案3安装gityum –y install git cd /usr/local mkdir git cd git git init --bare learngit.git useradd git passwd git chown -R git:git learngit.git vi /etc/passwd git:x:1000:1000::/home/git:/usr/bin/git-shell 复制客户端的ssh-keygen -t rsa -C &quot;你的邮箱&quot; 获得的id_rsa.pub公钥到/root/.ssh/authorized_keys和/root/.ssh/authorized_keys gitignore无效的解决方案 git rm -r --cached . git add . git commit -m &#39;.gitignore&#39; git push origin master *.cache *.cache.lock *.iml *.log **/target **/logs .idea **/.project **/.settingsgitlab安装git config --global http.sslVerify false yum -y install curl policycoreutils openssh-server openssh-clients postfix systemctl start sshd systemctl start postfix systemctl enable sshd systemctl enable postfix curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash yum -y install gitlab-ce 太慢的话使用清华的源，yum makecache再install vim /etc/yum.repos.d/gitlab_gitlab-ce.repo [gitlab-ce] name=gitlab-ce baseurl=http://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el6 repo_gpgcheck=0 gpgcheck=0 enabled=1 gpgkey=https://packages.gitlab.com/gpg.key mkdir -p /etc/gitlab/ssl openssl genrsa -out &quot;/etc/gitlab/ssl/gitlab.example.com.key&quot; 2048 openssl req -new -key &quot;/etc/gitlab/ssl/gitlab.example.com.key&quot; -out &quot;/etc/gitlab/ssl/gitlab.example.com.csr&quot; Country Name (2 letter code) [XX]:cn State or Province Name (full name) []:bj Locality Name (eg, city) (Default City]:bj Organization Name (eg, company) [Default Company Ltd]: Organizational Unit Name (eg, section) []: Common Name (eg, your name or your server&#39;s hostname) []:gitlab.example.com Email Address (]:admin@example.com Please enter the following &#39;extra&#39; attributes to be sent with your certificate request A challenge password [):123456 An optional company name []:GitLab基本配置openssl x509 -req -days 3650 -in &quot;/etc/gitlab/ssl/gitlab.example.com.csr&quot; -signkey &quot;/etc/gitlab/ssl/gitlab.example.com.key&quot; -out &quot;/etc/gitlab/ssl/gitlab.example.com.crt&quot; openssl dhparam -out /etc/gitlab/ssl/dhparams.pem 2048 chmod 600 /etc/gitlab/ssl/* vim /etc/gitlab/gitlab.rb 将external_url &#39;http://gitlab.example.com&#39;的http修改为https 将# nginx[&#39;redirect_http_to_https&#39;] = false的注释去掉，修改为nginx[&#39;redirect_http_to_https&#39;] = true 将# nginx[&#39;ssl_certificate&#39;] = &quot;/etc/gitlab/ssl/#{node[&#39;fqdn&#39;]}.crt&quot;修改为# nginx[&#39;ssl_certificate&#39;] = &quot;/etc/gitlab/ssl/gitlab.example.com.crt&quot; 将# nginx[&#39;ssl_certificate_key&#39;] = &quot;/etc/gitlab/ssl/#{node[&#39;fqdn&#39;]}.key&quot;修改为# nginx[&#39;ssl_certificate_key&#39;] = &quot;/etc/gitlab/ssl/gitlab.example.com.key&quot; 将# nginx[&#39;ssl_dhparam&#39;] = nil 修改为# nginx[&#39;ssl_dhparam&#39;] = &quot;/etc/gitlab/ssl/dhparams.pem&quot; gitlab-ctl reconfigure vim /var/opt/gitlab/nginx/conf/gitlab-http.conf 在server_name下添加如下配置内容：rewrite ^(.*)$ https://$host$1 permanent; 重启gitlab，使配置生效,gitlab-ctl restart，如遇到访问错误直接等待启动完成 修改本地hosts文件 将gitlab服务器的地址添加 gitlab.example.com 初始化时修改管理员密码，root 12345678 git -c http.sslVerify=false clone https://gitlab.example.com/root/test-repo.git git add . #git config --global user.email &quot;admin@example.com&quot; #git config --global user.name &quot;admin&quot; git commit -m &#39;init&#39; git -c http.sslVerify=false pull origin master git -c http.sslVerify=false push origin master https://gitlab.example.com/admin/system_info 查看系统资源状态值 https://gitlab.example.com/admin/logs 其中application.log记录了git用户的记录，production.log实时查看所有的访问链接 https://gitlab.example.com/admin/users/new 创建用户https://gitlab.example.com/root/test-repo/project_members 修改指定项目的成员，在项目的manage access中，修改登录密码 rm -rf test-repo/ git -c http.sslVerify=false clone https://gitlab.example.com/root/test-repo.git dev 12345678 cd test-repo/ git checkout -b release-1.0 git add . git commit -m &#39;release-1.0&#39; git -c http.sslVerify=false push origin release-1.0 dev登录后create merge request lead登录将受到release-1.0的merge申请，点击merge后可以填写comment并提交 Skywalkingmkdir /data2/skywalking vim docker-compose.yml version: &#39;3.3&#39; services: elasticsearch: image: wutang/elasticsearch-shanghai-zone:6.3.2 container_name: elasticsearch restart: always ports: - 191:9200 - 192:9300 environment: cluster.name: elasticsearch docker-compose up -d https://mirrors.huaweicloud.com/apache/incubator/skywalking/6.0.0-beta/apache-skywalking-apm-incubating-6.0.0-beta.tar.gz tar zxf apache-skywalking-apm-incubating-6.0.0-beta.tar.gz cd apache-skywalking-apm-incubating vim apache-skywalking-apm-bin/config/application.yml 注释h2,打开es并修改clusterNodes地址 # h2: # driver: ${SW_STORAGE_H2_DRIVER:org.h2.jdbcx.JdbcDataSource} # url: ${SW_STORAGE_H2_URL:jdbc:h2:mem:skywalking-oap-db} # user: ${SW_STORAGE_H2_USER:sa} elasticsearch: nameSpace: ${SW_NAMESPACE:&quot;&quot;} clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:192.168.2.7:191} indexShardsNumber: ${SW_STORAGE_ES_INDEX_SHARDS_NUMBER:2} indexReplicasNumber: ${SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:0} # Batch process setting, refer to https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.5/java-docs-bulk-processor.html bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:2000} # Execute the bulk every 2000 requests bulkSize: ${SW_STORAGE_ES_BULK_SIZE:20} # flush the bulk every 20mb flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10} # flush the bulk every 10 seconds whatever the number of requests concurrentRequests: ${SW_STORAGE_ES_CONCURRENT_REQUESTS:2} # the number of concurrent requests apache-skywalking-apm-incubating/webapp/webapp.yml port 193 ./apache-skywalking-apm-incubating/bin/startup.shSentinelcd /data2/ &amp;&amp; git clone https://github.com/alibaba/Sentinel.git cd Sentinel/ mvn clean package -DskipTests cd sentinel-dashboard/target/ nohup java -Dserver.port=190 -Dcsp.sentinel.dashboard.server=localhost:190 -Dproject.name=sentinel-dashboard -jar sentinel-dashboard.jar &amp;安装黑体字体yum -y install fontconfig fc-list :lang=zh cd /usr/share/fonts &amp;&amp; mkdir chinese chmod -R 755 /usr/share/fonts/chinese cd chinese/ &amp;&amp; rz simhei.ttf fs_GB2312.ttf fzxbsjt.ttf yum -y install ttmkfdir ttmkfdir -e /usr/share/X11/fonts/encodings/encodings.dir vim /etc/fonts/fonts.conf &lt;dir&gt;/usr/share/fonts&lt;/dir&gt; &lt;dir&gt;/usr/share/fonts/chinese&lt;/dir&gt; &lt;dir&gt;/usr/share/X11/fonts/Type1&lt;/dir&gt; &lt;dir&gt;/usr/share/X11/fonts/TTF&lt;/dir&gt; &lt;dir&gt;/usr/local/share/fonts&lt;/dir&gt; &lt;dir prefix=&quot;xdg&quot;&gt;fonts&lt;/dir&gt; &lt;!-- the following element will be removed in the future --&gt; &lt;dir&gt;~/.fonts&lt;/dir&gt; fc-cache fc-list :lang=zh RocketMQmkdir /data2/RocketMQ vim docker-compose.yml version: &#39;3.5&#39; services: rmqnamesrv: image: foxiswho/rocketmq:server container_name: rmqnamesrv ports: - 9876:9876 volumes: - ./data/logs:/opt/logs - ./data/store:/opt/store networks: rmq: aliases: - rmqnamesrv rmqbroker: image: foxiswho/rocketmq:broker container_name: rmqbroker ports: - 10909:10909 - 10911:10911 volumes: - ./data/logs:/opt/logs - ./data/store:/opt/store - ./data/brokerconf/broker.conf:/etc/rocketmq/broker.conf environment: NAMESRV_ADDR: &quot;rmqnamesrv:9876&quot; JAVA_OPTS: &quot; -Duser.home=/opt&quot; JAVA_OPT_EXT: &quot;-server -Xms128m -Xmx128m -Xmn128m&quot; command: mqbroker -c /etc/rocketmq/broker.conf depends_on: - rmqnamesrv networks: rmq: aliases: - rmqbroker rmqconsole: image: styletang/rocketmq-console-ng container_name: rmqconsole ports: - 8088:8080 environment: JAVA_OPTS: &quot;-Drocketmq.namesrv.addr=rmqnamesrv:9876 -Dcom.rocketmq.sendMessageWithVIPChannel=false&quot; depends_on: - rmqnamesrv networks: rmq: aliases: - rmqconsole networks: rmq: name: rmq driver: bridge vim /data2/RocketMQ/data/brokerconf/broker.conf brokerClusterName=DefaultCluster brokerName=broker-a brokerId=0 brokerIP1=192.168.2.7 defaultTopicQueueNums=4 autoCreateTopicEnable=true autoCreateSubscriptionGroup=true listenPort=10911 deleteWhen=04 fileReservedTime=120 mapedFileSizeCommitLog=1073741824 mapedFileSizeConsumeQueue=300000 diskMaxUsedSpaceRatio=88 maxMessageSize=65536 brokerRole=ASYNC_MASTER flushDiskType=ASYNC_FLUSH docker-compose up -d docker-compose down http://192.168.2.7:8088/RabbitMQdocker-compose.yml version: &#39;3&#39; services: rabbitmq: image: rabbitmq:management-alpine container_name: rabbitmq environment: - RABBITMQ_DEFAULT_USER=admin - RABBITMQ_DEFAULT_PASS=admin restart: always ports: - &quot;15672:15672&quot; - &quot;5672:5672&quot; logging: driver: &quot;json-file&quot; options: max-size: &quot;200k&quot; max-file: &quot;10&quot;docker-compose up -d,5672为rabbitmq的服务端口，15672为rabbitmq的web管理界面端口。 jenkinsrpm -qa|grep jenkins 搜索已安装 rpm -e --nodeps jenkins-2.83-1.1.noarch find / -name jenkins*|xargs rm -rf rpm -ivh jdk-8u171-linux-x64.rpm 安装java http://mirrors.jenkins.io/redhat/jenkins-2.180-1.1.noarch.rpm rpm -ivh jenkins-2.180-1.1.noarch.rpm 安装jenkins新版本 vi /etc/sysconfig/jenkins JENKINS_USER=&quot;root&quot; JENKINS_PORT=&quot;181&quot; vim /etc/rc.d/init.d/jenkins 修改candidates /data2/jdk/bin/java systemctl daemon-reload systemctl restart jenkins http://192.168.2.7:181 cat /var/lib/jenkins/secrets/initialAdminPassword 初始密码串并安装默认的插件 vim /var/lib/jenkins/hudson.model.UpdateCenter.xml 必须在填写完密码后修改 改https为http或者改为http://mirror.xmission.com/jenkins/updates/update-center.json systemctl restart jenkins 若页面空白/var/lib/jenkins/config.xml &lt;authorizationStrategy class=&quot;hudson.security.AuthorizationStrategy$Unsecured&quot;/&gt; &lt;securityRealm class=&quot;hudson.security.SecurityRealm$None&quot;/&gt; 系统管理-全局工具配置:/data2/maven /data2/jdk /data2/maven/conf/settings.xml 安装插件：Maven Integration，GitHub plugin，Git plugin 新建任务时，丢弃旧的构建，保持构建的天数3，保持构建的最大个数5 定时删除none的docker镜像手动执行：docker rmi $(docker images -f “dangling=true” -q)定时构建语法： 每天凌晨2:00跑一次 H 2 * * * 每隔5分钟构建一次 H/5 * * * * 每两小时构建一次 H H/2 * * * 每天中午12点定时构建一次 H 12 * * * 或0 12 * * *（0这种写法也被H替代了） 每天下午18点前定时构建一次 H 18 * * * 每15分钟构建一次 H/15 * * * * 或*/5 * * * *(这种方式已经被第一种替代了，jenkins也不推荐这种写法了) 周六到周日，18点-23点，三小时构建一次 H 18-23/3 * * 6-7 shell脚本 echo ---------------Stop-Rm-Containers...------------------ docker stop `docker ps -a| grep expire | awk &#39;{print $1}&#39;`|xargs docker rm echo ---------------Clear-Images...------------------ clearImagesList=$(docker images -f &quot;dangling=true&quot; -q) if [ ! -n &quot;$clearImagesList&quot; ]; then echo &quot;no images need clean up.&quot; else docker rmi $(docker images -f &quot;dangling=true&quot; -q) echo &quot;clear success.&quot; fi cron 每隔5秒执行一次：*/5 * * * * ? 每隔1分钟执行一次：0 */1 * * * ? 每天23点执行一次：0 0 23 * * ? 每天凌晨1点执行一次：0 0 1 * * ? 每月1号凌晨1点执行一次：0 0 1 1 * ? 每月最后一天23点执行一次：0 0 23 L * ? 每周星期天凌晨1点实行一次：0 0 1 ? * L 在26分、29分、33分执行一次：0 26,29,33 * * * ? 每天的0点、13点、18点、21点都执行一次：0 0 0,13,18,21 * * ? toptop -d 2 -c -p 123456 //每隔2秒显示pid是12345的进程的资源使用情况，并显式该进程启动的命令行参数 M —根据驻留内存大小进行排序 P —根据CPU使用百分比大小进行排序 T —根据时间/累计时间进行排序 c —切换显示命令名称和完整命令行 t —切换显示进程和CPU信息 m —切换显示内存信息 l —切换显示平均负载和启动时间信息 o —改变显示项目的顺序 f —从当前显示中添加或删除项目 S —切换到累计模式 s —改变两次刷新之间的延迟时间。系统将提示用户输入新的时间，单位为s。如果有小数，就换算成ms。 q —退出top程序 i —忽略闲置和僵尸进程。这是一个开关式的命令 k —终止一个进程更改显示内容通过 f 键可以选择显示的内容。按 f 键之后会显示列的列表，按 a-z 即可显示或隐藏对应的列，最后按回车键确定。 Exampletop - 01:06:48 up 1:22, 1 user, load average: 0.06, 0.60, 0.48 Tasks: 29 total, 1 running, 28 sleeping, 0 stopped, 0 zombie Cpu(s): 0.3% us, 1.0% sy, 0.0% ni, 98.7% id, 0.0% wa, 0.0% hi, 0.0% si Mem: 191272k total, 173656k used, 17616k free, 22052k buffers Swap: 192772k total, 0k used, 192772k free, 123988k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND root 16 0 7976 2456 1980 S 0.7 1.3 0:11.03 sshd root 16 0 2128 980 796 R 0.7 0.5 0:02.72 top root 16 0 1992 632 544 S 0.0 0.3 0:00.90 init root 34 19 0 0 0 S 0.0 0.0 0:00.00 ksoftirqd/0 root RT 0 0 0 0 S 0.0 0.0 0:00.00 watchdog/0 统计信息区前五行是系统整体的统计信息。第一行是任务队列信息，同 uptime 命令的执行结果。其内容如下： 01:06:48 当前时间 up 1:22 系统运行时间，格式为时:分 1 user 当前登录用户数 load average: 0.06, 0.60, 0.48 系统负载，即任务队列的平均长度。三个数值分别为 1分钟、5分钟、15分钟前到现在的平均值。 第二、三行为进程和CPU的信息。当有多个CPU时，这些内容可能会超过两行。内容如下： total 进程总数 running 正在运行的进程数 sleeping 睡眠的进程数 stopped 停止的进程数 zombie 僵尸进程数 Cpu(s): 0.3% us 用户空间占用CPU百分比 1.0% sy 内核空间占用CPU百分比 0.0% ni 用户进程空间内改变过优先级的进程占用CPU百分比 98.7% id 空闲CPU百分比 0.0% wa 等待输入输出的CPU时间百分比 0.0%hi：硬件CPU中断占用百分比 0.0%si：软中断占用百分比 0.0%st：虚拟机占用百分比最后两行为内存信息。内容如下： Mem: 191272k total 物理内存总量 173656k used 使用的物理内存总量 17616k free 空闲内存总量 22052k buffers 用作内核缓存的内存量 Swap: 192772k total 交换区总量 0k used 使用的交换区总量 192772k free 空闲交换区总量 123988k cached 缓冲的交换区总量,内存中的内容被换出到交换区，而后又被换入到内存，但使用过的交换区尚未被覆盖，该数值即为这些内容已存在于内存中的交换区的大小,相应的内存再次被换出时可不必再对交换区写入。进程信息区统计信息区域的下方显示了各个进程的详细信息。首先来认识一下各列的含义 序号 列名 含义 a PID 进程id b PPID 父进程id c RUSER Real user name d UID 进程所有者的用户id e USER 进程所有者的用户名 f GROUP 进程所有者的组名 g TTY 启动进程的终端名。不是从终端启动的进程则显示为 ? h PR 优先级 i NI nice值。负值表示高优先级，正值表示低优先级 j P 最后使用的CPU，仅在多CPU环境下有意义 k %CPU 上次更新到现在的CPU时间占用百分比 l TIME 进程使用的CPU时间总计，单位秒 m TIME+ 进程使用的CPU时间总计，单位1/100秒 n %MEM 进程使用的物理内存百分比 o VIRT 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES p SWAP 进程使用的虚拟内存中，被换出的大小，单位kb。 q RES 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA r CODE 可执行代码占用的物理内存大小，单位kb s DATA 可执行代码以外的部分(数据段+栈)占用的物理内存大小，单位kb t SHR 共享内存大小，单位kb u nFLT 页面错误次数 v nDRT 最后一次写入到现在，被修改过的页面数。 w S 进程状态(D=不可中断的睡眠状态,R=运行,S=睡眠,T=跟踪/停止,Z=僵尸进程) x COMMAND 命令名/命令行 y WCHAN 若该进程在睡眠，则显示睡眠中的系统函数名 z Flags 任务标志，参考 sched.h VIRT：virtual memory usage。Virtual这个词很神，一般解释是：virtual adj.虚的, 实质的,[物]有效的, 事实上的。到底是虚的还是实的？让Google给Define之后，将就明白一点，就是这东西还是非物质的，但是有效果的，不发生在真实世界的，发生在软件世界的等等。这个内存使用就是一个应用占有的地址空间，只是要应用程序要求的，就全算在这里，而不管它真的用了没有。写程序怕出错，又不在乎占用的时候，多开点内存也是很正常的。RES：resident memory usage。常驻内存。这个值就是该应用程序真的使用的内存，但还有两个小问题，一是有些东西可能放在交换盘上了（SWAP），二是有些内存可能是共享的。SHR：shared memory。共享内存。就是说这一块内存空间有可能也被其他应用程序使用着；而Virt － Shr似乎就是这个程序所要求的并且没有共享的内存空间。DATA：数据占用的内存。如果top没有显示，按f键可以显示出来。这一块是真正的该程序要求的数据空间，是真正在运行中要使用的。SHR是一个潜在的可能会被共享的数字，如果只开一个程序，也没有别人共同使用它；VIRT里面的可能性更多，比如它可能计算了被许多X的库所共享的内存；RES应该是比较准确的，但不含有交换出去的空间；但基本可以说RES是程序当前使用的内存量。 Q1:-bash: fork: Cannot allocate memory进程数满了,echo 1000000 &gt; /proc/sys/kernel/pid_max,echo “kernel.pid_max=1000000 “ &gt;&gt; /etc/sysctl.conf,sysctl -ptop:展示进程视图，监控服务器进程数值默认进入top时，各进程是按照CPU的占用量来排序的,-f查看实际内存占用量]]></content>
      <categories>
        <category>系统</category>
      </categories>
      <tags>
        <tag>gitlab</tag>
        <tag>jenkins</tag>
        <tag>sentinel</tag>
        <tag>linux</tag>
        <tag>rocketmq</tag>
        <tag>rabbitmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker]]></title>
    <url>%2F2019%2F08%2F20%2Fdocker%2F</url>
    <content type="text"><![CDATA[docker安装使用及镜像制作 安装docker基本配置yum install -y vim lrzsz git setenforce 0 vim /etc/selinux/config SELINUX=disabled systemctl stop firewalld systemctl disable firewalldcentosyum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-selinux \ docker-engine-selinux \ docker-engine -y 移除旧版本 rm -rf /etc/systemd/system/docker.service.d /var/lib/docker /var/run/docker yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum makecache fast yum list docker-ce --showduplicates | sort -r yum install -y docker-ce-18.09.5 systemctl restart docker yum update -y nss curl libcurl curl -L https://github.com/docker/compose/releases/download/1.20.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose docker-compose --versionubuntuapt-get update apt-get -y install apt-transport-https ca-certificates curl software-properties-common # step 2: 安装GPG证书 curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add - # Step 3: 写入软件源信息 sudo add-apt-repository &quot;deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable&quot; # Step 4: 更新并安装 Docker-CE sudo apt-get -y update sudo apt-get -y install docker-ce # 安装指定版本的Docker-CE: # Step 1: 查找Docker-CE的版本: # apt-cache madison docker-ce # docker-ce | 17.03.1~ce-0~ubuntu-xenial | http://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages # docker-ce | 17.03.0~ce-0~ubuntu-xenial | http://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages # Step 2: 安装指定版本的Docker-CE: (VERSION 例如上面的 17.03.1~ce-0~ubuntu-xenial) # sudo apt-get -y install docker-ce=[VERSION]Docker配置及基本使用vim ~/.bashrc alias dops=&#39;docker ps --format &quot;table {{.ID}}\t{{.Names}}\t{{.Image}}\t&quot;&#39; alias dopsa=&#39;docker ps -a --format &quot;table {{.ID}}\t{{.Names}}\t{{.Image}}\t&quot;&#39; alias dolo=&#39;docker logs -ft&#39; vim /etc/docker/daemon.json { &quot;registry-mirrors&quot;: [&quot;https://3gki6pei.mirror.aliyuncs.com&quot;], &quot;storage-driver&quot;:&quot;devicemapper&quot;, &quot;insecure-registries&quot;:[&quot;192.168.2.7:5000&quot;], &quot;log-driver&quot;: &quot;json-file&quot;, &quot;log-opt&quot;: { &quot;max-size&quot;: &quot;10m&quot;, &quot;max-file&quot;: &quot;10&quot; } } systemctl daemon-reload systemctl restart docker.service service docker stop 删除所有镜像 rm -rf /var/lib/docker systemctl start docker.service --restart=always 设置重启docker自动启动容器 docker update --restart=always es-node2 docker run -itd myimage:test /bin/bash -c &quot;命令1;命令2&quot; 启动容器自动执行命令 docker export registry &gt; /home/registry.tar 将容器打成tar包 scp /home/registry.tar root@192.168.2.7:/root cat ~/registry.tar | docker import - registry/2.5 将tar包打成镜像 docker save jdk1.8 &gt; /home/java.tar.gz 导出镜像 docker load &lt; /home/java.tar.gz 导入镜像 docker stop sentine1 | xargs docker rm docker rm -f redis-slave1|true docker logs -f -t --since=&quot;2018-02-08&quot; --tail=100 CONTAINER_ID 查看指定时间后的日志，只显示最后100行 docker logs --since 30m CONTAINER_ID 查看最近30分钟的日志 /var/lib/docker/containers/contain id 下rm -rf *.log 删除docker日志 进入https://homenew.console.aliyun.com/ 搜索容器镜像服务，进入侧边栏的镜像加速器获取自己的Docker加速镜像地址。 安装Registry私服方案1docker run -di --name=registry -p 5000:5000 docker.io/registry访问 http://192.168.2.5:5000/v2/_catalog 方案2vim config.yml version: 0.1 log: fields: service: registry storage: cache: blobdescriptor: inmemory filesystem: rootdirectory: /var/lib/registry delete: enabled: true http: addr: :5000 headers: X-Content-Type-Options: [nosniff] health: storagedriver: enabled: true interval: 10s threshold: 3 vim /usr/local/docker/registry/docker-compose.yml version: &#39;3.1&#39; services: registry: image: registry restart: always container_name: registry ports: - 5000:5000 volumes: - ./data:/var/lib/registry - ./config.yml:/etc/docker/registry/config.yml frontend: image: konradkleine/docker-registry-frontend:v2 container_name: registry-frontend restart: always ports: - 184:80 volumes: - ./certs/frontend.crt:/etc/apache2/server.crt:ro - ./certs/frontend.key:/etc/apache2/server.key:ro environment: - ENV_DOCKER_REGISTRY_HOST=192.168.2.5 - ENV_DOCKER_REGISTRY_PORT=5000使用docker-compose up -d启动registry容器，http://192.168.2.5:184/ 访问私有镜像库 curl -I -H &quot;Accept: application/vnd.docker.distribution.manifest.v2+json&quot; 192.168.2.7:5000/v2/ht-micro-record-service-user/manifests/1.0.0-SNAPSHOT 查看镜像Etag curl -I -H &quot;Accept: application/vnd.docker.distribution.manifest.v2+json&quot; 192.168.2.5:5000/v2/ht-micro-record-commons/manifests/1.0.0-SNAPSHOT 查看镜像Etag curl -i -X DELETE 192.168.2.5:5000/v2/codewj-redis-cluster/manifests/sha256:d6d6fad1ac67310ee34adbaa72986c6b233bd713906013961c722ecb10a049e5 删除codewj-redis-cluster:latest镜像 curl -I -X DELETE http://192.168.2.7:5000/v2/ht-micro-record-service-user/manifests/sha256:a6d4e02fa593f0ae30476bda8d992dcb0fc2341e6fef85a9887444b5e4b75a04 删除user镜像 docker exec -it registry registry garbage-collect /etc/docker/registry/config.yml 垃圾回收blob docker exec registry rm -rf /var/lib/registry/docker/registry/v2/repositories/codewj-redis-cluster 强制删除 docker exec registry rm -rf /var/lib/registry/docker/registry/v2/repositories/ht-micro-record-service-user 强制删除JDK镜像制作环境配置vim /etc/docker/daemon.json { &quot;registry-mirrors&quot;: [&quot;https://3gki6pei.mirror.aliyuncs.com&quot;], &quot;storage-driver&quot;:&quot;devicemapper&quot;, &quot;insecure-registries&quot;:[&quot;192.168.2.7:5000&quot;] } vim /lib/systemd/system/docker.service 开放访问 ExecStart 新增 -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock \ systemctl daemon-reload systemctl restart docker docker start registry制作镜像mkdir -p /usr/local/dockerjdk8 &amp;&amp; cd /usr/local/dockerjdk8sz jdk-8u60-linux-x64.tar.gz 传到该目录下vim Dockerfile #依赖镜像名称和ID FROM docker.io/centos:7 #指定镜像创建者信息 MAINTAINER OneJane #切换工作目录 WORKDIR /usr RUN mkdir /usr/local/java #ADD 是相对路径jar,把java添加到容器中 ADD jdk-8u60-linux-x64.tar.gz /usr/local/java/ ENV LANG en_US.UTF-8 #配置java环境变量 ENV JAVA_HOME /usr/local/java/jdk1.8.0_60 ENV JRE_HOME $JAVA_HOME/jre ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib:$CLASSPATH ENV PATH $JAVA_HOME/bin:$PATHdocker build -t='onejane-jdk1.8' .安装中文环境 yum -y install fontconfig fc-list :lang=zh cd /usr/share/fonts &amp;&amp; mkdir chinese chmod -R 755 /usr/share/fonts/chinese cd chinese/ &amp;&amp; rz simhei.ttf fs_GB2312.ttf fzxbsjt.ttf yum -y install ttmkfdir ttmkfdir -e /usr/share/X11/fonts/encodings/encodings.dir vim /etc/fonts/fonts.conf &lt;dir&gt;/usr/share/fonts&lt;/dir&gt; &lt;dir&gt;/usr/share/fonts/chinese&lt;/dir&gt; &lt;dir&gt;/usr/share/X11/fonts/Type1&lt;/dir&gt; &lt;dir&gt;/usr/share/X11/fonts/TTF&lt;/dir&gt; &lt;dir&gt;/usr/local/share/fonts&lt;/dir&gt; &lt;dir prefix=&quot;xdg&quot;&gt;fonts&lt;/dir&gt; &lt;!-- the following element will be removed in the future --&gt; &lt;dir&gt;~/.fonts&lt;/dir&gt; fc-cache fc-list :lang=zh 上传到私服docker commit 6ea1085dfc2a pxc:v1.0 将镜像保存本地 docker commit -m &quot;容器说明&quot; -a &quot;OneJane&quot; [CONTAINER ID] [给新的镜像命名] 将容器打包成镜像 docker tag onejane-jdk1.8 192.168.2.7:5000/onejane-jdk1.8 docker push 192.168.2.7:5000/onejane-jdk1.8 将镜像推到仓库Redis镜像制作vim entrypoint.sh #!/bin/sh #只作用于当前进程,不作用于其创建的子进程 set -e #$0--Shell本身的文件名 $1--第一个参数 $@--所有参数列表 # allow the container to be started with `--user` if [ &quot;$1&quot; = &#39;redis-server&#39; -a &quot;$(id -u)&quot; = &#39;0&#39; ]; then sed -i &#39;s/REDIS_PORT/&#39;$REDIS_PORT&#39;/g&#39; /usr/local/etc/redis.conf chown -R redis . #改变当前文件所有者 exec gosu redis &quot;$0&quot; &quot;$@&quot; #gosu是sudo轻量级”替代品” fi exec &quot;$@&quot; vim redis.conf #端口 port REDIS_PORT #开启集群 cluster-enabled yes #配置文件 cluster-config-file nodes.conf cluster-node-timeout 5000 #更新操作后进行日志记录 appendonly yes #设置主服务的连接密码 # masterauth #设置从服务的连接密码 # requirepassvi Dockerfile #基础镜像 FROM redis #修复时区 RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime RUN echo &#39;Asia/Shanghai&#39; &gt;/etc/timezone #环境变量 ENV REDIS_PORT 8000 #ENV REDIS_PORT_NODE 18000 #暴露变量 EXPOSE $REDIS_PORT #EXPOSE $REDIS_PORT_NODE #复制 COPY entrypoint.sh /usr/local/bin/ COPY redis.conf /usr/local/etc/ #for config rewrite RUN chmod 777 /usr/local/etc/redis.conf RUN chmod +x /usr/local/bin/entrypoint.sh #入口 ENTRYPOINT [&quot;/usr/local/bin/entrypoint.sh&quot;] #命令 CMD [&quot;redis-server&quot;, &quot;/usr/local/etc/redis.conf&quot;]docker build -t codewj/redis-cluster:1.0 . 上传到私服docker tag codewj/redis-cluster:1.0 192.168.2.5:5000/codewj-redis-cluster docker push 192.168.2.5:5000/codewj-redis-cluster上传到docker hubdocker login 输入https://hub.docker.com/账户密码 docker tag onejane-jdk1.8 onejane/onejane-jdk1.8 docker push onejane/onejane-jdk1.8:latest docker logout Redisdocker run -d –privileged=true -p 6379:6379 -v $PWD/redis.conf:/etc/redis/redis.conf -v $PWD/data:/data –name redis redis redis-server /etc/redis/redis.conf –appendonly yeshttp://download.redis.io/redis-stable/redis.conf bind 0.0.0.0 port 6379 daemonize no appendonly yes protected-mode no 手动部署 mvn clean install deploy docker:build -DpushImage docker run -di --name=panchip -v /tmp/saas:/tmp/saas --net=host 192.168.2.7:5000/panchip:1.0.0-SNAPSHOT docker export panchip &gt; /home/panchip.tar 容器打成tar cat panchip.tar | docker import - panchip tar转镜像Q1 Failed to start Docker Application Container Engine的解决办法 vim /etc/docker/daemon.json vim /usr/lib/systemd/system/docker.service]]></content>
      <categories>
        <category>持续集成</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitLab+jenkins+docker]]></title>
    <url>%2F2019%2F08%2F20%2FGitLab-jenkins-docker%2F</url>
    <content type="text"><![CDATA[GitLab+jenkins+docker自动发布 http://192.168.2.5:181/view/all/newJob 构建一个maven项目ht-micro-record-service-note-provider添加jenkins主机公钥到gitlab，并生成全局凭据1.Username with password root/1234562.SSH Username with private key Enter Directly,添加gitlab服务器私钥 parent.relativePath修改为，发布单个服务时设定一个空值将始终从仓库中获取，不从本地路径获取 vim /usr/lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix://var/run/docker.sock 基础服务&lt;build&gt; &lt;finalName&gt;ht-micro-record-commons-domain&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;classifier&gt;exec&lt;/classifier&gt; &lt;!--被依赖的包加该配置--&gt; &lt;mainClass&gt;com.ht.micro.record.commons.CommonsMapperApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 微服务提供者 &lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;name&gt;Nexus Release Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;name&gt;Nexus Snapshot Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.7:182/repository/maven-snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt; &lt;build&gt; &lt;finalName&gt;ht-micro-record-service-note-consumer&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;!--被其他服务引用必须增加该配置--&gt; &lt;classifier&gt;exec&lt;/classifier&gt; &lt;mainClass&gt;com.ht.micro.record.service.consumer.NoteConsumerServiceApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- docker的maven插件，官网 https://github.com/spotify/docker-maven-plugin --&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt; &lt;version&gt;0.4.13&lt;/version&gt; &lt;configuration&gt; &lt;imageName&gt;192.168.2.5:5000/${project.artifactId}:${project.version}&lt;/imageName&gt; &lt;baseImage&gt;jdk1.8&lt;/baseImage&gt; &lt;entryPoint&gt;[&quot;java&quot;, &quot;-jar&quot;,&quot;/${project.build.finalName}.jar&quot;]&lt;/entryPoint&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/&lt;/targetPath&gt; &lt;directory&gt;${project.build.directory}&lt;/directory&gt; &lt;include&gt;${project.build.finalName}.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;dockerHost&gt;http://192.168.2.5:2375&lt;/dockerHost&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; Jenkinsexport app_name=”ht-micro-record-service-note-consumer”export app_version=”1.0.0-SNAPSHOT”sh /usr/local/docker/ht-micro-record/deploy.sh #!/bin/bash # 判断项目是否在运行 DATE=`date +%s` last_app_name=`echo &quot;$app_name-expire-$DATE&quot;` if docker ps | grep $app_name;then docker stop $app_name docker rename $app_name $last_app_name docker run -di --name=$app_name -v /opt/template/:/opt/template/ -e JAVA_OPTS=&#39;-Xmx3g -Xms3g&#39; --net=host 192.168.2.7:5000/$app_name:$app_version # 判断项目是否存在 elif docker ps -a | grep $app_name;then docker start $app_name else docker run -di --name=$app_name -v /opt/template/:/opt/template/ -e JAVA_OPTS=&#39;-Xmx3g -Xms3g&#39; --net=host 192.168.2.7:5000/$app_name:$app_version fi vim /usr/local/bin/dokill docker images | grep -e $*|awk &#39;{print $3}&#39;|sed &#39;2p&#39;|xargs docker rmi chmod a+x /usr/local/bin/dokill dokill tensquare_recruit Build时必须clean，项目名必须小写 Q1:Failed to execute goal on project : Could not resolve dependencies for 对最父级项目clean install，再最子项目clean install Q2: repackage failed: Unable to find main class -&gt; [Help 1] 构建显示缺少主类 @SpringBootApplication public class CommonsApplication { public static void main(String[] args) { } }分布式构建192.168.2.7:181 访问jenkins，系统管理-节点管理-新建节点ln -s /data2/jdk/bin/java /usr/bin/java 分布式部署通过网关负载时，需要重新部署网关gateway服务。 免密登陆192.168.2.7 ssh-keygen -t rsa ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.2.5 ssh root@192.168.2.5 新建jenkins任务192.168.2.7ht-micro-record-service-user-2.5vim /data2/deploy2.5.sh #!/bin/bash echo &quot;------------------服务信息---------------&quot; echo $1 $2 echo &quot;------------------开始部署---------------&quot; ssh root@192.168.2.5 sh /data2/deploy.sh $1 $2 192.168.2.5vim /data2/deploy.sh #!/bin/bash # 判断项目是否在运行 if docker ps | grep $1;then docker stop $1|xargs docker rm docker rmi 192.168.2.7:5000/$1:$2 # 判断项目是否存在 elif docker ps -a | grep $1;then docker rm $1 docker rmi 192.168.2.7:5000/$1:$2 elif docker images|grep $1;then docker rmi 192.168.2.7:5000/$1:$2 fi docker run -di --name=$1 -v /opt/:/opt/ -e JAVA_OPTS=&#39;-Xmx3g -Xms3g&#39; --net=host 192.168.2.7:5000/$1:$2]]></content>
      <categories>
        <category>持续集成</category>
      </categories>
      <tags>
        <tag>gitlab</tag>
        <tag>jenkins</tag>
        <tag>docker</tag>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch]]></title>
    <url>%2F2019%2F08%2F20%2FElasticSearch%2F</url>
    <content type="text"><![CDATA[ElasticSearch基础及数据迁移 https://github.com/medcl/esm-abandoned/releases/download/v0.4.1/linux64-esm--2019.4.20.tar.gz 基本原理 写：发送一个请求到协调节点，对document进行hash路由到对应由primary shard的节点上，处理请求并同步到replica shard，协调节点检测同步后返回相应给客户端。读：发送一个请求到协调节点，根据doc id对document进行hash路由到对应node，通过随机轮询算法从primary和replica的shard中随机选择让读请求负载均衡，返回document给协调节点后给客户端。一个索引拆分为多个shard存储部分数据，每个shard由primary shard和replica shard组成，primary写入将同步到replica，类似kafka的partition副本。保证高可用。Es多个节点选举一个为master，管理和切换主副shard，若宕机则重新选举，并将宕机节点primary shard身份转移到其他机器的replica shard。重启将修改原primary为replica同步数据。 每个在文档上执行的写操作，包括删除，都会使其版本增加。 真正的删除时机： deleting a document doesn’t immediately remove the document from disk; it just marks it as deleted. Elasticsearch will clean up deleted documents in the background as you continue to index more data. 删除索引是会立即释放空间的，不存在所谓的“标记”逻辑。 删除文档的时候，是将新文档写入，同时将旧文档标记为已删除。 磁盘空间是否释放取决于新旧文档是否在同一个segment file里面，因此ES后台的segment merge在合并segment file的过程中有可能触发旧文档的物理删除。但因为一个shard可能会有上百个segment file，还是有很大几率新旧文档存在于不同的segment里而无法物理删除。想要手动释放空间，只能是定期做一下force merge，并且将max_num_segments设置为1。多机器集群搭建 vi /etc/sysctl.conf vm.max_map_count=262144 sysctl -pNode1 192.168.2.5最好挂载到大磁盘上 mkdir /usr/local/docker/ElasticSearch/data -p &amp;&amp; chmod 777 /usr/local/docker/ElasticSearch/data mkdir /usr/local/docker/ElasticSearch/config/ -p &amp;&amp; cd /usr/local/docker/ElasticSearch/config/ vi es.yml cluster.name: elasticsearch-cluster node.name: es-node1 network.bind_host: 0.0.0.0 network.publish_host: 192.168.2.5 http.port: 1800 transport.tcp.port: 1801 http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; node.master: true node.data: true discovery.zen.ping.unicast.hosts: [&quot;192.168.2.5:1801&quot;,&quot;192.168.2.6:1801&quot;] # 可配置多个 discovery.zen.minimum_master_nodes: 1 修改/usr/share/elasticsearch/config/jvm.options中-Xms10g -Xmx10g docker run -d -v /data3/elasticsearch/config/jvm.options:/usr/share/elasticsearch/config/jvm.options -v /data3/elasticsearch/config/es.yml:/usr/share/elasticsearch/config/elasticsearch.yml -v /data3/elasticsearch/data:/usr/share/elasticsearch/data --name es-node1 --net host --privileged elasticsearch:6.6.0 docker exec -it es-node1 bash elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.6.0/elasticsearch-analysis-ik-6.6.0.zip docker restart es-node1Node2 192.168.2.6mkdir /usr/local/docker/ElasticSearch/data -p &amp;&amp; chmod 777 /usr/local/docker/ElasticSearch/data mkdir /usr/local/docker/ElasticSearch/config/ -p &amp;&amp; cd /usr/local/docker/ElasticSearch/config/ vi es.yml cluster.name: elasticsearch-cluster node.name: es-node2 network.bind_host: 0.0.0.0 network.publish_host: 192.168.2.6 http.port: 1800 transport.tcp.port: 1801 http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; node.master: true node.data: true discovery.zen.ping.unicast.hosts: [&quot;192.168.2.5:1801&quot;,&quot;192.168.2.6:1801&quot;] # 可配置多个 discovery.zen.minimum_master_nodes: 1 # 集群最少需要有两个 node , 才能保证既可以不脑裂, 又可以高可用 修改/usr/share/elasticsearch/config/jvm.options中-Xms10g -Xmx10g docker run -d -v /data2/elasticsearch/config/jvm.options:/usr/share/elasticsearch/config/jvm.options -v /data2/elasticsearch/config/es.yml:/usr/share/elasticsearch/config/elasticsearch.yml -v /data3/elasticsearch/data:/usr/share/elasticsearch/data --name es-node2 --net host --privileged elasticsearch:6.6.0 docker exec -it es-node2 bash elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.6.0/elasticsearch-analysis-ik-6.6.0.zip docker restart es-node2http://192.168.2.5:1800/_cat/plugins 查看插件信息 esm数据迁移./esm -s http://192.168.2.6:9200 -d http://192.168.2.7:1800 -x p_answer_handle_alarm -w=5 -b=10 -c 10000; 同步数据 curl 192.168.2.7:1800/_cat/indices?v 查看所有索引信息 常见参数使用： ./esm -s http://192.168.2.6:9200 -d http://192.168.2.7:1800 -x t_record_analyze -y record_test --copy_settings --copy_mappings --shards=4 -w=5 -b=10 -c 10000;数据迁移完美方案通过两集群都建立同样索引，保证mappings和settings一致，再同步数据 curl -XGET 192.168.2.7:1800/p_answer_handle_alarm 查看索引信息 curl -XGET 192.168.2.7:1800/p_answer_handle_alarm/_search; curl -XGET http://192.168.2.5:1800/record_test/_settings?pretty 查看settings信息 PUT http://192.168.2.5:1800/record_set/_settings 修改副本数，片的信息分又重新做了调整，同curl -XPUT &#39;192.168.2.7:1800/record_set/_settings&#39; -d&#39;{&quot;index&quot;:{&quot;number_of_replicas&quot;:1}}&#39; { &quot;index&quot;:{ &quot;number_of_replicas&quot;:1 } } index.blocks.read_only //设置为 true 使索引和索引元数据为只读，false 为允许写入和元数据更改。 index.blocks.read // 设置为 true 可禁用对索引的读取操作 index.blocks.write //设置为 true 可禁用对索引的写入操作。 index.blocks.metadata // 设置为 true 可禁用索引元数据的读取和写入 index.mapping.total_fields.limit //1000 防止字段过多引起崩溃 curl -XGET http://192.168.2.5:1800/record_set/_mappings?pretty 查看mappings信息 http://192.168.2.5:1800/_cluster/settings?pretty 查看settings信息 PUT http:///192.168.2.5:1800/record_set/doc/_mapping 新增字段 { &quot;properties&quot;: { &quot;col1&quot;: { &quot;type&quot;: &quot;text&quot;, &quot;index&quot;: false } } } PUT 192.168.2.6:9200/test 新建索引 { &quot;mappings&quot;:{ &quot;doc&quot;:{ &quot;properties&quot;:{ &quot;jjbh&quot;:{&quot;type&quot;: &quot;keyword&quot;,&quot;index&quot;: &quot;true&quot;}, &quot;bccljg&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;index&quot;:&quot;true&quot;,&quot;analyzer&quot;: &quot;ik_max_word&quot;,&quot;search_analyzer&quot;: &quot;ik_max_word&quot;}, &quot;bjnr&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;index&quot;: &quot;true&quot;,&quot;analyzer&quot;:&quot;ik_max_word&quot;,&quot;search_analyzer&quot;:&quot;ik_max_word&quot;}, &quot;cjlb&quot;:{&quot;type&quot;: &quot;keyword&quot;,&quot;index&quot;: &quot;false&quot;} } } } } 192.168.2.7:1800/test/doc/{_id} 插入数据，_id不加默认随机字符串 { &quot;jjbh&quot;: &quot;4654132465&quot;, &quot;bccljg&quot;: &quot;我是一名合格的程序员&quot;, &quot;bjnr&quot;: &quot;今天天气真的好啊&quot;, &quot;cjlb&quot;: &quot;天上地下飞禽走兽&quot; } 192.168.2.7:1800/test/doc/{_id} 更新 { &quot;jjbh&quot;: &quot;11&quot;, &quot;bccljg&quot;: &quot;我是一名合格的程序员&quot;, &quot;bjnr&quot;: &quot;今天天气真的好啊&quot;, &quot;cjlb&quot;: &quot;天上地下飞禽走兽&quot; } PUT 192.168.2.5:1800/test 新建索引 { &quot;mappings&quot;:{ &quot;doc&quot;:{ &quot;properties&quot;:{ &quot;jjbh&quot;:{&quot;type&quot;: &quot;keyword&quot;,&quot;index&quot;: &quot;true&quot;}, &quot;bccljg&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;index&quot;:&quot;true&quot;,&quot;analyzer&quot;: &quot;ik_max_word&quot;,&quot;search_analyzer&quot;: &quot;ik_max_word&quot;}, &quot;bjnr&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;index&quot;: &quot;true&quot;,&quot;analyzer&quot;:&quot;ik_max_word&quot;,&quot;search_analyzer&quot;:&quot;ik_max_word&quot;}, &quot;cjlb&quot;:{&quot;type&quot;: &quot;keyword&quot;,&quot;index&quot;: &quot;false&quot;} } } } } ./esm -s http://192.168.2.6:9200 -d http://192.168.2.5:1800 -x test -y test -w=5 -b=10 -c 10000; 同步数据 POST 192.168.2.5:1800/test/doc 全局加ik分词器 { &quot;settings&quot;:{ &quot;analysis&quot;:{ &quot;analyzer&quot;:{ &quot;ik&quot;:{&quot;tokenizer&quot;: &quot;ik_max_keyword&quot;} } } } } POST 192.168.2.5:1800/test/doc/_search 查询 POST 192.168.2.5:1800/test/doc/_delete_by_query 删除 精确查询/删除 { &quot;query&quot;:{ &quot;term&quot;:{ &quot;bccljg.keyword&quot;:&quot;我是一名合格的程序员&quot; } } } 模糊查询/删除 { &quot;query&quot;: { &quot;match&quot;: { &quot;bccljg&quot;: &quot;合格&quot; } } } 正则模糊查询 { &quot;query&quot;: { &quot;regexp&quot;: { &quot;bccljg.keyword&quot;: &quot;.*我是.*&quot; } } } 文本开头查询 { &quot;query&quot;: { &quot;prefix&quot;: { &quot;bccljg.keyword&quot;: &quot;我是&quot; } } } Q1:若数据结果不一致，可能是磁盘不足。Q2:”caused_by”:{“type”:”search_context_missing_exception”,”reason”:”No search context found for id [69218326]”}} ./esm -s http://192.168.2.6:9200 -d http://192.168.2.7:1800 -x t_record_analyze -y t_record_analyze -w=5 -b=10 -c 10000; ./esm -s http://192.168.2.6:9200 -d http://192.168.2.7:1800 -x t_neo4j_data -y t_neo4j_data -w=5 -b=10 -c 10000; ./esm -s http://192.168.2.6:9200 -d http://192.168.2.7:1800 -x t_data_analysis -y t_data_analysis -w=5 -b=10 -c 10000; ./esm -s http://192.168.2.6:9200 -d http://192.168.2.7:1800 -x t_cjjxq -y t_cjjxq -w=5 -b=10 -c 10000; ./esm -s http://192.168.1.225:9200 -d http://192.168.2.7:1800 -x t_alarm_analysis_result -y t_alarm_analysis_result -w=5 -b=10 -c 10000; ./esm -s http://192.168.2.6:9200 -d http://192.168.2.7:1800 -x p_answer_handle_alarm -y p_answer_handle_alarm -t=10m -w=5 -b=10 -c 5000; ./esm -s http://192.168.2.6:9200 -d http://192.168.2.7:1800 -x city_answer_handle_alarm -y city_answer_handle_alarm -t=10m -w=5 -b=10 -c 5000;scroll time 超时，设置-t参数，默认是1m 同步后的数据结构 logstashES单节点安装docker run -di --name=tensquare_es -p 9200:9200 -p 9300:9300 --privileged docker.io/elasticsearch:6.6.0 docker cp tensquare_es:/usr/share/elasticsearch/config/elasticsearch.yml /usr/share/elasticsearch.yml docker stop tensquare_es docker rm tensquare_es vim /usr/share/elasticsearch.yml transport.host: 0.0.0.0 docker restart tensquare_es vim /etc/security/limits.conf * soft nofile 65536 * hard nofile 65536 nofile是单个进程允许打开的最大文件个数 soft nofile 是软限制 hard nofile是硬限制 vim /etc/sysctl.conf vm.max_map_count=655360 sysctl -p 修改内核参数立马生效 我们需要以文件挂载的 方式创建容器才行，这样我们就可以通过修改宿主机中的某个文件来实现对容器内配置 文件的修改 docker run -di --name=tensquare_es -p 9200:9200 -p 9300:9300 --privileged -v /usr/share/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml docker.io/elasticsearch:6.6.0 docker restart tensquare_es 才可以远程连接使用 同步mysqlhttps://www.elastic.co/cn/downloads/past-releases/logstash-6-6-0tar zxf logstash-6.6.0.tar.gz &amp;&amp; cd /root/logstash-6.6.0/binmkdir mysql &amp;&amp; cd mysql &amp;&amp; vim mysql.conf jdbc { # mysql jdbc connection string to our backup databse jdbc_connection_string =&gt; &quot;jdbc:mysql://192.168.2.5:185/ht_micro_record?characterEncoding=UTF8&quot; # the user we wish to excute our statement as jdbc_user =&gt; &quot;root&quot; jdbc_password =&gt; &quot;123456&quot; # the path to our downloaded jdbc driver jdbc_driver_library =&gt; &quot;/root/logstash-6.6.0/bin/mysql/mysql-connector-java-5.1.47.jar&quot; # the name of the driver class for mysql jdbc_driver_class =&gt; &quot;com.mysql.jdbc.Driver&quot; jdbc_paging_enabled =&gt; &quot;true&quot; jdbc_page_size =&gt; &quot;500000&quot; #以下对应着要执行的sql的绝对路径。 #statement_filepath =&gt; &quot;select id,title,content from tb_article&quot; statement =&gt; &quot;SELECT id,applyer_police_num,applyer_name FROM t_apply&quot; #定时字段 各字段含义（由左至右）分、时、天、月、年，全部为*默认含义为每分钟都更新（测试结果，不同的话请留言指出） schedule =&gt; &quot;* * * * *&quot; } } output { elasticsearch { #ESIP地址与端口 hosts =&gt; &quot;192.168.3.224:9200&quot; #ES索引名称（自己定义的） index =&gt; &quot;t_apply&quot; #自增ID编号 document_id =&gt; &quot;%{id}&quot; document_type =&gt; &quot;article&quot; } stdout { #以JSON格式输出 codec =&gt; json_lines } } nohup ./logstash -f mysql/mysql.conf &amp; 同步es/root/logstash-6.6.0/bin/es/es.conf input { elasticsearch { hosts =&gt; [&quot;192.168.2.5:1800&quot;,&quot;192.168.2.7:1800&quot;] index =&gt; &quot;t_neo4j_data&quot; size =&gt; 1000 scroll =&gt; &quot;1m&quot; codec =&gt; &quot;json&quot; docinfo =&gt; true } } filter { mutate { remove_field =&gt; [&quot;@timestamp&quot;, &quot;@version&quot;] } } output { elasticsearch { hosts =&gt; [&quot;192.168.3.224:9200&quot;] index =&gt; &quot;%{[@metadata][_index]}&quot; } stdout { codec =&gt; rubydebug { metadata =&gt; true } } } ./logstash -f es/es.conf –path.data ../logs/ elasticdumpwget https://nodejs.org/dist/v8.11.2/node-v8.11.2-linux-x64.tar.xz tar xf node-v8.11.2-linux-x64.tar.xz -C /usr/local/ ln -s /usr/local/node-v8.11.2-linux-x64/bin/npm /usr/local/bin/npm ln -s /usr/local/node-v8.11.2-linux-x64/bin/node /usr/local/bin/node npm install -g cnpm --registry=https://registry.npm.taobao.org ln -s /usr/local/node-v8.11.2-linux-x64/bin/cnpm /usr/local/bin/cnpm cnpm init -f cnpm install elasticdump cd node_modules/elasticdump/bin ./elasticdump \ --input=http://192.168.2.6:9200/t_record_analyze \ --output=http://192.168.3.224:9200/t_record_analyze \ --type=analyzer ./elasticdump \ --input=http://192.168.2.6:9200/t_record_analyze \ --output=http://192.168.3.224:9200/t_record_analyze \ --type=mapping ./elasticdump \ --input=http://192.168.2.6:9200/t_record_analyze \ --output=http://192.168.3.224:9200/t_record_analyze \ --type=data ./elasticdump \ --input=http://192.168.2.6:9200:9200/t_record_analyze \ --output=data.json \ --searchBody &#39;{&quot;query&quot;:{&quot;term&quot;:{&quot;username&quot;: &quot;admin&quot;}}} ./elasticdump \ --input=./data.json \ --output=http://192.168.2.7:1800整合springboot父pom &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.2.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt;子pom &lt;!--elasticsearch--&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.java.dev.jna&lt;/groupId&gt; &lt;artifactId&gt;jna&lt;/artifactId&gt; &lt;!-- &lt;version&gt;3.0.9&lt;/version&gt; --&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt; &lt;artifactId&gt;gson&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt;配置文件bootstrap.yml es: nodes: 192.168.2.5:1800,192.168.2.7:1800 host: 192.168.2.6,192.168.2.7 port: 1801,1801 types: doc clusterName: elasticsearch-cluster #笔录分析库 blDbName: t_record_analyze #接警信息库 jjDbName: p_answer_alarm #处警信息库 cjDbName: p_handle_alarm #警情分析结果信息库 jqfxDbName: t_alarm_analysis_result #标准化分析 cjjxqDbName: t_cjjxq #接处警整合库 jcjDbName: p_answer_handle_alarm #警情分析 daDbName: t_data_analysis #警情结果表(neo4j使用) neo4jData: t_neo4j_data #市局接处警表 cityDbName: city_answer_handle_alarm配置类com/ht/micro/record/service/dubbo/provider/utils/EsConfig.java @Service public class EsConfig { @Value(&quot;${es.nodes}&quot;) private String nodes; @Value(&quot;${es.host}&quot;) private String host; @Value(&quot;${es.port}&quot;) private String port; @Value(&quot;${es.blDbName}&quot;) private String blDbName; @Value(&quot;${es.jjDbName}&quot;) private String jjDbName; @Value(&quot;${es.cjDbName}&quot;) private String cjDbName; @Value(&quot;${es.jqfxDbName}&quot;) private String jqfxDbName; @Value(&quot;${es.clusterName}&quot;) private String clusterName; @Value(&quot;${es.jjDbName}&quot;) private String answerDbName; @Value(&quot;${es.cjDbName}&quot;) private String handleDbName; @Value(&quot;${es.cjjxqDbName}&quot;) private String cjjxqDbName; @Value(&quot;${es.jcjDbName}&quot;) private String jcjDbName; @Value(&quot;${es.daDbName}&quot;) private String daDbName; @Value(&quot;${es.daDbName}&quot;) private String fxDbName; @Value(&quot;${es.types}&quot;) private String types; @Value(&quot;${es.neo4jData}&quot;) private String neo4jData; @Value(&quot;${es.cityDbName}&quot;) private String cityDbName; } 配置类 com/ht/micro/record/service/dubbo/provider/utils/ElasticClientSingleton.java @Service public class ElasticClientSingleton { protected final Logger logger = LoggerFactory.getLogger(ElasticClientSingleton.class); private AtomicInteger atomicPass = new AtomicInteger(); // 0 未初始化, 1 已初始化 private TransportClient transportClient; private BulkRequestBuilder bulkRequest; public synchronized void init(EsConfig esConfig) { try { String ipArray = esConfig.getHost(); String portArray = esConfig.getPort(); String cluster = esConfig.getClusterName(); Settings settings = Settings.builder() .put(&quot;cluster.name&quot;, cluster) //连接的集群名 .put(&quot;client.transport.ignore_cluster_name&quot;, true) .put(&quot;client.transport.sniff&quot;, false)//如果集群名不对，也能连接 .build(); transportClient = new PreBuiltTransportClient(settings); String[] ips = ipArray.split(&quot;,&quot;); String[] ports = portArray.split(&quot;,&quot;); for (int i = 0; i &lt; ips.length; i++) { transportClient.addTransportAddress(new TransportAddress(InetAddress.getByName(ips[i]), Integer .parseInt(ports[i]))); } atomicPass.set(1); } catch (Exception e) { e.printStackTrace(); logger.error(e.getMessage()); atomicPass.set(0); destroy(); } } public void destroy() { if (transportClient != null) { transportClient.close(); transportClient = null; } } public BulkRequestBuilder getBulkRequest(EsConfig esConfig) { if (atomicPass.get() == 0) { // 初始化 init(esConfig); } bulkRequest = transportClient.prepareBulk(); return bulkRequest; } public TransportClient getTransportClient(EsConfig esConfig) { if (atomicPass.get() == 0) { // 初始化 init(esConfig); } return transportClient; } }配置类 com/ht/micro/record/service/dubbo/provider/utils/ElasticClient.java @Service public class ElasticClient { @Autowired private EsConfig esConfig; private static final Logger logger = LoggerFactory.getLogger(ElasticClient.class); private static BulkRequestBuilder bulkRequest; @Autowired private ElasticClientSingleton elasticClientSingleton; @PostConstruct public void init() { bulkRequest = elasticClientSingleton.getBulkRequest(esConfig); } public static String postRequest(String url, String query) { RestTemplate restTemplate = new RestTemplate(); MediaType type = MediaType.parseMediaType(&quot;application/json; charset=UTF-8&quot;); HttpHeaders headers = new HttpHeaders(); headers.setContentType(type); headers.add(&quot;Accept&quot;, MediaType.APPLICATION_JSON.toString()); HttpEntity&lt;String&gt; formEntity = new HttpEntity&lt;String&gt;(query, headers); String result = restTemplate.postForObject(url, formEntity, String.class); return result; } /** * action 提交操作 */ // public void action() { // int reqSize = bulkRequest.numberOfActions(); // //读不到数据了，默认已经全部读取 // if (reqSize == 0) { // bulkRequest.request().requests().clear(); // } // bulkRequest.setTimeout(new TimeValue(1000 * 60 * 5)); //超时30秒 // BulkResponse bulkResponse = bulkRequest.execute().actionGet(); // //持久化异常 // if (bulkResponse.hasFailures()) { // logger.error(bulkResponse.buildFailureMessage()); // bulkRequest.request().requests().clear(); // } // logger.info(&quot;import over....&quot; + bulkResponse.getItems().length); // } }测试com/ht/micro/record/service/dubbo/provider/controller/ProviderController.java @Autowired ElasticClientSingleton elasticClientSingleton; @Autowired EsConfig esConfig; @GetMapping(&quot;/test&quot;) public void test(){ SearchRequestBuilder srb = elasticClientSingleton.getTransportClient(esConfig).prepareSearch(esConfig.getCityDbName()).setTypes(esConfig.getTypes()); TermsQueryBuilder cjbsBuilder = QueryBuilders.termsQuery(&quot;cjbs&quot;, &quot;4&quot;); SearchResponse weekSearchResponse = srb.setQuery(cjbsBuilder).execute().actionGet(); System.out.println(weekSearchResponse.getHits().getTotalHits()); } ElasticSearch-SQL使用sql操作elasticsearch https://github.com/NLPchina/elasticsearch-sqlhttps://artifacts.elastic.co/maven/org/elasticsearch/client/x-pack-transport/6.6.0/x-pack-transport-6.6.0.jar 在github上查找相关案例，需要相关的jar并没有找到，后去elasticsearch-sql-6.6.0.0.zip插件包中找到elasticsearch-sql-6.6.0.0.jar，启动测试程序 Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/elasticsearch/xpack/client/PreBuiltXPackTransportClient at com.alibaba.druid.pool.ElasticSearchDruidDataSource.createPhysicalConnection(ElasticSearchDruidDataSource.java:686) at com.alibaba.druid.pool.ElasticSearchDruidDataSource.createPhysicalConnection(ElasticSearchDruidDataSource.java:632) at com.alibaba.druid.pool.ElasticSearchDruidDataSource.init(ElasticSearchDruidDataSource.java:579) at com.alibaba.druid.pool.ElasticSearchDruidDataSource.getConnection(ElasticSearchDruidDataSource.java:930) at com.alibaba.druid.pool.ElasticSearchDruidDataSource.getConnection(ElasticSearchDruidDataSource.java:926) at com.ht.micro.record.service.dubbo.provider.controller.Test.main(Test.java:20) Caused by: java.lang.ClassNotFoundException: org.elasticsearch.xpack.client.PreBuiltXPackTransportClient at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 6 more x-pack-transportx-pack-core后继续报错，陆续加入x-pack-core，unboundid-ldapsdk，bcpkix-jdk15on，bcprov-jdk15on等依赖才跑通流程。 安装elasticsearch-sqldocker exec -it es-node1 bash elasticsearch-plugin install https://github.com/NLPchina/elasticsearch-sql/releases/download/6.6.0.0/elasticsearch-sql-6.6.0.0.zip docker restart es-node1 本地安装jarmvn install:install-file -DgroupId=org.elasticsearch.plugin -DartifactId=x-pack-core -Dversion=6.6.0 -Dpackaging=jar -Dfile=x-pack-core-6.6.0.jarmvn install:install-file -DgroupId=org.elasticsearch.client -DartifactId=x-pack-transport -Dversion=6.6.0 -Dpackaging=jar -Dfile=x-pack-transport-6.6.0.jarmvn install:install-file -DgroupId=org.nlpcn -DartifactId=elasticsearch-sql -Dversion=6.6.0.0 -Dpackaging=jar -Dfile=elasticsearch-sql-6.6.0.0.jar 引入依赖&lt;dependency&gt; &lt;groupId&gt;org.nlpcn&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-sql&lt;/artifactId&gt; &lt;version&gt;6.6.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.15&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-client&lt;/artifactId&gt; &lt;version&gt;6.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;6.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;transport&lt;/artifactId&gt; &lt;version&gt;6.6.0&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.elasticsearch.plugin&lt;/groupId&gt; &lt;artifactId&gt;transport-netty4-client&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;artifactId&gt;elasticsearch-rest-client&lt;/artifactId&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.plugin&lt;/groupId&gt; &lt;artifactId&gt;transport-netty4-client&lt;/artifactId&gt; &lt;version&gt;6.6.0&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.38&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.unboundid&lt;/groupId&gt; &lt;artifactId&gt;unboundid-ldapsdk&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.bouncycastle&lt;/groupId&gt; &lt;artifactId&gt;bcprov-jdk15on&lt;/artifactId&gt; &lt;version&gt;1.58&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.bouncycastle&lt;/groupId&gt; &lt;artifactId&gt;bcpkix-jdk15on&lt;/artifactId&gt; &lt;version&gt;1.58&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;x-pack-transport&lt;/artifactId&gt; &lt;version&gt;6.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.plugin&lt;/groupId&gt; &lt;artifactId&gt;x-pack-core&lt;/artifactId&gt; &lt;version&gt;6.6.0&lt;/version&gt; &lt;/dependency&gt; 测试@RunWith(SpringRunner.class) @SpringBootTest(classes={DubboProviderApplication .class})// 指定启动类 public class ElasticSearchSql { @Test public void testselect() throws Exception { Properties properties = new Properties(); properties.put(&quot;url&quot;, &quot;jdbc:elasticsearch://192.168.2.7:1801,192.168.2.5:1801/&quot;); properties.put(PROP_CONNECTIONPROPERTIES, &quot;client.transport.ignore_cluster_name=true&quot;); DruidDataSource dds = (DruidDataSource) ElasticSearchDruidDataSourceFactory.createDataSource(properties); dds.setInitialSize(1); Connection connection = dds.getConnection(); String sql2 = &quot;select * FROM t_word_freq limit 10&quot;; PreparedStatement ps = connection.prepareStatement(sql2); ResultSet resultSet = ps.executeQuery(); while (resultSet.next()) { //sql对应输出 System.out.println(resultSet.getString(&quot;jjbh&quot;) ); } ps.close(); connection.close(); dds.close(); } } 由于springboot启动时自动加载druid autoconfigration,而低版本druid配合springboot2.x报错ClassNotFoundException Log4j2Filter，需要手动exclude druid包 &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-commons-service&lt;/artifactId&gt; &lt;version&gt;${project.parent.version}&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.16&lt;/version&gt; &lt;/dependency&gt; Q1:重启服务 停止所有index服务 执行curl -XPUT $url/_cluster/settings?pretty -d ‘{“transient” : {“cluster.routing.allocation.enable” : “none”}}’ 执行curl -XPOST $url/_flush/synced?pretty 重启ES集群 等待集群分片全部分配成功，执行curl -XPUT $url/_cluster/settings?pretty -d ‘{“transient” : {“cluster.routing.allocation.enable” : “all”}}’ 开启所有index服务 PUT http://192.168.2.7:1800/_cluster/settings 禁止分片分配。这一步阻止 Elasticsearch 再平衡缺失的分片，直到你告诉它可以进行了。 { &quot;transient&quot; : { &quot;cluster.routing.allocation.enable&quot; : &quot;none&quot; } }启动完毕后PUT http://192.168.2.7:1800/_cluster/settings 重启分片分配 { &quot;transient&quot; : { &quot;cluster.routing.allocation.enable&quot; : &quot;all&quot; } }Q2Elasticsearch默认安装后设置的内存是1GB，这是远远不够用于生产环境的。有两种方式修改Elasticsearch的堆内存： 设置环境变量：export ES_HEAP_SIZE=10g 在es启动时会读取该变量； 启动时作为参数传递给es： ./bin/elasticsearch -Xmx10g -Xms10gQ3 operations are blocked on license expiration. All data operations (read and curl -XPOST -u elastic:changeme &#39;http://192.168.2.5:1800/_xpack/license/start_basic?acknowledge=true&#39; -H &quot;Content-Type: application/json&quot; -d @one-jane-b9ac97b5-0b80-4d0d-9f1d-363b6fb3ce3c-v5.json]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>springboot</tag>
        <tag>elastisearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-Cluster]]></title>
    <url>%2F2019%2F08%2F20%2FRedis-Cluster%2F</url>
    <content type="text"><![CDATA[redis-cluster集群搭建使用 支撑多个master node，每个master挂载多个slave，master写对应slave读，每个master都有slave节点，若挂掉则将某个slave转为master。与相比Sentinel（哨兵）实现的高可用，集群（cluster）更多的是强调数据的分片或者是节点的伸缩性，如果在集群的主节点上加入对应的从节点，集群还可以自动故障转移。主从复制： 通过把这个RDB文件或AOF文件传给slave服务器，slave服务器重新加载RDB文件，来实现复制的功能！当建立一个从服务器后，从服务器会想主服务器发送一个SYNC的命令，主服务器接收到SYNC命令之后会执行BGSAVE，然后保存到RDB文件，然后发送到从服务器！收到RDB文件然后就载入到内存！ Redis 集群中内置了 16384 个哈希槽，当需要在 Redis 集群中放置一个 key-value 时，redis 先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点 集群中所有master参与,如果半数以上master节点与master节点通信超过(cluster-node-timeout),认为当前master节点挂掉.如果集群任意master挂掉,且当前master没有slave.集群进入fail状态,也可以理解成集群的slot映射[0-16383]不完成时进入fail状态如果集群超过半数以上master挂掉，无论是否有slave集群进入fail状态. Sentinel和Cluster区别 Redis-Sentinel(哨兵模式)是Redis官方推荐的高可用性(HA)解决方案，当用Redis做Master-slave的高可用方案时，假如master宕机了，Redis本身(包括它的很多客户端)都没有实现自动进行主备切换，而Redis-sentinel本身也是一个独立运行的进程，它能监控多个master-slave集群，发现master宕机后能进行自动切换。 Redis-Cluster当遇到单机内存、并发、流量等瓶颈时，可以采用Cluster架构达到负载均衡的目的。分布式集群首要解决把整个数据集按照分区规则映射到多个节点的问题，即把数据集划分到多个节点上，每个节点负责整个数据的一个子集。 多机集群redis-cluster，也叫分布式redis集群，可以有多个master，数据分片分布在这些master上。systemctl stop firewalld.servicesystemctl disable firewalld.service 192.168.2.5vim /usr/local/docker/redis/docker-compose.yml version: &#39;3&#39; services: redis1: image: publicisworldwide/redis-cluster container_name: redis1 network_mode: host restart: always volumes: - ./8001/data:/data environment: - REDIS_PORT=8001 redis2: image: publicisworldwide/redis-cluster container_name: redis2 network_mode: host restart: always volumes: - ./8002/data:/data environment: - REDIS_PORT=8002 redis3: image: publicisworldwide/redis-cluster container_name: redis3 network_mode: host restart: always volumes: - ./8003/data:/data environment: - REDIS_PORT=8003docker-compose up -d 192.168.2.7vim /usr/local/docker/redis/docker-compose.yml version: &#39;3&#39; services: redis1: image: publicisworldwide/redis-cluster network_mode: host container_name: redis4 restart: always volumes: - ./8004/data:/data environment: - REDIS_PORT=8004 redis2: image: publicisworldwide/redis-cluster network_mode: host container_name: redis5 restart: always volumes: - ./8005/data:/data environment: - REDIS_PORT=8005 redis3: image: publicisworldwide/redis-cluster network_mode: host container_name: redis6 restart: always volumes: - ./8006/data:/data environment: - REDIS_PORT=8006docker-compose up -d 启动方式直接启动docker run --rm -it inem0o/redis-trib create --replicas 1 192.168.2.5:8001 192.168.2.5:8002 192.168.2.5:8003 192.168.2.7:8004 192.168.2.7:8005 192.168.2.7:8006 docker exec -it redis1 redis-cli -h 127.0.0.1 -p 8001 -c set a 100 cluster info cluster nodes 自动指定master slave 指定masterdocker run --rm -it inem0o/redis-trib create 192.168.2.5:8001 192.168.2.5:8002 192.168.2.5:8003 docker run --rm -it inem0o/redis-trib add-node --slave --master-id 84c3b7ecbc4933e1368a6927f26c79ecc76810b3 192.168.2.7:8004 192.168.2.5:8001 docker run --rm -it inem0o/redis-trib add-node --slave --master-id 716f11f2971e9494183937abd61f7a4baf0b3959 192.168.2.7:8005 192.168.2.5:8002 docker run --rm -it inem0o/redis-trib add-node --slave --master-id c93060613a8f1531c82b97d97eeac402048f0b25 192.168.2.7:8006 192.168.2.5:8003 docker run --rm -it inem0o/redis-trib info 192.168.2.5:8001 docker run --rm -it inem0o/redis-trib help 若同一台宿主机，不想使用host模式同一台，也可以把network_mode去掉，但就要加ports映射。redis-cluster的节点端口共分为2种，一种是节点提供服务的端口，如6379；一种是节点间通信的端口，固定格式为：10000+6379。 docker-compose.yml version: &#39;3&#39; services: redis1: image: publicisworldwide/redis-cluster restart: always volumes: - /app/app/redis/8001/data:/data environment: - REDIS_PORT=8001 ports: - &#39;8001:8001&#39; - &#39;18001:18001&#39; redis2: image: publicisworldwide/redis-cluster restart: always volumes: - /app/app/redis/8002/data:/data environment: - REDIS_PORT=8002 ports: - &#39;8002:8002&#39; - &#39;18002:18002&#39; redis3: image: publicisworldwide/redis-cluster restart: always volumes: - /app/app/redis/8003/data:/data environment: - REDIS_PORT=8003 ports: - &#39;8003:8003&#39; - &#39;18003:18003&#39; redis4: image: publicisworldwide/redis-cluster restart: always volumes: - /app/app/redis/8004/data:/data environment: - REDIS_PORT=8004 ports: - &#39;8004:8004&#39; - &#39;18004:18004&#39; redis5: image: publicisworldwide/redis-cluster restart: always volumes: - /app/app/redis/8005/data:/data environment: - REDIS_PORT=8005 ports: - &#39;8005:8005&#39; - &#39;18005:18005&#39; redis6: image: publicisworldwide/redis-cluster restart: always volumes: - /app/app/redis/8006/data:/data environment: - REDIS_PORT=8006 ports: - &#39;8006:8006&#39; - &#39;18006:18006&#39;自定义Redis集群制作redis镜像vim entrypoint.sh #!/bin/sh #只作用于当前进程,不作用于其创建的子进程 set -e #$0--Shell本身的文件名 $1--第一个参数 $@--所有参数列表 # allow the container to be started with `--user` if [ &quot;$1&quot; = &#39;redis-server&#39; -a &quot;$(id -u)&quot; = &#39;0&#39; ]; then sed -i &#39;s/REDIS_PORT/&#39;$REDIS_PORT&#39;/g&#39; /usr/local/etc/redis.conf chown -R redis . #改变当前文件所有者 exec gosu redis &quot;$0&quot; &quot;$@&quot; #gosu是sudo轻量级”替代品” fi exec &quot;$@&quot;vim redis.conf #端口 port REDIS_PORT #开启集群 cluster-enabled yes #配置文件 cluster-config-file nodes.conf cluster-node-timeout 5000 #更新操作后进行日志记录 appendonly yes #设置主服务的连接密码 # masterauth #设置从服务的连接密码 # requirepassvi Dockerfile #基础镜像 FROM redis #修复时区 RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime RUN echo &#39;Asia/Shanghai&#39; &gt;/etc/timezone #环境变量 ENV REDIS_PORT 8000 #ENV REDIS_PORT_NODE 18000 #暴露变量 EXPOSE $REDIS_PORT #EXPOSE $REDIS_PORT_NODE #复制 COPY entrypoint.sh /usr/local/bin/ COPY redis.conf /usr/local/etc/ #for config rewrite RUN chmod 777 /usr/local/etc/redis.conf RUN chmod +x /usr/local/bin/entrypoint.sh #入口 ENTRYPOINT [&quot;/usr/local/bin/entrypoint.sh&quot;] #命令 CMD [&quot;redis-server&quot;, &quot;/usr/local/etc/redis.conf&quot;] docker build -t codewj/redis-cluster:1.0 .docker tag codewj/redis-cluster:1.0 192.168.2.5:5000/codewj-redis-clusterdocker push 192.168.2.5:5000/codewj-redis-cluster 192.168.2.5version: &#39;3&#39; services: redis1: image: 192.168.2.7:5000/onejane-redis-cluster container_name: redis1 network_mode: host restart: always volumes: - ./8001/data:/data environment: - REDIS_PORT=8001 redis2: image: 192.168.2.7:5000/onejane-redis-cluster container_name: redis2 network_mode: host restart: always volumes: - ./8002/data:/data environment: - REDIS_PORT=8002 redis3: image: 192.168.2.7:5000/onejane-redis-cluster container_name: redis3 network_mode: host restart: always volumes: - ./8003/data:/data environment: - REDIS_PORT=8003192.168.2.7version: &#39;3&#39; services: redis1: image: 192.168.2.7:5000/onejane-redis-cluster network_mode: host container_name: redis4 restart: always volumes: - ./8004/data:/data environment: - REDIS_PORT=8004 redis2: image: 192.168.2.7:5000/onejane-redis-cluster network_mode: host container_name: redis5 restart: always volumes: - ./8005/data:/data environment: - REDIS_PORT=8005 redis3: image: 192.168.2.7:5000/onejane-redis-cluster network_mode: host container_name: redis6 restart: always volumes: - ./8006/data:/data environment: - REDIS_PORT=8006]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>redis-cluster</tag>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven]]></title>
    <url>%2F2019%2F08%2F20%2Fmaven%2F</url>
    <content type="text"><![CDATA[maven基本用法 一个GroupId（项目）下面可以有很多个ArtifactId（模块），每个ArtifactId（模块）会有很多个Version（版本），每个Version（版本）一般被Packaging（打包）为jar、war、pom中的一种。 父模块的packaging必须为pom,packaging的默认值为jar module“组织”子模块,基于父模块pom的子模块相对目录名，不是子模块的artifactId 多模块deploy的时候，根据子模块的相互依赖关系整理一个build顺序，然后依次build，独立模块之间根据配置顺序build 子pom 会直接继承 父pom 中声明的属性 父pom 中仅仅使用dependencies 中dependency 依赖的包，会被 子pom 直接继承（不需要显式依赖） 父pom 中仅仅使用plugins 中plugin依赖的插件，会被 子pom 直接继承（不需要显式依赖） 父pom中可以使用dependecyManagement和pluginManagement来统一管理jar包和插件pugin，不会被子pom 直接继承。子pom如果希望继承该包或插件，则需要显式依赖，同时像 等配置项可以不被显式写出，默认从父pom继承,同pluginManagement mvn 命令对应着maven项目生命周期，顺序为compile、test、package、install、depoly，执行其中任一周期，都会把前面周期 统一执行 具有依赖传递性，而不具有依赖传递性 安装jar包到本地 mvn install:install-file -DgroupId=org.csource.fastdfs -DartifactId=fastdfs -Dversion=1.2 -Dpackaging=jar -Dfile=D:\Project\pinyougou\fastDFSdemo\src\main\webapp\WEB-INF\lib\fastdfs_client_v1.20.jar properties&lt;properties&gt; &lt;spring.version&gt;2.5&lt;/spring.version&gt; &lt;/properties&gt; &lt;version&gt;${spring.version}&lt;/version&gt; pluginManagement父 &lt;pluginManagement&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.8.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; 子 &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; 由于 Maven 内置了 maven-compiler-plugin 与生命周期的绑定，因此子模块就不再需要任何 maven-compiler-plugin 的配置了。 资源打包配置 &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;${project.build.directory}/classes&lt;/targetPath&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;includes&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;include&gt;**/*.yml&lt;/include&gt; &lt;include&gt;**/*.txt&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt; &lt;/resources&gt; dependencyManagement&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.juvenxu.sample&lt;/groupId&gt; &lt;artifactid&gt;sample-dependency-infrastructure&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; 优势 Parent的dependencyManagement节点的作用是管理整个项目所需要用到的第三方依赖。只要约定了第三方依赖的坐标（GroupId:ArtifactId:version），后代模块即可通过GroupId:ArtifactId进行依赖的引入。其它元素如 version 和 scope 都能通过继承父 POM 的 dependencyManagement 得到这样能够避免依赖版本的冲突。当然，这里只是进行约定，并不会真正地引用依赖。 依赖统一管理(parent中定义，需要变动dependency版本，只要修改一处即可)； 代码简洁(子model只需要指定groupId、artifactId即可) dependencyManagement只会影响现有依赖的配置，但不会引入依赖，即子model不会继承parent中dependencyManagement所有预定义的depandency，只引入需要的依赖即可，简单说就是“按需引入依赖”或者“按需继承”；因此，在parent中严禁直接使用depandencys预定义依赖，坏处是子model会自动继承depandencys中所有预定义依赖；劣势单继承：maven的继承跟java一样，单继承，也就是说子model中只能出现一个parent标签；parent模块中，dependencyManagement中预定义太多的依赖，造成pom文件过长，而且很乱； 通过父pom的parent继承的方法，只能继承一个parent。实际开发中，用户很可能需要继承自己公司的标准parent配置，这个时候可以使用 scope=import 来实现多继承。 解决：scope=import只能用在dependencyManagement里面,且仅用于type=pom的dependency,要继承多个，可以在dependencyManagement通过非继承的方式来引入这段依赖管理配置，添加依赖scope=import，type=pom &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;1.3.3.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.test.sample&lt;/groupId&gt; &lt;artifactid&gt;base-parent1&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;自己的项目里面就不需要继承SpringBoot的module了，而可以继承自己项目的module了。 Scope provided 被依赖项目理论上可以参与编译、测试、运行等阶段，相当于compile，但是再打包阶段做了exclude的动作。,servlet-api和jsp-api都是由tomcat等servlet容器负责提供的包,这个 jar 包已由应用服务器提供,这里引入只用于开发,不进行打包 runtime被依赖项目无需参与项目的编译，但是会参与到项目的测试和运行,在编译的时候我们不需要 JDBC API 的 jar 包，而在运行的时候我们才需要 JDBC 驱动包。 compile（默认）被依赖项目需要参与到当前项目的编译，测试，打包，运行等阶段。打包的时候通常会包含被依赖项目。 test 被依赖项目仅仅参与测试相关的工作，包括测试代码的编译，执行,Junit 测试。 system 被依赖项不会从 maven 仓库中查找，而是从本地系统中获取，systemPath 元素用于制定本地系统中 jar 文件的路径发布 &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;!-- ID 名称必须要与 settings.xml 中 Servers 配置的 ID 名称保持一致。--&gt; &lt;name&gt;Nexus Release Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.5:182/repository/maven-releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;name&gt;Nexus Snapshot Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.5:182/repository/maven-snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt; mvn deploy发布到私服,在项目 pom.xml 中设置的版本号添加 SNAPSHOT 标识的都会发布为 SNAPSHOT 版本，没有 SNAPSHOT 标识的都会发布为 RELEASE 版本Nexus 3.0 不支持页面上传，可使用 maven 命令： mvn deploy:deploy-file -DgroupId=com.github.axet -DartifactId=kaptcha -Dversion=0.0.9 -Dpackaging=jar -Dfile=E:\kaptcha-0.0.9.jar -Durl=http://192.168.2.5:182/repository/maven-releases/ -DrepositoryId=nexus-releases 要求jar的pom中的repository.id和settings.xml中一致 上传第三方jarproxy：即你可以设置代理，设置了代理之后，在你的nexus中找不到的依赖就会去配置的代理的地址中找hosted：你可以上传你自己的项目到这里面group：它可以包含前面两个，是一个聚合体。一般用来给客户一个访问nexus的统一地址。 你可以上传私有的项目到hosted，以及配置proxy以获取第三方的依赖（比如可以配置中央仓库的地址）。前面两个都 弄好了之后，在通过group聚合给客户提供统一的访问地址。 settings.xml&lt;server&gt; &lt;id&gt;3rdParty&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;123456&lt;/password&gt; &lt;/server&gt; &lt;repository&gt; &lt;id&gt;3rdParty&lt;/id&gt; &lt;url&gt;http://192.168.2.7:182/repository/3rdParty/&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/repository&gt; mvn deploy:deploy-file -DgroupId=org.nlpcn -DartifactId=elasticsearch-sql -Dversion=6.6.0.0 -Dpackaging=jar -Dfile=elasticsearch-sql-6.6.0.0.jar -Durl=http://192.168.2.7:182/repository/3rdParty/ -DrepositoryId=3rdParty pom.xml &lt;repository&gt; &lt;id&gt;3rdParty&lt;/id&gt; &lt;name&gt;3rdParty Repository&lt;/name&gt; &lt;url&gt;http://192.168.2.7:182/repository/3rdParty/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; parent 必须放在group Id 上]]></content>
      <categories>
        <category>持续集成</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xxl-job]]></title>
    <url>%2F2019%2F08%2F20%2Fxxl-job%2F</url>
    <content type="text"><![CDATA[springboot整合xxl-job定时器 搭建wget https://github.com/xuxueli/xxl-job/archive/2.1.0.tar.gz tar zxf xxl-job-2.1.0.tar.gz docker cp xxl-job-2.1.0/doc/db/tables_xxl_job.sql ht-mysql-slave:/root/ docker exec -it ht-mysql-slave mysql -u root -p use xxl_job; source /root/tables_xxl_job.sql docker run -d --rm \ -e PARAMS=&quot;--spring.datasource.url=jdbc:mysql://192.168.2.7:186/xxl_job?Unicode=true&amp;characterEncoding=UTF-8 --spring.datasource.username=root --spring.datasource.password=123456&quot; \ -p 183:8080 \ --name xxl-job-admin xuxueli/xxl-job-admin:2.1.0http://192.168.2.7:183/xxl-job-admin/ admin 123456 开发&lt;dependency&gt; &lt;groupId&gt;com.xuxueli&lt;/groupId&gt; &lt;artifactId&gt;xxl-job-core&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt;ht-micro-record-service-job/pom.xml &lt;parent&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../ht-micro-record-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;ht-micro-record-service-job&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;ht-micro-record-service-job&lt;/name&gt; &lt;url&gt;http://www.htdatacloud.com/&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.xuxueli&lt;/groupId&gt; &lt;artifactId&gt;xxl-job-core&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt; &lt;artifactId&gt;jsr305&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt; &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rocketmq&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.objenesis&lt;/groupId&gt; &lt;artifactId&gt;objenesis&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;!-- Projects Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;com.htdc&lt;/groupId&gt; &lt;artifactId&gt;ht-micro-record-commons-service&lt;/artifactId&gt; &lt;version&gt;${project.parent.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Projects End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;ht.micro.record.service.job.JobServiceApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;application.yaml spring: application: name: ht-micro-record-service-job datasource: druid: url: jdbc:mysql://192.168.2.88:189/ht-micro-record?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false username: root password: 123456 initial-size: 1 min-idle: 1 max-active: 20 test-on-borrow: true driver-class-name: com.mysql.jdbc.Driver cloud: nacos: discovery: server-addr: 192.168.2.5:8848,192.168.2.5:8849,192.168.2.5:8850 config: server-addr: 192.168.2.5:8848,192.168.2.5:8849,192.168.2.5:8850 sentinel: transport: port: 8719 dashboard: 192.168.2.5:190 server: port: 9701 xxl: job: executor: logpath: logs/xxl-job/jobhandler appname: xxl-job-executor port: 9999 logretentiondays: -1 ip: 192.168.3.233 admin: addresses: http://192.168.2.7:183/xxl-job-admin accessTokencom.ht.micro.record.service.job.JobServiceApplication @SpringBootApplication(scanBasePackages = &quot;com.ht.micro.record&quot;) @EnableDiscoveryClient @MapperScan(basePackages = &quot;com.ht.micro.record.commons.mapper&quot;) public class JobServiceApplication { public static void main(String[] args) { SpringApplication.run(JobServiceApplication.class, args); } }com.ht.micro.record.service.job.config.XxlJobConfig @Configuration @ComponentScan(basePackages = &quot;com.ht.micro.record.service.job.handler&quot;) public class XxlJobConfig { private Logger logger = LoggerFactory.getLogger(XxlJobConfig.class); @Value(&quot;${xxl.job.admin.addresses}&quot;) private String adminAddresses; @Value(&quot;${xxl.job.executor.appname}&quot;) private String appName; @Value(&quot;${xxl.job.executor.ip}&quot;) private String ip; @Value(&quot;${xxl.job.executor.port}&quot;) private int port; @Value(&quot;${xxl.job.accessToken}&quot;) private String accessToken; @Value(&quot;${xxl.job.executor.logpath}&quot;) private String logPath; @Value(&quot;${xxl.job.executor.logretentiondays}&quot;) private int logRetentionDays; @Bean(initMethod = &quot;start&quot;, destroyMethod = &quot;destroy&quot;) public XxlJobSpringExecutor xxlJobExecutor() { logger.info(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; xxl-job config init.&quot;); XxlJobSpringExecutor xxlJobSpringExecutor = new XxlJobSpringExecutor(); xxlJobSpringExecutor.setAdminAddresses(adminAddresses); xxlJobSpringExecutor.setAppName(appName); xxlJobSpringExecutor.setIp(ip); xxlJobSpringExecutor.setPort(port); xxlJobSpringExecutor.setAccessToken(accessToken); xxlJobSpringExecutor.setLogPath(logPath); xxlJobSpringExecutor.setLogRetentionDays(logRetentionDays); return xxlJobSpringExecutor; } }com.ht.micro.record.service.job.handler.TestJobHandler @JobHandler(value=&quot;testJobHandler&quot;) @Component public class TestJobHandler extends IJobHandler { @Override public ReturnT&lt;String&gt; execute(String param) throws Exception { XxlJobLogger.log(&quot;XXL-JOB, Hello World.&quot;); for (int i = 0; i &lt; 5; i++) { XxlJobLogger.log(&quot;beat at:&quot; + i); TimeUnit.SECONDS.sleep(2); } return SUCCESS; } }]]></content>
      <categories>
        <category>定时器</category>
      </categories>
      <tags>
        <tag>xxl-job</tag>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot2-multi-druid]]></title>
    <url>%2F2019%2F08%2F20%2Fspringboot2-multi-druid%2F</url>
    <content type="text"><![CDATA[基于springboot2+druid实现多数据源 配置application.yml server: port: 10205 spring: datasource: base: #监控统计拦截的filters filters: stat type: com.alibaba.druid.pool.DruidDataSource jdbc-url: jdbc:mysql://192.168.1.250:3306/htbl_test?useUnicode=true&amp;characterEncoding=UTF-8 username: root password: 123456 driver-class-name: com.mysql.jdbc.Driver max-idle: 10 max-wait: 10000 min-idle: 5 initial-size: 5 validation-query: SELECT 1 test-on-borrow: false test-while-idle: true time-between-eviction-runs-millis: 18800 dic: #监控统计拦截的filters filters: stat type: com.alibaba.druid.pool.DruidDataSource jdbc-url: jdbc:mysql://192.168.1.48:3306/ht_nlp?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false username: root password: Sonar@1234 driver-class-name: com.mysql.jdbc.Driver max-idle: 10 max-wait: 10000 min-idle: 5 initial-size: 5 validation-query: SELECT 1 test-on-borrow: false test-while-idle: true time-between-eviction-runs-millis: 18800数据源1配置BaseMybatisConfig @Configuration @MapperScan(basePackages = {&quot;com.ht.micro.record.commons.mapper.baseMapper&quot;}, sqlSessionTemplateRef = &quot;baseSqlSessionTemplate&quot;) public class BaseMybatisConfig { @Value(&quot;${spring.datasource.base.filters}&quot;) String filters; @Value(&quot;${spring.datasource.base.driver-class-name}&quot;) String driverClassName; @Value(&quot;${spring.datasource.base.username}&quot;) String username; @Value(&quot;${spring.datasource.base.password}&quot;) String password; @Value(&quot;${spring.datasource.base.jdbc-url}&quot;) String url; @Bean(name=&quot;baseDataSource&quot;) @Primary//必须加此注解，不然报错，下一个类则不需要添加 spring.datasource @ConfigurationProperties(prefix=&quot;spring.datasource.base&quot;)//prefix值必须是application.properteis中对应属性的前缀 public DataSource baseDataSource() throws SQLException { DruidDataSource druid = new DruidDataSource(); // 监控统计拦截的filters druid.setFilters(filters); // 配置基本属性 druid.setDriverClassName(driverClassName); druid.setUsername(username); druid.setPassword(password); druid.setUrl(url); return druid; } @Bean(name=&quot;baseSqlSessionFactory&quot;) @Primary public SqlSessionFactory baseSqlSessionFactory(@Qualifier(&quot;baseDataSource&quot;)DataSource dataSource)throws Exception{ // 创建Mybatis的连接会话工厂实例 SqlSessionFactoryBean bean=new SqlSessionFactoryBean(); bean.setDataSource(dataSource);//// 设置数据源bean //添加XML目录 ResourcePatternResolver resolver=new PathMatchingResourcePatternResolver(); try{ bean.setMapperLocations(resolver.getResources(&quot;classpath:baseMapper/*Mapper.xml&quot;));//// 设置mapper文件路径 return bean.getObject(); }catch(Exception e){ e.printStackTrace(); throw new RuntimeException(e); } } @Bean(name=&quot;baseSqlSessionTemplate&quot;) @Primary public SqlSessionTemplate baseSqlSessionTemplate(@Qualifier(&quot;baseSqlSessionFactory&quot;)SqlSessionFactory sqlSessionFactory)throws Exception{ SqlSessionTemplate template=new SqlSessionTemplate(sqlSessionFactory);//使用上面配置的Factory return template; } } 数据源2配置@Configuration @MapperScan(basePackages = {&quot;com.ht.micro.record.commons.mapper.dicMapper&quot;}, sqlSessionTemplateRef = &quot;dicSqlSessionTemplate&quot;) public class DicMybatisConfig { @Value(&quot;${spring.datasource.dic.filters}&quot;) String filters; @Value(&quot;${spring.datasource.dic.driver-class-name}&quot;) String driverClassName; @Value(&quot;${spring.datasource.dic.username}&quot;) String username; @Value(&quot;${spring.datasource.dic.password}&quot;) String password; @Value(&quot;${spring.datasource.dic.jdbc-url}&quot;) String url; @Bean(name = &quot;dicDataSource&quot;) @ConfigurationProperties(prefix=&quot;spring.datasource.dic&quot;) public DataSource dicDataSource() throws SQLException { DruidDataSource druid = new DruidDataSource(); // 监控统计拦截的filters // druid.setFilters(filters); // 配置基本属性 druid.setDriverClassName(driverClassName); druid.setUsername(username); druid.setPassword(password); druid.setUrl(url); /* //初始化时建立物理连接的个数 druid.setInitialSize(initialSize); //最大连接池数量 druid.setMaxActive(maxActive); //最小连接池数量 druid.setMinIdle(minIdle); //获取连接时最大等待时间，单位毫秒。 druid.setMaxWait(maxWait); //间隔多久进行一次检测，检测需要关闭的空闲连接 druid.setTimeBetweenEvictionRunsMillis(timeBetweenEvictionRunsMillis); //一个连接在池中最小生存的时间 druid.setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis); //用来检测连接是否有效的sql druid.setValidationQuery(validationQuery); //建议配置为true，不影响性能，并且保证安全性。 druid.setTestWhileIdle(testWhileIdle); //申请连接时执行validationQuery检测连接是否有效 druid.setTestOnBorrow(testOnBorrow); druid.setTestOnReturn(testOnReturn); //是否缓存preparedStatement，也就是PSCache，oracle设为true，mysql设为false。分库分表较多推荐设置为false druid.setPoolPreparedStatements(poolPreparedStatements); // 打开PSCache时，指定每个连接上PSCache的大小 druid.setMaxPoolPreparedStatementPerConnectionSize(maxPoolPreparedStatementPerConnectionSize); */ return druid; } @Bean(name = &quot;dicSqlSessionFactory&quot;) public SqlSessionFactory dicSqlSessionFactory(@Qualifier(&quot;dicDataSource&quot;)DataSource dataSource)throws Exception{ SqlSessionFactoryBean bean=new SqlSessionFactoryBean(); bean.setDataSource(dataSource); //添加XML目录 ResourcePatternResolver resolver=new PathMatchingResourcePatternResolver(); try{ bean.setMapperLocations(resolver.getResources(&quot;classpath:dicMapper/*Mapper.xml&quot;)); return bean.getObject(); }catch(Exception e){ e.printStackTrace(); throw new RuntimeException(e); } } @Bean(name=&quot;dicSqlSessionTemplate&quot;) public SqlSessionTemplate dicSqlSessionTemplate(@Qualifier(&quot;dicSqlSessionFactory&quot;)SqlSessionFactory sqlSessionFactory)throws Exception{ SqlSessionTemplate template=new SqlSessionTemplate(sqlSessionFactory);//使用上面配置的Factory return template; } }启动类NoteProviderServiceApplication @SpringBootApplication(scanBasePackages = &quot;com.ht.micro.record&quot;) @EnableDiscoveryClient @MapperScan(basePackages = &quot;com.ht.micro.record.commons.mapper&quot;) @EnableSwagger2 public class NoteProviderServiceApplication { public static void main(String[] args) { SpringApplication.run(NoteProviderServiceApplication.class, args); } }测试服务层WebDicController @RestController @RequestMapping(value = &quot;webdic&quot;) public class WebDicController extends AbstractBaseController&lt;TbUser&gt; { @Autowired private WebDicService webDicService; @ApiOperation(value = &quot;获取所有字典&quot;) @RequestMapping public List&lt;WebDic&gt; getAll() { return webDicService.getAll(); } }TApplyController @RestController @RequestMapping(value = &quot;apply&quot;) public class TApplyController extends AbstractBaseController { @Autowired private TApplyService tApplyService; /** http://localhost:10105/apply/asked/任大龙 * @param name * @return */ @ApiImplicitParams({ @ApiImplicitParam(name = &quot;askedName&quot;, value = &quot;被询问人名&quot;, required = true, paramType = &quot;path&quot;) }) @GetMapping(value = &quot;asked/{name}&quot;) public List&lt;TApply&gt; getByAskedName(@PathVariable String name){ return tApplyService.getByAskedName(name); } }WebDicService @Service public class WebDicService { @Autowired WebDicMapper webDicMapper; public List&lt;WebDic&gt; getAll() { return webDicMapper.selectAll(); } }TApplyService public interface TApplyService extends BaseCrudService&lt;TApply&gt; { List&lt;TApply&gt; getByAskedName(String name); }TApplyServiceImpl @Service public class TApplyServiceImpl extends BaseCrudServiceImpl&lt;TApply, TApplyMapper&gt; implements TApplyService { @Autowired private TApplyMapper tApplyMapper; public List&lt;TApply&gt; getByAskedName(String name) { return tApplyMapper.selectByAskedName(name); } }mapper层com.ht.micro.record.commons.mapper.baseMapper.TApplyMapper public interface TApplyMapper extends MyMapper&lt;TApply&gt; { /** * 根据名称查找 * @param name * @return */ List&lt;TApply&gt; selectByAskedName(String name); }com.ht.micro.record.commons.mapper.dicMapper.WebDicMapper public interface WebDicMapper extends MyMapper&lt;WebDic&gt; { }tk.mybatis.mapper.MyMapper public interface MyMapper&lt;T&gt; extends Mapper&lt;T&gt;, MySqlMapper&lt;T&gt; { }baseMapper/TApplyMapper.xml &lt;mapper namespace=&quot;com.ht.micro.record.commons.mapper.baseMapper.TApplyMapper&quot;&gt; &lt;resultMap id=&quot;BaseResultMap&quot; type=&quot;com.ht.micro.record.commons.domain.TApply&quot;&gt; &lt;!-- WARNING - @mbg.generated --&gt; &lt;id column=&quot;id&quot; jdbcType=&quot;INTEGER&quot; property=&quot;id&quot; /&gt; &lt;result column=&quot;applyer_police_num&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;applyerPoliceNum&quot; /&gt; &lt;result column=&quot;applyer_name&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;applyerName&quot; /&gt; &lt;result column=&quot;asked_name&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;askedName&quot; /&gt; &lt;result column=&quot;applyer_id&quot; jdbcType=&quot;INTEGER&quot; property=&quot;applyerId&quot; /&gt; &lt;result column=&quot;unit_id&quot; jdbcType=&quot;INTEGER&quot; property=&quot;unitId&quot; /&gt; &lt;result column=&quot;case_info_ids&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;caseInfoIds&quot; /&gt; &lt;result column=&quot;case_count&quot; jdbcType=&quot;INTEGER&quot; property=&quot;caseCount&quot; /&gt; &lt;result column=&quot;apply_time&quot; jdbcType=&quot;TIMESTAMP&quot; property=&quot;applyTime&quot; /&gt; &lt;result column=&quot;apply_state&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;applyState&quot; /&gt; &lt;result column=&quot;apply_goal&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;applyGoal&quot; /&gt; &lt;result column=&quot;approve_time&quot; jdbcType=&quot;TIMESTAMP&quot; property=&quot;approveTime&quot; /&gt; &lt;result column=&quot;approve_state&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;approveState&quot; /&gt; &lt;result column=&quot;approver_id&quot; jdbcType=&quot;INTEGER&quot; property=&quot;approverId&quot; /&gt; &lt;result column=&quot;approve_prop&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;approveProp&quot; /&gt; &lt;/resultMap&gt; &lt;select id=&quot;selectByAskedName&quot; parameterType=&quot;string&quot; resultMap=&quot;BaseResultMap&quot;&gt; select * from t_apply where asked_name = #{name,jdbcType=VARCHAR} &lt;/select&gt; &lt;/mapper&gt;dicMapper/WebDicMapper.xml &lt;mapper namespace=&quot;com.ht.micro.record.commons.mapper.dicMapper&quot;&gt; &lt;resultMap id=&quot;BaseResultMap&quot; type=&quot;com.ht.micro.record.commons.domain.WebDic&quot;&gt; &lt;!-- WARNING - @mbg.generated --&gt; &lt;id column=&quot;id&quot; jdbcType=&quot;INTEGER&quot; property=&quot;id&quot; /&gt; &lt;result column=&quot;name&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;name&quot; /&gt; &lt;result column=&quot;type&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;type&quot; /&gt; &lt;/resultMap&gt; &lt;/mapper&gt;]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>springboot</tag>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[etcd+overlay+pxc集群搭建]]></title>
    <url>%2F2019%2F08%2F20%2Fetcd-overlay-pxc%2F</url>
    <content type="text"><![CDATA[etcd+overlay+pxc实现基于swarm自定义网络数据库高可用 curl -L https://github.com/coreos/etcd/releases/download/v2.2.1/etcd-v2.2.1-linux-amd64.tar.gz -o etcd-v2.2.1-linux-amd64.tar.gz etcd集群tar xzvf etcd-v2.2.1-linux-amd64.tar.gz &amp;&amp; cd etcd-v2.2.1-linux-amd64 {NODE_NAME}:etcd节点名称，需要和命令中的-initial-cluster的对应的{NODE1_NAME}或{NODE2_NAME}对应 {NODE_IP}/{NODE1_IP}/{NODE2_NAME}：节点的IP ./etcd -name {NODE_NAME} -initial-advertise-peer-urls [http://{NODE_IP}:2380](http://NODE_IP:2380) \ -listen-peer-urls &lt;http://0.0.0.0:2380&gt; \ -listen-client-urls [http://0.0.0.0:2379,http://127.0.0.1:4001](http://0.0.0.0:2379,http:/127.0.0.1:4001) \ -advertise-client-urls &lt;http://0.0.0.0:2379&gt; \ -initial-cluster-token etcd-cluster \ -initial-cluster {NODE1_NAME}=http://{NODE1_IP}:2380,{NODE2_NAME}=http://{NODE2_IP}:2380 \ -initial-cluster-state new 单机tar xzvf etcd-v2.2.1-linux-amd64.tar.gz &amp;&amp; cd etcd-v2.2.1-linux-amd64 nohup ./etcd --advertise-client-urls &#39;http://192.168.3.226:2379&#39; --listen-client-urls &#39;http://0.0.0.0:2379&#39; &amp; ./etcdctl member list 查看启动情况 ./etcdctl mk name OneJane ./etcdctl get name 其他主机 ./etcdctl -endpoint http://192.168.3.226:2379 get name ./etcdctl -endpoint http://192.168.3.226:2379 mk age 22 swarmdocker配置192.168.3.224 vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --cluster-store=etcd://192.168.3.224:2379 --cluster-advertise=192.168.3.224:2375 systemctl daemon-reload service docker start 192.168.3.227 vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --cluster-store=etcd://192.168.3.224:2379 --cluster-advertise=192.168.3.227:2375 systemctl daemon-reload service docker start overlay192.168.3.224 docker swarm init --advertise-addr 192.168.3.224 192.168.3.227 docker swarm join --token SWMTKN-1-4qdkodh0g0c73iw5oehhn4rmsxxxca1cdfushtujspjsn1i827-3x1zyoo4tm4d4qm9x8f4o658q 192.168.3.227:2377 192.168.3.224 docker network create --driver overlay --attachable overnet docker run -itd --name=worker-1 --net=overnet ubuntu docker exec worker-1 apt-get update docker exec worker-1 apt-get install net-tools docker exec worker-1 ifconfig 10.0.0.5 192.168.3.227 docker run -itd --name=worker-2 --net=overnet ubuntu docker exec worker-2 apt-get update docker exec worker-2 apt-get install net-tools docker exec worker-2 apt-get install -y inetutils-ping docker exec worker-2 ifconfig docker exec worker-2 ping 10.0.0.5 PXC多机器集群vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock systemctl daemon-reload service docker start 192.168.3.224 docker swarm init --advertise-addr 192.168.3.224 192.168.3.227 docker swarm join --token SWMTKN-1-4qdkodh0g0c73iw5oehhn4rmsxxxca1cdfushtujspjsn1i827-3x1zyoo4tm4d4qm9x8f4o658q 192.168.3.227:2377 192.168.3.224 docker network create --driver overlay --attachable overnet 192.168.3.224 docker volume create v01 docker run -d \ -p 3306:3306 \ -e MYSQL_ROOT_PASSWORD=123456 \ -e CLUSTER_NAME=PXC \ -e XTRABACKUP_PASSWORD=123456 \ -v v01:/var/lib/mysql \ --privileged \ --name=node1 \ --net=overnet \ percona/percona-xtradb-cluster:5.6 192.168.3.227 docker volume create v02 docker run -d \ -p 3306:3306 \ -e MYSQL_ROOT_PASSWORD=123456 \ -e CLUSTER_NAME=PXC \ -e XTRABACKUP_PASSWORD=123456 \ -e CLUSTER_JOIN=node1 \ -v v02:/var/lib/mysql \ --name=node2 \ --net=overnet \ percona/percona-xtradb-cluster:5.6]]></content>
      <categories>
        <category>高可用</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>pxc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux-partition]]></title>
    <url>%2F2019%2F08%2F20%2Flinux-partition%2F</url>
    <content type="text"><![CDATA[parted实现linux大磁盘分区 已挂载磁盘分区分区需先把磁盘unmount /dev/mapper/centos_ht05-homeQ1:Error: /dev/dm-2: unrecognised disk labelmklabel gpt 转成gpt格式再分区 开始分区du -h –max-depth=1 查看各文件夹大小 parted /dev/dm-2 查看/dev/mapper/centos_ht05-home是l文件属性，链接到dm-2,分区dm-2磁盘 (parted) mkpart Partition name? []? data1 File system type? [ext2]? ext4 Start? 0 End? 3584GB (parted) mkpart Partition name? []? data2 File system type? [ext2]? ext4 Start? 3584GB End? 7168GB (parted) mkpart Partition name? []? data3 File system type? [ext2]? ext4 Start? 7168GB End? -1 (parted) p Model: Linux device-mapper (linear) (dm) Disk /dev/dm-2: 11.9TB Sector size (logical/physical): 512B/512B Partition Table: gpt Disk Flags: Number Start End Size File system Name Flags 1 17.4kB 3584GB 3584GB data1 2 3584GB 7168GB 3584GB data2 3 7168GB 11.9TB 4745GB data3 若分区错误：rm 1 挂载目录mkdir /data1 /data2 /data3 ll -t /dev/dm-* 发现 dm-2 dm-17 dm-28 dm-27最新 mkfs -t ext4 /dev/dm-2 mkfs -t ext4 /dev/dm-17 mkfs -t ext4 /dev/dm-27 mkfs -t ext4 /dev/dm-28 fdisk -l 查看/dev/mapper/centos_ht05-home3 /dev/mapper/centos_ht05-home2 /dev/mapper/centos_ht05-home1生成 mount /dev/mapper/centos_ht05-home1 /data1 mount /dev/mapper/centos_ht05-home2 /data2 mount /dev/mapper/centos_ht05-home3 /data3 开机自动挂载vi /etc/fstab /dev/mapper/centos_ht05-home1 /data1 ext4 defaults 0 0 /dev/mapper/centos_ht05-home2 /data2 ext4 defaults 0 0 /dev/mapper/centos_ht05-home3 /data3 ext4 defaults 0 0 df -hl 查看磁盘分区挂载情况]]></content>
      <categories>
        <category>系统</category>
      </categories>
      <tags>
        <tag>partition</tag>
        <tag>sentinel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis-sentinel]]></title>
    <url>%2F2019%2F08%2F20%2Fredis-sentinel%2F</url>
    <content type="text"><![CDATA[基于redis-sentinel实现redis哨兵集群 redis-sentinel，只有一个master，各实例数据保持一致； 单个redis-sentinel进程来监控redis集群是不可靠的，由于redis-sentinel本身也有single-point-of-failure-problem(单点问题)，当出现问题时整个redis集群系统将无法按照预期的方式切换主从。官方推荐：一个健康的集群部署，至少需要3个Sentinel实例。另外，redis-sentinel只需要配置监控redis master，而集群之间可以通过master相互通信。 首先Sentinel是集群部署的，Client可以链接任何一个Sentinel服务所获的结果都是一致的。其次，所有的Sentinel服务都会对Redis的主从服务进行监控，当监控到Master服务无响应的时候，Sentinel内部进行仲裁，从所有的 Slave选举出一个做为新的Master。并且把其他的slave作为新的Master的Slave。 name ip port redis-master 192.168.2.5 6300 redis-slave1 192.168.2.7 6301 redis-slave2 192.168.2.7 6302 sentinel1 192.168.2.5 26000 sentinel2 192.168.2.7 26001 sentinel3 192.168.2.7 26002 Redis部署在192.168.2.5上运行 docker run -it --name redis-master --network host -d redis --appendonly yes --port 6300 在192.168.2.7上运行 docker run -it --name redis-slave1 --network host -d redis --appendonly yes --port 6301 --slaveof 192.168.2.5 6300 docker run -it --name redis-slave2 --network host -d redis --appendonly yes --port 6302 --slaveof 192.168.2.5 6300 Sentinel部署wget http://download.redis.io/redis-stable/sentinel.conf mkdir /app/{sentine1,sentine2,sentine3}/{data,conf} -p 并复制sentinel1.conf，sentinel2.conf，sentinel3.conf /app├── sentine1│ ├── conf│ └── data├── sentine2│ ├── conf│ └── data└── sentine3 ├── conf └── data 192.168.2.5主节点chmod a+w -R /app/ port 26000 pidfile /var/run/redis-sentinel.pid logfile &quot;&quot; daemonize no dir /tmp sentinel monitor mymaster 192.168.2.5 6300 2 sentinel down-after-milliseconds mymaster 30000 sentinel parallel-syncs mymaster 1 sentinel failover-timeout mymaster 180000 sentinel deny-scripts-reconfig yes创建主节点 docker run -d --network host --name sentine1 \ -v /app/sentine1/data:/var/redis/data \ -v /app/sentine1/conf/sentinel.conf:/usr/local/etc/redis/sentinel.conf \ redis /usr/local/etc/redis/sentinel.conf --sentinel 192.168.2.7 从节点sentinel2.conf port 26001 daemonize no dir /tmp sentinel monitor mymaster 192.168.2.5 6300 2 sentinel down-after-milliseconds mymaster 30000 sentinel parallel-syncs mymaster 1 sentinel failover-timeout mymaster 180000 sentinel deny-scripts-reconfig yes创建从节点1 docker run -d --network host --name sentine2 \ -v /app/sentine2/data:/var/redis/data \ -v /app/sentine2/conf/sentinel.conf:/usr/local/etc/redis/sentinel.conf \ redis /usr/local/etc/redis/sentinel.conf --sentinel 192.168.2.7 从节点sentinel3.conf port 26002 daemonize no dir /tmp sentinel monitor mymaster 192.168.2.5 6300 2 sentinel down-after-milliseconds mymaster 30000 sentinel parallel-syncs mymaster 1 sentinel failover-timeout mymaster 180000 sentinel deny-scripts-reconfig yes创建从节点2 docker run -d --network host --name sentine3 \ -v /app/sentine3/data:/var/redis/data \ -v /app/sentine3/conf/sentinel.conf:/usr/local/etc/redis/sentinel.conf \ redis /usr/local/etc/redis/sentinel.conf --sentinel 测试[root@centoss2 app]# redis-cli -p 26000 127.0.0.1:26000&gt; sentinel master mymaster]]></content>
      <categories>
        <category>高可用</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>sentinel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Replication+Haproxy+Keepalived]]></title>
    <url>%2F2019%2F08%2F20%2FReplication-Haproxy-Keepalived%2F</url>
    <content type="text"><![CDATA[基于Replication+Haproxy+Keepalived实现数据库高可用 Haproxy 负载均衡haproxy提供负载均衡，并自动切换故障容器vim /usr/local/docker/mysql/haproxy/haproxy.cfg 编写配置文件 global #工作目录 chroot /usr/local/etc/haproxy #日志文件，使用rsyslog服务中local5日志设备（/var/log/local5），等级info log 127.0.0.1 local5 info #守护进程运行 daemon defaults log global mode http #日志格式 option httplog #日志中不记录负载均衡的心跳检测记录 option dontlognull #连接超时（毫秒） timeout connect 5000 #客户端超时（毫秒） timeout client 50000 #服务器超时（毫秒） timeout server 50000 #监控界面 listen admin_stats #监控界面的访问的IP和端口 bind 0.0.0.0:8888 #访问协议 mode http #URI相对地址 stats uri /dbs #统计报告格式 stats realm Global\ statistics #登陆帐户信息 stats auth admin:abc123456 #数据库负载均衡 listen proxy-mysql #访问的IP和端口 bind 0.0.0.0:185 #网络协议 mode tcp #负载均衡算法（轮询算法） #轮询算法：roundrobin #权重算法：static-rr #最少连接算法：leastconn #请求源IP算法：source balance roundrobin #日志格式 option tcplog #在MySQL中创建一个没有权限的haproxy用户，密码为空。Haproxy使用这个账户对MySQL数据库心跳检测 option mysql-check user haproxy server MySQL_1 192.168.3.226:3317 check weight 1 maxconn 2000 server MySQL_2 192.168.3.225:3318 check weight 1 maxconn 2000 #使用keepalive检测死链 option tcpka 在两台Replication组建的mysql集群同时创建mysql_cluster的haproxy容器，形成集群。 docker run -itd -v /data2/haproxy:/usr/local/etc/haproxy --name mysql-haproxy --privileged --net host haproxy docker exec -it ht-mysql-master mysql -u root -p drop user &#39;haproxy&#39;@&#39;%&#39;; create user &#39;haproxy&#39;@&#39;%&#39; IDENTIFIED BY &#39;&#39;;http://192.168.3.226:4001/dbs admin abc123456 实时查看haproxy监控页面 admin:abc123456192.168.3.226 185 root 123456 访问数据库，与Replication数据同步一致。 Keepalived双机热备高可用应用程序向宿主机65的发起请求，宿主机的Keepalived路由到docker内部的虚拟IP15。Haproxy容器内Keepalived抢占虚拟IP，接收到所有数据库请求将被转发到抢占虚拟IP的Haproxy，keepalived互相心跳检测，一旦主服务器挂了，备用服务器将有权抢到虚拟ip，再通过负载均衡分发到某一个PXC节点，并通过主从复制实现数据同步。 192.168.3.226docker exec -it mysql_cluster bash 进入h1 echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty main restricted&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty-updates main restricted&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty universe&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty-updates universe&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty multiverse&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty-updates multiverse&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty-backports main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list echo &quot;# deb http://security.ubuntu.com/ubuntu zesty-security main restricted&quot; &gt;&gt; /etc/apt/sources.list echo &quot;# deb http://security.ubuntu.com/ubuntu zesty-security universe&quot; &gt;&gt; /etc/apt/sources.list echo &quot;# deb http://security.ubuntu.com/ubuntu zesty-security multiverse&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://archive.canonical.com/ubuntu zesty partner&quot; &gt;&gt; /etc/apt/sources.list apt-get update apt-get install gnupg -y --allow-unauthenticated apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 3B4FE6ACC0B21F32 apt-get update apt-get install keepalived vim -y vim /etc/keepalived/keepalived.conf vrrp_instance VI_1 { state MASTER interface enp1s0f0 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 123456 } virtual_ipaddress { 192.168.2.88 } } virtual_server 192.168.2.88 3306 { delay_loop 3 lb_algo rr lb_kind NAT persistence_timeout 50 protocol TCP real_server 192.168.2.5 185 { weight 1 } } service keepalived start ping 192.168.2.88192.168.3.225docker exec -it mysql_cluster bash 进入h2 echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty main restricted&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty-updates main restricted&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty universe&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty-updates universe&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty multiverse&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty-updates multiverse&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://old-releases.ubuntu.com/ubuntu/ zesty-backports main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list echo &quot;# deb http://security.ubuntu.com/ubuntu zesty-security main restricted&quot; &gt;&gt; /etc/apt/sources.list echo &quot;# deb http://security.ubuntu.com/ubuntu zesty-security universe&quot; &gt;&gt; /etc/apt/sources.list echo &quot;# deb http://security.ubuntu.com/ubuntu zesty-security multiverse&quot; &gt;&gt; /etc/apt/sources.list echo &quot;deb http://archive.canonical.com/ubuntu zesty partner&quot; &gt;&gt; /etc/apt/sources.list apt-get update apt-get install gnupg -y --allow-unauthenticated apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 3B4FE6ACC0B21F32 apt-get update apt-get install keepalived vim -y vim /etc/keepalived/keepalived.conf vrrp_instance VI_1 { state BACKUP interface enp1s0f0 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 123456 } virtual_ipaddress { 192.168.2.88 } } virtual_server 192.168.2.88 3306 { delay_loop 3 lb_algo rr lb_kind NAT persistence_timeout 50 protocol TCP real_server 192.168.2.7 186 { weight 1 } } service keepalived start ping 192.168.2.88访问192.168.3.222 189 root 123456]]></content>
      <categories>
        <category>高可用</category>
      </categories>
      <tags>
        <tag>haproxy</tag>
        <tag>replication</tag>
        <tag>keepalived</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Replication主从复制]]></title>
    <url>%2F2019%2F08%2F20%2FReplication%2F</url>
    <content type="text"><![CDATA[基于Replication实现mysql主从复制 简单介绍本文具体讲述mysql基于多机器的数据库高可用的一些解决方案。 主从复制：常见方案有PXC以及Replication。 Replication的主从在主库中操作，速度较快，弱一致性，单向异步，一旦stop slave将无法同步；PXC集群速度慢，强一致性，高价值数据，双向同步。 负载均衡：Nginx更适用于HTTP协议的应用负载，刚刚支持TCP；Haproxy提供负载，故障自动切换。 双机热备：Keepalived通过虚拟IP将请求分发，让抢占到虚拟IP的Haproxy通过负载分发给某一数据库节点。 Replication单机Masterdocker run -di --name=mysql_master -p 3300:3306 -e MYSQL_ROOT_PASSWORD=123456 mysql docker cp mysql_master:/etc/mysql/my.cnf /usr/share/mysql/my_master.cnf docker cp mysql_master:/etc/mysql/my.cnf /usr/share/mysql/my_slaver.cnf docker stop mysql_master docker rm mysql_master docker run -di --name=mysql_master -p 3300:3306 -e MYSQL_ROOT_PASSWORD=123456 -v /usr/share/mysql/my_master.cnf:/etc/mysql/my.cnf mysql:5.7.25 mkdir -p /usr/local/mysql_master chown -R 777 /usr/local/mysql_master/ 以上主要取出配置文件模板类型 vim /usr/share/mysql/my_master.cnf basedir = /usr/local/mysql_master port = 3306 server_id = 98 log_bin=zlinux01 docker restart mysql_master docker exec -it mysql_master /bin/bash mysql -u root -p ALTER USER &#39;root&#39;@&#39;%&#39; IDENTIFIED WITH mysql_native_password BY &#39;123456&#39;; GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39; WITH GRANT OPTION; FLUSH PRIVILEGES; grant replication slave on *.* to &#39;root&#39;@&#39;192.168.12.98&#39; identified by &#39;123456&#39;; flush tables with read lock; show master status; +-----------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +-----------------+----------+--------------+------------------+-------------------+ | zlinux01.000004 | 154 | | | | +-----------------+----------+--------------+------------------+-------------------+ Slavedocker run -di --name=mysql_slaver -p 3301:3306 -e MYSQL_ROOT_PASSWORD=123456 -v /usr/share/mysql/my_slaver.cnf:/etc/mysql/my.cnf mysql:5.7.25 mkdir -p /usr/local/mysql_slaver chown -R mysql.mysql /usr/local/mysql_slaver/ vim /usr/share/mysql/my_slaver.cnf basedir = /usr/local/mysql_slaver port = 3306 server_id = 89 log_bin=zlinux02 docker restart mysql_slaver docker exec -it mysql_slaver /bin/bash mysql -u root -p ALTER USER &#39;root&#39;@&#39;%&#39; IDENTIFIED WITH mysql_native_password BY &#39;123456&#39;; stop slave; change master to master_host=&#39;192.168.12.98&#39;,master_user=&#39;root&#39;, master_password=&#39;123456&#39;,master_port=3300, master_log_file=&#39;zlinux01.000004&#39;,master_log_pos=154; start slave; show slave status\G Slave_IO_Running: Yes Slave_SQL_Running: Yes则成功单机集群在实际应用中毫无意义，仅供参考。 多机一主多从Master 192.168.3.226mkdir -p /usr/local/mysql_master chown -R 777 /usr/local/mysql_master docker run -di --name=mysql_master -p 3300:3306 -e MYSQL_ROOT_PASSWORD=123456 mysql docker cp mysql_master:/etc/mysql/my.cnf /usr/share/mysql/my_master.cnf 并修改 basedir = /usr/local/mysql_master port = 3306 server_id = 98 log_bin=zlinux01 docker stop mysql_master docker rm mysql_master docker run -di --name=mysql_master -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -v /usr/share/mysql/my_master.cnf:/etc/mysql/my.cnf mysql:5.7.25 docker exec -it mysql_master /bin/bash mysql -u root -p ALTER USER &#39;root&#39;@&#39;%&#39; IDENTIFIED WITH mysql_native_password BY &#39;123456&#39;; GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39; WITH GRANT OPTION; FLUSH PRIVILEGES; grant replication slave on *.* to &#39;root&#39;@&#39;192.168.3.225&#39; identified by &#39;123456&#39;; flush tables with read lock; show master status; +-----------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +-----------------+----------+--------------+------------------+-------------------+ | zlinux01.000004 | 1135 | | | | +-----------------+----------+--------------+------------------+-------------------+Slaver 192.168.3.225mkdir -p /usr/local/mysql_slaver chown -R 777 /usr/local/mysql_slaver docker cp mysql_master:/etc/mysql/my.cnf /usr/share/mysql/my_slaver.cnf 并修改 basedir = /usr/local/mysql_slaver port = 3306 server_id = 89 log_bin=zlinux02 docker run -di --name=mysql_slaver -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -v /usr/share/mysql/my_slaver.cnf:/etc/mysql/my.cnf mysql:5.7.25 docker exec -it mysql_slaver /bin/bash mysql -u root -p ALTER USER &#39;root&#39;@&#39;%&#39; IDENTIFIED WITH mysql_native_password BY &#39;123456&#39;; stop slave; change master to master_host=&#39;192.168.3.226&#39;,master_user=&#39;root&#39;, master_password=&#39;123456&#39;,master_port=3306, master_log_file=&#39;zlinux01.000004&#39;,ster_logmaster_log_pos=1135; start slave; show slave status\G 证明主从复制实现 Slave_IO_Running: Yes Slave_SQL_Running: Yes以上即Replication的主从复制，单向复制，只可作为热备使用。 最终方案Master_1 192.168.3.226/root └── test └── mysql_test1 ├── haproxy │ └── haproxy.cfg ├── log ├── mone │ ├── conf │ │ └── my.cnf │ └── data └── mtwo ├── conf │ └── my.cnf └── data mkdir test/mysql_test1/{mone,mtwo}/{data,conf} -p vim test/mysql_test1/mone/conf/my.cnf [mysqld] server_id = 1 log-bin= mysql-bin replicate-ignore-db=mysql replicate-ignore-db=sys replicate-ignore-db=information_schema replicate-ignore-db=performance_schema read-only=0 relay_log=mysql-relay-bin log-slave-updates=on auto-increment-offset=1 auto-increment-increment=2 !includedir /etc/mysql/conf.d/ !includedir /etc/mysql/mysql.conf.d/ vim test/mysql_test1/mysql/mtwo/conf/my.cnf [mysqld] server_id = 2 log-bin= mysql-bin replicate-ignore-db=mysql replicate-ignore-db=sys replicate-ignore-db=information_schema replicate-ignore-db=performance_schema read-only=0 relay_log=mysql-relay-bin log-slave-updates=on auto-increment-offset=2 auto-increment-increment=2 !includedir /etc/mysql/conf.d/ !includedir /etc/mysql/mysql.conf.d/ scp -r test root@192.168.3.225:/root/ docker run --name monemysql -d -p 3317:3306 -e MYSQL_ROOT_PASSWORD=root -v ~/test/mysql_test1/mone/data:/var/lib/mysql -v ~/test/mysql_test1/mone/conf/my.cnf:/etc/mysql/my.cnf mysql:5.7 docker exec -it monemysql mysql -u root -p 输入root stop slave; GRANT REPLICATION SLAVE ON *.* to &#39;slave&#39;@&#39;%&#39; identified by &#39;123456&#39;; 创建一个slave同步账号slave，允许访问的IP地址为%，%表示通配符用来同步数据 show master status; +------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+-------------------+ | mysql-bin.000003 | 443 | | | | +------------------+----------+--------------+------------------+-------------------+ docker inspect monemysql | grep IPA 查看容器ip &quot;SecondaryIPAddresses&quot;: null, &quot;IPAddress&quot;: &quot;172.17.0.2&quot;, &quot;IPAMConfig&quot;: null, &quot;IPAddress&quot;: &quot;172.17.0.2&quot;,Master_2 192.168.3.225docker run --name mtwomysql -d -p 3318:3306 -e MYSQL_ROOT_PASSWORD=root -v ~/test/mysql_test1/mtwo/data:/var/lib/mysql -v ~/test/mysql_test1/mtwo/conf/my.cnf:/etc/mysql/my.cnf mysql:5.7 docker exec -it mtwomysql mysql -u root -p 输入root stop slave; change master to master_host=&#39;192.168.3.226&#39;,master_user=&#39;slave&#39;, master_password=&#39;123456&#39;,master_log_file=&#39;mysql-bin.000003&#39;, master_log_pos=443,master_port=3317; GRANT REPLICATION SLAVE ON *.* to &#39;slave&#39;@&#39;%&#39; identified by &#39;123456&#39;; 创建一个用户来同步数据 start slave ; 启动同步 show master status; 查看状态 +------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+-------------------+ | mysql-bin.000003 | 443 | | | | +------------------+----------+--------------+------------------+-------------------+双向同步Master_1 192.168.3.226 stop slave; change master to master_host=&#39;192.168.3.225&#39;,master_user=&#39;slave&#39;, master_password=&#39;123456&#39;,master_log_file=&#39;mysql-bin.000003&#39;, master_log_pos=443,master_port=3318; start slave ; 在两个容器中查看 show slave status\G; Slave_IO_Running: Yes Slave_SQL_Running: Yes 双向验证，数据同步 实例 映射路径data为空 2.7docker run --name ht-mysql-master -d -p 3317:3306 -e MYSQL_ROOT_PASSWORD=123456 -v /data3/replication/data:/var/lib/mysql -v /data2/replication/conf/my.cnf:/etc/mysql/my.cnf mysql:5.7 docker exec -it ht-mysql-master mysql -u root -p GRANT REPLICATION SLAVE ON *.* TO &#39;repl_user&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; FLUSH PRIVILEGES; reset master; #清空master的binlog，平时慎用，可选 flush tables with read lock; #只读 flush logs; show master status; 2.5docker run --name ht-mysql-slave -d -p 3318:3306 -e MYSQL_ROOT_PASSWORD=123456 -v /data3/replication/data:/var/lib/mysql -v /data3/replication/conf/my.cnf:/etc/mysql/my.cnf mysql:5.7 docker exec -it ht-mysql-slave mysql -u root -p GRANT REPLICATION SLAVE ON *.* TO &#39;repl_user&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; FLUSH PRIVILEGES; stop slave; CHANGE MASTER TO MASTER_HOST=&#39;192.168.2.7&#39;,MASTER_PORT=3317, MASTER_USER=&#39;repl_user&#39;, MASTER_PASSWORD=&#39;123456&#39;, MASTER_LOG_FILE=&#39;mysql-bin.000002&#39;, MASTER_LOG_POS=154; start slave; show slave status\G reset master;(清空master的binlog，平时慎用，可选) flush tables with read lock; flush logs; show master status;2.7unlock stop slave; CHANGE MASTER TO MASTER_HOST=&#39;192.168.2.5&#39;,MASTER_PORT=3318, MASTER_USER=&#39;repl_user&#39;, MASTER_PASSWORD=&#39;123456&#39;, MASTER_LOG_FILE=&#39;mysql-bin.000002&#39;, MASTER_LOG_POS=154; start slave; show slave status\G2.5unlock tables;Semisync半同步配置nodeA和nodeB上执行： mysql&gt; INSTALL PLUGIN rpl_semi_sync_master SONAME &#39;semisync_master.so&#39;; mysql&gt; INSTALL PLUGIN rpl_semi_sync_slave SONAME &#39;semisync_slave.so&#39;; mysql&gt; show variables like &#39;%semi%&#39;; rpl_semi_sync_master_timeout=10000 表示主库在某次事务中，如果等待时间超过10秒，则降级为普通模式，不再等待备库。如果主库再次探测到备库恢复了，则会自动再次回到semisync模式。 rpl_semi_sync_master_wait_point=AFTER_SYNCAFTER_SYNC工作流程： 客户端提交一个事务，master将事务写入binlog并刷新到磁盘，发送到slave，master等待slave反馈。 slave接收master的binlog，写到本地的relaylog里。发送确认信息给master。 当接收到slave反馈，master提交事务并返回结果给客户端。这样就保证了主从数据一致。 mysql&gt; SET GLOBAL rpl_semi_sync_master_enabled = 1; mysql&gt; SET GLOBAL rpl_semi_sync_slave_enabled = 1; mysql&gt; stop slave;start slave; mysql&gt; show status like &#39;%semi%&#39;; +--------------------------------------------+-------+ | Variable_name | Value | +--------------------------------------------+-------+ | Rpl_semi_sync_master_clients | 1 | | Rpl_semi_sync_master_net_avg_wait_time | 0 | | Rpl_semi_sync_master_net_wait_time | 0 | | Rpl_semi_sync_master_net_waits | 0 | | Rpl_semi_sync_master_no_times | 0 | | Rpl_semi_sync_master_no_tx | 0 | | Rpl_semi_sync_master_status | ON | (master同步） | Rpl_semi_sync_master_timefunc_failures | 0 | | Rpl_semi_sync_master_tx_avg_wait_time | 0 | | Rpl_semi_sync_master_tx_wait_time | 0 | | Rpl_semi_sync_master_tx_waits | 0 | | Rpl_semi_sync_master_wait_pos_backtraverse | 0 | | Rpl_semi_sync_master_wait_sessions | 0 | | Rpl_semi_sync_master_yes_tx | 0 | | Rpl_semi_sync_slave_status | ON |（从同步） +--------------------------------------------+-------+ 15 rows in set (0.00 sec)并修改my.cnf，添加下面两行： rpl_semi_sync_master_enabled = 1 rpl_semi_sync_slave_enabled = 1]]></content>
      <categories>
        <category>高可用</category>
      </categories>
      <tags>
        <tag>replication</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[个人博客]]></title>
    <url>%2F2019%2F08%2F20%2Fblog%2F</url>
    <content type="text"><![CDATA[基于Nexmoe的搭建的个人博客 Hexohttps://nodejs.org/download/release/v10.15.3/ 安装node npm install -g cnpm --registry=https://registry.npm.taobao.org 安装cnpm npm config set registry https://registry.npm.taobao.org 使用npm淘宝源 npm install -g hexo-cli hexo init blog cd blog npm installNexmoecd themes git clone https://github.com/nexmoe/hexo-theme-nexmoe.git nexmoe cd nexmoe git checkout master npm i --save hexo-wordcount npm i hexo-deployer-git --save npm i -g gulp --save cp -i _config.example.yml _config.ymlvim package.json &quot;scripts&quot;: { &quot;build&quot;: &quot;hexo clean &amp;&amp; hexo g &amp;&amp; gulp &amp;&amp; hexo d &amp; git add * &amp; git commit -m &#39;加油&#39; &amp; git push origin master&quot;, &quot;test&quot;: &quot;hexo clean &amp;&amp; hexo g &amp;&amp; gulp &amp;&amp; hexo s&quot;, &quot;dev&quot;: &quot;hexo clean &amp;&amp; hexo g &amp;&amp; hexo s&quot; },默认自动开启github pages 使用npm install gulp npm install --save-dev gulp npm install gulp-htmlclean gulp-htmlmin gulp-minify-css gulp-uglify gulp-imagemin --savegulpfile.js var gulp = require(&#39;gulp&#39;); var minifycss = require(&#39;gulp-minify-css&#39;); var uglify = require(&#39;gulp-uglify&#39;); var htmlmin = require(&#39;gulp-htmlmin&#39;); var htmlclean = require(&#39;gulp-htmlclean&#39;); var imagemin = require(&#39;gulp-imagemin&#39;); // 压缩html gulp.task(&#39;minify-html&#39;, function() { return gulp.src(&#39;./public/**/*.html&#39;) .pipe(htmlclean()) .pipe(htmlmin({ removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, })) .pipe(gulp.dest(&#39;./public&#39;)) }); // 压缩css gulp.task(&#39;minify-css&#39;, function() { return gulp.src(&#39;./public/**/*.css&#39;) .pipe(minifycss({ compatibility: &#39;ie8&#39; })) .pipe(gulp.dest(&#39;./public&#39;)); }); // 压缩js gulp.task(&#39;minify-js&#39;, function() { return gulp.src(&#39;./public/js/**/*.js&#39;) .pipe(uglify()) .pipe(gulp.dest(&#39;./public&#39;)); }); // 压缩图片 gulp.task(&#39;minify-images&#39;, function() { return gulp.src(&#39;./public/images/**/*.*&#39;) .pipe(imagemin( [imagemin.gifsicle({&#39;optimizationLevel&#39;: 3}), imagemin.jpegtran({&#39;progressive&#39;: true}), imagemin.optipng({&#39;optimizationLevel&#39;: 7}), imagemin.svgo()], {&#39;verbose&#39;: true})) .pipe(gulp.dest(&#39;./public/images&#39;)) }); // 默认任务 gulp.task(&#39;default&#39;, [ &#39;minify-html&#39;,&#39;minify-css&#39;,&#39;minify-js&#39;,&#39;minify-images&#39; ]);hexo g &amp;&amp; gulp _config.ymltitle: OneJane subtitle: 码农养成记 description: keywords: Spring Cloud,Docker,Dubbo author: OneJane language: zh-CN timezone: Hongkong url: https://onejane.github.io/ theme: nexmoe deploy: type: git repo: github: git@github.com:OneJane/OneJane.github.io.git branch: master message: github highlight: enable: false line_number: true auto_detect: false tab_replace: themes/nexmoe/_config.ymlavatar: https://i.loli.net/2019/08/20/UIhTqdQiPasxLtr.jpg # 网站 Logo background: https://i.loli.net/2019/01/13/5c3aec85a4343.jpg # 既是博客的背景，又是文章默认头图 favicon: href: /img/a.ico # 网站图标 type: image/png # 图标类型，可能的值有(image/png, image/vnd.microsoft.icon, image/x-icon, image/gif) social: zhihu: - https://www.zhihu.com/people/codewj/activities - icon-zhihu - rgb(231, 106, 141) - rgba(231, 106, 141, .15) GitHub: - https://github.com/OneJane - icon-github - rgb(25, 23, 23) - rgba(25, 23, 23, .15) analytics: la_site_id: 20279757 comment: gitment gitment: owner: onejane # 持有该 repo 的 GitHub username repo: onejane.github.io # 存放评论的 issue 所在的 repo clientID: e677e59382e1c7a468fd # GitHub Client ID clientSecret: 717d041bc4ab749f069314862232cfb6ec8adc15 # GitHub Client Secret 打赏themes/nexmoe/layout/_partial/donate.ejs &lt;! -- 添加捐赠图标 --&gt; &lt;div class =&quot;post-donate&quot;&gt; &lt;div id=&quot;donate_board&quot; class=&quot;donate_bar center&quot;&gt; &lt;a id=&quot;btn_donate&quot; class=&quot;btn_donate&quot; href=&quot;javascript:;&quot; title=&quot;打赏&quot;&gt;&lt;/a&gt; &lt;span class=&quot;donate_txt&quot;&gt; ↑&lt;br&gt; &lt;%=theme.donate_message%&gt; &lt;/span&gt; &lt;br&gt; &lt;/div&gt; &lt;div id=&quot;donate_guide&quot; class=&quot;donate_bar center hidden&quot; &gt; ![](/images/alipay.jpg) ![](/images/alipay.jpg) &lt;!-- 支付宝打赏图案 --&gt; &lt;img src=&quot;&lt;%- theme.root_url %&gt;/images/alipay.jpg&quot; alt=&quot;支付宝打赏&quot;&gt; 666 &lt;!-- 微信打赏图案 --&gt; &lt;img src=&quot;&lt;%- theme.root_url %&gt;/images/wechatpay.png&quot; alt=&quot;微信打赏&quot;&gt; &lt;/div&gt; &lt;script type=&quot;text/javascript&quot;&gt; document.getElementById(&#39;btn_donate&#39;).onclick = function(){ $(&#39;#donate_board&#39;).addClass(&#39;hidden&#39;); $(&#39;#donate_guide&#39;).removeClass(&#39;hidden&#39;); } &lt;/script&gt; &lt;/div&gt; &lt;! -- 添加捐赠图标 --&gt; themes/nexmoe/source/css/_partial/donate.styl .donate_bar { text-align: center; margin-top: 5% } .donate_bar a.btn_donate { display: inline-block; width: 82px; height: 82px; margin-left: auto; margin-right: auto; background: url(http://img.t.sinajs.cn/t5/style/images/apps_PRF/e_media/btn_reward.gif)no-repeat; -webkit-transition: background 0s; -moz-transition: background 0s; -o-transition: background 0s; -ms-transition: background 0s; transition: background 0s } .donate_bar a.btn_donate:hover { background-position: 0 -82px } .donate_bar .donate_txt { display: block; color: #9d9d9d; font: 14px/2 &quot;Microsoft Yahei&quot; } .donate_bar.hidden{ display: none } .post-donate{ margin-top: 80px; } #donate_guide{ height: 210px; width: 420px; margin: 0 auto; } #donate_guide img{ height: 200px; height: 200px; } 在source\css\style.styl中添加@import ‘_partial/donate’themes/nexmoe/layout/post.ejs &lt;% if (theme.donate){ %&gt; &lt;%- partial(&#39;donate&#39;) %&gt; &lt;% } %&gt; themes/nexmoe/_config.yml #是否开启打赏功能 donate: true #打赏文案 donate_message: 欣赏此文？求鼓励，求支持！404插入音乐hexo new page 404 生成source/404.md --- title: 404 permalink: /404 cover: https://i.loli.net/2019/08/20/DMuWHOGTq4AR1iQ.png --- &lt;div class=&quot;aplayer&quot; data-id=&quot;439625244&quot; data-server=&quot;netease&quot; data-type=&quot;song&quot; data-autoplay=&quot;true&quot; data-mode=&quot;single&quot;&gt;&lt;/div&gt;themes/nexmoe/layout/layout.ejs &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.css&quot;&gt; &lt;script src=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.js&quot;&gt;&lt;/script&gt; ... &lt;script src=&quot;https://cdn.jsdelivr.net/npm/meting@1.1.0/dist/Meting.min.js&quot;&gt;&lt;/script&gt; windows右键复制路径Windows Registry Editor Version 5.00 [HKEY_CLASSES_ROOT\Directory\shell\copypath] @=&quot;copy path of dir&quot; [HKEY_CLASSES_ROOT\Directory\shell\copypath\command] @=&quot;mshta vbscript:clipboarddata.setdata(\&quot;text\&quot;,\&quot;%1\&quot;)(close)&quot; [HKEY_CLASSES_ROOT\*\shell\copypath] @=&quot;copy path of file&quot; [HKEY_CLASSES_ROOT\*\shell\copypath\command] @=&quot;mshta vbscript:clipboarddata.setdata(\&quot;text\&quot;,\&quot;%1\&quot;)(close)&quot;windows定时任务此电脑–&gt;管理–&gt;任务计划管理–&gt;任务计划程序 –&gt;任务计划程序库–&gt;Microsoft –&gt;windows vim C:\Users\codewj\Desktop\build-blog.bat E: &amp;&amp; cd E:\Project\blog &amp;&amp; npm run build robots.txtUser-agent: * Disallow: Disallow: /bin/ Sitemap: https://onejane.github.io/sitemap.txt 加入blog\themes\nexmoe\source google收录 https://search.google.com/search-console 添加资源 将google验证文件放入blog\themes\nexmoe\source，发布校验 站点地图 将生成的站点地图放入blog\themes\nexmoe\source bing收录https://www.bing.com/toolbox/webmaster/ 登陆后进入https://www.bing.com/webmaster/home/mysites#https://www.bing.com/webmaster/home/dashboard?url=https%3A%2F%2Fonejane.github.io%2F 使用小书匠小书匠 新建github reository为blog，并初始化 新建token:https://github.com/settings/tokens/new 并Generate token得到129483a01745abe39ed1ac109ec09f1d71b9e8c3数据存储&amp;图床服务 QuestionAssertionError [ERR_ASSERTION]: Task function must be specified?&quot;devDependencies&quot;: { &quot;gulp&quot;: &quot;^3.9.1&quot; } npm install GulpUglifyError:unable to minify JavaScriptnpm install gulp-util –save-dev var gutil = require(&#39;gulp-util&#39;); // 压缩public目录下的所有js gulp.task(&#39;minify-js&#39;, function() { return gulp.src(&#39;./public/**/*.js&#39;) .pipe(uglify()) .on(&#39;error&#39;, function (err) { gutil.log(gutil.colors.red(&#39;[Error]&#39;), err.toString()); }) //增加这一行 .pipe(gulp.dest(&#39;./public&#39;)); }); gittalk Error：validation failedgitalk.ejs id: window.location.pathname改为id: decodeURI(window.location.pathname)]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>nexmoe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单机Nacos集群]]></title>
    <url>%2F2019%2F08%2F20%2Fnacos%2F</url>
    <content type="text"><![CDATA[基于docker的单机nacos集群安装配置 环境搭建yum update -y nss curl libcurl curl -L https://github.com/docker/compose/releases/download/1.20.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose docker-compose --version git clone https://github.com/nacos-group/nacos-docker.git cd nacos-docker 映射cluster-hostname启动脚本 - ../docker-startup.sh:/home/nacos/bin/docker-startup.sh 修改docker-startup.sh中环境变量 JAVA_OPT=&quot;${JAVA_OPT} -server -Xms512m -Xmx512m -Xmn256m -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m&quot; vim ~/.bashrc alias dofo=&#39;docker ps --format &quot;table {{.Names}}\t{{.Ports}}\t{{.Image}}\t&quot;&#39; alias dolo=&#39;docker logs -ft&#39; source ~/.bashrc #docker-compose up -d #docker-compose logs -ft #docker-compose down docker-compose -f example/cluster-hostname.yaml up -d docker stop nacos1 nacos2 nacos3 docker start nacos1 nacos2 nacos3 docker-compose -f example/cluster-hostname.yaml downhttp://192.168.2.7:9503/actuator/nacos-discoveryhttp://192.168.2.5:8848/nacos http://192.168.2.5:8849/nacos http://192.168.2.5:8850/nacos nacos/nacos Nginx负载均衡mkdir /usr/local/docker/nginx/nacos -p vim n1.conf upstream nacos { # 配置负载均衡 server 192.168.2.5:8848; server 192.168.2.5:8849; server 192.168.2.5:8850; } server { listen 8841; server_name 192.168.2.5; location / { proxy_pass http://nacos; index index.html index.htm; } vim n2.conf upstream nacos { # 配置负载均衡 server 192.168.2.5:8848; server 192.168.2.5:8849; server 192.168.2.5:8850; } server { listen 8842; server_name 192.168.2.5; location / { proxy_pass http://nacos; index index.html index.htm; } docker run -it -d –name nginx2 -v /usr/local/docker/nginx/nacos/n2.conf:/etc/nginx/nginx.conf –net=host –privileged nginxdocker run -it -d –name nginx1 -v /usr/local/docker/nginx/nacos/n1.conf:/etc/nginx/nginx.conf –net=host –privileged nginx http://192.168.2.5:8841/nacos/#/login http://192.168.2.5:8842/nacos/#/login docker pause nacos1 测试页面维持访问 Keepalive双机热备docker exec -it nginx1 bash mv /etc/apt/sources.list sources.list.bak echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list apt-get update apt-get install gnupg -y --allow-unauthenticated apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 3B4FE6ACC0B21F32 apt-get update apt-get install keepalived vim -y vim /etc/keepalived/keepalived.conf vrrp_instance VI_1 { state MASTER interface docker0 # 填写docker网卡名 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 123456 } virtual_ipaddress { 172.17.0.100 # 定义该网卡下的虚拟ip地址段地址 } } service keepalived start docker exec -it nginx2 bash mv /etc/apt/sources.list sources.list.bak echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list echo &quot;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse&quot;&gt;&gt;/etc/apt/sources.list apt-get update apt-get install gnupg -y --allow-unauthenticated apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 3B4FE6ACC0B21F32 apt-get update apt-get install keepalived vim -y vim /etc/keepalived/keepalived.conf vrrp_instance VI_1 { state BACKUP interface docker0 # 填写docker网卡名 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 123456 } virtual_ipaddress { 172.17.0.100 # 定义虚拟ip地址段地址 } } service keepalived start 由于docker内的虚拟ip不能被外界访问借助宿主机keepalived映射外网可以访问的虚拟ip， exit yum install keepalived -y mv /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak vim /etc/keepalived/keepalived.conf vrrp_instance VI_1 { state MASTER interface enp1s0f0 # 宿主机网卡 virtual_router_id 51 # 保持一致 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.2.155 # 宿主机虚拟ip } } virtual_server 192.168.2.155 183 { # 宿主机虚拟ip及开放端口 delay_loop 3 lb_algo rr lb_kind NAT persistence_timeout 50 protocol TCP real_server 172.17.0.100 8841 { # nginx1容器虚拟ip及开放nacos端口 weight 1 } } virtual_server 192.168.2.155 183 { delay_loop 3 lb_algo rr lb_kind NAT persistence_timeout 50 protocol TCP real_server 172.17.0.100 8842 { # nginx2容器虚拟ip及开放nacos端口 weight 1 } } #apt-get --purge remove keepalived -y #/sbin/ip addr del 192.168.12.100/32 dev enp1s0f0 删除虚拟ip 如docker0没有则重启docker，再次检查创建systemctl daemon-reloadsystemctl restart docker 访问 http://192.168.2.155:183/nacos/#/login nacos nacos 关闭nacos心跳日志logging: level: com.alibaba.nacos.client.naming: error]]></content>
      <categories>
        <category>注册中心</category>
      </categories>
      <tags>
        <tag>spring cloud alibaba</tag>
        <tag>docker</tag>
      </tags>
  </entry>
</search>
